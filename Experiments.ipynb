{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "# os.environ[\"BART_TOOLBOX_PATH\"] = \"/Users/chunxuguo/bart\"\n",
    "import torch\n",
    "import torchopt\n",
    "from einops import rearrange, reduce\n",
    "from fastmri import complex_abs\n",
    "from fastmri.data import mri_data, subsample, transforms\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from ese5934_project.datasets.csm_estimation import espirit_csm_estimation\n",
    "from ese5934_project.evaluate_metric import Evaluate_MT1, Evaluate_MT2\n",
    "from ese5934_project.models.GridField import Grid\n",
    "from ese5934_project.models.operators import (\n",
    "    C_adj,\n",
    "    F_adj,\n",
    "    ForwardModel,\n",
    "    MaskedForwardModel,\n",
    ")\n",
    "from ese5934_project.models.SIREN import Siren, get_coordinates\n",
    "from ese5934_project.tasks.mri_reconstruction_2d import reconstruct\n",
    "\n",
    "# Create a mask function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_transform_2(\n",
    "    kspace,\n",
    "    mask,\n",
    "    target,\n",
    "    data_attributes,\n",
    "    filename,\n",
    "    slice_num,\n",
    "):\n",
    "    # Transform the data into appropriate format\n",
    "    # Here we simply mask the k-space and return the result\n",
    "    kspace = transforms.to_tensor(kspace * 1e5)\n",
    "    mean = reduce(kspace, \"ch h w complex-> () () complex\", \"mean\")\n",
    "    std = reduce(kspace, \"ch h w complex-> () () complex\", torch.std)\n",
    "    print(mean, std)\n",
    "    print(mean.shape, std.shape)\n",
    "    mask_func = subsample.RandomMaskFunc(center_fractions=[0.08], accelerations=[4])\n",
    "    masked_kspace, mask, num_low_frequencies = transforms.apply_mask(kspace, mask_func)\n",
    "    csm = transforms.to_tensor(espirit_csm_estimation(kspace, num_low_frequencies))\n",
    "    csm = rearrange(csm, \"() h w ch complex-> ch h w complex\")\n",
    "    return kspace, (mean, std), masked_kspace, mask, csm\n",
    "\n",
    "\n",
    "def data_transform_4(\n",
    "    kspace,\n",
    "    mask,\n",
    "    target,\n",
    "    data_attributes,\n",
    "    filename,\n",
    "    slice_num,\n",
    "):\n",
    "    # Transform the data into appropriate format\n",
    "    # Here we simply mask the k-space and return the result\n",
    "    kspace = transforms.to_tensor(kspace * 1e5)\n",
    "    mean = reduce(kspace, \"ch h w complex-> () () complex\", \"mean\")\n",
    "    std = reduce(kspace, \"ch h w complex-> () () complex\", torch.std)\n",
    "    print(mean, std)\n",
    "    print(mean.shape, std.shape)\n",
    "    mask_func = subsample.RandomMaskFunc(center_fractions=[0.08], accelerations=[4])\n",
    "    masked_kspace, mask, num_low_frequencies = transforms.apply_mask(kspace, mask_func)\n",
    "    csm = transforms.to_tensor(espirit_csm_estimation(kspace, num_low_frequencies))\n",
    "    csm = rearrange(csm, \"() h w ch complex-> ch h w complex\")\n",
    "    return kspace, (mean, std), masked_kspace, mask, csm\n",
    "\n",
    "\n",
    "def data_transform_8(\n",
    "    kspace,\n",
    "    mask,\n",
    "    target,\n",
    "    data_attributes,\n",
    "    filename,\n",
    "    slice_num,\n",
    "):\n",
    "    # Transform the data into appropriate format\n",
    "    # Here we simply mask the k-space and return the result\n",
    "    kspace = transforms.to_tensor(kspace * 1e5)\n",
    "    mean = reduce(kspace, \"ch h w complex-> () () complex\", \"mean\")\n",
    "    std = reduce(kspace, \"ch h w complex-> () () complex\", torch.std)\n",
    "    print(mean, std)\n",
    "    print(mean.shape, std.shape)\n",
    "    mask_func = subsample.RandomMaskFunc(center_fractions=[0.08], accelerations=[4])\n",
    "    masked_kspace, mask, num_low_frequencies = transforms.apply_mask(kspace, mask_func)\n",
    "    csm = transforms.to_tensor(espirit_csm_estimation(kspace, num_low_frequencies))\n",
    "    csm = rearrange(csm, \"() h w ch complex-> ch h w complex\")\n",
    "    return kspace, (mean, std), masked_kspace, mask, csm\n",
    "\n",
    "\n",
    "def data_transform_gt(kspace, mask, target, data_attributes, filename, slice_num):\n",
    "    # Transform the data into appropriate format\n",
    "    # Here we simply mask the k-space and return the result\n",
    "    kspace = transforms.to_tensor(kspace * 1e5)\n",
    "    mean = reduce(kspace, \"ch h w complex-> () () complex\", \"mean\")\n",
    "    std = reduce(kspace, \"ch h w complex-> () () complex\", torch.std)\n",
    "    print(mean, std)\n",
    "    print(mean.shape, std.shape)\n",
    "    mask_func = subsample.RandomMaskFunc(center_fractions=[0.08], accelerations=[1])\n",
    "    masked_kspace, mask, num_low_frequencies = transforms.apply_mask(kspace, mask_func)\n",
    "    csm = transforms.to_tensor(espirit_csm_estimation(kspace, num_low_frequencies))\n",
    "    csm = rearrange(csm, \"() h w ch complex-> ch h w complex\")\n",
    "    return kspace, (mean, std), masked_kspace, mask, csm\n",
    "\n",
    "\n",
    "dataset_gt = mri_data.SliceDataset(\n",
    "    root=pathlib.Path(\"/bmrc-homes/nmrgrp/nmr219/ese5934_project/data\"),\n",
    "    transform=data_transform_gt,\n",
    "    challenge=\"multicoil\",\n",
    ")\n",
    "dataset_2 = mri_data.SliceDataset(\n",
    "    root=pathlib.Path(\"/bmrc-homes/nmrgrp/nmr219/ese5934_project/data\"),\n",
    "    transform=data_transform_2,\n",
    "    challenge=\"multicoil\",\n",
    ")\n",
    "dataset_4 = mri_data.SliceDataset(\n",
    "    root=pathlib.Path(\"/bmrc-homes/nmrgrp/nmr219/ese5934_project/data\"),\n",
    "    transform=data_transform_4,\n",
    "    challenge=\"multicoil\",\n",
    ")\n",
    "dataset_8 = mri_data.SliceDataset(\n",
    "    root=pathlib.Path(\"/bmrc-homes/nmrgrp/nmr219/ese5934_project/data\"),\n",
    "    transform=data_transform_8,\n",
    "    challenge=\"multicoil\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[6.0409e-04, 7.4484e-05]]]) tensor([[[1.2870, 1.2171]]])\n",
      "torch.Size([1, 1, 2]) torch.Size([1, 1, 2])\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1ecce00950>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kspace, (mean, std), masked_kspace, mask, csm = dataset_gt[15]\n",
    "f_adj = F_adj()\n",
    "c_adj = C_adj()\n",
    "image_gt = c_adj(f_adj(kspace), csm)\n",
    "ground_truth_image = image_gt.squeeze()\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.imshow(torch.view_as_complex(image_gt).abs()[0], cmap=\"gray\", vmax=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Case Images with Acceleration Rate of 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Field (Voxel Representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[6.0409e-04, 7.4484e-05]]]) tensor([[[1.2870, 1.2171]]])\n",
      "torch.Size([1, 1, 2]) torch.Size([1, 1, 2])\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| params.keys(): dict_keys(['grid'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1, dc_loss: 2.9770655632019043, tv_loss: 0.0\n",
      "iteration 2, dc_loss: 2.9171924591064453, tv_loss: 0.0002476871886756271\n",
      "iteration 3, dc_loss: 2.8584272861480713, tv_loss: 0.00048622602480463684\n",
      "iteration 4, dc_loss: 2.8007476329803467, tv_loss: 0.000722508761100471\n",
      "iteration 5, dc_loss: 2.74413800239563, tv_loss: 0.0009582258644513786\n",
      "iteration 6, dc_loss: 2.6885812282562256, tv_loss: 0.001193525968119502\n",
      "iteration 7, dc_loss: 2.634061813354492, tv_loss: 0.001427414477802813\n",
      "iteration 8, dc_loss: 2.580565929412842, tv_loss: 0.0016614214982837439\n",
      "iteration 9, dc_loss: 2.5280795097351074, tv_loss: 0.0018910674843937159\n",
      "iteration 10, dc_loss: 2.476588487625122, tv_loss: 0.0021193281281739473\n",
      "iteration 11, dc_loss: 2.4260783195495605, tv_loss: 0.0023439370561391115\n",
      "iteration 12, dc_loss: 2.376535177230835, tv_loss: 0.0025646330323070288\n",
      "iteration 13, dc_loss: 2.327944278717041, tv_loss: 0.0027830027975142\n",
      "iteration 14, dc_loss: 2.28029203414917, tv_loss: 0.002997355768457055\n",
      "iteration 15, dc_loss: 2.2335636615753174, tv_loss: 0.003208736190572381\n",
      "iteration 16, dc_loss: 2.1877448558807373, tv_loss: 0.0034161382354795933\n",
      "iteration 17, dc_loss: 2.1428213119506836, tv_loss: 0.0036200713366270065\n",
      "iteration 18, dc_loss: 2.09877872467041, tv_loss: 0.0038218628615140915\n",
      "iteration 19, dc_loss: 2.055602550506592, tv_loss: 0.0040200818330049515\n",
      "iteration 20, dc_loss: 2.0132784843444824, tv_loss: 0.004214799962937832\n",
      "iteration 21, dc_loss: 1.971792221069336, tv_loss: 0.0044058943167328835\n",
      "iteration 22, dc_loss: 1.9311294555664062, tv_loss: 0.004593051970005035\n",
      "iteration 23, dc_loss: 1.8912760019302368, tv_loss: 0.004777814727276564\n",
      "iteration 24, dc_loss: 1.852217435836792, tv_loss: 0.004960660357028246\n",
      "iteration 25, dc_loss: 1.813940167427063, tv_loss: 0.005140224471688271\n",
      "iteration 26, dc_loss: 1.7764301300048828, tv_loss: 0.0053145745769143105\n",
      "iteration 27, dc_loss: 1.7396734952926636, tv_loss: 0.005487787537276745\n",
      "iteration 28, dc_loss: 1.7036570310592651, tv_loss: 0.005657619331032038\n",
      "iteration 29, dc_loss: 1.6683675050735474, tv_loss: 0.005824370309710503\n",
      "iteration 30, dc_loss: 1.633791446685791, tv_loss: 0.005987553857266903\n",
      "iteration 31, dc_loss: 1.5999157428741455, tv_loss: 0.006149135064333677\n",
      "iteration 32, dc_loss: 1.566727876663208, tv_loss: 0.006308088079094887\n",
      "iteration 33, dc_loss: 1.534214973449707, tv_loss: 0.006464391015470028\n",
      "iteration 34, dc_loss: 1.5023647546768188, tv_loss: 0.0066174292005598545\n",
      "iteration 35, dc_loss: 1.4711648225784302, tv_loss: 0.0067681120708584785\n",
      "iteration 36, dc_loss: 1.4406031370162964, tv_loss: 0.006917515769600868\n",
      "iteration 37, dc_loss: 1.4106677770614624, tv_loss: 0.007064477074891329\n",
      "iteration 38, dc_loss: 1.3813472986221313, tv_loss: 0.00720847537741065\n",
      "iteration 39, dc_loss: 1.352629542350769, tv_loss: 0.007349757943302393\n",
      "iteration 40, dc_loss: 1.324503779411316, tv_loss: 0.0074892789125442505\n",
      "iteration 41, dc_loss: 1.2969584465026855, tv_loss: 0.007627138867974281\n",
      "iteration 42, dc_loss: 1.2699826955795288, tv_loss: 0.00776272639632225\n",
      "iteration 43, dc_loss: 1.2435652017593384, tv_loss: 0.007896563969552517\n",
      "iteration 44, dc_loss: 1.217695713043213, tv_loss: 0.008029203861951828\n",
      "iteration 45, dc_loss: 1.1923633813858032, tv_loss: 0.008159673772752285\n",
      "iteration 46, dc_loss: 1.1675578355789185, tv_loss: 0.00828802864998579\n",
      "iteration 47, dc_loss: 1.1432687044143677, tv_loss: 0.008413827046751976\n",
      "iteration 48, dc_loss: 1.119485855102539, tv_loss: 0.008539155125617981\n",
      "iteration 49, dc_loss: 1.0961993932724, tv_loss: 0.008662798441946507\n",
      "iteration 50, dc_loss: 1.0733991861343384, tv_loss: 0.008784470148384571\n",
      "iteration 51, dc_loss: 1.05107581615448, tv_loss: 0.008903833106160164\n",
      "iteration 52, dc_loss: 1.029219627380371, tv_loss: 0.009020635858178139\n",
      "iteration 53, dc_loss: 1.0078212022781372, tv_loss: 0.009137796238064766\n",
      "iteration 54, dc_loss: 0.9868713021278381, tv_loss: 0.009254316799342632\n",
      "iteration 55, dc_loss: 0.9663607478141785, tv_loss: 0.00936893466860056\n",
      "iteration 56, dc_loss: 0.946280837059021, tv_loss: 0.00948052667081356\n",
      "iteration 57, dc_loss: 0.9266225099563599, tv_loss: 0.009591159410774708\n",
      "iteration 58, dc_loss: 0.9073775410652161, tv_loss: 0.009700745344161987\n",
      "iteration 59, dc_loss: 0.8885371088981628, tv_loss: 0.009808936156332493\n",
      "iteration 60, dc_loss: 0.8700929284095764, tv_loss: 0.009915696457028389\n",
      "iteration 61, dc_loss: 0.8520371317863464, tv_loss: 0.010021650232374668\n",
      "iteration 62, dc_loss: 0.8343614935874939, tv_loss: 0.010125569999217987\n",
      "iteration 63, dc_loss: 0.817058265209198, tv_loss: 0.010227524675428867\n",
      "iteration 64, dc_loss: 0.8001195788383484, tv_loss: 0.010329728946089745\n",
      "iteration 65, dc_loss: 0.7835379242897034, tv_loss: 0.010430311784148216\n",
      "iteration 66, dc_loss: 0.7673060894012451, tv_loss: 0.010527816601097584\n",
      "iteration 67, dc_loss: 0.7514166235923767, tv_loss: 0.010626045987010002\n",
      "iteration 68, dc_loss: 0.7358624339103699, tv_loss: 0.010723299346864223\n",
      "iteration 69, dc_loss: 0.7206367254257202, tv_loss: 0.010818645358085632\n",
      "iteration 70, dc_loss: 0.705732524394989, tv_loss: 0.010911032557487488\n",
      "iteration 71, dc_loss: 0.6911430954933167, tv_loss: 0.011005177162587643\n",
      "iteration 72, dc_loss: 0.6768618822097778, tv_loss: 0.01109769195318222\n",
      "iteration 73, dc_loss: 0.6628824472427368, tv_loss: 0.011187992058694363\n",
      "iteration 74, dc_loss: 0.6491984724998474, tv_loss: 0.011276856064796448\n",
      "iteration 75, dc_loss: 0.6358038187026978, tv_loss: 0.011365759186446667\n",
      "iteration 76, dc_loss: 0.6226924657821655, tv_loss: 0.01145439874380827\n",
      "iteration 77, dc_loss: 0.6098582744598389, tv_loss: 0.011540194042026997\n",
      "iteration 78, dc_loss: 0.5972957611083984, tv_loss: 0.011624895967543125\n",
      "iteration 79, dc_loss: 0.5849990248680115, tv_loss: 0.011709296144545078\n",
      "iteration 80, dc_loss: 0.5729625821113586, tv_loss: 0.011793175712227821\n",
      "iteration 81, dc_loss: 0.5611808896064758, tv_loss: 0.011876030825078487\n",
      "iteration 82, dc_loss: 0.549648642539978, tv_loss: 0.011957382783293724\n",
      "iteration 83, dc_loss: 0.5383606553077698, tv_loss: 0.012036689557135105\n",
      "iteration 84, dc_loss: 0.5273117423057556, tv_loss: 0.012115832418203354\n",
      "iteration 85, dc_loss: 0.5164967775344849, tv_loss: 0.01219607051461935\n",
      "iteration 86, dc_loss: 0.5059109926223755, tv_loss: 0.012273325584828854\n",
      "iteration 87, dc_loss: 0.4955495297908783, tv_loss: 0.012349975295364857\n",
      "iteration 88, dc_loss: 0.48540762066841125, tv_loss: 0.01242547482252121\n",
      "iteration 89, dc_loss: 0.4754807651042938, tv_loss: 0.0124993110075593\n",
      "iteration 90, dc_loss: 0.4657643139362335, tv_loss: 0.012574232183396816\n",
      "iteration 91, dc_loss: 0.45625391602516174, tv_loss: 0.012648182921111584\n",
      "iteration 92, dc_loss: 0.4469451308250427, tv_loss: 0.012719878926873207\n",
      "iteration 93, dc_loss: 0.4378337860107422, tv_loss: 0.01279049925506115\n",
      "iteration 94, dc_loss: 0.42891567945480347, tv_loss: 0.012860258109867573\n",
      "iteration 95, dc_loss: 0.42018672823905945, tv_loss: 0.01293039321899414\n",
      "iteration 96, dc_loss: 0.41164296865463257, tv_loss: 0.013000134378671646\n",
      "iteration 97, dc_loss: 0.40328049659729004, tv_loss: 0.013067781925201416\n",
      "iteration 98, dc_loss: 0.39509546756744385, tv_loss: 0.013133772648870945\n",
      "iteration 99, dc_loss: 0.38708409667015076, tv_loss: 0.01320067048072815\n",
      "iteration 100, dc_loss: 0.379242867231369, tv_loss: 0.013266235589981079\n",
      "iteration 101, dc_loss: 0.3715679943561554, tv_loss: 0.013330696150660515\n",
      "iteration 102, dc_loss: 0.3640560507774353, tv_loss: 0.013393654488027096\n",
      "iteration 103, dc_loss: 0.35670363903045654, tv_loss: 0.013456452637910843\n",
      "iteration 104, dc_loss: 0.34950727224349976, tv_loss: 0.013519181869924068\n",
      "iteration 105, dc_loss: 0.34246373176574707, tv_loss: 0.013580468483269215\n",
      "iteration 106, dc_loss: 0.3355697989463806, tv_loss: 0.013639544136822224\n",
      "iteration 107, dc_loss: 0.32882222533226013, tv_loss: 0.013699223287403584\n",
      "iteration 108, dc_loss: 0.3222179710865021, tv_loss: 0.0137582803145051\n",
      "iteration 109, dc_loss: 0.3157540559768677, tv_loss: 0.013816907070577145\n",
      "iteration 110, dc_loss: 0.309427410364151, tv_loss: 0.013873054645955563\n",
      "iteration 111, dc_loss: 0.30323517322540283, tv_loss: 0.013929752632975578\n",
      "iteration 112, dc_loss: 0.2971745431423187, tv_loss: 0.013986210338771343\n",
      "iteration 113, dc_loss: 0.29124265909194946, tv_loss: 0.014040817506611347\n",
      "iteration 114, dc_loss: 0.28543680906295776, tv_loss: 0.014093911275267601\n",
      "iteration 115, dc_loss: 0.27975431084632874, tv_loss: 0.01414862833917141\n",
      "iteration 116, dc_loss: 0.2741926312446594, tv_loss: 0.014202676713466644\n",
      "iteration 117, dc_loss: 0.2687491476535797, tv_loss: 0.014253365807235241\n",
      "iteration 118, dc_loss: 0.26342135667800903, tv_loss: 0.014302960596978664\n",
      "iteration 119, dc_loss: 0.2582068145275116, tv_loss: 0.014356745406985283\n",
      "iteration 120, dc_loss: 0.253103107213974, tv_loss: 0.014407690614461899\n",
      "iteration 121, dc_loss: 0.2481078952550888, tv_loss: 0.014455382712185383\n",
      "iteration 122, dc_loss: 0.24321888387203217, tv_loss: 0.014503159560263157\n",
      "iteration 123, dc_loss: 0.2384338080883026, tv_loss: 0.014552752487361431\n",
      "iteration 124, dc_loss: 0.23375047743320465, tv_loss: 0.01460127905011177\n",
      "iteration 125, dc_loss: 0.22916673123836517, tv_loss: 0.01464750710874796\n",
      "iteration 126, dc_loss: 0.22468048334121704, tv_loss: 0.014692981727421284\n",
      "iteration 127, dc_loss: 0.22028964757919312, tv_loss: 0.014738362282514572\n",
      "iteration 128, dc_loss: 0.21599219739437103, tv_loss: 0.014784583821892738\n",
      "iteration 129, dc_loss: 0.2117861807346344, tv_loss: 0.014829443767666817\n",
      "iteration 130, dc_loss: 0.20766963064670563, tv_loss: 0.01487328577786684\n",
      "iteration 131, dc_loss: 0.2036406248807907, tv_loss: 0.014915921725332737\n",
      "iteration 132, dc_loss: 0.1996973305940628, tv_loss: 0.014957423321902752\n",
      "iteration 133, dc_loss: 0.19583788514137268, tv_loss: 0.015001209452748299\n",
      "iteration 134, dc_loss: 0.19206058979034424, tv_loss: 0.01504287775605917\n",
      "iteration 135, dc_loss: 0.18836364150047302, tv_loss: 0.015082355588674545\n",
      "iteration 136, dc_loss: 0.1847454160451889, tv_loss: 0.015122619457542896\n",
      "iteration 137, dc_loss: 0.1812041699886322, tv_loss: 0.015162848867475986\n",
      "iteration 138, dc_loss: 0.17773829400539398, tv_loss: 0.015201178379356861\n",
      "iteration 139, dc_loss: 0.1743461936712265, tv_loss: 0.015240282751619816\n",
      "iteration 140, dc_loss: 0.17102627456188202, tv_loss: 0.015278681181371212\n",
      "iteration 141, dc_loss: 0.16777700185775757, tv_loss: 0.015316478908061981\n",
      "iteration 142, dc_loss: 0.16459691524505615, tv_loss: 0.015353060327470303\n",
      "iteration 143, dc_loss: 0.1614845097064972, tv_loss: 0.015388887375593185\n",
      "iteration 144, dc_loss: 0.15843833982944489, tv_loss: 0.01542493887245655\n",
      "iteration 145, dc_loss: 0.15545706450939178, tv_loss: 0.01546120923012495\n",
      "iteration 146, dc_loss: 0.15253928303718567, tv_loss: 0.015496086329221725\n",
      "iteration 147, dc_loss: 0.1496836394071579, tv_loss: 0.015529855154454708\n",
      "iteration 148, dc_loss: 0.14688879251480103, tv_loss: 0.015564954839646816\n",
      "iteration 149, dc_loss: 0.14415347576141357, tv_loss: 0.015598065219819546\n",
      "iteration 150, dc_loss: 0.14147645235061646, tv_loss: 0.01563132181763649\n",
      "iteration 151, dc_loss: 0.138856440782547, tv_loss: 0.015664154663681984\n",
      "iteration 152, dc_loss: 0.1362922340631485, tv_loss: 0.015696171671152115\n",
      "iteration 153, dc_loss: 0.13378268480300903, tv_loss: 0.01572810299694538\n",
      "iteration 154, dc_loss: 0.1313265711069107, tv_loss: 0.015759633854031563\n",
      "iteration 155, dc_loss: 0.12892279028892517, tv_loss: 0.015790708363056183\n",
      "iteration 156, dc_loss: 0.1265702098608017, tv_loss: 0.01582111045718193\n",
      "iteration 157, dc_loss: 0.12426776438951492, tv_loss: 0.01585080847144127\n",
      "iteration 158, dc_loss: 0.12201438099145889, tv_loss: 0.015881409868597984\n",
      "iteration 159, dc_loss: 0.11980903148651123, tv_loss: 0.015911348164081573\n",
      "iteration 160, dc_loss: 0.11765069514513016, tv_loss: 0.015939779579639435\n",
      "iteration 161, dc_loss: 0.11553839594125748, tv_loss: 0.01596808433532715\n",
      "iteration 162, dc_loss: 0.1134711280465126, tv_loss: 0.015995770692825317\n",
      "iteration 163, dc_loss: 0.11144791543483734, tv_loss: 0.016023876145482063\n",
      "iteration 164, dc_loss: 0.10946781933307648, tv_loss: 0.016051795333623886\n",
      "iteration 165, dc_loss: 0.10752995312213898, tv_loss: 0.01607823185622692\n",
      "iteration 166, dc_loss: 0.10563340783119202, tv_loss: 0.016104744747281075\n",
      "iteration 167, dc_loss: 0.10377731919288635, tv_loss: 0.01613081619143486\n",
      "iteration 168, dc_loss: 0.10196083039045334, tv_loss: 0.016157571226358414\n",
      "iteration 169, dc_loss: 0.10018309205770493, tv_loss: 0.016182251274585724\n",
      "iteration 170, dc_loss: 0.09844327718019485, tv_loss: 0.016206808388233185\n",
      "iteration 171, dc_loss: 0.09674055874347687, tv_loss: 0.016231955960392952\n",
      "iteration 172, dc_loss: 0.09507415443658829, tv_loss: 0.016256198287010193\n",
      "iteration 173, dc_loss: 0.09344331920146942, tv_loss: 0.01628018170595169\n",
      "iteration 174, dc_loss: 0.09184728562831879, tv_loss: 0.01630343683063984\n",
      "iteration 175, dc_loss: 0.0902853012084961, tv_loss: 0.016326379030942917\n",
      "iteration 176, dc_loss: 0.08875665068626404, tv_loss: 0.016349587589502335\n",
      "iteration 177, dc_loss: 0.08726061135530472, tv_loss: 0.01637253351509571\n",
      "iteration 178, dc_loss: 0.0857965350151062, tv_loss: 0.01639372482895851\n",
      "iteration 179, dc_loss: 0.08436370640993118, tv_loss: 0.016415417194366455\n",
      "iteration 180, dc_loss: 0.08296150714159012, tv_loss: 0.016437876969575882\n",
      "iteration 181, dc_loss: 0.08158928155899048, tv_loss: 0.01645953767001629\n",
      "iteration 182, dc_loss: 0.08024627715349197, tv_loss: 0.01647958531975746\n",
      "iteration 183, dc_loss: 0.07893195003271103, tv_loss: 0.016500016674399376\n",
      "iteration 184, dc_loss: 0.07764565199613571, tv_loss: 0.016520392149686813\n",
      "iteration 185, dc_loss: 0.07638680189847946, tv_loss: 0.016540508717298508\n",
      "iteration 186, dc_loss: 0.07515484094619751, tv_loss: 0.016560552641749382\n",
      "iteration 187, dc_loss: 0.0739491805434227, tv_loss: 0.016580354422330856\n",
      "iteration 188, dc_loss: 0.07276929169893265, tv_loss: 0.016599923372268677\n",
      "iteration 189, dc_loss: 0.07161460816860199, tv_loss: 0.01661793701350689\n",
      "iteration 190, dc_loss: 0.07048456370830536, tv_loss: 0.01663641817867756\n",
      "iteration 191, dc_loss: 0.06937862187623978, tv_loss: 0.01665622740983963\n",
      "iteration 192, dc_loss: 0.06829628348350525, tv_loss: 0.016675012186169624\n",
      "iteration 193, dc_loss: 0.06723708659410477, tv_loss: 0.01669197529554367\n",
      "iteration 194, dc_loss: 0.06620051711797714, tv_loss: 0.016708742827177048\n",
      "iteration 195, dc_loss: 0.06518609076738358, tv_loss: 0.01672760397195816\n",
      "iteration 196, dc_loss: 0.06419327855110168, tv_loss: 0.016745248809456825\n",
      "iteration 197, dc_loss: 0.06322167813777924, tv_loss: 0.016761720180511475\n",
      "iteration 198, dc_loss: 0.06227083131670952, tv_loss: 0.016777710989117622\n",
      "iteration 199, dc_loss: 0.06134027615189552, tv_loss: 0.016794228926301003\n",
      "iteration 200, dc_loss: 0.060429587960243225, tv_loss: 0.016810862347483635\n",
      "iteration 201, dc_loss: 0.059538353234529495, tv_loss: 0.016826419159770012\n",
      "iteration 202, dc_loss: 0.05866613984107971, tv_loss: 0.016841458156704903\n",
      "iteration 203, dc_loss: 0.05781254172325134, tv_loss: 0.016856985166668892\n",
      "iteration 204, dc_loss: 0.05697714909911156, tv_loss: 0.016873279586434364\n",
      "iteration 205, dc_loss: 0.05615958943963051, tv_loss: 0.01688835211098194\n",
      "iteration 206, dc_loss: 0.05535949394106865, tv_loss: 0.01690211519598961\n",
      "iteration 207, dc_loss: 0.05457646772265434, tv_loss: 0.016916103661060333\n",
      "iteration 208, dc_loss: 0.05381016060709953, tv_loss: 0.016930993646383286\n",
      "iteration 209, dc_loss: 0.05306019261479378, tv_loss: 0.01694546267390251\n",
      "iteration 210, dc_loss: 0.05232623219490051, tv_loss: 0.016958149150013924\n",
      "iteration 211, dc_loss: 0.05160791426897049, tv_loss: 0.016971774399280548\n",
      "iteration 212, dc_loss: 0.050904933363199234, tv_loss: 0.01698572374880314\n",
      "iteration 213, dc_loss: 0.05021696165204048, tv_loss: 0.016998721286654472\n",
      "iteration 214, dc_loss: 0.04954364895820618, tv_loss: 0.017010891810059547\n",
      "iteration 215, dc_loss: 0.048884693533182144, tv_loss: 0.01702442206442356\n",
      "iteration 216, dc_loss: 0.04823978245258331, tv_loss: 0.01703796349465847\n",
      "iteration 217, dc_loss: 0.0476086400449276, tv_loss: 0.017050111666321754\n",
      "iteration 218, dc_loss: 0.04699094593524933, tv_loss: 0.017061026766896248\n",
      "iteration 219, dc_loss: 0.04638643190264702, tv_loss: 0.01707358844578266\n",
      "iteration 220, dc_loss: 0.04579479247331619, tv_loss: 0.017086125910282135\n",
      "iteration 221, dc_loss: 0.04521576315164566, tv_loss: 0.017097432166337967\n",
      "iteration 222, dc_loss: 0.044649045914411545, tv_loss: 0.017108537256717682\n",
      "iteration 223, dc_loss: 0.044094376266002655, tv_loss: 0.017120113596320152\n",
      "iteration 224, dc_loss: 0.04355150833725929, tv_loss: 0.01713174395263195\n",
      "iteration 225, dc_loss: 0.043020181357860565, tv_loss: 0.01714255101978779\n",
      "iteration 226, dc_loss: 0.04250015690922737, tv_loss: 0.017153380438685417\n",
      "iteration 227, dc_loss: 0.0419912114739418, tv_loss: 0.017164448276162148\n",
      "iteration 228, dc_loss: 0.04149309918284416, tv_loss: 0.017174212262034416\n",
      "iteration 229, dc_loss: 0.041005611419677734, tv_loss: 0.017185209318995476\n",
      "iteration 230, dc_loss: 0.040528442710638046, tv_loss: 0.017195330932736397\n",
      "iteration 231, dc_loss: 0.04006141796708107, tv_loss: 0.017205126583576202\n",
      "iteration 232, dc_loss: 0.039604321122169495, tv_loss: 0.017215179279446602\n",
      "iteration 233, dc_loss: 0.039156924933195114, tv_loss: 0.01722526364028454\n",
      "iteration 234, dc_loss: 0.03871902823448181, tv_loss: 0.0172348041087389\n",
      "iteration 235, dc_loss: 0.03829042240977287, tv_loss: 0.01724463514983654\n",
      "iteration 236, dc_loss: 0.0378708690404892, tv_loss: 0.01725395768880844\n",
      "iteration 237, dc_loss: 0.03746022656559944, tv_loss: 0.01726226508617401\n",
      "iteration 238, dc_loss: 0.03705828636884689, tv_loss: 0.01727171055972576\n",
      "iteration 239, dc_loss: 0.03666486591100693, tv_loss: 0.017280615866184235\n",
      "iteration 240, dc_loss: 0.03627976030111313, tv_loss: 0.017289089038968086\n",
      "iteration 241, dc_loss: 0.03590279072523117, tv_loss: 0.017297618091106415\n",
      "iteration 242, dc_loss: 0.03553376719355583, tv_loss: 0.017306571826338768\n",
      "iteration 243, dc_loss: 0.03517255559563637, tv_loss: 0.017315123230218887\n",
      "iteration 244, dc_loss: 0.03481896594166756, tv_loss: 0.017322489991784096\n",
      "iteration 245, dc_loss: 0.034472834318876266, tv_loss: 0.017330847680568695\n",
      "iteration 246, dc_loss: 0.034134022891521454, tv_loss: 0.017339373007416725\n",
      "iteration 247, dc_loss: 0.03380236402153969, tv_loss: 0.01734643429517746\n",
      "iteration 248, dc_loss: 0.03347767889499664, tv_loss: 0.017354073002934456\n",
      "iteration 249, dc_loss: 0.03315981477499008, tv_loss: 0.017362257465720177\n",
      "iteration 250, dc_loss: 0.03284865617752075, tv_loss: 0.01736973039805889\n",
      "iteration 251, dc_loss: 0.032544028013944626, tv_loss: 0.017376909032464027\n",
      "iteration 252, dc_loss: 0.03224579617381096, tv_loss: 0.017383843660354614\n",
      "iteration 253, dc_loss: 0.03195380046963692, tv_loss: 0.017390934750437737\n",
      "iteration 254, dc_loss: 0.03166792914271355, tv_loss: 0.017397867515683174\n",
      "iteration 255, dc_loss: 0.031388066709041595, tv_loss: 0.017405180260539055\n",
      "iteration 256, dc_loss: 0.031114067882299423, tv_loss: 0.01741078495979309\n",
      "iteration 257, dc_loss: 0.030845830217003822, tv_loss: 0.017417851835489273\n",
      "iteration 258, dc_loss: 0.03058319352567196, tv_loss: 0.0174248144030571\n",
      "iteration 259, dc_loss: 0.030326027423143387, tv_loss: 0.017431212589144707\n",
      "iteration 260, dc_loss: 0.0300742294639349, tv_loss: 0.017436623573303223\n",
      "iteration 261, dc_loss: 0.02982768416404724, tv_loss: 0.017442887648940086\n",
      "iteration 262, dc_loss: 0.02958625927567482, tv_loss: 0.017449472099542618\n",
      "iteration 263, dc_loss: 0.02934986725449562, tv_loss: 0.017455095425248146\n",
      "iteration 264, dc_loss: 0.02911839820444584, tv_loss: 0.017460882663726807\n",
      "iteration 265, dc_loss: 0.02889174222946167, tv_loss: 0.017466751858592033\n",
      "iteration 266, dc_loss: 0.028669795021414757, tv_loss: 0.017472906038165092\n",
      "iteration 267, dc_loss: 0.0284524355083704, tv_loss: 0.01747800037264824\n",
      "iteration 268, dc_loss: 0.028239594772458076, tv_loss: 0.017482439056038857\n",
      "iteration 269, dc_loss: 0.028031162917613983, tv_loss: 0.017488103359937668\n",
      "iteration 270, dc_loss: 0.027827037498354912, tv_loss: 0.017494136467576027\n",
      "iteration 271, dc_loss: 0.027627132833003998, tv_loss: 0.017499007284641266\n",
      "iteration 272, dc_loss: 0.027431365102529526, tv_loss: 0.017503583803772926\n",
      "iteration 273, dc_loss: 0.027239609509706497, tv_loss: 0.01750892400741577\n",
      "iteration 274, dc_loss: 0.027051761746406555, tv_loss: 0.01751399226486683\n",
      "iteration 275, dc_loss: 0.02686774544417858, tv_loss: 0.017518922686576843\n",
      "iteration 276, dc_loss: 0.026687512174248695, tv_loss: 0.01752348802983761\n",
      "iteration 277, dc_loss: 0.026510994881391525, tv_loss: 0.01752876304090023\n",
      "iteration 278, dc_loss: 0.026338128373026848, tv_loss: 0.01753280498087406\n",
      "iteration 279, dc_loss: 0.026168813928961754, tv_loss: 0.017537612468004227\n",
      "iteration 280, dc_loss: 0.02600291557610035, tv_loss: 0.0175420381128788\n",
      "iteration 281, dc_loss: 0.025840383023023605, tv_loss: 0.017546413466334343\n",
      "iteration 282, dc_loss: 0.025681147351861, tv_loss: 0.01755080744624138\n",
      "iteration 283, dc_loss: 0.025525139644742012, tv_loss: 0.017554400488734245\n",
      "iteration 284, dc_loss: 0.02537228912115097, tv_loss: 0.017558883875608444\n",
      "iteration 285, dc_loss: 0.0252225399017334, tv_loss: 0.017563946545124054\n",
      "iteration 286, dc_loss: 0.02507583051919937, tv_loss: 0.01756790652871132\n",
      "iteration 287, dc_loss: 0.02493208646774292, tv_loss: 0.01757057197391987\n",
      "iteration 288, dc_loss: 0.024791225790977478, tv_loss: 0.017574837431311607\n",
      "iteration 289, dc_loss: 0.024653201922774315, tv_loss: 0.017579488456249237\n",
      "iteration 290, dc_loss: 0.02451794035732746, tv_loss: 0.01758338138461113\n",
      "iteration 291, dc_loss: 0.024385403841733932, tv_loss: 0.017586255446076393\n",
      "iteration 292, dc_loss: 0.024255532771348953, tv_loss: 0.01758945733308792\n",
      "iteration 293, dc_loss: 0.02412826381623745, tv_loss: 0.017593616619706154\n",
      "iteration 294, dc_loss: 0.0240035317838192, tv_loss: 0.017597513273358345\n",
      "iteration 295, dc_loss: 0.02388126403093338, tv_loss: 0.01760062947869301\n",
      "iteration 296, dc_loss: 0.02376141957938671, tv_loss: 0.017603550106287003\n",
      "iteration 297, dc_loss: 0.023643936961889267, tv_loss: 0.01760667935013771\n",
      "iteration 298, dc_loss: 0.023528773337602615, tv_loss: 0.017609797418117523\n",
      "iteration 299, dc_loss: 0.023415880277752876, tv_loss: 0.017613235861063004\n",
      "iteration 300, dc_loss: 0.02330523356795311, tv_loss: 0.017616018652915955\n",
      "iteration 301, dc_loss: 0.023196766152977943, tv_loss: 0.01761864311993122\n",
      "iteration 302, dc_loss: 0.02309044823050499, tv_loss: 0.017621969804167747\n",
      "iteration 303, dc_loss: 0.022986192256212234, tv_loss: 0.017624961212277412\n",
      "iteration 304, dc_loss: 0.022883959114551544, tv_loss: 0.01762738823890686\n",
      "iteration 305, dc_loss: 0.02278369665145874, tv_loss: 0.017630087211728096\n",
      "iteration 306, dc_loss: 0.02268538624048233, tv_loss: 0.01763320155441761\n",
      "iteration 307, dc_loss: 0.02258899249136448, tv_loss: 0.017635861411690712\n",
      "iteration 308, dc_loss: 0.02249445766210556, tv_loss: 0.017638294026255608\n",
      "iteration 309, dc_loss: 0.02240176312625408, tv_loss: 0.01764063537120819\n",
      "iteration 310, dc_loss: 0.022310856729745865, tv_loss: 0.017643144354224205\n",
      "iteration 311, dc_loss: 0.02222166769206524, tv_loss: 0.01764562539756298\n",
      "iteration 312, dc_loss: 0.022134169936180115, tv_loss: 0.017648112028837204\n",
      "iteration 313, dc_loss: 0.02204832062125206, tv_loss: 0.017650406807661057\n",
      "iteration 314, dc_loss: 0.02196412719786167, tv_loss: 0.017652856186032295\n",
      "iteration 315, dc_loss: 0.021881526336073875, tv_loss: 0.017655188217759132\n",
      "iteration 316, dc_loss: 0.02180050127208233, tv_loss: 0.01765749603509903\n",
      "iteration 317, dc_loss: 0.02172100730240345, tv_loss: 0.01765965111553669\n",
      "iteration 318, dc_loss: 0.021642977371811867, tv_loss: 0.01766137406229973\n",
      "iteration 319, dc_loss: 0.021566422656178474, tv_loss: 0.017664030194282532\n",
      "iteration 320, dc_loss: 0.02149129845201969, tv_loss: 0.017666438594460487\n",
      "iteration 321, dc_loss: 0.021417591720819473, tv_loss: 0.01766842231154442\n",
      "iteration 322, dc_loss: 0.021345272660255432, tv_loss: 0.017670154571533203\n",
      "iteration 323, dc_loss: 0.021274268627166748, tv_loss: 0.017671901732683182\n",
      "iteration 324, dc_loss: 0.02120460569858551, tv_loss: 0.017673898488283157\n",
      "iteration 325, dc_loss: 0.021136252209544182, tv_loss: 0.017675617709755898\n",
      "iteration 326, dc_loss: 0.021069154143333435, tv_loss: 0.01767728477716446\n",
      "iteration 327, dc_loss: 0.021003304049372673, tv_loss: 0.01767924800515175\n",
      "iteration 328, dc_loss: 0.020938649773597717, tv_loss: 0.01768074557185173\n",
      "iteration 329, dc_loss: 0.020875146612524986, tv_loss: 0.017682041972875595\n",
      "iteration 330, dc_loss: 0.02081281505525112, tv_loss: 0.017684094607830048\n",
      "iteration 331, dc_loss: 0.0207515936344862, tv_loss: 0.01768539287149906\n",
      "iteration 332, dc_loss: 0.02069147489964962, tv_loss: 0.01768673211336136\n",
      "iteration 333, dc_loss: 0.020632412284612656, tv_loss: 0.017688589170575142\n",
      "iteration 334, dc_loss: 0.02057439647614956, tv_loss: 0.01768992468714714\n",
      "iteration 335, dc_loss: 0.020517414435744286, tv_loss: 0.017691142857074738\n",
      "iteration 336, dc_loss: 0.020461449399590492, tv_loss: 0.01769273541867733\n",
      "iteration 337, dc_loss: 0.02040649764239788, tv_loss: 0.017694395035505295\n",
      "iteration 338, dc_loss: 0.020352521911263466, tv_loss: 0.017695503309369087\n",
      "iteration 339, dc_loss: 0.020299486815929413, tv_loss: 0.01769641414284706\n",
      "iteration 340, dc_loss: 0.020247407257556915, tv_loss: 0.01769738271832466\n",
      "iteration 341, dc_loss: 0.020196253433823586, tv_loss: 0.017698513343930244\n",
      "iteration 342, dc_loss: 0.020145971328020096, tv_loss: 0.017700109630823135\n",
      "iteration 343, dc_loss: 0.020096562802791595, tv_loss: 0.017700819298624992\n",
      "iteration 344, dc_loss: 0.02004801295697689, tv_loss: 0.0177020113915205\n",
      "iteration 345, dc_loss: 0.020000280812382698, tv_loss: 0.01770291104912758\n",
      "iteration 346, dc_loss: 0.019953355193138123, tv_loss: 0.01770390197634697\n",
      "iteration 347, dc_loss: 0.01990724727511406, tv_loss: 0.01770489476621151\n",
      "iteration 348, dc_loss: 0.019861936569213867, tv_loss: 0.017706045880913734\n",
      "iteration 349, dc_loss: 0.01981736533343792, tv_loss: 0.017707282677292824\n",
      "iteration 350, dc_loss: 0.01977354846894741, tv_loss: 0.017708322033286095\n",
      "iteration 351, dc_loss: 0.0197304654866457, tv_loss: 0.017709143459796906\n",
      "iteration 352, dc_loss: 0.019688114523887634, tv_loss: 0.01770995371043682\n",
      "iteration 353, dc_loss: 0.019646503031253815, tv_loss: 0.017711320891976357\n",
      "iteration 354, dc_loss: 0.019605588167905807, tv_loss: 0.017712047323584557\n",
      "iteration 355, dc_loss: 0.019565340131521225, tv_loss: 0.017712406814098358\n",
      "iteration 356, dc_loss: 0.01952575519680977, tv_loss: 0.017713379114866257\n",
      "iteration 357, dc_loss: 0.019486822187900543, tv_loss: 0.017714254558086395\n",
      "iteration 358, dc_loss: 0.01944851502776146, tv_loss: 0.01771504618227482\n",
      "iteration 359, dc_loss: 0.01941082440316677, tv_loss: 0.01771530881524086\n",
      "iteration 360, dc_loss: 0.019373726099729538, tv_loss: 0.01771620474755764\n",
      "iteration 361, dc_loss: 0.019337214529514313, tv_loss: 0.01771690882742405\n",
      "iteration 362, dc_loss: 0.01930129900574684, tv_loss: 0.017717761918902397\n",
      "iteration 363, dc_loss: 0.01926598511636257, tv_loss: 0.01771828904747963\n",
      "iteration 364, dc_loss: 0.01923127844929695, tv_loss: 0.01771860010921955\n",
      "iteration 365, dc_loss: 0.019197117537260056, tv_loss: 0.017719406634569168\n",
      "iteration 366, dc_loss: 0.019163452088832855, tv_loss: 0.017719894647598267\n",
      "iteration 367, dc_loss: 0.019130297005176544, tv_loss: 0.01772022247314453\n",
      "iteration 368, dc_loss: 0.019097689539194107, tv_loss: 0.01772044226527214\n",
      "iteration 369, dc_loss: 0.01906559430062771, tv_loss: 0.017721250653266907\n",
      "iteration 370, dc_loss: 0.019033994525671005, tv_loss: 0.01772191748023033\n",
      "iteration 371, dc_loss: 0.01900288835167885, tv_loss: 0.017722077667713165\n",
      "iteration 372, dc_loss: 0.018972249701619148, tv_loss: 0.017722342163324356\n",
      "iteration 373, dc_loss: 0.0189420934766531, tv_loss: 0.01772305741906166\n",
      "iteration 374, dc_loss: 0.018912388011813164, tv_loss: 0.017723646014928818\n",
      "iteration 375, dc_loss: 0.01888311468064785, tv_loss: 0.01772323250770569\n",
      "iteration 376, dc_loss: 0.018854284659028053, tv_loss: 0.017723917961120605\n",
      "iteration 377, dc_loss: 0.018825886771082878, tv_loss: 0.017724642530083656\n",
      "iteration 378, dc_loss: 0.01879788376390934, tv_loss: 0.017724482342600822\n",
      "iteration 379, dc_loss: 0.018770286813378334, tv_loss: 0.017724404111504555\n",
      "iteration 380, dc_loss: 0.018743108958005905, tv_loss: 0.017724649980664253\n",
      "iteration 381, dc_loss: 0.018716342747211456, tv_loss: 0.017725137993693352\n",
      "iteration 382, dc_loss: 0.018689993768930435, tv_loss: 0.01772521249949932\n",
      "iteration 383, dc_loss: 0.018664034083485603, tv_loss: 0.01772543229162693\n",
      "iteration 384, dc_loss: 0.018638433888554573, tv_loss: 0.01772548444569111\n",
      "iteration 385, dc_loss: 0.018613183870911598, tv_loss: 0.017725294455885887\n",
      "iteration 386, dc_loss: 0.01858827844262123, tv_loss: 0.017725789919495583\n",
      "iteration 387, dc_loss: 0.018563712015748024, tv_loss: 0.01772587187588215\n",
      "iteration 388, dc_loss: 0.01853950507938862, tv_loss: 0.017726007848978043\n",
      "iteration 389, dc_loss: 0.018515674397349358, tv_loss: 0.01772592030465603\n",
      "iteration 390, dc_loss: 0.01849217899143696, tv_loss: 0.017726365476846695\n",
      "iteration 391, dc_loss: 0.01846900023519993, tv_loss: 0.017726195976138115\n",
      "iteration 392, dc_loss: 0.018446166068315506, tv_loss: 0.017725888639688492\n",
      "iteration 393, dc_loss: 0.018423669040203094, tv_loss: 0.01772628352046013\n",
      "iteration 394, dc_loss: 0.018401501700282097, tv_loss: 0.017726177349686623\n",
      "iteration 395, dc_loss: 0.01837961934506893, tv_loss: 0.017725776880979538\n",
      "iteration 396, dc_loss: 0.01835799776017666, tv_loss: 0.017726054415106773\n",
      "iteration 397, dc_loss: 0.018336674198508263, tv_loss: 0.017726115882396698\n",
      "iteration 398, dc_loss: 0.018315643072128296, tv_loss: 0.01772594265639782\n",
      "iteration 399, dc_loss: 0.01829487644135952, tv_loss: 0.017725683748722076\n",
      "iteration 400, dc_loss: 0.01827438361942768, tv_loss: 0.01772548444569111\n",
      "iteration 401, dc_loss: 0.018254172056913376, tv_loss: 0.017725583165884018\n",
      "iteration 402, dc_loss: 0.018234243616461754, tv_loss: 0.017725428566336632\n",
      "iteration 403, dc_loss: 0.018214590847492218, tv_loss: 0.017725378274917603\n",
      "iteration 404, dc_loss: 0.018195178359746933, tv_loss: 0.017724845558404922\n",
      "iteration 405, dc_loss: 0.0181760061532259, tv_loss: 0.01772451587021351\n",
      "iteration 406, dc_loss: 0.018157102167606354, tv_loss: 0.017724763602018356\n",
      "iteration 407, dc_loss: 0.01813841611146927, tv_loss: 0.01772451028227806\n",
      "iteration 408, dc_loss: 0.01811996102333069, tv_loss: 0.017724042758345604\n",
      "iteration 409, dc_loss: 0.01810174435377121, tv_loss: 0.017723726108670235\n",
      "iteration 410, dc_loss: 0.018083756789565086, tv_loss: 0.01772383227944374\n",
      "iteration 411, dc_loss: 0.018066000193357468, tv_loss: 0.01772361993789673\n",
      "iteration 412, dc_loss: 0.01804846152663231, tv_loss: 0.017723213881254196\n",
      "iteration 413, dc_loss: 0.018031137064099312, tv_loss: 0.01772278919816017\n",
      "iteration 414, dc_loss: 0.018014054745435715, tv_loss: 0.01772266998887062\n",
      "iteration 415, dc_loss: 0.01799718849360943, tv_loss: 0.017722666263580322\n",
      "iteration 416, dc_loss: 0.017980532720685005, tv_loss: 0.0177223589271307\n",
      "iteration 417, dc_loss: 0.017964089289307594, tv_loss: 0.017721571028232574\n",
      "iteration 418, dc_loss: 0.017947837710380554, tv_loss: 0.01772128976881504\n",
      "iteration 419, dc_loss: 0.017931781709194183, tv_loss: 0.01772141270339489\n",
      "iteration 420, dc_loss: 0.01791592687368393, tv_loss: 0.017721064388751984\n",
      "iteration 421, dc_loss: 0.01790027506649494, tv_loss: 0.017720209434628487\n",
      "iteration 422, dc_loss: 0.017884807661175728, tv_loss: 0.01771971955895424\n",
      "iteration 423, dc_loss: 0.0178694948554039, tv_loss: 0.01771984063088894\n",
      "iteration 424, dc_loss: 0.017854325473308563, tv_loss: 0.017719535157084465\n",
      "iteration 425, dc_loss: 0.01783933863043785, tv_loss: 0.01771879941225052\n",
      "iteration 426, dc_loss: 0.017824536189436913, tv_loss: 0.017718156799674034\n",
      "iteration 427, dc_loss: 0.017809908837080002, tv_loss: 0.0177181176841259\n",
      "iteration 428, dc_loss: 0.01779547519981861, tv_loss: 0.017717693001031876\n",
      "iteration 429, dc_loss: 0.017781220376491547, tv_loss: 0.01771680638194084\n",
      "iteration 430, dc_loss: 0.0177671629935503, tv_loss: 0.017716575413942337\n",
      "iteration 431, dc_loss: 0.017753247171640396, tv_loss: 0.0177164189517498\n",
      "iteration 432, dc_loss: 0.01773943565785885, tv_loss: 0.017715729773044586\n",
      "iteration 433, dc_loss: 0.0177257489413023, tv_loss: 0.017714999616146088\n",
      "iteration 434, dc_loss: 0.017712220549583435, tv_loss: 0.017714835703372955\n",
      "iteration 435, dc_loss: 0.01769886538386345, tv_loss: 0.017714690417051315\n",
      "iteration 436, dc_loss: 0.017685651779174805, tv_loss: 0.017714258283376694\n",
      "iteration 437, dc_loss: 0.017672594636678696, tv_loss: 0.01771337166428566\n",
      "iteration 438, dc_loss: 0.01765969954431057, tv_loss: 0.01771276257932186\n",
      "iteration 439, dc_loss: 0.017646940425038338, tv_loss: 0.017712710425257683\n",
      "iteration 440, dc_loss: 0.017634332180023193, tv_loss: 0.017712416127324104\n",
      "iteration 441, dc_loss: 0.0176218431442976, tv_loss: 0.017711404711008072\n",
      "iteration 442, dc_loss: 0.017609523609280586, tv_loss: 0.017710447311401367\n",
      "iteration 443, dc_loss: 0.01759735308587551, tv_loss: 0.017710190266370773\n",
      "iteration 444, dc_loss: 0.01758531853556633, tv_loss: 0.017709769308567047\n",
      "iteration 445, dc_loss: 0.01757338084280491, tv_loss: 0.01770905777812004\n",
      "iteration 446, dc_loss: 0.017561526969075203, tv_loss: 0.017708048224449158\n",
      "iteration 447, dc_loss: 0.0175497867166996, tv_loss: 0.017707310616970062\n",
      "iteration 448, dc_loss: 0.01753818988800049, tv_loss: 0.01770715042948723\n",
      "iteration 449, dc_loss: 0.01752672903239727, tv_loss: 0.01770670898258686\n",
      "iteration 450, dc_loss: 0.017515387386083603, tv_loss: 0.017705770209431648\n",
      "iteration 451, dc_loss: 0.017504150047898293, tv_loss: 0.017704814672470093\n",
      "iteration 452, dc_loss: 0.01749303564429283, tv_loss: 0.017704321071505547\n",
      "iteration 453, dc_loss: 0.017482029274106026, tv_loss: 0.01770365610718727\n",
      "iteration 454, dc_loss: 0.01747114397585392, tv_loss: 0.01770327426493168\n",
      "iteration 455, dc_loss: 0.017460346221923828, tv_loss: 0.01770247146487236\n",
      "iteration 456, dc_loss: 0.017449667677283287, tv_loss: 0.01770208589732647\n",
      "iteration 457, dc_loss: 0.01743912324309349, tv_loss: 0.017701050266623497\n",
      "iteration 458, dc_loss: 0.017428698018193245, tv_loss: 0.017699746415019035\n",
      "iteration 459, dc_loss: 0.01741836778819561, tv_loss: 0.01769939810037613\n",
      "iteration 460, dc_loss: 0.017408128827810287, tv_loss: 0.017698828130960464\n",
      "iteration 461, dc_loss: 0.017397945746779442, tv_loss: 0.01769784651696682\n",
      "iteration 462, dc_loss: 0.017387865111231804, tv_loss: 0.017696939408779144\n",
      "iteration 463, dc_loss: 0.01737789437174797, tv_loss: 0.017696766182780266\n",
      "iteration 464, dc_loss: 0.017368018627166748, tv_loss: 0.017696110531687737\n",
      "iteration 465, dc_loss: 0.01735825277864933, tv_loss: 0.017695391550660133\n",
      "iteration 466, dc_loss: 0.017348598688840866, tv_loss: 0.017694631591439247\n",
      "iteration 467, dc_loss: 0.017339009791612625, tv_loss: 0.017693964764475822\n",
      "iteration 468, dc_loss: 0.0173295047134161, tv_loss: 0.017693016678094864\n",
      "iteration 469, dc_loss: 0.01732012815773487, tv_loss: 0.017692431807518005\n",
      "iteration 470, dc_loss: 0.017310855910182, tv_loss: 0.017691606655716896\n",
      "iteration 471, dc_loss: 0.017301665619015694, tv_loss: 0.017691008746623993\n",
      "iteration 472, dc_loss: 0.017292562872171402, tv_loss: 0.017690205946564674\n",
      "iteration 473, dc_loss: 0.01728353649377823, tv_loss: 0.01768934540450573\n",
      "iteration 474, dc_loss: 0.017274564132094383, tv_loss: 0.017688464373350143\n",
      "iteration 475, dc_loss: 0.01726567931473255, tv_loss: 0.017688149586319923\n",
      "iteration 476, dc_loss: 0.01725686714053154, tv_loss: 0.017687590792775154\n",
      "iteration 477, dc_loss: 0.017248131334781647, tv_loss: 0.017686231061816216\n",
      "iteration 478, dc_loss: 0.017239464446902275, tv_loss: 0.017685119062662125\n",
      "iteration 479, dc_loss: 0.017230911180377007, tv_loss: 0.017684947699308395\n",
      "iteration 480, dc_loss: 0.0172224473208189, tv_loss: 0.01768428459763527\n",
      "iteration 481, dc_loss: 0.017214076593518257, tv_loss: 0.017683090642094612\n",
      "iteration 482, dc_loss: 0.017205772921442986, tv_loss: 0.017681855708360672\n",
      "iteration 483, dc_loss: 0.017197521403431892, tv_loss: 0.017681637778878212\n",
      "iteration 484, dc_loss: 0.017189335078001022, tv_loss: 0.017681138589978218\n",
      "iteration 485, dc_loss: 0.017181243747472763, tv_loss: 0.017680201679468155\n",
      "iteration 486, dc_loss: 0.017173228785395622, tv_loss: 0.017679033800959587\n",
      "iteration 487, dc_loss: 0.01716526411473751, tv_loss: 0.017678065225481987\n",
      "iteration 488, dc_loss: 0.017157355323433876, tv_loss: 0.017677737399935722\n",
      "iteration 489, dc_loss: 0.01714952103793621, tv_loss: 0.017676901072263718\n",
      "iteration 490, dc_loss: 0.017141755670309067, tv_loss: 0.01767582818865776\n",
      "iteration 491, dc_loss: 0.017134075984358788, tv_loss: 0.017674913629889488\n",
      "iteration 492, dc_loss: 0.017126480117440224, tv_loss: 0.017674149945378304\n",
      "iteration 493, dc_loss: 0.017118951305747032, tv_loss: 0.017673054710030556\n",
      "iteration 494, dc_loss: 0.017111506313085556, tv_loss: 0.01767243631184101\n",
      "iteration 495, dc_loss: 0.01710408926010132, tv_loss: 0.01767173409461975\n",
      "iteration 496, dc_loss: 0.017096711322665215, tv_loss: 0.017670493572950363\n",
      "iteration 497, dc_loss: 0.01708938367664814, tv_loss: 0.017669672146439552\n",
      "iteration 498, dc_loss: 0.01708214171230793, tv_loss: 0.017669251188635826\n",
      "iteration 499, dc_loss: 0.017074957489967346, tv_loss: 0.0176682248711586\n",
      "iteration 500, dc_loss: 0.017067834734916687, tv_loss: 0.017667081207036972\n",
      "iteration 501, dc_loss: 0.017060790210962296, tv_loss: 0.017666617408394814\n",
      "iteration 502, dc_loss: 0.01705382950603962, tv_loss: 0.017665807157754898\n",
      "iteration 503, dc_loss: 0.017046919092535973, tv_loss: 0.017664771527051926\n",
      "iteration 504, dc_loss: 0.017040075734257698, tv_loss: 0.017663689330220222\n",
      "iteration 505, dc_loss: 0.017033275216817856, tv_loss: 0.017662806436419487\n",
      "iteration 506, dc_loss: 0.017026526853442192, tv_loss: 0.017661912366747856\n",
      "iteration 507, dc_loss: 0.017019838094711304, tv_loss: 0.017661236226558685\n",
      "iteration 508, dc_loss: 0.01701320894062519, tv_loss: 0.01766030862927437\n",
      "iteration 509, dc_loss: 0.017006609588861465, tv_loss: 0.01765935681760311\n",
      "iteration 510, dc_loss: 0.017000047490000725, tv_loss: 0.017658475786447525\n",
      "iteration 511, dc_loss: 0.01699354313313961, tv_loss: 0.017657695338129997\n",
      "iteration 512, dc_loss: 0.01698707975447178, tv_loss: 0.017656929790973663\n",
      "iteration 513, dc_loss: 0.01698068343102932, tv_loss: 0.017655951902270317\n",
      "iteration 514, dc_loss: 0.01697433926165104, tv_loss: 0.017654892057180405\n",
      "iteration 515, dc_loss: 0.01696806214749813, tv_loss: 0.017653902992606163\n",
      "iteration 516, dc_loss: 0.01696183532476425, tv_loss: 0.017653299495577812\n",
      "iteration 517, dc_loss: 0.016955668106675148, tv_loss: 0.01765253022313118\n",
      "iteration 518, dc_loss: 0.016949566081166267, tv_loss: 0.017651531845331192\n",
      "iteration 519, dc_loss: 0.016943491995334625, tv_loss: 0.017650412395596504\n",
      "iteration 520, dc_loss: 0.016937486827373505, tv_loss: 0.0176498144865036\n",
      "iteration 521, dc_loss: 0.016931507736444473, tv_loss: 0.017649006098508835\n",
      "iteration 522, dc_loss: 0.016925562173128128, tv_loss: 0.017647909000515938\n",
      "iteration 523, dc_loss: 0.016919661313295364, tv_loss: 0.017646830528974533\n",
      "iteration 524, dc_loss: 0.01691380888223648, tv_loss: 0.017645830288529396\n",
      "iteration 525, dc_loss: 0.016908006742596626, tv_loss: 0.0176450926810503\n",
      "iteration 526, dc_loss: 0.01690223254263401, tv_loss: 0.01764441281557083\n",
      "iteration 527, dc_loss: 0.016896521672606468, tv_loss: 0.017643621191382408\n",
      "iteration 528, dc_loss: 0.01689085364341736, tv_loss: 0.017642449587583542\n",
      "iteration 529, dc_loss: 0.01688522845506668, tv_loss: 0.01764131896197796\n",
      "iteration 530, dc_loss: 0.016879640519618988, tv_loss: 0.017640607431530952\n",
      "iteration 531, dc_loss: 0.016874121502041817, tv_loss: 0.01763976365327835\n",
      "iteration 532, dc_loss: 0.016868656501173973, tv_loss: 0.017638592049479485\n",
      "iteration 533, dc_loss: 0.016863256692886353, tv_loss: 0.01763744466006756\n",
      "iteration 534, dc_loss: 0.016857896000146866, tv_loss: 0.017636558040976524\n",
      "iteration 535, dc_loss: 0.016852548345923424, tv_loss: 0.017635339871048927\n",
      "iteration 536, dc_loss: 0.016847234219312668, tv_loss: 0.017634479328989983\n",
      "iteration 537, dc_loss: 0.016841938719153404, tv_loss: 0.01763363555073738\n",
      "iteration 538, dc_loss: 0.01683669351041317, tv_loss: 0.01763276942074299\n",
      "iteration 539, dc_loss: 0.016831455752253532, tv_loss: 0.0176312867552042\n",
      "iteration 540, dc_loss: 0.016826264560222626, tv_loss: 0.017630817368626595\n",
      "iteration 541, dc_loss: 0.01682111993432045, tv_loss: 0.017630042508244514\n",
      "iteration 542, dc_loss: 0.016816025599837303, tv_loss: 0.017628859728574753\n",
      "iteration 543, dc_loss: 0.01681099459528923, tv_loss: 0.017627865076065063\n",
      "iteration 544, dc_loss: 0.01680605672299862, tv_loss: 0.017626872286200523\n",
      "iteration 545, dc_loss: 0.016801146790385246, tv_loss: 0.017626069486141205\n",
      "iteration 546, dc_loss: 0.016796229407191277, tv_loss: 0.01762547716498375\n",
      "iteration 547, dc_loss: 0.016791319474577904, tv_loss: 0.017624257132411003\n",
      "iteration 548, dc_loss: 0.016786424443125725, tv_loss: 0.017623141407966614\n",
      "iteration 549, dc_loss: 0.016781585291028023, tv_loss: 0.017622215673327446\n",
      "iteration 550, dc_loss: 0.01677677594125271, tv_loss: 0.017621463164687157\n",
      "iteration 551, dc_loss: 0.016771988943219185, tv_loss: 0.017620699480175972\n",
      "iteration 552, dc_loss: 0.016767241060733795, tv_loss: 0.017619570717215538\n",
      "iteration 553, dc_loss: 0.01676255464553833, tv_loss: 0.017618464305996895\n",
      "iteration 554, dc_loss: 0.016757909208536148, tv_loss: 0.017617477104067802\n",
      "iteration 555, dc_loss: 0.016753291711211205, tv_loss: 0.01761658675968647\n",
      "iteration 556, dc_loss: 0.016748690977692604, tv_loss: 0.017615586519241333\n",
      "iteration 557, dc_loss: 0.01674412377178669, tv_loss: 0.01761476695537567\n",
      "iteration 558, dc_loss: 0.016739603132009506, tv_loss: 0.01761374995112419\n",
      "iteration 559, dc_loss: 0.01673511043190956, tv_loss: 0.017612656578421593\n",
      "iteration 560, dc_loss: 0.016730649396777153, tv_loss: 0.01761196367442608\n",
      "iteration 561, dc_loss: 0.016726238653063774, tv_loss: 0.01761092059314251\n",
      "iteration 562, dc_loss: 0.016721881926059723, tv_loss: 0.017609985545277596\n",
      "iteration 563, dc_loss: 0.016717541962862015, tv_loss: 0.017609069123864174\n",
      "iteration 564, dc_loss: 0.016713205724954605, tv_loss: 0.01760818250477314\n",
      "iteration 565, dc_loss: 0.016708899289369583, tv_loss: 0.017607413232326508\n",
      "iteration 566, dc_loss: 0.016704635694622993, tv_loss: 0.017606103792786598\n",
      "iteration 567, dc_loss: 0.0167003832757473, tv_loss: 0.017605038359761238\n",
      "iteration 568, dc_loss: 0.016696155071258545, tv_loss: 0.01760442554950714\n",
      "iteration 569, dc_loss: 0.01669197715818882, tv_loss: 0.0176035538315773\n",
      "iteration 570, dc_loss: 0.016687855124473572, tv_loss: 0.01760210283100605\n",
      "iteration 571, dc_loss: 0.01668379083275795, tv_loss: 0.0176008939743042\n",
      "iteration 572, dc_loss: 0.016679752618074417, tv_loss: 0.017600324004888535\n",
      "iteration 573, dc_loss: 0.016675716266036034, tv_loss: 0.017599161714315414\n",
      "iteration 574, dc_loss: 0.016671698540449142, tv_loss: 0.017598386853933334\n",
      "iteration 575, dc_loss: 0.0166676826775074, tv_loss: 0.01759733073413372\n",
      "iteration 576, dc_loss: 0.01666366122663021, tv_loss: 0.017596280202269554\n",
      "iteration 577, dc_loss: 0.016659682616591454, tv_loss: 0.01759549230337143\n",
      "iteration 578, dc_loss: 0.01665574312210083, tv_loss: 0.01759479008615017\n",
      "iteration 579, dc_loss: 0.01665182039141655, tv_loss: 0.01759369671344757\n",
      "iteration 580, dc_loss: 0.01664792373776436, tv_loss: 0.017592689022421837\n",
      "iteration 581, dc_loss: 0.01664406806230545, tv_loss: 0.017591705545783043\n",
      "iteration 582, dc_loss: 0.01664027012884617, tv_loss: 0.017590880393981934\n",
      "iteration 583, dc_loss: 0.016636556014418602, tv_loss: 0.017589423805475235\n",
      "iteration 584, dc_loss: 0.01663287542760372, tv_loss: 0.01758863963186741\n",
      "iteration 585, dc_loss: 0.016629209741950035, tv_loss: 0.017587820068001747\n",
      "iteration 586, dc_loss: 0.0166255384683609, tv_loss: 0.017586834728717804\n",
      "iteration 587, dc_loss: 0.016621846705675125, tv_loss: 0.017585763707756996\n",
      "iteration 588, dc_loss: 0.016618140041828156, tv_loss: 0.0175846666097641\n",
      "iteration 589, dc_loss: 0.016614453867077827, tv_loss: 0.017583664506673813\n",
      "iteration 590, dc_loss: 0.01661083661019802, tv_loss: 0.017582744359970093\n",
      "iteration 591, dc_loss: 0.016607265919446945, tv_loss: 0.017581777647137642\n",
      "iteration 592, dc_loss: 0.01660369150340557, tv_loss: 0.017580756917595863\n",
      "iteration 593, dc_loss: 0.016600146889686584, tv_loss: 0.017579806968569756\n",
      "iteration 594, dc_loss: 0.01659662276506424, tv_loss: 0.01757887750864029\n",
      "iteration 595, dc_loss: 0.016593145206570625, tv_loss: 0.01757783815264702\n",
      "iteration 596, dc_loss: 0.0165896974503994, tv_loss: 0.017576970160007477\n",
      "iteration 597, dc_loss: 0.01658628322184086, tv_loss: 0.017576079815626144\n",
      "iteration 598, dc_loss: 0.016582870855927467, tv_loss: 0.017574654892086983\n",
      "iteration 599, dc_loss: 0.016579482704401016, tv_loss: 0.017573563382029533\n",
      "iteration 600, dc_loss: 0.016576111316680908, tv_loss: 0.01757299341261387\n",
      "iteration 601, dc_loss: 0.016572769731283188, tv_loss: 0.017571980133652687\n",
      "iteration 602, dc_loss: 0.016569431871175766, tv_loss: 0.017570732161402702\n",
      "iteration 603, dc_loss: 0.016566120088100433, tv_loss: 0.01756991818547249\n",
      "iteration 604, dc_loss: 0.01656283624470234, tv_loss: 0.017568839713931084\n",
      "iteration 605, dc_loss: 0.016559595242142677, tv_loss: 0.01756790280342102\n",
      "iteration 606, dc_loss: 0.016556402668356895, tv_loss: 0.017566876485943794\n",
      "iteration 607, dc_loss: 0.01655321940779686, tv_loss: 0.017566217109560966\n",
      "iteration 608, dc_loss: 0.01655004546046257, tv_loss: 0.017564861103892326\n",
      "iteration 609, dc_loss: 0.016546880826354027, tv_loss: 0.017563674598932266\n",
      "iteration 610, dc_loss: 0.016543708741664886, tv_loss: 0.01756289228796959\n",
      "iteration 611, dc_loss: 0.016540542244911194, tv_loss: 0.017562221735715866\n",
      "iteration 612, dc_loss: 0.016537358984351158, tv_loss: 0.017561165615916252\n",
      "iteration 613, dc_loss: 0.016534224152565002, tv_loss: 0.017560040578246117\n",
      "iteration 614, dc_loss: 0.016531167551875114, tv_loss: 0.01755896396934986\n",
      "iteration 615, dc_loss: 0.016528142616152763, tv_loss: 0.01755816861987114\n",
      "iteration 616, dc_loss: 0.016525132581591606, tv_loss: 0.0175572969019413\n",
      "iteration 617, dc_loss: 0.016522139310836792, tv_loss: 0.01755625009536743\n",
      "iteration 618, dc_loss: 0.016519146040081978, tv_loss: 0.017555151134729385\n",
      "iteration 619, dc_loss: 0.0165161844342947, tv_loss: 0.01755431666970253\n",
      "iteration 620, dc_loss: 0.01651325449347496, tv_loss: 0.01755344308912754\n",
      "iteration 621, dc_loss: 0.016510341316461563, tv_loss: 0.017552345991134644\n",
      "iteration 622, dc_loss: 0.016507448628544807, tv_loss: 0.01755153201520443\n",
      "iteration 623, dc_loss: 0.016504542902112007, tv_loss: 0.017550548538565636\n",
      "iteration 624, dc_loss: 0.01650162599980831, tv_loss: 0.01754947565495968\n",
      "iteration 625, dc_loss: 0.01649872027337551, tv_loss: 0.017548371106386185\n",
      "iteration 626, dc_loss: 0.0164958443492651, tv_loss: 0.0175477284938097\n",
      "iteration 627, dc_loss: 0.016492990776896477, tv_loss: 0.017546886578202248\n",
      "iteration 628, dc_loss: 0.01649017632007599, tv_loss: 0.017545636743307114\n",
      "iteration 629, dc_loss: 0.016487397253513336, tv_loss: 0.017544681206345558\n",
      "iteration 630, dc_loss: 0.016484655439853668, tv_loss: 0.017543965950608253\n",
      "iteration 631, dc_loss: 0.016481926664710045, tv_loss: 0.017542880028486252\n",
      "iteration 632, dc_loss: 0.01647922769188881, tv_loss: 0.017541708424687386\n",
      "iteration 633, dc_loss: 0.016476547345519066, tv_loss: 0.017540492117404938\n",
      "iteration 634, dc_loss: 0.01647382602095604, tv_loss: 0.01753997802734375\n",
      "iteration 635, dc_loss: 0.016471104696393013, tv_loss: 0.017539246007800102\n",
      "iteration 636, dc_loss: 0.016468413174152374, tv_loss: 0.01753801852464676\n",
      "iteration 637, dc_loss: 0.016465727239847183, tv_loss: 0.017536912113428116\n",
      "iteration 638, dc_loss: 0.016463058069348335, tv_loss: 0.017536187544465065\n",
      "iteration 639, dc_loss: 0.016460414975881577, tv_loss: 0.017535356804728508\n",
      "iteration 640, dc_loss: 0.016457809135317802, tv_loss: 0.017534106969833374\n",
      "iteration 641, dc_loss: 0.016455242410302162, tv_loss: 0.01753311976790428\n",
      "iteration 642, dc_loss: 0.016452694311738014, tv_loss: 0.01753217913210392\n",
      "iteration 643, dc_loss: 0.01645013689994812, tv_loss: 0.0175316259264946\n",
      "iteration 644, dc_loss: 0.016447583213448524, tv_loss: 0.017530521377921104\n",
      "iteration 645, dc_loss: 0.01644502580165863, tv_loss: 0.017529545351862907\n",
      "iteration 646, dc_loss: 0.016442490741610527, tv_loss: 0.0175288375467062\n",
      "iteration 647, dc_loss: 0.016439978033304214, tv_loss: 0.017527958378195763\n",
      "iteration 648, dc_loss: 0.016437508165836334, tv_loss: 0.01752684824168682\n",
      "iteration 649, dc_loss: 0.016435083001852036, tv_loss: 0.017525795847177505\n",
      "iteration 650, dc_loss: 0.016432667151093483, tv_loss: 0.01752476394176483\n",
      "iteration 651, dc_loss: 0.016430264338850975, tv_loss: 0.017523935064673424\n",
      "iteration 652, dc_loss: 0.016427835449576378, tv_loss: 0.017522767186164856\n",
      "iteration 653, dc_loss: 0.01642538234591484, tv_loss: 0.01752154342830181\n",
      "iteration 654, dc_loss: 0.016422946006059647, tv_loss: 0.017521116882562637\n",
      "iteration 655, dc_loss: 0.016420552507042885, tv_loss: 0.01752018928527832\n",
      "iteration 656, dc_loss: 0.016418196260929108, tv_loss: 0.017518822103738785\n",
      "iteration 657, dc_loss: 0.01641583815217018, tv_loss: 0.01751803606748581\n",
      "iteration 658, dc_loss: 0.016413480043411255, tv_loss: 0.017517229542136192\n",
      "iteration 659, dc_loss: 0.016411133110523224, tv_loss: 0.01751612313091755\n",
      "iteration 660, dc_loss: 0.016408802941441536, tv_loss: 0.01751544326543808\n",
      "iteration 661, dc_loss: 0.016406536102294922, tv_loss: 0.01751442439854145\n",
      "iteration 662, dc_loss: 0.016404269263148308, tv_loss: 0.017513245344161987\n",
      "iteration 663, dc_loss: 0.01640201173722744, tv_loss: 0.017512409016489983\n",
      "iteration 664, dc_loss: 0.016399767249822617, tv_loss: 0.017511414363980293\n",
      "iteration 665, dc_loss: 0.016397541388869286, tv_loss: 0.01751065067946911\n",
      "iteration 666, dc_loss: 0.016395308077335358, tv_loss: 0.017509834840893745\n",
      "iteration 667, dc_loss: 0.016393080353736877, tv_loss: 0.017508450895547867\n",
      "iteration 668, dc_loss: 0.0163908489048481, tv_loss: 0.01750744879245758\n",
      "iteration 669, dc_loss: 0.016388608142733574, tv_loss: 0.017506791278719902\n",
      "iteration 670, dc_loss: 0.01638641208410263, tv_loss: 0.017505742609500885\n",
      "iteration 671, dc_loss: 0.016384240239858627, tv_loss: 0.01750466413795948\n",
      "iteration 672, dc_loss: 0.01638210006058216, tv_loss: 0.017503855749964714\n",
      "iteration 673, dc_loss: 0.01637997105717659, tv_loss: 0.017502950504422188\n",
      "iteration 674, dc_loss: 0.016377853229641914, tv_loss: 0.017501885071396828\n",
      "iteration 675, dc_loss: 0.016375744715332985, tv_loss: 0.01750115677714348\n",
      "iteration 676, dc_loss: 0.01637366972863674, tv_loss: 0.017500685527920723\n",
      "iteration 677, dc_loss: 0.01637158915400505, tv_loss: 0.017499634996056557\n",
      "iteration 678, dc_loss: 0.016369495540857315, tv_loss: 0.017498549073934555\n",
      "iteration 679, dc_loss: 0.016367405652999878, tv_loss: 0.017497805878520012\n",
      "iteration 680, dc_loss: 0.01636531762778759, tv_loss: 0.017496958374977112\n",
      "iteration 681, dc_loss: 0.016363244503736496, tv_loss: 0.017495814710855484\n",
      "iteration 682, dc_loss: 0.01636120304465294, tv_loss: 0.017494898289442062\n",
      "iteration 683, dc_loss: 0.016359200701117516, tv_loss: 0.01749384216964245\n",
      "iteration 684, dc_loss: 0.01635723002254963, tv_loss: 0.01749281771481037\n",
      "iteration 685, dc_loss: 0.016355255618691444, tv_loss: 0.017492195591330528\n",
      "iteration 686, dc_loss: 0.016353286802768707, tv_loss: 0.017491426318883896\n",
      "iteration 687, dc_loss: 0.016351310536265373, tv_loss: 0.017490122467279434\n",
      "iteration 688, dc_loss: 0.016349349170923233, tv_loss: 0.01748933084309101\n",
      "iteration 689, dc_loss: 0.016347384080290794, tv_loss: 0.017488449811935425\n",
      "iteration 690, dc_loss: 0.01634540781378746, tv_loss: 0.017487604171037674\n",
      "iteration 691, dc_loss: 0.01634344644844532, tv_loss: 0.01748673804104328\n",
      "iteration 692, dc_loss: 0.01634152978658676, tv_loss: 0.01748555526137352\n",
      "iteration 693, dc_loss: 0.016339613124728203, tv_loss: 0.01748473569750786\n",
      "iteration 694, dc_loss: 0.016337696462869644, tv_loss: 0.01748407445847988\n",
      "iteration 695, dc_loss: 0.016335779801011086, tv_loss: 0.01748296059668064\n",
      "iteration 696, dc_loss: 0.01633387804031372, tv_loss: 0.017482100054621696\n",
      "iteration 697, dc_loss: 0.01633199118077755, tv_loss: 0.017481250688433647\n",
      "iteration 698, dc_loss: 0.01633015275001526, tv_loss: 0.017480364069342613\n",
      "iteration 699, dc_loss: 0.01632833667099476, tv_loss: 0.017479512840509415\n",
      "iteration 700, dc_loss: 0.01632651686668396, tv_loss: 0.017478421330451965\n",
      "iteration 701, dc_loss: 0.016324715688824654, tv_loss: 0.0174776092171669\n",
      "iteration 702, dc_loss: 0.016322901472449303, tv_loss: 0.017476581037044525\n",
      "iteration 703, dc_loss: 0.01632106304168701, tv_loss: 0.017475582659244537\n",
      "iteration 704, dc_loss: 0.01631923019886017, tv_loss: 0.01747453771531582\n",
      "iteration 705, dc_loss: 0.01631743460893631, tv_loss: 0.017473841086030006\n",
      "iteration 706, dc_loss: 0.01631566323339939, tv_loss: 0.01747305318713188\n",
      "iteration 707, dc_loss: 0.01631390117108822, tv_loss: 0.01747213676571846\n",
      "iteration 708, dc_loss: 0.01631212793290615, tv_loss: 0.017471211031079292\n",
      "iteration 709, dc_loss: 0.01631036214530468, tv_loss: 0.01747030019760132\n",
      "iteration 710, dc_loss: 0.01630859449505806, tv_loss: 0.017469478771090508\n",
      "iteration 711, dc_loss: 0.01630682870745659, tv_loss: 0.017468811944127083\n",
      "iteration 712, dc_loss: 0.01630508527159691, tv_loss: 0.017467917874455452\n",
      "iteration 713, dc_loss: 0.016303380951285362, tv_loss: 0.0174668338149786\n",
      "iteration 714, dc_loss: 0.016301672905683517, tv_loss: 0.017465980723500252\n",
      "iteration 715, dc_loss: 0.01629997044801712, tv_loss: 0.017465148121118546\n",
      "iteration 716, dc_loss: 0.01629827171564102, tv_loss: 0.01746409758925438\n",
      "iteration 717, dc_loss: 0.016296587884426117, tv_loss: 0.017463352531194687\n",
      "iteration 718, dc_loss: 0.01629491150379181, tv_loss: 0.01746228337287903\n",
      "iteration 719, dc_loss: 0.016293270513415337, tv_loss: 0.01746159978210926\n",
      "iteration 720, dc_loss: 0.016291627660393715, tv_loss: 0.01746094599366188\n",
      "iteration 721, dc_loss: 0.016290003433823586, tv_loss: 0.017459576949477196\n",
      "iteration 722, dc_loss: 0.016288386657834053, tv_loss: 0.01745845563709736\n",
      "iteration 723, dc_loss: 0.016286766156554222, tv_loss: 0.017457829788327217\n",
      "iteration 724, dc_loss: 0.0162851270288229, tv_loss: 0.017456889152526855\n",
      "iteration 725, dc_loss: 0.016283519566059113, tv_loss: 0.017456064000725746\n",
      "iteration 726, dc_loss: 0.016281921416521072, tv_loss: 0.017455024644732475\n",
      "iteration 727, dc_loss: 0.016280319541692734, tv_loss: 0.017454087734222412\n",
      "iteration 728, dc_loss: 0.016278743743896484, tv_loss: 0.017453061416745186\n",
      "iteration 729, dc_loss: 0.016277143731713295, tv_loss: 0.017452353611588478\n",
      "iteration 730, dc_loss: 0.016275517642498016, tv_loss: 0.017451690509915352\n",
      "iteration 731, dc_loss: 0.016273904591798782, tv_loss: 0.01745086908340454\n",
      "iteration 732, dc_loss: 0.016272328794002533, tv_loss: 0.01744997873902321\n",
      "iteration 733, dc_loss: 0.016270814463496208, tv_loss: 0.01744898036122322\n",
      "iteration 734, dc_loss: 0.01626930758357048, tv_loss: 0.017448222264647484\n",
      "iteration 735, dc_loss: 0.016267787665128708, tv_loss: 0.01744760386645794\n",
      "iteration 736, dc_loss: 0.01626625843346119, tv_loss: 0.01744675636291504\n",
      "iteration 737, dc_loss: 0.016264725476503372, tv_loss: 0.017445553094148636\n",
      "iteration 738, dc_loss: 0.016263214871287346, tv_loss: 0.01744479313492775\n",
      "iteration 739, dc_loss: 0.01626168005168438, tv_loss: 0.017444299533963203\n",
      "iteration 740, dc_loss: 0.016260163858532906, tv_loss: 0.017443381249904633\n",
      "iteration 741, dc_loss: 0.016258692368865013, tv_loss: 0.017442304641008377\n",
      "iteration 742, dc_loss: 0.016257237643003464, tv_loss: 0.01744144596159458\n",
      "iteration 743, dc_loss: 0.016255808994174004, tv_loss: 0.017440708354115486\n",
      "iteration 744, dc_loss: 0.01625439152121544, tv_loss: 0.01743977889418602\n",
      "iteration 745, dc_loss: 0.01625296287238598, tv_loss: 0.017438771203160286\n",
      "iteration 746, dc_loss: 0.016251519322395325, tv_loss: 0.017438018694519997\n",
      "iteration 747, dc_loss: 0.016250044107437134, tv_loss: 0.017437007278203964\n",
      "iteration 748, dc_loss: 0.016248567029833794, tv_loss: 0.017436273396015167\n",
      "iteration 749, dc_loss: 0.016247104853391647, tv_loss: 0.01743544079363346\n",
      "iteration 750, dc_loss: 0.016245655715465546, tv_loss: 0.01743455044925213\n",
      "iteration 751, dc_loss: 0.01624421775341034, tv_loss: 0.01743360236287117\n",
      "iteration 752, dc_loss: 0.016242796555161476, tv_loss: 0.017432918772101402\n",
      "iteration 753, dc_loss: 0.01624138094484806, tv_loss: 0.017432233318686485\n",
      "iteration 754, dc_loss: 0.016239982098340988, tv_loss: 0.01743125729262829\n",
      "iteration 755, dc_loss: 0.016238613054156303, tv_loss: 0.01743008755147457\n",
      "iteration 756, dc_loss: 0.01623724400997162, tv_loss: 0.017429236322641373\n",
      "iteration 757, dc_loss: 0.01623588800430298, tv_loss: 0.017428619787096977\n",
      "iteration 758, dc_loss: 0.016234541311860085, tv_loss: 0.017427602782845497\n",
      "iteration 759, dc_loss: 0.016233189031481743, tv_loss: 0.017426710575819016\n",
      "iteration 760, dc_loss: 0.01623183861374855, tv_loss: 0.017425822094082832\n",
      "iteration 761, dc_loss: 0.016230473294854164, tv_loss: 0.01742498017847538\n",
      "iteration 762, dc_loss: 0.016229119151830673, tv_loss: 0.017424385994672775\n",
      "iteration 763, dc_loss: 0.016227789223194122, tv_loss: 0.017423508688807487\n",
      "iteration 764, dc_loss: 0.016226455569267273, tv_loss: 0.017422696575522423\n",
      "iteration 765, dc_loss: 0.016225114464759827, tv_loss: 0.017421824857592583\n",
      "iteration 766, dc_loss: 0.01622380129992962, tv_loss: 0.017421169206500053\n",
      "iteration 767, dc_loss: 0.016222471371293068, tv_loss: 0.017420243471860886\n",
      "iteration 768, dc_loss: 0.016221139580011368, tv_loss: 0.01741967536509037\n",
      "iteration 769, dc_loss: 0.016219839453697205, tv_loss: 0.01741868630051613\n",
      "iteration 770, dc_loss: 0.016218531876802444, tv_loss: 0.01741795428097248\n",
      "iteration 771, dc_loss: 0.016217224299907684, tv_loss: 0.01741717755794525\n",
      "iteration 772, dc_loss: 0.016215935349464417, tv_loss: 0.0174164567142725\n",
      "iteration 773, dc_loss: 0.016214657574892044, tv_loss: 0.017415562644600868\n",
      "iteration 774, dc_loss: 0.016213392838835716, tv_loss: 0.017414640635252\n",
      "iteration 775, dc_loss: 0.016212135553359985, tv_loss: 0.017413882538676262\n",
      "iteration 776, dc_loss: 0.016210880130529404, tv_loss: 0.01741330325603485\n",
      "iteration 777, dc_loss: 0.01620962843298912, tv_loss: 0.017412446439266205\n",
      "iteration 778, dc_loss: 0.016208387911319733, tv_loss: 0.0174113716930151\n",
      "iteration 779, dc_loss: 0.016207128763198853, tv_loss: 0.017410511150956154\n",
      "iteration 780, dc_loss: 0.01620587892830372, tv_loss: 0.017409756779670715\n",
      "iteration 781, dc_loss: 0.01620464213192463, tv_loss: 0.017408989369869232\n",
      "iteration 782, dc_loss: 0.016203418374061584, tv_loss: 0.01740814931690693\n",
      "iteration 783, dc_loss: 0.01620221510529518, tv_loss: 0.017407288774847984\n",
      "iteration 784, dc_loss: 0.016200989484786987, tv_loss: 0.017406534403562546\n",
      "iteration 785, dc_loss: 0.016199778765439987, tv_loss: 0.017405936494469643\n",
      "iteration 786, dc_loss: 0.01619861088693142, tv_loss: 0.01740480773150921\n",
      "iteration 787, dc_loss: 0.01619742438197136, tv_loss: 0.017403824254870415\n",
      "iteration 788, dc_loss: 0.01619623601436615, tv_loss: 0.01740335114300251\n",
      "iteration 789, dc_loss: 0.01619502529501915, tv_loss: 0.01740245334804058\n",
      "iteration 790, dc_loss: 0.01619384065270424, tv_loss: 0.01740177720785141\n",
      "iteration 791, dc_loss: 0.016192659735679626, tv_loss: 0.017400773242115974\n",
      "iteration 792, dc_loss: 0.016191529110074043, tv_loss: 0.017400149255990982\n",
      "iteration 793, dc_loss: 0.01619040220975876, tv_loss: 0.017399463802576065\n",
      "iteration 794, dc_loss: 0.01618923433125019, tv_loss: 0.017398571595549583\n",
      "iteration 795, dc_loss: 0.016188064590096474, tv_loss: 0.01739758439362049\n",
      "iteration 796, dc_loss: 0.016186872497200966, tv_loss: 0.017396964132785797\n",
      "iteration 797, dc_loss: 0.016185719519853592, tv_loss: 0.017396412789821625\n",
      "iteration 798, dc_loss: 0.01618456095457077, tv_loss: 0.01739533618092537\n",
      "iteration 799, dc_loss: 0.016183404251933098, tv_loss: 0.01739436946809292\n",
      "iteration 800, dc_loss: 0.01618223264813423, tv_loss: 0.017393741756677628\n",
      "iteration 801, dc_loss: 0.016181083396077156, tv_loss: 0.017393222078680992\n",
      "iteration 802, dc_loss: 0.016179975122213364, tv_loss: 0.017392123118042946\n",
      "iteration 803, dc_loss: 0.01617889478802681, tv_loss: 0.017391011118888855\n",
      "iteration 804, dc_loss: 0.01617780327796936, tv_loss: 0.017390578985214233\n",
      "iteration 805, dc_loss: 0.01617671549320221, tv_loss: 0.017389768734574318\n",
      "iteration 806, dc_loss: 0.016175640746951103, tv_loss: 0.01738888956606388\n",
      "iteration 807, dc_loss: 0.01617453247308731, tv_loss: 0.01738813892006874\n",
      "iteration 808, dc_loss: 0.01617339625954628, tv_loss: 0.017387522384524345\n",
      "iteration 809, dc_loss: 0.01617228426039219, tv_loss: 0.01738693006336689\n",
      "iteration 810, dc_loss: 0.016171175986528397, tv_loss: 0.017386049032211304\n",
      "iteration 811, dc_loss: 0.01617012917995453, tv_loss: 0.017385171726346016\n",
      "iteration 812, dc_loss: 0.016169100999832153, tv_loss: 0.017384404316544533\n",
      "iteration 813, dc_loss: 0.016168061643838882, tv_loss: 0.017383595928549767\n",
      "iteration 814, dc_loss: 0.016166988760232925, tv_loss: 0.017382783815264702\n",
      "iteration 815, dc_loss: 0.01616589166224003, tv_loss: 0.017382068559527397\n",
      "iteration 816, dc_loss: 0.016164803877472878, tv_loss: 0.01738140359520912\n",
      "iteration 817, dc_loss: 0.01616373099386692, tv_loss: 0.01738051138818264\n",
      "iteration 818, dc_loss: 0.016162671148777008, tv_loss: 0.01737959124147892\n",
      "iteration 819, dc_loss: 0.016161633655428886, tv_loss: 0.01737927831709385\n",
      "iteration 820, dc_loss: 0.0161606278270483, tv_loss: 0.01737826131284237\n",
      "iteration 821, dc_loss: 0.016159653663635254, tv_loss: 0.017377303913235664\n",
      "iteration 822, dc_loss: 0.016158685088157654, tv_loss: 0.017376592382788658\n",
      "iteration 823, dc_loss: 0.0161577258259058, tv_loss: 0.017375923693180084\n",
      "iteration 824, dc_loss: 0.016156714409589767, tv_loss: 0.017374984920024872\n",
      "iteration 825, dc_loss: 0.01615564525127411, tv_loss: 0.017374221235513687\n",
      "iteration 826, dc_loss: 0.016154592856764793, tv_loss: 0.01737358048558235\n",
      "iteration 827, dc_loss: 0.01615355722606182, tv_loss: 0.017372846603393555\n",
      "iteration 828, dc_loss: 0.016152499243617058, tv_loss: 0.01737227849662304\n",
      "iteration 829, dc_loss: 0.016151471063494682, tv_loss: 0.01737136021256447\n",
      "iteration 830, dc_loss: 0.016150448471307755, tv_loss: 0.01737075112760067\n",
      "iteration 831, dc_loss: 0.016149448230862617, tv_loss: 0.017370322719216347\n",
      "iteration 832, dc_loss: 0.01614847220480442, tv_loss: 0.01736939139664173\n",
      "iteration 833, dc_loss: 0.016147514805197716, tv_loss: 0.017368506640195847\n",
      "iteration 834, dc_loss: 0.01614655926823616, tv_loss: 0.01736775040626526\n",
      "iteration 835, dc_loss: 0.016145581379532814, tv_loss: 0.017367111518979073\n",
      "iteration 836, dc_loss: 0.016144560649991035, tv_loss: 0.01736661233007908\n",
      "iteration 837, dc_loss: 0.0161435566842556, tv_loss: 0.017366081476211548\n",
      "iteration 838, dc_loss: 0.01614258624613285, tv_loss: 0.017365170642733574\n",
      "iteration 839, dc_loss: 0.016141662374138832, tv_loss: 0.017364313825964928\n",
      "iteration 840, dc_loss: 0.01614074967801571, tv_loss: 0.017363760620355606\n",
      "iteration 841, dc_loss: 0.016139859333634377, tv_loss: 0.01736290194094181\n",
      "iteration 842, dc_loss: 0.016138965263962746, tv_loss: 0.017361929640173912\n",
      "iteration 843, dc_loss: 0.016138028353452682, tv_loss: 0.017361214384436607\n",
      "iteration 844, dc_loss: 0.016137078404426575, tv_loss: 0.01736035943031311\n",
      "iteration 845, dc_loss: 0.016136106103658676, tv_loss: 0.017359811812639236\n",
      "iteration 846, dc_loss: 0.016135087236762047, tv_loss: 0.01735926978290081\n",
      "iteration 847, dc_loss: 0.01613408885896206, tv_loss: 0.01735835336148739\n",
      "iteration 848, dc_loss: 0.016133135184645653, tv_loss: 0.017357703298330307\n",
      "iteration 849, dc_loss: 0.016132226213812828, tv_loss: 0.017356961965560913\n",
      "iteration 850, dc_loss: 0.016131339594721794, tv_loss: 0.01735617406666279\n",
      "iteration 851, dc_loss: 0.016130411997437477, tv_loss: 0.017355529591441154\n",
      "iteration 852, dc_loss: 0.016129491850733757, tv_loss: 0.017354629933834076\n",
      "iteration 853, dc_loss: 0.016128620132803917, tv_loss: 0.017353801056742668\n",
      "iteration 854, dc_loss: 0.016127722337841988, tv_loss: 0.0173532385379076\n",
      "iteration 855, dc_loss: 0.0161268450319767, tv_loss: 0.01735270582139492\n",
      "iteration 856, dc_loss: 0.016125962138175964, tv_loss: 0.017351901158690453\n",
      "iteration 857, dc_loss: 0.016125047579407692, tv_loss: 0.017350783571600914\n",
      "iteration 858, dc_loss: 0.016124118119478226, tv_loss: 0.01735028065741062\n",
      "iteration 859, dc_loss: 0.0161232128739357, tv_loss: 0.017349762842059135\n",
      "iteration 860, dc_loss: 0.01612231880426407, tv_loss: 0.01734873652458191\n",
      "iteration 861, dc_loss: 0.01612146943807602, tv_loss: 0.01734820008277893\n",
      "iteration 862, dc_loss: 0.016120631247758865, tv_loss: 0.017347590997815132\n",
      "iteration 863, dc_loss: 0.016119781881570816, tv_loss: 0.017346547916531563\n",
      "iteration 864, dc_loss: 0.016118939965963364, tv_loss: 0.017345907166600227\n",
      "iteration 865, dc_loss: 0.016118088737130165, tv_loss: 0.01734546385705471\n",
      "iteration 866, dc_loss: 0.01611723005771637, tv_loss: 0.017344815656542778\n",
      "iteration 867, dc_loss: 0.016116345301270485, tv_loss: 0.01734418049454689\n",
      "iteration 868, dc_loss: 0.01611548475921154, tv_loss: 0.017343444749712944\n",
      "iteration 869, dc_loss: 0.016114618629217148, tv_loss: 0.0173425804823637\n",
      "iteration 870, dc_loss: 0.01611376740038395, tv_loss: 0.017342131584882736\n",
      "iteration 871, dc_loss: 0.016112880781292915, tv_loss: 0.017341462895274162\n",
      "iteration 872, dc_loss: 0.016111988574266434, tv_loss: 0.01734071783721447\n",
      "iteration 873, dc_loss: 0.01611112803220749, tv_loss: 0.01734008640050888\n",
      "iteration 874, dc_loss: 0.01611027680337429, tv_loss: 0.01733941026031971\n",
      "iteration 875, dc_loss: 0.0161094032227993, tv_loss: 0.017338670790195465\n",
      "iteration 876, dc_loss: 0.016108563169836998, tv_loss: 0.017337912693619728\n",
      "iteration 877, dc_loss: 0.016107752919197083, tv_loss: 0.017337266355752945\n",
      "iteration 878, dc_loss: 0.016106950119137764, tv_loss: 0.01733657531440258\n",
      "iteration 879, dc_loss: 0.0161061342805624, tv_loss: 0.0173360463231802\n",
      "iteration 880, dc_loss: 0.016105325892567635, tv_loss: 0.01733509451150894\n",
      "iteration 881, dc_loss: 0.01610453613102436, tv_loss: 0.01733410358428955\n",
      "iteration 882, dc_loss: 0.016103733330965042, tv_loss: 0.017333848401904106\n",
      "iteration 883, dc_loss: 0.016102895140647888, tv_loss: 0.017333177849650383\n",
      "iteration 884, dc_loss: 0.016102055087685585, tv_loss: 0.017332248389720917\n",
      "iteration 885, dc_loss: 0.016101207584142685, tv_loss: 0.01733182929456234\n",
      "iteration 886, dc_loss: 0.016100404784083366, tv_loss: 0.017331387847661972\n",
      "iteration 887, dc_loss: 0.01609964668750763, tv_loss: 0.017330454662442207\n",
      "iteration 888, dc_loss: 0.016098903492093086, tv_loss: 0.01732960157096386\n",
      "iteration 889, dc_loss: 0.016098160296678543, tv_loss: 0.01732892170548439\n",
      "iteration 890, dc_loss: 0.01609737239778042, tv_loss: 0.017328141257166862\n",
      "iteration 891, dc_loss: 0.016096558421850204, tv_loss: 0.01732759177684784\n",
      "iteration 892, dc_loss: 0.01609574258327484, tv_loss: 0.01732718013226986\n",
      "iteration 893, dc_loss: 0.01609494350850582, tv_loss: 0.017326319590210915\n",
      "iteration 894, dc_loss: 0.016094190999865532, tv_loss: 0.01732548512518406\n",
      "iteration 895, dc_loss: 0.01609342359006405, tv_loss: 0.017325103282928467\n",
      "iteration 896, dc_loss: 0.016092630103230476, tv_loss: 0.01732451096177101\n",
      "iteration 897, dc_loss: 0.016091851517558098, tv_loss: 0.01732349768280983\n",
      "iteration 898, dc_loss: 0.01609107479453087, tv_loss: 0.017322756350040436\n",
      "iteration 899, dc_loss: 0.01609029807150364, tv_loss: 0.017322378233075142\n",
      "iteration 900, dc_loss: 0.016089526936411858, tv_loss: 0.0173217561095953\n",
      "iteration 901, dc_loss: 0.016088737174868584, tv_loss: 0.017320970073342323\n",
      "iteration 902, dc_loss: 0.016087951138615608, tv_loss: 0.01732022874057293\n",
      "iteration 903, dc_loss: 0.016087200492620468, tv_loss: 0.01731971837580204\n",
      "iteration 904, dc_loss: 0.016086440533399582, tv_loss: 0.017319170758128166\n",
      "iteration 905, dc_loss: 0.016085682436823845, tv_loss: 0.01731853000819683\n",
      "iteration 906, dc_loss: 0.016084939241409302, tv_loss: 0.017317719757556915\n",
      "iteration 907, dc_loss: 0.016084197908639908, tv_loss: 0.017317013815045357\n",
      "iteration 908, dc_loss: 0.01608344539999962, tv_loss: 0.017316294834017754\n",
      "iteration 909, dc_loss: 0.01608271896839142, tv_loss: 0.01731577329337597\n",
      "iteration 910, dc_loss: 0.01608201302587986, tv_loss: 0.01731489598751068\n",
      "iteration 911, dc_loss: 0.016081318259239197, tv_loss: 0.017314130440354347\n",
      "iteration 912, dc_loss: 0.016080601140856743, tv_loss: 0.01731381006538868\n",
      "iteration 913, dc_loss: 0.016079850494861603, tv_loss: 0.017313050106167793\n",
      "iteration 914, dc_loss: 0.016079114750027657, tv_loss: 0.017312195152044296\n",
      "iteration 915, dc_loss: 0.016078373417258263, tv_loss: 0.01731165312230587\n",
      "iteration 916, dc_loss: 0.01607768051326275, tv_loss: 0.01731099747121334\n",
      "iteration 917, dc_loss: 0.016076957806944847, tv_loss: 0.017310231924057007\n",
      "iteration 918, dc_loss: 0.016076238825917244, tv_loss: 0.017309686169028282\n",
      "iteration 919, dc_loss: 0.016075512394309044, tv_loss: 0.017309177666902542\n",
      "iteration 920, dc_loss: 0.01607479900121689, tv_loss: 0.017308490350842476\n",
      "iteration 921, dc_loss: 0.01607407070696354, tv_loss: 0.017307860776782036\n",
      "iteration 922, dc_loss: 0.01607333868741989, tv_loss: 0.017307307571172714\n",
      "iteration 923, dc_loss: 0.01607263833284378, tv_loss: 0.017306672409176826\n",
      "iteration 924, dc_loss: 0.016071954742074013, tv_loss: 0.017306013032794\n",
      "iteration 925, dc_loss: 0.01607128418982029, tv_loss: 0.01730557531118393\n",
      "iteration 926, dc_loss: 0.016070609912276268, tv_loss: 0.017304984852671623\n",
      "iteration 927, dc_loss: 0.0160699263215065, tv_loss: 0.01730419509112835\n",
      "iteration 928, dc_loss: 0.016069216653704643, tv_loss: 0.01730339042842388\n",
      "iteration 929, dc_loss: 0.016068505123257637, tv_loss: 0.017302893102169037\n",
      "iteration 930, dc_loss: 0.016067801043391228, tv_loss: 0.017302433028817177\n",
      "iteration 931, dc_loss: 0.016067102551460266, tv_loss: 0.0173017755150795\n",
      "iteration 932, dc_loss: 0.016066396608948708, tv_loss: 0.017300855368375778\n",
      "iteration 933, dc_loss: 0.016065675765275955, tv_loss: 0.017300482839345932\n",
      "iteration 934, dc_loss: 0.016064977273344994, tv_loss: 0.017299873754382133\n",
      "iteration 935, dc_loss: 0.01606428623199463, tv_loss: 0.017299311235547066\n",
      "iteration 936, dc_loss: 0.01606360636651516, tv_loss: 0.017298666760325432\n",
      "iteration 937, dc_loss: 0.01606292650103569, tv_loss: 0.01729801669716835\n",
      "iteration 938, dc_loss: 0.016062255948781967, tv_loss: 0.017297454178333282\n",
      "iteration 939, dc_loss: 0.016061600297689438, tv_loss: 0.017296846956014633\n",
      "iteration 940, dc_loss: 0.01606094464659691, tv_loss: 0.01729612797498703\n",
      "iteration 941, dc_loss: 0.01606028713285923, tv_loss: 0.017295528203248978\n",
      "iteration 942, dc_loss: 0.01605960726737976, tv_loss: 0.017295224592089653\n",
      "iteration 943, dc_loss: 0.01605898141860962, tv_loss: 0.01729460246860981\n",
      "iteration 944, dc_loss: 0.016058357432484627, tv_loss: 0.017293618991971016\n",
      "iteration 945, dc_loss: 0.016057727858424187, tv_loss: 0.017292894423007965\n",
      "iteration 946, dc_loss: 0.016057077795267105, tv_loss: 0.017292570322752\n",
      "iteration 947, dc_loss: 0.016056416556239128, tv_loss: 0.017291948199272156\n",
      "iteration 948, dc_loss: 0.016055775806307793, tv_loss: 0.017291132360696793\n",
      "iteration 949, dc_loss: 0.016055095940828323, tv_loss: 0.017290616407990456\n",
      "iteration 950, dc_loss: 0.016054419800639153, tv_loss: 0.017290115356445312\n",
      "iteration 951, dc_loss: 0.016053762286901474, tv_loss: 0.017289312556385994\n",
      "iteration 952, dc_loss: 0.016053108498454094, tv_loss: 0.01728885993361473\n",
      "iteration 953, dc_loss: 0.016052493825554848, tv_loss: 0.017288479954004288\n",
      "iteration 954, dc_loss: 0.016051867976784706, tv_loss: 0.01728774793446064\n",
      "iteration 955, dc_loss: 0.016051258891820908, tv_loss: 0.017286989837884903\n",
      "iteration 956, dc_loss: 0.01605067029595375, tv_loss: 0.0172863882035017\n",
      "iteration 957, dc_loss: 0.01605006866157055, tv_loss: 0.01728576421737671\n",
      "iteration 958, dc_loss: 0.016049420461058617, tv_loss: 0.017285345122218132\n",
      "iteration 959, dc_loss: 0.016048729419708252, tv_loss: 0.017284922301769257\n",
      "iteration 960, dc_loss: 0.01604803465306759, tv_loss: 0.017284244298934937\n",
      "iteration 961, dc_loss: 0.016047373414039612, tv_loss: 0.01728355698287487\n",
      "iteration 962, dc_loss: 0.01604674756526947, tv_loss: 0.0172831192612648\n",
      "iteration 963, dc_loss: 0.01604611985385418, tv_loss: 0.017282599583268166\n",
      "iteration 964, dc_loss: 0.016045507043600082, tv_loss: 0.01728214882314205\n",
      "iteration 965, dc_loss: 0.016044870018959045, tv_loss: 0.017281509935855865\n",
      "iteration 966, dc_loss: 0.016044223681092262, tv_loss: 0.017280979081988335\n",
      "iteration 967, dc_loss: 0.01604357734322548, tv_loss: 0.01728041097521782\n",
      "iteration 968, dc_loss: 0.01604294963181019, tv_loss: 0.01727985590696335\n",
      "iteration 969, dc_loss: 0.016042349860072136, tv_loss: 0.017279207706451416\n",
      "iteration 970, dc_loss: 0.016041753813624382, tv_loss: 0.017278490588068962\n",
      "iteration 971, dc_loss: 0.01604118011891842, tv_loss: 0.01727808080613613\n",
      "iteration 972, dc_loss: 0.016040600836277008, tv_loss: 0.01727749966084957\n",
      "iteration 973, dc_loss: 0.016039982438087463, tv_loss: 0.017276903614401817\n",
      "iteration 974, dc_loss: 0.01603936403989792, tv_loss: 0.017276225611567497\n",
      "iteration 975, dc_loss: 0.016038786619901657, tv_loss: 0.017275763675570488\n",
      "iteration 976, dc_loss: 0.01603821851313114, tv_loss: 0.017275264486670494\n",
      "iteration 977, dc_loss: 0.016037624329328537, tv_loss: 0.017274703830480576\n",
      "iteration 978, dc_loss: 0.01603703573346138, tv_loss: 0.01727420836687088\n",
      "iteration 979, dc_loss: 0.01603640988469124, tv_loss: 0.017273634672164917\n",
      "iteration 980, dc_loss: 0.016035767272114754, tv_loss: 0.017273152247071266\n",
      "iteration 981, dc_loss: 0.016035115346312523, tv_loss: 0.017272857949137688\n",
      "iteration 982, dc_loss: 0.016034476459026337, tv_loss: 0.017272228375077248\n",
      "iteration 983, dc_loss: 0.01603388972580433, tv_loss: 0.017271650955080986\n",
      "iteration 984, dc_loss: 0.01603330858051777, tv_loss: 0.01727115549147129\n",
      "iteration 985, dc_loss: 0.016032718122005463, tv_loss: 0.017270628362894058\n",
      "iteration 986, dc_loss: 0.01603212207555771, tv_loss: 0.017270218580961227\n",
      "iteration 987, dc_loss: 0.016031567007303238, tv_loss: 0.017269589006900787\n",
      "iteration 988, dc_loss: 0.016031043604016304, tv_loss: 0.01726887933909893\n",
      "iteration 989, dc_loss: 0.01603054814040661, tv_loss: 0.017268244177103043\n",
      "iteration 990, dc_loss: 0.016029980033636093, tv_loss: 0.0172678641974926\n",
      "iteration 991, dc_loss: 0.01602940633893013, tv_loss: 0.01726723089814186\n",
      "iteration 992, dc_loss: 0.016028856858611107, tv_loss: 0.017266511917114258\n",
      "iteration 993, dc_loss: 0.01602828875184059, tv_loss: 0.017266038805246353\n",
      "iteration 994, dc_loss: 0.016027700155973434, tv_loss: 0.01726534217596054\n",
      "iteration 995, dc_loss: 0.016027063131332397, tv_loss: 0.017264751717448235\n",
      "iteration 996, dc_loss: 0.016026441007852554, tv_loss: 0.017264438793063164\n",
      "iteration 997, dc_loss: 0.01602586917579174, tv_loss: 0.017264053225517273\n",
      "iteration 998, dc_loss: 0.016025317832827568, tv_loss: 0.01726323738694191\n",
      "iteration 999, dc_loss: 0.016024766489863396, tv_loss: 0.01726282760500908\n",
      "iteration 1000, dc_loss: 0.016024233773350716, tv_loss: 0.01726231724023819\n",
      "iteration 1001, dc_loss: 0.016023708507418633, tv_loss: 0.01726151630282402\n",
      "iteration 1002, dc_loss: 0.016023172065615654, tv_loss: 0.01726110652089119\n",
      "iteration 1003, dc_loss: 0.01602262631058693, tv_loss: 0.017260676249861717\n",
      "iteration 1004, dc_loss: 0.01602206751704216, tv_loss: 0.017260149121284485\n",
      "iteration 1005, dc_loss: 0.0160214863717556, tv_loss: 0.01725945994257927\n",
      "iteration 1006, dc_loss: 0.01602092571556568, tv_loss: 0.017258867621421814\n",
      "iteration 1007, dc_loss: 0.01602037250995636, tv_loss: 0.017258690670132637\n",
      "iteration 1008, dc_loss: 0.016019823029637337, tv_loss: 0.017258303239941597\n",
      "iteration 1009, dc_loss: 0.01601928472518921, tv_loss: 0.017257582396268845\n",
      "iteration 1010, dc_loss: 0.016018718481063843, tv_loss: 0.01725679263472557\n",
      "iteration 1011, dc_loss: 0.016018198803067207, tv_loss: 0.017256474122405052\n",
      "iteration 1012, dc_loss: 0.016017671674489975, tv_loss: 0.017256103456020355\n",
      "iteration 1013, dc_loss: 0.016017146408557892, tv_loss: 0.017255516722798347\n",
      "iteration 1014, dc_loss: 0.016016609966754913, tv_loss: 0.01725502498447895\n",
      "iteration 1015, dc_loss: 0.01601608656346798, tv_loss: 0.017254376783967018\n",
      "iteration 1016, dc_loss: 0.01601555198431015, tv_loss: 0.01725410297513008\n",
      "iteration 1017, dc_loss: 0.016015009954571724, tv_loss: 0.017253601923584938\n",
      "iteration 1018, dc_loss: 0.016014443710446358, tv_loss: 0.017252933233976364\n",
      "iteration 1019, dc_loss: 0.0160138588398695, tv_loss: 0.017252465710043907\n",
      "iteration 1020, dc_loss: 0.016013329848647118, tv_loss: 0.017252188175916672\n",
      "iteration 1021, dc_loss: 0.01601283624768257, tv_loss: 0.017251476645469666\n",
      "iteration 1022, dc_loss: 0.01601235754787922, tv_loss: 0.01725086197257042\n",
      "iteration 1023, dc_loss: 0.01601187326014042, tv_loss: 0.017250489443540573\n",
      "iteration 1024, dc_loss: 0.01601138338446617, tv_loss: 0.017249910160899162\n",
      "iteration 1025, dc_loss: 0.016010865569114685, tv_loss: 0.01724921353161335\n",
      "iteration 1026, dc_loss: 0.0160103440284729, tv_loss: 0.01724899932742119\n",
      "iteration 1027, dc_loss: 0.016009801998734474, tv_loss: 0.017248600721359253\n",
      "iteration 1028, dc_loss: 0.016009261831641197, tv_loss: 0.0172480046749115\n",
      "iteration 1029, dc_loss: 0.0160087738186121, tv_loss: 0.01724741980433464\n",
      "iteration 1030, dc_loss: 0.016008270904421806, tv_loss: 0.01724700815975666\n",
      "iteration 1031, dc_loss: 0.016007741913199425, tv_loss: 0.017246661707758904\n",
      "iteration 1032, dc_loss: 0.016007188707590103, tv_loss: 0.01724598929286003\n",
      "iteration 1033, dc_loss: 0.01600663550198078, tv_loss: 0.017245564609766006\n",
      "iteration 1034, dc_loss: 0.016006099060177803, tv_loss: 0.01724516972899437\n",
      "iteration 1035, dc_loss: 0.016005586832761765, tv_loss: 0.0172446109354496\n",
      "iteration 1036, dc_loss: 0.016005108132958412, tv_loss: 0.01724393665790558\n",
      "iteration 1037, dc_loss: 0.0160046499222517, tv_loss: 0.01724345050752163\n",
      "iteration 1038, dc_loss: 0.01600418984889984, tv_loss: 0.017243269830942154\n",
      "iteration 1039, dc_loss: 0.016003694385290146, tv_loss: 0.017242535948753357\n",
      "iteration 1040, dc_loss: 0.01600315049290657, tv_loss: 0.01724211499094963\n",
      "iteration 1041, dc_loss: 0.016002614051103592, tv_loss: 0.017241833731532097\n",
      "iteration 1042, dc_loss: 0.016002118587493896, tv_loss: 0.017241403460502625\n",
      "iteration 1043, dc_loss: 0.01600164733827114, tv_loss: 0.01724088005721569\n",
      "iteration 1044, dc_loss: 0.01600119285285473, tv_loss: 0.017240434885025024\n",
      "iteration 1045, dc_loss: 0.01600070856511593, tv_loss: 0.01723993942141533\n",
      "iteration 1046, dc_loss: 0.016000228002667427, tv_loss: 0.01723949797451496\n",
      "iteration 1047, dc_loss: 0.015999743714928627, tv_loss: 0.017238812521100044\n",
      "iteration 1048, dc_loss: 0.015999261289834976, tv_loss: 0.01723838970065117\n",
      "iteration 1049, dc_loss: 0.015998784452676773, tv_loss: 0.01723804511129856\n",
      "iteration 1050, dc_loss: 0.01599828526377678, tv_loss: 0.017237527295947075\n",
      "iteration 1051, dc_loss: 0.01599779538810253, tv_loss: 0.017236992716789246\n",
      "iteration 1052, dc_loss: 0.015997352078557014, tv_loss: 0.017236554995179176\n",
      "iteration 1053, dc_loss: 0.015996893867850304, tv_loss: 0.017236098647117615\n",
      "iteration 1054, dc_loss: 0.015996428206562996, tv_loss: 0.01723576709628105\n",
      "iteration 1055, dc_loss: 0.015995938330888748, tv_loss: 0.017235230654478073\n",
      "iteration 1056, dc_loss: 0.015995431691408157, tv_loss: 0.017234720289707184\n",
      "iteration 1057, dc_loss: 0.015994910150766373, tv_loss: 0.017234209924936295\n",
      "iteration 1058, dc_loss: 0.0159943625330925, tv_loss: 0.01723388023674488\n",
      "iteration 1059, dc_loss: 0.01599385403096676, tv_loss: 0.017233647406101227\n",
      "iteration 1060, dc_loss: 0.015993405133485794, tv_loss: 0.017232950776815414\n",
      "iteration 1061, dc_loss: 0.015992987900972366, tv_loss: 0.017232296988368034\n",
      "iteration 1062, dc_loss: 0.01599257066845894, tv_loss: 0.01723197102546692\n",
      "iteration 1063, dc_loss: 0.015992138534784317, tv_loss: 0.017231544479727745\n",
      "iteration 1064, dc_loss: 0.015991706401109695, tv_loss: 0.017230818048119545\n",
      "iteration 1065, dc_loss: 0.015991227701306343, tv_loss: 0.017230503261089325\n",
      "iteration 1066, dc_loss: 0.015990719199180603, tv_loss: 0.01723015494644642\n",
      "iteration 1067, dc_loss: 0.01599019579589367, tv_loss: 0.017229653894901276\n",
      "iteration 1068, dc_loss: 0.015989700332283974, tv_loss: 0.017229266464710236\n",
      "iteration 1069, dc_loss: 0.015989208593964577, tv_loss: 0.017228981480002403\n",
      "iteration 1070, dc_loss: 0.01598869450390339, tv_loss: 0.017228521406650543\n",
      "iteration 1071, dc_loss: 0.015988225117325783, tv_loss: 0.017228109762072563\n",
      "iteration 1072, dc_loss: 0.015987783670425415, tv_loss: 0.017227670177817345\n",
      "iteration 1073, dc_loss: 0.015987327322363853, tv_loss: 0.0172270555049181\n",
      "iteration 1074, dc_loss: 0.015986910089850426, tv_loss: 0.0172265637665987\n",
      "iteration 1075, dc_loss: 0.01598651334643364, tv_loss: 0.017226289957761765\n",
      "iteration 1076, dc_loss: 0.015986114740371704, tv_loss: 0.017225803807377815\n",
      "iteration 1077, dc_loss: 0.015985701233148575, tv_loss: 0.017225157469511032\n",
      "iteration 1078, dc_loss: 0.01598528027534485, tv_loss: 0.017224757000803947\n",
      "iteration 1079, dc_loss: 0.015984831377863884, tv_loss: 0.017224183306097984\n",
      "iteration 1080, dc_loss: 0.01598437689244747, tv_loss: 0.01722388155758381\n",
      "iteration 1081, dc_loss: 0.015983927994966507, tv_loss: 0.01722344383597374\n",
      "iteration 1082, dc_loss: 0.01598348096013069, tv_loss: 0.01722295768558979\n",
      "iteration 1083, dc_loss: 0.01598302088677883, tv_loss: 0.017222600057721138\n",
      "iteration 1084, dc_loss: 0.01598253659904003, tv_loss: 0.01722225919365883\n",
      "iteration 1085, dc_loss: 0.015982065349817276, tv_loss: 0.017221812158823013\n",
      "iteration 1086, dc_loss: 0.015981633216142654, tv_loss: 0.01722150854766369\n",
      "iteration 1087, dc_loss: 0.015981197357177734, tv_loss: 0.017221001908183098\n",
      "iteration 1088, dc_loss: 0.015980755910277367, tv_loss: 0.017220627516508102\n",
      "iteration 1089, dc_loss: 0.015980305150151253, tv_loss: 0.01722029782831669\n",
      "iteration 1090, dc_loss: 0.015979869291186333, tv_loss: 0.017219940200448036\n",
      "iteration 1091, dc_loss: 0.015979424118995667, tv_loss: 0.01721947453916073\n",
      "iteration 1092, dc_loss: 0.015978984534740448, tv_loss: 0.017219077795743942\n",
      "iteration 1093, dc_loss: 0.015978550538420677, tv_loss: 0.017218735069036484\n",
      "iteration 1094, dc_loss: 0.01597810536623001, tv_loss: 0.017218343913555145\n",
      "iteration 1095, dc_loss: 0.01597769930958748, tv_loss: 0.017217911779880524\n",
      "iteration 1096, dc_loss: 0.015977324917912483, tv_loss: 0.017217490822076797\n",
      "iteration 1097, dc_loss: 0.015976961702108383, tv_loss: 0.017217084765434265\n",
      "iteration 1098, dc_loss: 0.01597658172249794, tv_loss: 0.017216525971889496\n",
      "iteration 1099, dc_loss: 0.015976184979081154, tv_loss: 0.01721593551337719\n",
      "iteration 1100, dc_loss: 0.015975739806890488, tv_loss: 0.0172156672924757\n",
      "iteration 1101, dc_loss: 0.015975244343280792, tv_loss: 0.01721530221402645\n",
      "iteration 1102, dc_loss: 0.015974774956703186, tv_loss: 0.01721489615738392\n",
      "iteration 1103, dc_loss: 0.015974298119544983, tv_loss: 0.017214428633451462\n",
      "iteration 1104, dc_loss: 0.01597381941974163, tv_loss: 0.01721419207751751\n",
      "iteration 1105, dc_loss: 0.015973389148712158, tv_loss: 0.017213817685842514\n",
      "iteration 1106, dc_loss: 0.015972958877682686, tv_loss: 0.01721316948533058\n",
      "iteration 1107, dc_loss: 0.015972556546330452, tv_loss: 0.017212826758623123\n",
      "iteration 1108, dc_loss: 0.015972189605236053, tv_loss: 0.01721241883933544\n",
      "iteration 1109, dc_loss: 0.01597180776298046, tv_loss: 0.017211928963661194\n",
      "iteration 1110, dc_loss: 0.01597142405807972, tv_loss: 0.01721145212650299\n",
      "iteration 1111, dc_loss: 0.015971023589372635, tv_loss: 0.01721092127263546\n",
      "iteration 1112, dc_loss: 0.015970610082149506, tv_loss: 0.017210567370057106\n",
      "iteration 1113, dc_loss: 0.015970205888152122, tv_loss: 0.01721027120947838\n",
      "iteration 1114, dc_loss: 0.015969792380928993, tv_loss: 0.01721005141735077\n",
      "iteration 1115, dc_loss: 0.01596941612660885, tv_loss: 0.017209460958838463\n",
      "iteration 1116, dc_loss: 0.01596902310848236, tv_loss: 0.017209051176905632\n",
      "iteration 1117, dc_loss: 0.015968605875968933, tv_loss: 0.0172086413949728\n",
      "iteration 1118, dc_loss: 0.015968170017004013, tv_loss: 0.017208309844136238\n",
      "iteration 1119, dc_loss: 0.015967752784490585, tv_loss: 0.017207903787493706\n",
      "iteration 1120, dc_loss: 0.015967346727848053, tv_loss: 0.017207708209753036\n",
      "iteration 1121, dc_loss: 0.015966948121786118, tv_loss: 0.017207205295562744\n",
      "iteration 1122, dc_loss: 0.01596653088927269, tv_loss: 0.01720675267279148\n",
      "iteration 1123, dc_loss: 0.01596609316766262, tv_loss: 0.017206335440278053\n",
      "iteration 1124, dc_loss: 0.015965670347213745, tv_loss: 0.01720607280731201\n",
      "iteration 1125, dc_loss: 0.01596527174115181, tv_loss: 0.017205795273184776\n",
      "iteration 1126, dc_loss: 0.015964912250638008, tv_loss: 0.017205415293574333\n",
      "iteration 1127, dc_loss: 0.015964573249220848, tv_loss: 0.017204655334353447\n",
      "iteration 1128, dc_loss: 0.0159642081707716, tv_loss: 0.017204435542225838\n",
      "iteration 1129, dc_loss: 0.01596381701529026, tv_loss: 0.01720377616584301\n",
      "iteration 1130, dc_loss: 0.01596343331038952, tv_loss: 0.017203420400619507\n",
      "iteration 1131, dc_loss: 0.015963038429617882, tv_loss: 0.017203237861394882\n",
      "iteration 1132, dc_loss: 0.015962660312652588, tv_loss: 0.017202721908688545\n",
      "iteration 1133, dc_loss: 0.0159622635692358, tv_loss: 0.017202449962496758\n",
      "iteration 1134, dc_loss: 0.015961874276399612, tv_loss: 0.017202144488692284\n",
      "iteration 1135, dc_loss: 0.01596151851117611, tv_loss: 0.017201757058501244\n",
      "iteration 1136, dc_loss: 0.01596113294363022, tv_loss: 0.017201272770762444\n",
      "iteration 1137, dc_loss: 0.015960698947310448, tv_loss: 0.017201023176312447\n",
      "iteration 1138, dc_loss: 0.015960276126861572, tv_loss: 0.017200717702507973\n",
      "iteration 1139, dc_loss: 0.01595986634492874, tv_loss: 0.017200298607349396\n",
      "iteration 1140, dc_loss: 0.015959477052092552, tv_loss: 0.01719990372657776\n",
      "iteration 1141, dc_loss: 0.0159591156989336, tv_loss: 0.0171994399279356\n",
      "iteration 1142, dc_loss: 0.015958739444613457, tv_loss: 0.017198992893099785\n",
      "iteration 1143, dc_loss: 0.015958381816744804, tv_loss: 0.017198842018842697\n",
      "iteration 1144, dc_loss: 0.01595798321068287, tv_loss: 0.01719834841787815\n",
      "iteration 1145, dc_loss: 0.01595759391784668, tv_loss: 0.017197951674461365\n",
      "iteration 1146, dc_loss: 0.015957197174429893, tv_loss: 0.017197761684656143\n",
      "iteration 1147, dc_loss: 0.015956809744238853, tv_loss: 0.01719753071665764\n",
      "iteration 1148, dc_loss: 0.01595643162727356, tv_loss: 0.017196958884596825\n",
      "iteration 1149, dc_loss: 0.01595606841146946, tv_loss: 0.017196428030729294\n",
      "iteration 1150, dc_loss: 0.015955716371536255, tv_loss: 0.017196139320731163\n",
      "iteration 1151, dc_loss: 0.015955381095409393, tv_loss: 0.017195897176861763\n",
      "iteration 1152, dc_loss: 0.015955058857798576, tv_loss: 0.017195506021380424\n",
      "iteration 1153, dc_loss: 0.015954704955220222, tv_loss: 0.0171949565410614\n",
      "iteration 1154, dc_loss: 0.01595434546470642, tv_loss: 0.01719455048441887\n",
      "iteration 1155, dc_loss: 0.015953967347741127, tv_loss: 0.017194287851452827\n",
      "iteration 1156, dc_loss: 0.015953607857227325, tv_loss: 0.0171938668936491\n",
      "iteration 1157, dc_loss: 0.015953265130519867, tv_loss: 0.01719321496784687\n",
      "iteration 1158, dc_loss: 0.015952864661812782, tv_loss: 0.01719311997294426\n",
      "iteration 1159, dc_loss: 0.015952426940202713, tv_loss: 0.017192982137203217\n",
      "iteration 1160, dc_loss: 0.015952013432979584, tv_loss: 0.017192533239722252\n",
      "iteration 1161, dc_loss: 0.01595163904130459, tv_loss: 0.01719198003411293\n",
      "iteration 1162, dc_loss: 0.015951303765177727, tv_loss: 0.017191609367728233\n",
      "iteration 1163, dc_loss: 0.015950974076986313, tv_loss: 0.01719135418534279\n",
      "iteration 1164, dc_loss: 0.015950677916407585, tv_loss: 0.017191026359796524\n",
      "iteration 1165, dc_loss: 0.01595035195350647, tv_loss: 0.01719050109386444\n",
      "iteration 1166, dc_loss: 0.015950007364153862, tv_loss: 0.017190169543027878\n",
      "iteration 1167, dc_loss: 0.015949618071317673, tv_loss: 0.017189737409353256\n",
      "iteration 1168, dc_loss: 0.015949197113513947, tv_loss: 0.01718941144645214\n",
      "iteration 1169, dc_loss: 0.01594877801835537, tv_loss: 0.01718936115503311\n",
      "iteration 1170, dc_loss: 0.015948405489325523, tv_loss: 0.017189031466841698\n",
      "iteration 1171, dc_loss: 0.015948062762618065, tv_loss: 0.01718846708536148\n",
      "iteration 1172, dc_loss: 0.015947725623846054, tv_loss: 0.01718812994658947\n",
      "iteration 1173, dc_loss: 0.01594739593565464, tv_loss: 0.017187830060720444\n",
      "iteration 1174, dc_loss: 0.01594707742333412, tv_loss: 0.01718749664723873\n",
      "iteration 1175, dc_loss: 0.015946757048368454, tv_loss: 0.017187094315886497\n",
      "iteration 1176, dc_loss: 0.015946414321660995, tv_loss: 0.01718669943511486\n",
      "iteration 1177, dc_loss: 0.015946079045534134, tv_loss: 0.01718638464808464\n",
      "iteration 1178, dc_loss: 0.015945732593536377, tv_loss: 0.017186053097248077\n",
      "iteration 1179, dc_loss: 0.015945391729474068, tv_loss: 0.01718577928841114\n",
      "iteration 1180, dc_loss: 0.015945056453347206, tv_loss: 0.01718534156680107\n",
      "iteration 1181, dc_loss: 0.01594468392431736, tv_loss: 0.017185069620609283\n",
      "iteration 1182, dc_loss: 0.015944279730319977, tv_loss: 0.017184551805257797\n",
      "iteration 1183, dc_loss: 0.015943938866257668, tv_loss: 0.01718420907855034\n",
      "iteration 1184, dc_loss: 0.015943611040711403, tv_loss: 0.017184076830744743\n",
      "iteration 1185, dc_loss: 0.01594330184161663, tv_loss: 0.01718343421816826\n",
      "iteration 1186, dc_loss: 0.015943005681037903, tv_loss: 0.017183000221848488\n",
      "iteration 1187, dc_loss: 0.015942687168717384, tv_loss: 0.017182668671011925\n",
      "iteration 1188, dc_loss: 0.01594235561788082, tv_loss: 0.01718239113688469\n",
      "iteration 1189, dc_loss: 0.015941988676786423, tv_loss: 0.017182020470499992\n",
      "iteration 1190, dc_loss: 0.015941621735692024, tv_loss: 0.01718197762966156\n",
      "iteration 1191, dc_loss: 0.015941280871629715, tv_loss: 0.017181560397148132\n",
      "iteration 1192, dc_loss: 0.015940994024276733, tv_loss: 0.017181066796183586\n",
      "iteration 1193, dc_loss: 0.015940705314278603, tv_loss: 0.017180757597088814\n",
      "iteration 1194, dc_loss: 0.015940360724925995, tv_loss: 0.017180413007736206\n",
      "iteration 1195, dc_loss: 0.01593998447060585, tv_loss: 0.01718011498451233\n",
      "iteration 1196, dc_loss: 0.0159396193921566, tv_loss: 0.01717975176870823\n",
      "iteration 1197, dc_loss: 0.015939289703965187, tv_loss: 0.01717943698167801\n",
      "iteration 1198, dc_loss: 0.01593896932899952, tv_loss: 0.017179157584905624\n",
      "iteration 1199, dc_loss: 0.01593867875635624, tv_loss: 0.017178760841488838\n",
      "iteration 1200, dc_loss: 0.01593838632106781, tv_loss: 0.017178470268845558\n",
      "iteration 1201, dc_loss: 0.015938056632876396, tv_loss: 0.017178084701299667\n",
      "iteration 1202, dc_loss: 0.01593771018087864, tv_loss: 0.01717754453420639\n",
      "iteration 1203, dc_loss: 0.015937360003590584, tv_loss: 0.017177406698465347\n",
      "iteration 1204, dc_loss: 0.01593702845275402, tv_loss: 0.017177386209368706\n",
      "iteration 1205, dc_loss: 0.015936709940433502, tv_loss: 0.017176993191242218\n",
      "iteration 1206, dc_loss: 0.01593639887869358, tv_loss: 0.017176512628793716\n",
      "iteration 1207, dc_loss: 0.015936072915792465, tv_loss: 0.017176160588860512\n",
      "iteration 1208, dc_loss: 0.01593569852411747, tv_loss: 0.017176005989313126\n",
      "iteration 1209, dc_loss: 0.015935329720377922, tv_loss: 0.01717563346028328\n",
      "iteration 1210, dc_loss: 0.015934983268380165, tv_loss: 0.01717517338693142\n",
      "iteration 1211, dc_loss: 0.015934668481349945, tv_loss: 0.017174849286675453\n",
      "iteration 1212, dc_loss: 0.015934355556964874, tv_loss: 0.017174798995256424\n",
      "iteration 1213, dc_loss: 0.015934044495224953, tv_loss: 0.01717434823513031\n",
      "iteration 1214, dc_loss: 0.015933765098452568, tv_loss: 0.017173856496810913\n",
      "iteration 1215, dc_loss: 0.015933489426970482, tv_loss: 0.01717361807823181\n",
      "iteration 1216, dc_loss: 0.01593318209052086, tv_loss: 0.017173225060105324\n",
      "iteration 1217, dc_loss: 0.015932882204651833, tv_loss: 0.01717287115752697\n",
      "iteration 1218, dc_loss: 0.015932615846395493, tv_loss: 0.01717253588140011\n",
      "iteration 1219, dc_loss: 0.015932338312268257, tv_loss: 0.017172057181596756\n",
      "iteration 1220, dc_loss: 0.015932029113173485, tv_loss: 0.017171798273921013\n",
      "iteration 1221, dc_loss: 0.015931706875562668, tv_loss: 0.017171628773212433\n",
      "iteration 1222, dc_loss: 0.015931373462080956, tv_loss: 0.01717129722237587\n",
      "iteration 1223, dc_loss: 0.015931015834212303, tv_loss: 0.017170937731862068\n",
      "iteration 1224, dc_loss: 0.015930665656924248, tv_loss: 0.017170697450637817\n",
      "iteration 1225, dc_loss: 0.015930337831377983, tv_loss: 0.017170334234833717\n",
      "iteration 1226, dc_loss: 0.01593003422021866, tv_loss: 0.01717013493180275\n",
      "iteration 1227, dc_loss: 0.01592978648841381, tv_loss: 0.017169730737805367\n",
      "iteration 1228, dc_loss: 0.01592954434454441, tv_loss: 0.01716918870806694\n",
      "iteration 1229, dc_loss: 0.015929263085126877, tv_loss: 0.01716899871826172\n",
      "iteration 1230, dc_loss: 0.01592893712222576, tv_loss: 0.017168723046779633\n",
      "iteration 1231, dc_loss: 0.015928586944937706, tv_loss: 0.017168601974844933\n",
      "iteration 1232, dc_loss: 0.015928251668810844, tv_loss: 0.017168253660202026\n",
      "iteration 1233, dc_loss: 0.015927935019135475, tv_loss: 0.01716790534555912\n",
      "iteration 1234, dc_loss: 0.015927644446492195, tv_loss: 0.017167609184980392\n",
      "iteration 1235, dc_loss: 0.015927355736494064, tv_loss: 0.017167283222079277\n",
      "iteration 1236, dc_loss: 0.015927070751786232, tv_loss: 0.01716691255569458\n",
      "iteration 1237, dc_loss: 0.015926772728562355, tv_loss: 0.01716662384569645\n",
      "iteration 1238, dc_loss: 0.015926485881209373, tv_loss: 0.017166495323181152\n",
      "iteration 1239, dc_loss: 0.0159261804074049, tv_loss: 0.01716604270040989\n",
      "iteration 1240, dc_loss: 0.015925873070955276, tv_loss: 0.017165813595056534\n",
      "iteration 1241, dc_loss: 0.0159255713224411, tv_loss: 0.0171655286103487\n",
      "iteration 1242, dc_loss: 0.015925247222185135, tv_loss: 0.01716521754860878\n",
      "iteration 1243, dc_loss: 0.015924951061606407, tv_loss: 0.01716507226228714\n",
      "iteration 1244, dc_loss: 0.01592465490102768, tv_loss: 0.017164867371320724\n",
      "iteration 1245, dc_loss: 0.015924371778964996, tv_loss: 0.017164453864097595\n",
      "iteration 1246, dc_loss: 0.015924111008644104, tv_loss: 0.017163850367069244\n",
      "iteration 1247, dc_loss: 0.01592383347451687, tv_loss: 0.01716354489326477\n",
      "iteration 1248, dc_loss: 0.015923531726002693, tv_loss: 0.017163407057523727\n",
      "iteration 1249, dc_loss: 0.015923231840133667, tv_loss: 0.017163095995783806\n",
      "iteration 1250, dc_loss: 0.015922928228974342, tv_loss: 0.017162851989269257\n",
      "iteration 1251, dc_loss: 0.015922648832201958, tv_loss: 0.01716269925236702\n",
      "iteration 1252, dc_loss: 0.01592237502336502, tv_loss: 0.017162246629595757\n",
      "iteration 1253, dc_loss: 0.015922067686915398, tv_loss: 0.01716190204024315\n",
      "iteration 1254, dc_loss: 0.01592174917459488, tv_loss: 0.0171616580337286\n",
      "iteration 1255, dc_loss: 0.015921447426080704, tv_loss: 0.01716148667037487\n",
      "iteration 1256, dc_loss: 0.01592112146317959, tv_loss: 0.017161061987280846\n",
      "iteration 1257, dc_loss: 0.015920797362923622, tv_loss: 0.017160657793283463\n",
      "iteration 1258, dc_loss: 0.015920506790280342, tv_loss: 0.01716051809489727\n",
      "iteration 1259, dc_loss: 0.015920259058475494, tv_loss: 0.017160337418317795\n",
      "iteration 1260, dc_loss: 0.015920018777251244, tv_loss: 0.017159966751933098\n",
      "iteration 1261, dc_loss: 0.015919772908091545, tv_loss: 0.017159361392259598\n",
      "iteration 1262, dc_loss: 0.015919499099254608, tv_loss: 0.017159078270196915\n",
      "iteration 1263, dc_loss: 0.015919234603643417, tv_loss: 0.017158880829811096\n",
      "iteration 1264, dc_loss: 0.01591896452009678, tv_loss: 0.01715852878987789\n",
      "iteration 1265, dc_loss: 0.015918681398034096, tv_loss: 0.017158204689621925\n",
      "iteration 1266, dc_loss: 0.015918400138616562, tv_loss: 0.01715790294110775\n",
      "iteration 1267, dc_loss: 0.015918118879199028, tv_loss: 0.01715766079723835\n",
      "iteration 1268, dc_loss: 0.015917818993330002, tv_loss: 0.017157400026917458\n",
      "iteration 1269, dc_loss: 0.015917522832751274, tv_loss: 0.017157090827822685\n",
      "iteration 1270, dc_loss: 0.01591721549630165, tv_loss: 0.01715676113963127\n",
      "iteration 1271, dc_loss: 0.015916932374238968, tv_loss: 0.017156442627310753\n",
      "iteration 1272, dc_loss: 0.015916652977466583, tv_loss: 0.01715630106627941\n",
      "iteration 1273, dc_loss: 0.0159163661301136, tv_loss: 0.017155921086668968\n",
      "iteration 1274, dc_loss: 0.015916094183921814, tv_loss: 0.01715557835996151\n",
      "iteration 1275, dc_loss: 0.015915844589471817, tv_loss: 0.017155563458800316\n",
      "iteration 1276, dc_loss: 0.015915580093860626, tv_loss: 0.01715533621609211\n",
      "iteration 1277, dc_loss: 0.01591530255973339, tv_loss: 0.017154846340417862\n",
      "iteration 1278, dc_loss: 0.015915049239993095, tv_loss: 0.01715458184480667\n",
      "iteration 1279, dc_loss: 0.015914805233478546, tv_loss: 0.017154451459646225\n",
      "iteration 1280, dc_loss: 0.015914540737867355, tv_loss: 0.017154237255454063\n",
      "iteration 1281, dc_loss: 0.015914224088191986, tv_loss: 0.017154032364487648\n",
      "iteration 1282, dc_loss: 0.015913933515548706, tv_loss: 0.01715371198952198\n",
      "iteration 1283, dc_loss: 0.015913665294647217, tv_loss: 0.017153402790427208\n",
      "iteration 1284, dc_loss: 0.015913410112261772, tv_loss: 0.017153240740299225\n",
      "iteration 1285, dc_loss: 0.015913154929876328, tv_loss: 0.017152829095721245\n",
      "iteration 1286, dc_loss: 0.015912892296910286, tv_loss: 0.017152493819594383\n",
      "iteration 1287, dc_loss: 0.0159126166254282, tv_loss: 0.017152296379208565\n",
      "iteration 1288, dc_loss: 0.01591232791543007, tv_loss: 0.01715211570262909\n",
      "iteration 1289, dc_loss: 0.01591203548014164, tv_loss: 0.0171516053378582\n",
      "iteration 1290, dc_loss: 0.015911776572465897, tv_loss: 0.017151352018117905\n",
      "iteration 1291, dc_loss: 0.015911556780338287, tv_loss: 0.017151325941085815\n",
      "iteration 1292, dc_loss: 0.015911327674984932, tv_loss: 0.017150921747088432\n",
      "iteration 1293, dc_loss: 0.015911061316728592, tv_loss: 0.017150606960058212\n",
      "iteration 1294, dc_loss: 0.015910765156149864, tv_loss: 0.017150303348898888\n",
      "iteration 1295, dc_loss: 0.015910470858216286, tv_loss: 0.01715012639760971\n",
      "iteration 1296, dc_loss: 0.0159101914614439, tv_loss: 0.017149830237030983\n",
      "iteration 1297, dc_loss: 0.015909958630800247, tv_loss: 0.017149480059742928\n",
      "iteration 1298, dc_loss: 0.015909695997834206, tv_loss: 0.01714925281703472\n",
      "iteration 1299, dc_loss: 0.01590944267809391, tv_loss: 0.017149141058325768\n",
      "iteration 1300, dc_loss: 0.015909170731902122, tv_loss: 0.017148880288004875\n",
      "iteration 1301, dc_loss: 0.01590890623629093, tv_loss: 0.01714855618774891\n",
      "iteration 1302, dc_loss: 0.01590864360332489, tv_loss: 0.017148280516266823\n",
      "iteration 1303, dc_loss: 0.015908367931842804, tv_loss: 0.017147978767752647\n",
      "iteration 1304, dc_loss: 0.01590808667242527, tv_loss: 0.017147770151495934\n",
      "iteration 1305, dc_loss: 0.015907807275652885, tv_loss: 0.017147710546851158\n",
      "iteration 1306, dc_loss: 0.015907544642686844, tv_loss: 0.01714739017188549\n",
      "iteration 1307, dc_loss: 0.01590728759765625, tv_loss: 0.01714692823588848\n",
      "iteration 1308, dc_loss: 0.015907039865851402, tv_loss: 0.017146794125437737\n",
      "iteration 1309, dc_loss: 0.015906769782304764, tv_loss: 0.01714649610221386\n",
      "iteration 1310, dc_loss: 0.01590648852288723, tv_loss: 0.017146166414022446\n",
      "iteration 1311, dc_loss: 0.015906233340501785, tv_loss: 0.017145896330475807\n",
      "iteration 1312, dc_loss: 0.01590600050985813, tv_loss: 0.01714557409286499\n",
      "iteration 1313, dc_loss: 0.01590581238269806, tv_loss: 0.017145385965704918\n",
      "iteration 1314, dc_loss: 0.015905629843473434, tv_loss: 0.017145099118351936\n",
      "iteration 1315, dc_loss: 0.015905380249023438, tv_loss: 0.017144618555903435\n",
      "iteration 1316, dc_loss: 0.01590510830283165, tv_loss: 0.017144398763775826\n",
      "iteration 1317, dc_loss: 0.015904800966382027, tv_loss: 0.017144419252872467\n",
      "iteration 1318, dc_loss: 0.015904489904642105, tv_loss: 0.017144085839390755\n",
      "iteration 1319, dc_loss: 0.015904201194643974, tv_loss: 0.01714378409087658\n",
      "iteration 1320, dc_loss: 0.015903931111097336, tv_loss: 0.0171437319368124\n",
      "iteration 1321, dc_loss: 0.01590365543961525, tv_loss: 0.01714354194700718\n",
      "iteration 1322, dc_loss: 0.015903422608971596, tv_loss: 0.0171431303024292\n",
      "iteration 1323, dc_loss: 0.01590319536626339, tv_loss: 0.01714273728430271\n",
      "iteration 1324, dc_loss: 0.015902983024716377, tv_loss: 0.01714271306991577\n",
      "iteration 1325, dc_loss: 0.015902746468782425, tv_loss: 0.01714247465133667\n",
      "iteration 1326, dc_loss: 0.015902521088719368, tv_loss: 0.017142053693532944\n",
      "iteration 1327, dc_loss: 0.01590225100517273, tv_loss: 0.017141688615083694\n",
      "iteration 1328, dc_loss: 0.015901992097496986, tv_loss: 0.017141681164503098\n",
      "iteration 1329, dc_loss: 0.01590171828866005, tv_loss: 0.017141379415988922\n",
      "iteration 1330, dc_loss: 0.015901437029242516, tv_loss: 0.017141252756118774\n",
      "iteration 1331, dc_loss: 0.015901178121566772, tv_loss: 0.017140977084636688\n",
      "iteration 1332, dc_loss: 0.015900954604148865, tv_loss: 0.01714063249528408\n",
      "iteration 1333, dc_loss: 0.015900734812021255, tv_loss: 0.017140304669737816\n",
      "iteration 1334, dc_loss: 0.015900535508990288, tv_loss: 0.0171399787068367\n",
      "iteration 1335, dc_loss: 0.015900354832410812, tv_loss: 0.017139872536063194\n",
      "iteration 1336, dc_loss: 0.015900149941444397, tv_loss: 0.017139462754130363\n",
      "iteration 1337, dc_loss: 0.015899915248155594, tv_loss: 0.017139147967100143\n",
      "iteration 1338, dc_loss: 0.015899620950222015, tv_loss: 0.01713906228542328\n",
      "iteration 1339, dc_loss: 0.015899324789643288, tv_loss: 0.017138928174972534\n",
      "iteration 1340, dc_loss: 0.015899023041129112, tv_loss: 0.017138686031103134\n",
      "iteration 1341, dc_loss: 0.015898743644356728, tv_loss: 0.017138531431555748\n",
      "iteration 1342, dc_loss: 0.01589851640164852, tv_loss: 0.017138134688138962\n",
      "iteration 1343, dc_loss: 0.0158983301371336, tv_loss: 0.017137866467237473\n",
      "iteration 1344, dc_loss: 0.01589813455939293, tv_loss: 0.017137739807367325\n",
      "iteration 1345, dc_loss: 0.015897924080491066, tv_loss: 0.017137465998530388\n",
      "iteration 1346, dc_loss: 0.01589769311249256, tv_loss: 0.01713724248111248\n",
      "iteration 1347, dc_loss: 0.015897443518042564, tv_loss: 0.017137011513113976\n",
      "iteration 1348, dc_loss: 0.015897179022431374, tv_loss: 0.017136799171566963\n",
      "iteration 1349, dc_loss: 0.01589692011475563, tv_loss: 0.017136581242084503\n",
      "iteration 1350, dc_loss: 0.015896689146757126, tv_loss: 0.017136456444859505\n",
      "iteration 1351, dc_loss: 0.01589646190404892, tv_loss: 0.017136070877313614\n",
      "iteration 1352, dc_loss: 0.01589622162282467, tv_loss: 0.017135772854089737\n",
      "iteration 1353, dc_loss: 0.015895966440439224, tv_loss: 0.017135614529252052\n",
      "iteration 1354, dc_loss: 0.015895714983344078, tv_loss: 0.017135458067059517\n",
      "iteration 1355, dc_loss: 0.015895502641797066, tv_loss: 0.017135266214609146\n",
      "iteration 1356, dc_loss: 0.015895308926701546, tv_loss: 0.01713492162525654\n",
      "iteration 1357, dc_loss: 0.015895098447799683, tv_loss: 0.017134595662355423\n",
      "iteration 1358, dc_loss: 0.015894856303930283, tv_loss: 0.017134306952357292\n",
      "iteration 1359, dc_loss: 0.015894634649157524, tv_loss: 0.017134178429841995\n",
      "iteration 1360, dc_loss: 0.015894409269094467, tv_loss: 0.01713399775326252\n",
      "iteration 1361, dc_loss: 0.015894174575805664, tv_loss: 0.017133750021457672\n",
      "iteration 1362, dc_loss: 0.015893952921032906, tv_loss: 0.017133593559265137\n",
      "iteration 1363, dc_loss: 0.01589369960129261, tv_loss: 0.017133254557847977\n",
      "iteration 1364, dc_loss: 0.015893470495939255, tv_loss: 0.0171329565346241\n",
      "iteration 1365, dc_loss: 0.015893246978521347, tv_loss: 0.017132902517914772\n",
      "iteration 1366, dc_loss: 0.015893027186393738, tv_loss: 0.01713266782462597\n",
      "iteration 1367, dc_loss: 0.01589280180633068, tv_loss: 0.017132291570305824\n",
      "iteration 1368, dc_loss: 0.015892567113041878, tv_loss: 0.01713218167424202\n",
      "iteration 1369, dc_loss: 0.015892326831817627, tv_loss: 0.01713200844824314\n",
      "iteration 1370, dc_loss: 0.01589207723736763, tv_loss: 0.01713174767792225\n",
      "iteration 1371, dc_loss: 0.015891840681433678, tv_loss: 0.017131485044956207\n",
      "iteration 1372, dc_loss: 0.015891598537564278, tv_loss: 0.017131313681602478\n",
      "iteration 1373, dc_loss: 0.015891363844275475, tv_loss: 0.017131293192505836\n",
      "iteration 1374, dc_loss: 0.015891142189502716, tv_loss: 0.017130807042121887\n",
      "iteration 1375, dc_loss: 0.015890954062342644, tv_loss: 0.01713041216135025\n",
      "iteration 1376, dc_loss: 0.015890778973698616, tv_loss: 0.017130499705672264\n",
      "iteration 1377, dc_loss: 0.015890566632151604, tv_loss: 0.017130322754383087\n",
      "iteration 1378, dc_loss: 0.01589031331241131, tv_loss: 0.01712987571954727\n",
      "iteration 1379, dc_loss: 0.015890058130025864, tv_loss: 0.017129607498645782\n",
      "iteration 1380, dc_loss: 0.01588980294764042, tv_loss: 0.017129385843873024\n",
      "iteration 1381, dc_loss: 0.01588958501815796, tv_loss: 0.01712939701974392\n",
      "iteration 1382, dc_loss: 0.015889383852481842, tv_loss: 0.017129216343164444\n",
      "iteration 1383, dc_loss: 0.015889178961515427, tv_loss: 0.01712886430323124\n",
      "iteration 1384, dc_loss: 0.01588895544409752, tv_loss: 0.017128752544522285\n",
      "iteration 1385, dc_loss: 0.015888728201389313, tv_loss: 0.017128368839621544\n",
      "iteration 1386, dc_loss: 0.01588853821158409, tv_loss: 0.01712822914123535\n",
      "iteration 1387, dc_loss: 0.015888331457972527, tv_loss: 0.017128048464655876\n",
      "iteration 1388, dc_loss: 0.01588805951178074, tv_loss: 0.01712782494723797\n",
      "iteration 1389, dc_loss: 0.015887804329395294, tv_loss: 0.01712743006646633\n",
      "iteration 1390, dc_loss: 0.015887556597590446, tv_loss: 0.017127299681305885\n",
      "iteration 1391, dc_loss: 0.015887310728430748, tv_loss: 0.01712728850543499\n",
      "iteration 1392, dc_loss: 0.015887092798948288, tv_loss: 0.01712697371840477\n",
      "iteration 1393, dc_loss: 0.015886861830949783, tv_loss: 0.017126740887761116\n",
      "iteration 1394, dc_loss: 0.015886660665273666, tv_loss: 0.01712649315595627\n",
      "iteration 1395, dc_loss: 0.015886444598436356, tv_loss: 0.017126448452472687\n",
      "iteration 1396, dc_loss: 0.01588623784482479, tv_loss: 0.017126042395830154\n",
      "iteration 1397, dc_loss: 0.0158860944211483, tv_loss: 0.01712583564221859\n",
      "iteration 1398, dc_loss: 0.01588594727218151, tv_loss: 0.017125628888607025\n",
      "iteration 1399, dc_loss: 0.015885761007666588, tv_loss: 0.017125381156802177\n",
      "iteration 1400, dc_loss: 0.01588558964431286, tv_loss: 0.017125090584158897\n",
      "iteration 1401, dc_loss: 0.01588539406657219, tv_loss: 0.017124980688095093\n",
      "iteration 1402, dc_loss: 0.015885192900896072, tv_loss: 0.01712479256093502\n",
      "iteration 1403, dc_loss: 0.015884945169091225, tv_loss: 0.017124615609645844\n",
      "iteration 1404, dc_loss: 0.01588466204702854, tv_loss: 0.017124416306614876\n",
      "iteration 1405, dc_loss: 0.01588435471057892, tv_loss: 0.017124168574810028\n",
      "iteration 1406, dc_loss: 0.01588411256670952, tv_loss: 0.017124077305197716\n",
      "iteration 1407, dc_loss: 0.015883896499872208, tv_loss: 0.017123965546488762\n",
      "iteration 1408, dc_loss: 0.015883713960647583, tv_loss: 0.01712377555668354\n",
      "iteration 1409, dc_loss: 0.015883538872003555, tv_loss: 0.017123540863394737\n",
      "iteration 1410, dc_loss: 0.015883352607488632, tv_loss: 0.017123153433203697\n",
      "iteration 1411, dc_loss: 0.015883134678006172, tv_loss: 0.017123086377978325\n",
      "iteration 1412, dc_loss: 0.015882935374975204, tv_loss: 0.017122860997915268\n",
      "iteration 1413, dc_loss: 0.015882736071944237, tv_loss: 0.017122505232691765\n",
      "iteration 1414, dc_loss: 0.015882553532719612, tv_loss: 0.017122235149145126\n",
      "iteration 1415, dc_loss: 0.01588236354291439, tv_loss: 0.017122119665145874\n",
      "iteration 1416, dc_loss: 0.01588217355310917, tv_loss: 0.017122050747275352\n",
      "iteration 1417, dc_loss: 0.0158819779753685, tv_loss: 0.01712159439921379\n",
      "iteration 1418, dc_loss: 0.015881802886724472, tv_loss: 0.017121341079473495\n",
      "iteration 1419, dc_loss: 0.015881618484854698, tv_loss: 0.017121119424700737\n",
      "iteration 1420, dc_loss: 0.015881411731243134, tv_loss: 0.01712111011147499\n",
      "iteration 1421, dc_loss: 0.015881163999438286, tv_loss: 0.01712086796760559\n",
      "iteration 1422, dc_loss: 0.015880923718214035, tv_loss: 0.017120717093348503\n",
      "iteration 1423, dc_loss: 0.015880698338150978, tv_loss: 0.017120392993092537\n",
      "iteration 1424, dc_loss: 0.015880513936281204, tv_loss: 0.01712038181722164\n",
      "iteration 1425, dc_loss: 0.015880312770605087, tv_loss: 0.01712014712393284\n",
      "iteration 1426, dc_loss: 0.015880117192864418, tv_loss: 0.017119955271482468\n",
      "iteration 1427, dc_loss: 0.015879923477768898, tv_loss: 0.017119549214839935\n",
      "iteration 1428, dc_loss: 0.015879733487963676, tv_loss: 0.01711936853826046\n",
      "iteration 1429, dc_loss: 0.015879536047577858, tv_loss: 0.017119185999035835\n",
      "iteration 1430, dc_loss: 0.015879333019256592, tv_loss: 0.01711905375123024\n",
      "iteration 1431, dc_loss: 0.015879148617386818, tv_loss: 0.01711885817348957\n",
      "iteration 1432, dc_loss: 0.01587895303964615, tv_loss: 0.01711856760084629\n",
      "iteration 1433, dc_loss: 0.015878789126873016, tv_loss: 0.017118332907557487\n",
      "iteration 1434, dc_loss: 0.015878641977906227, tv_loss: 0.01711803674697876\n",
      "iteration 1435, dc_loss: 0.015878481790423393, tv_loss: 0.017117805778980255\n",
      "iteration 1436, dc_loss: 0.01587827503681183, tv_loss: 0.0171175729483366\n",
      "iteration 1437, dc_loss: 0.015878036618232727, tv_loss: 0.017117615789175034\n",
      "iteration 1438, dc_loss: 0.01587778888642788, tv_loss: 0.017117418348789215\n",
      "iteration 1439, dc_loss: 0.015877539291977882, tv_loss: 0.017117125913500786\n",
      "iteration 1440, dc_loss: 0.015877321362495422, tv_loss: 0.01711713708937168\n",
      "iteration 1441, dc_loss: 0.01587711274623871, tv_loss: 0.017116792500019073\n",
      "iteration 1442, dc_loss: 0.015876933932304382, tv_loss: 0.01711685210466385\n",
      "iteration 1443, dc_loss: 0.015876753255724907, tv_loss: 0.017116613686084747\n",
      "iteration 1444, dc_loss: 0.015876565128564835, tv_loss: 0.017116190865635872\n",
      "iteration 1445, dc_loss: 0.015876369550824165, tv_loss: 0.017116134986281395\n",
      "iteration 1446, dc_loss: 0.01587618514895439, tv_loss: 0.01711612194776535\n",
      "iteration 1447, dc_loss: 0.015876026824116707, tv_loss: 0.017115719616413116\n",
      "iteration 1448, dc_loss: 0.015875866636633873, tv_loss: 0.017115477472543716\n",
      "iteration 1449, dc_loss: 0.015875698998570442, tv_loss: 0.017115354537963867\n",
      "iteration 1450, dc_loss: 0.0158755611628294, tv_loss: 0.0171150304377079\n",
      "iteration 1451, dc_loss: 0.01587541401386261, tv_loss: 0.017114784568548203\n",
      "iteration 1452, dc_loss: 0.015875225886702538, tv_loss: 0.017114752903580666\n",
      "iteration 1453, dc_loss: 0.015875007957220078, tv_loss: 0.01711445488035679\n",
      "iteration 1454, dc_loss: 0.015874790027737617, tv_loss: 0.01711432822048664\n",
      "iteration 1455, dc_loss: 0.015874555334448814, tv_loss: 0.01711425930261612\n",
      "iteration 1456, dc_loss: 0.015874287113547325, tv_loss: 0.01711420901119709\n",
      "iteration 1457, dc_loss: 0.01587403193116188, tv_loss: 0.017113907262682915\n",
      "iteration 1458, dc_loss: 0.01587384194135666, tv_loss: 0.01711370423436165\n",
      "iteration 1459, dc_loss: 0.015873700380325317, tv_loss: 0.017113525420427322\n",
      "iteration 1460, dc_loss: 0.015873592346906662, tv_loss: 0.0171134602278471\n",
      "iteration 1461, dc_loss: 0.015873467549681664, tv_loss: 0.017113083973526955\n",
      "iteration 1462, dc_loss: 0.015873314812779427, tv_loss: 0.01711294613778591\n",
      "iteration 1463, dc_loss: 0.015873122960329056, tv_loss: 0.017112761735916138\n",
      "iteration 1464, dc_loss: 0.015872912481427193, tv_loss: 0.01711251400411129\n",
      "iteration 1465, dc_loss: 0.015872696414589882, tv_loss: 0.017112424597144127\n",
      "iteration 1466, dc_loss: 0.015872474759817123, tv_loss: 0.017112240195274353\n",
      "iteration 1467, dc_loss: 0.015872282907366753, tv_loss: 0.017112111672759056\n",
      "iteration 1468, dc_loss: 0.015872083604335785, tv_loss: 0.01711185649037361\n",
      "iteration 1469, dc_loss: 0.01587187498807907, tv_loss: 0.01711157150566578\n",
      "iteration 1470, dc_loss: 0.01587170548737049, tv_loss: 0.017111321911215782\n",
      "iteration 1471, dc_loss: 0.015871543437242508, tv_loss: 0.01711125858128071\n",
      "iteration 1472, dc_loss: 0.015871386975049973, tv_loss: 0.017110873013734818\n",
      "iteration 1473, dc_loss: 0.01587119698524475, tv_loss: 0.017110759392380714\n",
      "iteration 1474, dc_loss: 0.015870997682213783, tv_loss: 0.017110729590058327\n",
      "iteration 1475, dc_loss: 0.015870805829763412, tv_loss: 0.017110472545027733\n",
      "iteration 1476, dc_loss: 0.015870630741119385, tv_loss: 0.017110206186771393\n",
      "iteration 1477, dc_loss: 0.015870433300733566, tv_loss: 0.017110079526901245\n",
      "iteration 1478, dc_loss: 0.015870220959186554, tv_loss: 0.01711004041135311\n",
      "iteration 1479, dc_loss: 0.015869999304413795, tv_loss: 0.01710979826748371\n",
      "iteration 1480, dc_loss: 0.015869785100221634, tv_loss: 0.017109762877225876\n",
      "iteration 1481, dc_loss: 0.01586959697306156, tv_loss: 0.01710965298116207\n",
      "iteration 1482, dc_loss: 0.01586942933499813, tv_loss: 0.017109360545873642\n",
      "iteration 1483, dc_loss: 0.015869291499257088, tv_loss: 0.017109226435422897\n",
      "iteration 1484, dc_loss: 0.01586918905377388, tv_loss: 0.017108937725424767\n",
      "iteration 1485, dc_loss: 0.01586906611919403, tv_loss: 0.017108721658587456\n",
      "iteration 1486, dc_loss: 0.015868905931711197, tv_loss: 0.017108483240008354\n",
      "iteration 1487, dc_loss: 0.015868723392486572, tv_loss: 0.017108293250203133\n",
      "iteration 1488, dc_loss: 0.01586853340268135, tv_loss: 0.01710817590355873\n",
      "iteration 1489, dc_loss: 0.01586831733584404, tv_loss: 0.01710803061723709\n",
      "iteration 1490, dc_loss: 0.01586810313165188, tv_loss: 0.017107980325818062\n",
      "iteration 1491, dc_loss: 0.01586790941655636, tv_loss: 0.01710769534111023\n",
      "iteration 1492, dc_loss: 0.01586776040494442, tv_loss: 0.017107510939240456\n",
      "iteration 1493, dc_loss: 0.01586761139333248, tv_loss: 0.01710733212530613\n",
      "iteration 1494, dc_loss: 0.015867464244365692, tv_loss: 0.017107026651501656\n",
      "iteration 1495, dc_loss: 0.01586730033159256, tv_loss: 0.01710694096982479\n",
      "iteration 1496, dc_loss: 0.015867145732045174, tv_loss: 0.01710689812898636\n",
      "iteration 1497, dc_loss: 0.015866974368691444, tv_loss: 0.01710665225982666\n",
      "iteration 1498, dc_loss: 0.015866784378886223, tv_loss: 0.0171064343303442\n",
      "iteration 1499, dc_loss: 0.015866613015532494, tv_loss: 0.017106277868151665\n",
      "iteration 1500, dc_loss: 0.015866462141275406, tv_loss: 0.01710624247789383\n",
      "iteration 1501, dc_loss: 0.01586630567908287, tv_loss: 0.017106084153056145\n",
      "iteration 1502, dc_loss: 0.015866128727793694, tv_loss: 0.017105722799897194\n",
      "iteration 1503, dc_loss: 0.015865912660956383, tv_loss: 0.017105529084801674\n",
      "iteration 1504, dc_loss: 0.015865707769989967, tv_loss: 0.017105573788285255\n",
      "iteration 1505, dc_loss: 0.01586552895605564, tv_loss: 0.017105448991060257\n",
      "iteration 1506, dc_loss: 0.01586538925766945, tv_loss: 0.017105042934417725\n",
      "iteration 1507, dc_loss: 0.01586524210870266, tv_loss: 0.01710493676364422\n",
      "iteration 1508, dc_loss: 0.01586505025625229, tv_loss: 0.0171047355979681\n",
      "iteration 1509, dc_loss: 0.01586483046412468, tv_loss: 0.01710476167500019\n",
      "iteration 1510, dc_loss: 0.015864599496126175, tv_loss: 0.0171047393232584\n",
      "iteration 1511, dc_loss: 0.015864383429288864, tv_loss: 0.017104510217905045\n",
      "iteration 1512, dc_loss: 0.015864208340644836, tv_loss: 0.01710420660674572\n",
      "iteration 1513, dc_loss: 0.015864064916968346, tv_loss: 0.01710405759513378\n",
      "iteration 1514, dc_loss: 0.015863949432969093, tv_loss: 0.017103848978877068\n",
      "iteration 1515, dc_loss: 0.015863869339227676, tv_loss: 0.017103463411331177\n",
      "iteration 1516, dc_loss: 0.01586376316845417, tv_loss: 0.017103279009461403\n",
      "iteration 1517, dc_loss: 0.015863610431551933, tv_loss: 0.017103008925914764\n",
      "iteration 1518, dc_loss: 0.01586342044174671, tv_loss: 0.01710277609527111\n",
      "iteration 1519, dc_loss: 0.015863219276070595, tv_loss: 0.017102686688303947\n",
      "iteration 1520, dc_loss: 0.01586303673684597, tv_loss: 0.017102764919400215\n",
      "iteration 1521, dc_loss: 0.015862824395298958, tv_loss: 0.017102492973208427\n",
      "iteration 1522, dc_loss: 0.01586262136697769, tv_loss: 0.017102262005209923\n",
      "iteration 1523, dc_loss: 0.015862446278333664, tv_loss: 0.01710205338895321\n",
      "iteration 1524, dc_loss: 0.01586228795349598, tv_loss: 0.017102038487792015\n",
      "iteration 1525, dc_loss: 0.015862111002206802, tv_loss: 0.017101924866437912\n",
      "iteration 1526, dc_loss: 0.015861941501498222, tv_loss: 0.017101678997278214\n",
      "iteration 1527, dc_loss: 0.01586179807782173, tv_loss: 0.01710149645805359\n",
      "iteration 1528, dc_loss: 0.01586165465414524, tv_loss: 0.017101339995861053\n",
      "iteration 1529, dc_loss: 0.015861526131629944, tv_loss: 0.017101198434829712\n",
      "iteration 1530, dc_loss: 0.01586136966943741, tv_loss: 0.017100699245929718\n",
      "iteration 1531, dc_loss: 0.01586117222905159, tv_loss: 0.01710071787238121\n",
      "iteration 1532, dc_loss: 0.01586095057427883, tv_loss: 0.017100699245929718\n",
      "iteration 1533, dc_loss: 0.015860773622989655, tv_loss: 0.01710060052573681\n",
      "iteration 1534, dc_loss: 0.01586064137518406, tv_loss: 0.017100410535931587\n",
      "iteration 1535, dc_loss: 0.015860503539443016, tv_loss: 0.017100250348448753\n",
      "iteration 1536, dc_loss: 0.01586036942899227, tv_loss: 0.017099857330322266\n",
      "iteration 1537, dc_loss: 0.015860266983509064, tv_loss: 0.017099672928452492\n",
      "iteration 1538, dc_loss: 0.01586013287305832, tv_loss: 0.017099743708968163\n",
      "iteration 1539, dc_loss: 0.015859970822930336, tv_loss: 0.017099613323807716\n",
      "iteration 1540, dc_loss: 0.015859805047512054, tv_loss: 0.01709935814142227\n",
      "iteration 1541, dc_loss: 0.015859641134738922, tv_loss: 0.01709917187690735\n",
      "iteration 1542, dc_loss: 0.015859471634030342, tv_loss: 0.017099035903811455\n",
      "iteration 1543, dc_loss: 0.015859274193644524, tv_loss: 0.01709907315671444\n",
      "iteration 1544, dc_loss: 0.015859080478549004, tv_loss: 0.01709894835948944\n",
      "iteration 1545, dc_loss: 0.01585887186229229, tv_loss: 0.017098790034651756\n",
      "iteration 1546, dc_loss: 0.01585870422422886, tv_loss: 0.01709873415529728\n",
      "iteration 1547, dc_loss: 0.015858540311455727, tv_loss: 0.017098603770136833\n",
      "iteration 1548, dc_loss: 0.01585838943719864, tv_loss: 0.017098436132073402\n",
      "iteration 1549, dc_loss: 0.015858249738812447, tv_loss: 0.017098261043429375\n",
      "iteration 1550, dc_loss: 0.015858136117458344, tv_loss: 0.017097920179367065\n",
      "iteration 1551, dc_loss: 0.01585804484784603, tv_loss: 0.017097754403948784\n",
      "iteration 1552, dc_loss: 0.01585792563855648, tv_loss: 0.017097484320402145\n",
      "iteration 1553, dc_loss: 0.0158577561378479, tv_loss: 0.01709737442433834\n",
      "iteration 1554, dc_loss: 0.015857581049203873, tv_loss: 0.01709727570414543\n",
      "iteration 1555, dc_loss: 0.015857402235269547, tv_loss: 0.017097188159823418\n",
      "iteration 1556, dc_loss: 0.015857215970754623, tv_loss: 0.01709694415330887\n",
      "iteration 1557, dc_loss: 0.015857050195336342, tv_loss: 0.01709669828414917\n",
      "iteration 1558, dc_loss: 0.01585688255727291, tv_loss: 0.017096709460020065\n",
      "iteration 1559, dc_loss: 0.015856727957725525, tv_loss: 0.017096566036343575\n",
      "iteration 1560, dc_loss: 0.015856582671403885, tv_loss: 0.01709611527621746\n",
      "iteration 1561, dc_loss: 0.01585645042359829, tv_loss: 0.017096063122153282\n",
      "iteration 1562, dc_loss: 0.01585634984076023, tv_loss: 0.017096063122153282\n",
      "iteration 1563, dc_loss: 0.015856247395277023, tv_loss: 0.01709580235183239\n",
      "iteration 1564, dc_loss: 0.015856102108955383, tv_loss: 0.017095638439059258\n",
      "iteration 1565, dc_loss: 0.015855908393859863, tv_loss: 0.017095400020480156\n",
      "iteration 1566, dc_loss: 0.015855690464377403, tv_loss: 0.017095305025577545\n",
      "iteration 1567, dc_loss: 0.015855472534894943, tv_loss: 0.017095396295189857\n",
      "iteration 1568, dc_loss: 0.015855271369218826, tv_loss: 0.017095297574996948\n",
      "iteration 1569, dc_loss: 0.0158550925552845, tv_loss: 0.017094986513257027\n",
      "iteration 1570, dc_loss: 0.015854937955737114, tv_loss: 0.017094921320676804\n",
      "iteration 1571, dc_loss: 0.01585482619702816, tv_loss: 0.01709466427564621\n",
      "iteration 1572, dc_loss: 0.015854740515351295, tv_loss: 0.017094489187002182\n",
      "iteration 1573, dc_loss: 0.015854643657803535, tv_loss: 0.017094383016228676\n",
      "iteration 1574, dc_loss: 0.015854543074965477, tv_loss: 0.017094172537326813\n",
      "iteration 1575, dc_loss: 0.015854420140385628, tv_loss: 0.017093835398554802\n",
      "iteration 1576, dc_loss: 0.015854254364967346, tv_loss: 0.01709374599158764\n",
      "iteration 1577, dc_loss: 0.015854107216000557, tv_loss: 0.017093628644943237\n",
      "iteration 1578, dc_loss: 0.015853969380259514, tv_loss: 0.017093591392040253\n",
      "iteration 1579, dc_loss: 0.01585381291806698, tv_loss: 0.01709326170384884\n",
      "iteration 1580, dc_loss: 0.015853628516197205, tv_loss: 0.017093205824494362\n",
      "iteration 1581, dc_loss: 0.015853464603424072, tv_loss: 0.017093252390623093\n",
      "iteration 1582, dc_loss: 0.015853285789489746, tv_loss: 0.017093034461140633\n",
      "iteration 1583, dc_loss: 0.015853114426136017, tv_loss: 0.017093049362301826\n",
      "iteration 1584, dc_loss: 0.01585296168923378, tv_loss: 0.017092978581786156\n",
      "iteration 1585, dc_loss: 0.0158527959138155, tv_loss: 0.017092816531658173\n",
      "iteration 1586, dc_loss: 0.01585264317691326, tv_loss: 0.017092673107981682\n",
      "iteration 1587, dc_loss: 0.015852490440011024, tv_loss: 0.017092429101467133\n",
      "iteration 1588, dc_loss: 0.01585237868130207, tv_loss: 0.01709217205643654\n",
      "iteration 1589, dc_loss: 0.015852266922593117, tv_loss: 0.017092162743210793\n",
      "iteration 1590, dc_loss: 0.015852181240916252, tv_loss: 0.017091883346438408\n",
      "iteration 1591, dc_loss: 0.0158520694822073, tv_loss: 0.01709161512553692\n",
      "iteration 1592, dc_loss: 0.015851927921175957, tv_loss: 0.01709158346056938\n",
      "iteration 1593, dc_loss: 0.015851786360144615, tv_loss: 0.017091497778892517\n",
      "iteration 1594, dc_loss: 0.015851633623242378, tv_loss: 0.017091382294893265\n",
      "iteration 1595, dc_loss: 0.015851464122533798, tv_loss: 0.0170911755412817\n",
      "iteration 1596, dc_loss: 0.015851294621825218, tv_loss: 0.01709107868373394\n",
      "iteration 1597, dc_loss: 0.015851108357310295, tv_loss: 0.017090993002057076\n",
      "iteration 1598, dc_loss: 0.015850935131311417, tv_loss: 0.017090870067477226\n",
      "iteration 1599, dc_loss: 0.015850786119699478, tv_loss: 0.017090747132897377\n",
      "iteration 1600, dc_loss: 0.01585063897073269, tv_loss: 0.01709057204425335\n",
      "iteration 1601, dc_loss: 0.01585049368441105, tv_loss: 0.017090214416384697\n",
      "iteration 1602, dc_loss: 0.01585034467279911, tv_loss: 0.01709027960896492\n",
      "iteration 1603, dc_loss: 0.015850186347961426, tv_loss: 0.01709025911986828\n",
      "iteration 1604, dc_loss: 0.015850067138671875, tv_loss: 0.017089955508708954\n",
      "iteration 1605, dc_loss: 0.0158500075340271, tv_loss: 0.017089787870645523\n",
      "iteration 1606, dc_loss: 0.01584998145699501, tv_loss: 0.017089473083615303\n",
      "iteration 1607, dc_loss: 0.01584991253912449, tv_loss: 0.017089204862713814\n",
      "iteration 1608, dc_loss: 0.015849776566028595, tv_loss: 0.017089085653424263\n",
      "iteration 1609, dc_loss: 0.015849603340029716, tv_loss: 0.017089035362005234\n",
      "iteration 1610, dc_loss: 0.01584937795996666, tv_loss: 0.017089128494262695\n",
      "iteration 1611, dc_loss: 0.015849152579903603, tv_loss: 0.017089001834392548\n",
      "iteration 1612, dc_loss: 0.015848983079195023, tv_loss: 0.017088757827878\n",
      "iteration 1613, dc_loss: 0.015848854556679726, tv_loss: 0.01708865538239479\n",
      "iteration 1614, dc_loss: 0.015848767012357712, tv_loss: 0.01708856038749218\n",
      "iteration 1615, dc_loss: 0.015848692506551743, tv_loss: 0.017088251188397408\n",
      "iteration 1616, dc_loss: 0.01584860496222973, tv_loss: 0.017087863758206367\n",
      "iteration 1617, dc_loss: 0.015848495066165924, tv_loss: 0.017087748274207115\n",
      "iteration 1618, dc_loss: 0.015848323702812195, tv_loss: 0.01708764210343361\n",
      "iteration 1619, dc_loss: 0.015848129987716675, tv_loss: 0.017087740823626518\n",
      "iteration 1620, dc_loss: 0.0158479455858469, tv_loss: 0.017087701708078384\n",
      "iteration 1621, dc_loss: 0.01584777981042862, tv_loss: 0.017087426036596298\n",
      "iteration 1622, dc_loss: 0.015847595408558846, tv_loss: 0.01708744466304779\n",
      "iteration 1623, dc_loss: 0.015847427770495415, tv_loss: 0.017087377607822418\n",
      "iteration 1624, dc_loss: 0.01584729738533497, tv_loss: 0.017087211832404137\n",
      "iteration 1625, dc_loss: 0.015847237780690193, tv_loss: 0.017086930572986603\n",
      "iteration 1626, dc_loss: 0.015847155824303627, tv_loss: 0.017086755484342575\n",
      "iteration 1627, dc_loss: 0.015847060829401016, tv_loss: 0.01708661951124668\n",
      "iteration 1628, dc_loss: 0.01584690995514393, tv_loss: 0.01708650216460228\n",
      "iteration 1629, dc_loss: 0.01584676094353199, tv_loss: 0.01708640158176422\n",
      "iteration 1630, dc_loss: 0.0158466175198555, tv_loss: 0.017086278647184372\n",
      "iteration 1631, dc_loss: 0.015846477821469307, tv_loss: 0.01708604022860527\n",
      "iteration 1632, dc_loss: 0.015846338123083115, tv_loss: 0.017086125910282135\n",
      "iteration 1633, dc_loss: 0.015846198424696922, tv_loss: 0.017085924744606018\n",
      "iteration 1634, dc_loss: 0.01584605872631073, tv_loss: 0.017085522413253784\n",
      "iteration 1635, dc_loss: 0.01584594137966633, tv_loss: 0.017085565254092216\n",
      "iteration 1636, dc_loss: 0.015845835208892822, tv_loss: 0.017085589468479156\n",
      "iteration 1637, dc_loss: 0.015845708549022675, tv_loss: 0.017085297033190727\n",
      "iteration 1638, dc_loss: 0.01584554836153984, tv_loss: 0.017085233703255653\n",
      "iteration 1639, dc_loss: 0.015845373272895813, tv_loss: 0.017085079103708267\n",
      "iteration 1640, dc_loss: 0.015845244750380516, tv_loss: 0.01708514615893364\n",
      "iteration 1641, dc_loss: 0.015845106914639473, tv_loss: 0.017084933817386627\n",
      "iteration 1642, dc_loss: 0.015844976529479027, tv_loss: 0.017084715887904167\n",
      "iteration 1643, dc_loss: 0.01584482192993164, tv_loss: 0.017084641382098198\n",
      "iteration 1644, dc_loss: 0.015844672918319702, tv_loss: 0.017084509134292603\n",
      "iteration 1645, dc_loss: 0.01584455743432045, tv_loss: 0.01708429493010044\n",
      "iteration 1646, dc_loss: 0.015844428911805153, tv_loss: 0.01708420366048813\n",
      "iteration 1647, dc_loss: 0.015844298526644707, tv_loss: 0.017084162682294846\n",
      "iteration 1648, dc_loss: 0.015844184905290604, tv_loss: 0.01708415523171425\n",
      "iteration 1649, dc_loss: 0.01584409363567829, tv_loss: 0.01708378456532955\n",
      "iteration 1650, dc_loss: 0.015844004228711128, tv_loss: 0.01708354987204075\n",
      "iteration 1651, dc_loss: 0.015843886882066727, tv_loss: 0.017083602026104927\n",
      "iteration 1652, dc_loss: 0.015843741595745087, tv_loss: 0.017083492130041122\n",
      "iteration 1653, dc_loss: 0.01584358885884285, tv_loss: 0.01708328165113926\n",
      "iteration 1654, dc_loss: 0.015843456611037254, tv_loss: 0.017083195969462395\n",
      "iteration 1655, dc_loss: 0.01584331877529621, tv_loss: 0.01708320342004299\n",
      "iteration 1656, dc_loss: 0.015843184664845467, tv_loss: 0.017083168029785156\n",
      "iteration 1657, dc_loss: 0.01584305241703987, tv_loss: 0.017083028331398964\n",
      "iteration 1658, dc_loss: 0.015842938795685768, tv_loss: 0.017082707956433296\n",
      "iteration 1659, dc_loss: 0.01584281586110592, tv_loss: 0.017082683742046356\n",
      "iteration 1660, dc_loss: 0.015842679888010025, tv_loss: 0.01708269864320755\n",
      "iteration 1661, dc_loss: 0.01584254764020443, tv_loss: 0.01708240620791912\n",
      "iteration 1662, dc_loss: 0.01584245078265667, tv_loss: 0.017082076519727707\n",
      "iteration 1663, dc_loss: 0.015842368826270103, tv_loss: 0.017081966623663902\n",
      "iteration 1664, dc_loss: 0.015842273831367493, tv_loss: 0.017081886529922485\n",
      "iteration 1665, dc_loss: 0.015842173248529434, tv_loss: 0.017081741243600845\n",
      "iteration 1666, dc_loss: 0.015842074528336525, tv_loss: 0.017081722617149353\n",
      "iteration 1667, dc_loss: 0.01584193855524063, tv_loss: 0.017081646248698235\n",
      "iteration 1668, dc_loss: 0.01584177277982235, tv_loss: 0.01708146557211876\n",
      "iteration 1669, dc_loss: 0.01584159955382347, tv_loss: 0.017081204801797867\n",
      "iteration 1670, dc_loss: 0.01584140956401825, tv_loss: 0.017081238329410553\n",
      "iteration 1671, dc_loss: 0.015841258689761162, tv_loss: 0.017081229016184807\n",
      "iteration 1672, dc_loss: 0.015841133892536163, tv_loss: 0.01708107441663742\n",
      "iteration 1673, dc_loss: 0.015841001644730568, tv_loss: 0.01708087883889675\n",
      "iteration 1674, dc_loss: 0.015840889886021614, tv_loss: 0.017080681398510933\n",
      "iteration 1675, dc_loss: 0.015840813517570496, tv_loss: 0.01708073541522026\n",
      "iteration 1676, dc_loss: 0.015840740874409676, tv_loss: 0.01708059199154377\n",
      "iteration 1677, dc_loss: 0.015840621665120125, tv_loss: 0.01708022877573967\n",
      "iteration 1678, dc_loss: 0.015840496867895126, tv_loss: 0.017080102115869522\n",
      "iteration 1679, dc_loss: 0.01584036275744438, tv_loss: 0.017080046236515045\n",
      "iteration 1680, dc_loss: 0.015840252861380577, tv_loss: 0.017079997807741165\n",
      "iteration 1681, dc_loss: 0.015840161591768265, tv_loss: 0.017079772427678108\n",
      "iteration 1682, dc_loss: 0.01583999954164028, tv_loss: 0.01707952283322811\n",
      "iteration 1683, dc_loss: 0.015839839354157448, tv_loss: 0.0170794278383255\n",
      "iteration 1684, dc_loss: 0.015839679166674614, tv_loss: 0.017079491168260574\n",
      "iteration 1685, dc_loss: 0.015839537605643272, tv_loss: 0.017079422250390053\n",
      "iteration 1686, dc_loss: 0.01583945006132126, tv_loss: 0.017079025506973267\n",
      "iteration 1687, dc_loss: 0.01583937555551529, tv_loss: 0.017078906297683716\n",
      "iteration 1688, dc_loss: 0.01583930104970932, tv_loss: 0.017078841105103493\n",
      "iteration 1689, dc_loss: 0.015839191153645515, tv_loss: 0.017078571021556854\n",
      "iteration 1690, dc_loss: 0.015839077532291412, tv_loss: 0.01707850769162178\n",
      "iteration 1691, dc_loss: 0.015838949009776115, tv_loss: 0.017078451812267303\n",
      "iteration 1692, dc_loss: 0.015838785097002983, tv_loss: 0.01707839034497738\n",
      "iteration 1693, dc_loss: 0.015838641673326492, tv_loss: 0.01707843504846096\n",
      "iteration 1694, dc_loss: 0.015838518738746643, tv_loss: 0.01707821525633335\n",
      "iteration 1695, dc_loss: 0.015838418155908585, tv_loss: 0.01707805134356022\n",
      "iteration 1696, dc_loss: 0.01583833061158657, tv_loss: 0.017077941447496414\n",
      "iteration 1697, dc_loss: 0.015838192775845528, tv_loss: 0.017077989876270294\n",
      "iteration 1698, dc_loss: 0.01583804562687874, tv_loss: 0.017077846452593803\n",
      "iteration 1699, dc_loss: 0.01583789475262165, tv_loss: 0.017077559605240822\n",
      "iteration 1700, dc_loss: 0.0158377792686224, tv_loss: 0.01707747019827366\n",
      "iteration 1701, dc_loss: 0.015837686136364937, tv_loss: 0.017077578231692314\n",
      "iteration 1702, dc_loss: 0.015837576240301132, tv_loss: 0.017077485099434853\n",
      "iteration 1703, dc_loss: 0.015837445855140686, tv_loss: 0.017077120020985603\n",
      "iteration 1704, dc_loss: 0.015837296843528748, tv_loss: 0.017077181488275528\n",
      "iteration 1705, dc_loss: 0.01583714969456196, tv_loss: 0.0170771274715662\n",
      "iteration 1706, dc_loss: 0.015837009996175766, tv_loss: 0.017076989635825157\n",
      "iteration 1707, dc_loss: 0.01583690755069256, tv_loss: 0.017076833173632622\n",
      "iteration 1708, dc_loss: 0.015836838632822037, tv_loss: 0.017076777294278145\n",
      "iteration 1709, dc_loss: 0.015836752951145172, tv_loss: 0.01707644946873188\n",
      "iteration 1710, dc_loss: 0.015836674720048904, tv_loss: 0.017076198011636734\n",
      "iteration 1711, dc_loss: 0.015836575999855995, tv_loss: 0.017076147720217705\n",
      "iteration 1712, dc_loss: 0.015836486592888832, tv_loss: 0.017076287418603897\n",
      "iteration 1713, dc_loss: 0.015836387872695923, tv_loss: 0.017075790092349052\n",
      "iteration 1714, dc_loss: 0.01583627052605152, tv_loss: 0.01707577146589756\n",
      "iteration 1715, dc_loss: 0.015836145728826523, tv_loss: 0.01707562245428562\n",
      "iteration 1716, dc_loss: 0.01583602838218212, tv_loss: 0.01707555539906025\n",
      "iteration 1717, dc_loss: 0.01583588868379593, tv_loss: 0.017075413838028908\n",
      "iteration 1718, dc_loss: 0.015835758298635483, tv_loss: 0.01707523874938488\n",
      "iteration 1719, dc_loss: 0.015835648402571678, tv_loss: 0.017075199633836746\n",
      "iteration 1720, dc_loss: 0.015835516154766083, tv_loss: 0.017075154930353165\n",
      "iteration 1721, dc_loss: 0.015835391357541084, tv_loss: 0.01707511581480503\n",
      "iteration 1722, dc_loss: 0.015835270285606384, tv_loss: 0.01707492768764496\n",
      "iteration 1723, dc_loss: 0.015835152938961983, tv_loss: 0.01707487367093563\n",
      "iteration 1724, dc_loss: 0.015835050493478775, tv_loss: 0.017074763774871826\n",
      "iteration 1725, dc_loss: 0.01583492010831833, tv_loss: 0.017074434086680412\n",
      "iteration 1726, dc_loss: 0.015834825113415718, tv_loss: 0.01707431674003601\n",
      "iteration 1727, dc_loss: 0.015834713354706764, tv_loss: 0.01707429066300392\n",
      "iteration 1728, dc_loss: 0.015834614634513855, tv_loss: 0.017074046656489372\n",
      "iteration 1729, dc_loss: 0.01583453081548214, tv_loss: 0.01707390882074833\n",
      "iteration 1730, dc_loss: 0.015834445133805275, tv_loss: 0.01707378402352333\n",
      "iteration 1731, dc_loss: 0.01583433896303177, tv_loss: 0.01707366853952408\n",
      "iteration 1732, dc_loss: 0.01583421416580677, tv_loss: 0.017073754221200943\n",
      "iteration 1733, dc_loss: 0.015834063291549683, tv_loss: 0.017073582857847214\n",
      "iteration 1734, dc_loss: 0.015833934769034386, tv_loss: 0.017073433846235275\n",
      "iteration 1735, dc_loss: 0.015833817422389984, tv_loss: 0.01707340031862259\n",
      "iteration 1736, dc_loss: 0.01583372801542282, tv_loss: 0.017073264345526695\n",
      "iteration 1737, dc_loss: 0.015833629295229912, tv_loss: 0.017073137685656548\n",
      "iteration 1738, dc_loss: 0.015833528712391853, tv_loss: 0.017073139548301697\n",
      "iteration 1739, dc_loss: 0.015833403915166855, tv_loss: 0.017072973772883415\n",
      "iteration 1740, dc_loss: 0.015833279117941856, tv_loss: 0.01707281358540058\n",
      "iteration 1741, dc_loss: 0.015833169221878052, tv_loss: 0.017072655260562897\n",
      "iteration 1742, dc_loss: 0.015833063051104546, tv_loss: 0.01707260124385357\n",
      "iteration 1743, dc_loss: 0.015832973644137383, tv_loss: 0.017072653397917747\n",
      "iteration 1744, dc_loss: 0.015832889825105667, tv_loss: 0.017072472721338272\n",
      "iteration 1745, dc_loss: 0.015832778066396713, tv_loss: 0.017072169110178947\n",
      "iteration 1746, dc_loss: 0.015832671895623207, tv_loss: 0.017072254791855812\n",
      "iteration 1747, dc_loss: 0.0158325657248497, tv_loss: 0.0170722808688879\n",
      "iteration 1748, dc_loss: 0.015832427889108658, tv_loss: 0.017071859911084175\n",
      "iteration 1749, dc_loss: 0.01583229750394821, tv_loss: 0.01707201823592186\n",
      "iteration 1750, dc_loss: 0.015832191333174706, tv_loss: 0.01707199215888977\n",
      "iteration 1751, dc_loss: 0.01583207957446575, tv_loss: 0.017071882262825966\n",
      "iteration 1752, dc_loss: 0.015831975266337395, tv_loss: 0.017071550711989403\n",
      "iteration 1753, dc_loss: 0.01583188958466053, tv_loss: 0.01707146316766739\n",
      "iteration 1754, dc_loss: 0.015831787139177322, tv_loss: 0.017071504145860672\n",
      "iteration 1755, dc_loss: 0.01583169773221016, tv_loss: 0.017071284353733063\n",
      "iteration 1756, dc_loss: 0.015831610187888145, tv_loss: 0.017070982605218887\n",
      "iteration 1757, dc_loss: 0.01583149842917919, tv_loss: 0.01707087643444538\n",
      "iteration 1758, dc_loss: 0.01583137921988964, tv_loss: 0.017070990055799484\n",
      "iteration 1759, dc_loss: 0.015831254422664642, tv_loss: 0.017070921137928963\n",
      "iteration 1760, dc_loss: 0.015831178054213524, tv_loss: 0.0170705895870924\n",
      "iteration 1761, dc_loss: 0.01583111099898815, tv_loss: 0.017070429399609566\n",
      "iteration 1762, dc_loss: 0.015831049531698227, tv_loss: 0.01707054115831852\n",
      "iteration 1763, dc_loss: 0.01583097130060196, tv_loss: 0.01707048900425434\n",
      "iteration 1764, dc_loss: 0.015830878168344498, tv_loss: 0.017070231959223747\n",
      "iteration 1765, dc_loss: 0.015830738469958305, tv_loss: 0.01706991344690323\n",
      "iteration 1766, dc_loss: 0.015830570831894875, tv_loss: 0.01707003265619278\n",
      "iteration 1767, dc_loss: 0.015830418094992638, tv_loss: 0.017070181667804718\n",
      "iteration 1768, dc_loss: 0.01583026349544525, tv_loss: 0.017070213332772255\n",
      "iteration 1769, dc_loss: 0.015830138698220253, tv_loss: 0.01706998236477375\n",
      "iteration 1770, dc_loss: 0.015830054879188538, tv_loss: 0.017069892957806587\n",
      "iteration 1771, dc_loss: 0.015829971060156822, tv_loss: 0.01706986129283905\n",
      "iteration 1772, dc_loss: 0.01582987792789936, tv_loss: 0.01706969551742077\n",
      "iteration 1773, dc_loss: 0.01582975871860981, tv_loss: 0.01706955023109913\n",
      "iteration 1774, dc_loss: 0.01582966558635235, tv_loss: 0.017069527879357338\n",
      "iteration 1775, dc_loss: 0.015829579904675484, tv_loss: 0.017069363966584206\n",
      "iteration 1776, dc_loss: 0.015829505398869514, tv_loss: 0.01706908456981182\n",
      "iteration 1777, dc_loss: 0.015829451382160187, tv_loss: 0.017069045454263687\n",
      "iteration 1778, dc_loss: 0.01582939922809601, tv_loss: 0.017069052904844284\n",
      "iteration 1779, dc_loss: 0.01582927256822586, tv_loss: 0.017068756744265556\n",
      "iteration 1780, dc_loss: 0.01582915522158146, tv_loss: 0.017068704590201378\n",
      "iteration 1781, dc_loss: 0.015829049050807953, tv_loss: 0.01706874743103981\n",
      "iteration 1782, dc_loss: 0.01582893542945385, tv_loss: 0.017068663612008095\n",
      "iteration 1783, dc_loss: 0.01582883857190609, tv_loss: 0.01706850156188011\n",
      "iteration 1784, dc_loss: 0.015828780829906464, tv_loss: 0.017068367451429367\n",
      "iteration 1785, dc_loss: 0.015828711912035942, tv_loss: 0.0170681681483984\n",
      "iteration 1786, dc_loss: 0.01582859270274639, tv_loss: 0.017068056389689445\n",
      "iteration 1787, dc_loss: 0.015828458592295647, tv_loss: 0.017067909240722656\n",
      "iteration 1788, dc_loss: 0.015828324481844902, tv_loss: 0.017068011686205864\n",
      "iteration 1789, dc_loss: 0.01582818292081356, tv_loss: 0.01706782355904579\n",
      "iteration 1790, dc_loss: 0.01582803763449192, tv_loss: 0.017067696899175644\n",
      "iteration 1791, dc_loss: 0.01582791656255722, tv_loss: 0.01706758327782154\n",
      "iteration 1792, dc_loss: 0.015827814117074013, tv_loss: 0.017067549750208855\n",
      "iteration 1793, dc_loss: 0.015827739611268044, tv_loss: 0.017067385837435722\n",
      "iteration 1794, dc_loss: 0.015827687457203865, tv_loss: 0.017067251726984978\n",
      "iteration 1795, dc_loss: 0.015827618539333344, tv_loss: 0.017067043110728264\n",
      "iteration 1796, dc_loss: 0.015827562659978867, tv_loss: 0.017066920176148415\n",
      "iteration 1797, dc_loss: 0.01582750491797924, tv_loss: 0.017066843807697296\n",
      "iteration 1798, dc_loss: 0.015827419236302376, tv_loss: 0.017066681757569313\n",
      "iteration 1799, dc_loss: 0.01582730934023857, tv_loss: 0.01706659235060215\n",
      "iteration 1800, dc_loss: 0.015827206894755363, tv_loss: 0.017066534608602524\n",
      "iteration 1801, dc_loss: 0.015827085822820663, tv_loss: 0.017066409811377525\n",
      "iteration 1802, dc_loss: 0.01582697220146656, tv_loss: 0.017066244035959244\n",
      "iteration 1803, dc_loss: 0.01582685112953186, tv_loss: 0.01706627383828163\n",
      "iteration 1804, dc_loss: 0.01582673005759716, tv_loss: 0.017066150903701782\n",
      "iteration 1805, dc_loss: 0.015826616436243057, tv_loss: 0.017066005617380142\n",
      "iteration 1806, dc_loss: 0.015826523303985596, tv_loss: 0.01706596650183201\n",
      "iteration 1807, dc_loss: 0.01582643948495388, tv_loss: 0.017065852880477905\n",
      "iteration 1808, dc_loss: 0.015826357528567314, tv_loss: 0.01706572063267231\n",
      "iteration 1809, dc_loss: 0.015826260671019554, tv_loss: 0.01706559583544731\n",
      "iteration 1810, dc_loss: 0.015826154500246048, tv_loss: 0.01706552878022194\n",
      "iteration 1811, dc_loss: 0.015826040878891945, tv_loss: 0.0170655008405447\n",
      "iteration 1812, dc_loss: 0.015825925394892693, tv_loss: 0.017065424472093582\n",
      "iteration 1813, dc_loss: 0.01582583785057068, tv_loss: 0.01706533320248127\n",
      "iteration 1814, dc_loss: 0.01582573913037777, tv_loss: 0.017065217718482018\n",
      "iteration 1815, dc_loss: 0.01582564413547516, tv_loss: 0.01706511154770851\n",
      "iteration 1816, dc_loss: 0.015825551003217697, tv_loss: 0.017064929008483887\n",
      "iteration 1817, dc_loss: 0.015825463458895683, tv_loss: 0.01706482283771038\n",
      "iteration 1818, dc_loss: 0.01582539826631546, tv_loss: 0.01706470176577568\n",
      "iteration 1819, dc_loss: 0.015825333073735237, tv_loss: 0.017064711079001427\n",
      "iteration 1820, dc_loss: 0.015825267881155014, tv_loss: 0.017064550891518593\n",
      "iteration 1821, dc_loss: 0.015825143083930016, tv_loss: 0.017064373940229416\n",
      "iteration 1822, dc_loss: 0.01582500711083412, tv_loss: 0.017064467072486877\n",
      "iteration 1823, dc_loss: 0.015824830159544945, tv_loss: 0.017064502462744713\n",
      "iteration 1824, dc_loss: 0.015824666246771812, tv_loss: 0.017064370214939117\n",
      "iteration 1825, dc_loss: 0.015824561938643456, tv_loss: 0.017064286395907402\n",
      "iteration 1826, dc_loss: 0.01582452841103077, tv_loss: 0.017064271494746208\n",
      "iteration 1827, dc_loss: 0.015824517235159874, tv_loss: 0.017064085230231285\n",
      "iteration 1828, dc_loss: 0.015824493020772934, tv_loss: 0.017063915729522705\n",
      "iteration 1829, dc_loss: 0.015824465081095695, tv_loss: 0.01706385426223278\n",
      "iteration 1830, dc_loss: 0.01582440920174122, tv_loss: 0.017063604667782784\n",
      "iteration 1831, dc_loss: 0.01582428440451622, tv_loss: 0.017063627019524574\n",
      "iteration 1832, dc_loss: 0.01582416146993637, tv_loss: 0.017063479870557785\n",
      "iteration 1833, dc_loss: 0.015824025496840477, tv_loss: 0.01706351526081562\n",
      "iteration 1834, dc_loss: 0.015823902562260628, tv_loss: 0.017063500359654427\n",
      "iteration 1835, dc_loss: 0.015823770314455032, tv_loss: 0.017063386738300323\n",
      "iteration 1836, dc_loss: 0.015823636204004288, tv_loss: 0.01706312596797943\n",
      "iteration 1837, dc_loss: 0.01582351140677929, tv_loss: 0.017063157632946968\n",
      "iteration 1838, dc_loss: 0.015823420137166977, tv_loss: 0.017063045874238014\n",
      "iteration 1839, dc_loss: 0.015823325142264366, tv_loss: 0.017062900587916374\n",
      "iteration 1840, dc_loss: 0.015823252499103546, tv_loss: 0.017062753438949585\n",
      "iteration 1841, dc_loss: 0.015823163092136383, tv_loss: 0.017062680795788765\n",
      "iteration 1842, dc_loss: 0.01582309603691101, tv_loss: 0.017062600702047348\n",
      "iteration 1843, dc_loss: 0.01582302898168564, tv_loss: 0.017062535509467125\n",
      "iteration 1844, dc_loss: 0.015822945162653923, tv_loss: 0.017062414437532425\n",
      "iteration 1845, dc_loss: 0.015822865068912506, tv_loss: 0.01706240326166153\n",
      "iteration 1846, dc_loss: 0.0158227626234293, tv_loss: 0.017062393948435783\n",
      "iteration 1847, dc_loss: 0.01582266204059124, tv_loss: 0.017062263563275337\n",
      "iteration 1848, dc_loss: 0.015822580084204674, tv_loss: 0.017062129452824593\n",
      "iteration 1849, dc_loss: 0.015822486951947212, tv_loss: 0.017062140628695488\n",
      "iteration 1850, dc_loss: 0.01582237146794796, tv_loss: 0.017061997205018997\n",
      "iteration 1851, dc_loss: 0.015822242945432663, tv_loss: 0.017061926424503326\n",
      "iteration 1852, dc_loss: 0.015822116285562515, tv_loss: 0.01706196554005146\n",
      "iteration 1853, dc_loss: 0.015822021290659904, tv_loss: 0.017061671242117882\n",
      "iteration 1854, dc_loss: 0.015821946784853935, tv_loss: 0.017061715945601463\n",
      "iteration 1855, dc_loss: 0.015821894630789757, tv_loss: 0.01706167869269848\n",
      "iteration 1856, dc_loss: 0.015821855515241623, tv_loss: 0.017061470076441765\n",
      "iteration 1857, dc_loss: 0.015821844339370728, tv_loss: 0.01706128753721714\n",
      "iteration 1858, dc_loss: 0.015821781009435654, tv_loss: 0.017061080783605576\n",
      "iteration 1859, dc_loss: 0.015821678563952446, tv_loss: 0.017060808837413788\n",
      "iteration 1860, dc_loss: 0.015821557492017746, tv_loss: 0.017060980200767517\n",
      "iteration 1861, dc_loss: 0.015821468085050583, tv_loss: 0.01706089824438095\n",
      "iteration 1862, dc_loss: 0.015821393579244614, tv_loss: 0.017060717567801476\n",
      "iteration 1863, dc_loss: 0.0158213060349226, tv_loss: 0.017060574144124985\n",
      "iteration 1864, dc_loss: 0.01582122966647148, tv_loss: 0.017060551792383194\n",
      "iteration 1865, dc_loss: 0.01582111045718193, tv_loss: 0.01706060953438282\n",
      "iteration 1866, dc_loss: 0.015821002423763275, tv_loss: 0.0170605331659317\n",
      "iteration 1867, dc_loss: 0.015820913016796112, tv_loss: 0.017060352489352226\n",
      "iteration 1868, dc_loss: 0.01582081988453865, tv_loss: 0.017060305923223495\n",
      "iteration 1869, dc_loss: 0.01582072675228119, tv_loss: 0.017060333862900734\n",
      "iteration 1870, dc_loss: 0.015820665284991264, tv_loss: 0.017060114070773125\n",
      "iteration 1871, dc_loss: 0.015820607542991638, tv_loss: 0.01705993339419365\n",
      "iteration 1872, dc_loss: 0.015820544213056564, tv_loss: 0.017059851437807083\n",
      "iteration 1873, dc_loss: 0.01582047902047634, tv_loss: 0.017059767618775368\n",
      "iteration 1874, dc_loss: 0.01582040637731552, tv_loss: 0.01705970987677574\n",
      "iteration 1875, dc_loss: 0.015820298343896866, tv_loss: 0.017059601843357086\n",
      "iteration 1876, dc_loss: 0.01582016609609127, tv_loss: 0.0170594472438097\n",
      "iteration 1877, dc_loss: 0.015820033848285675, tv_loss: 0.017059462144970894\n",
      "iteration 1878, dc_loss: 0.01581994630396366, tv_loss: 0.01705947332084179\n",
      "iteration 1879, dc_loss: 0.0158198494464159, tv_loss: 0.017059380188584328\n",
      "iteration 1880, dc_loss: 0.015819761902093887, tv_loss: 0.017059287056326866\n",
      "iteration 1881, dc_loss: 0.015819676220417023, tv_loss: 0.017059223726391792\n",
      "iteration 1882, dc_loss: 0.015819571912288666, tv_loss: 0.01705911196768284\n",
      "iteration 1883, dc_loss: 0.01581949181854725, tv_loss: 0.017058920115232468\n",
      "iteration 1884, dc_loss: 0.01581941358745098, tv_loss: 0.017058810219168663\n",
      "iteration 1885, dc_loss: 0.015819337218999863, tv_loss: 0.0170587170869112\n",
      "iteration 1886, dc_loss: 0.015819242224097252, tv_loss: 0.017058758065104485\n",
      "iteration 1887, dc_loss: 0.015819186344742775, tv_loss: 0.017058486118912697\n",
      "iteration 1888, dc_loss: 0.015819160267710686, tv_loss: 0.017058413475751877\n",
      "iteration 1889, dc_loss: 0.015819115564227104, tv_loss: 0.01705826073884964\n",
      "iteration 1890, dc_loss: 0.015819044783711433, tv_loss: 0.017058108001947403\n",
      "iteration 1891, dc_loss: 0.01581893116235733, tv_loss: 0.017058134078979492\n",
      "iteration 1892, dc_loss: 0.01581880822777748, tv_loss: 0.017058102414011955\n",
      "iteration 1893, dc_loss: 0.015818724408745766, tv_loss: 0.01705792360007763\n",
      "iteration 1894, dc_loss: 0.015818653628230095, tv_loss: 0.017058026045560837\n",
      "iteration 1895, dc_loss: 0.015818526968359947, tv_loss: 0.01705792546272278\n",
      "iteration 1896, dc_loss: 0.015818390995264053, tv_loss: 0.01705777831375599\n",
      "iteration 1897, dc_loss: 0.015818273648619652, tv_loss: 0.017057741060853004\n",
      "iteration 1898, dc_loss: 0.01581818051636219, tv_loss: 0.01705770380795002\n",
      "iteration 1899, dc_loss: 0.015818117186427116, tv_loss: 0.017057638615369797\n",
      "iteration 1900, dc_loss: 0.01581806130707264, tv_loss: 0.017057571560144424\n",
      "iteration 1901, dc_loss: 0.0158180333673954, tv_loss: 0.017057325690984726\n",
      "iteration 1902, dc_loss: 0.01581799052655697, tv_loss: 0.017057064920663834\n",
      "iteration 1903, dc_loss: 0.015817902982234955, tv_loss: 0.017057137563824654\n",
      "iteration 1904, dc_loss: 0.01581781730055809, tv_loss: 0.01705709844827652\n",
      "iteration 1905, dc_loss: 0.015817750245332718, tv_loss: 0.01705699786543846\n",
      "iteration 1906, dc_loss: 0.015817632898688316, tv_loss: 0.017057038843631744\n",
      "iteration 1907, dc_loss: 0.015817489475011826, tv_loss: 0.017056820914149284\n",
      "iteration 1908, dc_loss: 0.01581738144159317, tv_loss: 0.017056791111826897\n",
      "iteration 1909, dc_loss: 0.01581728644669056, tv_loss: 0.017056850716471672\n",
      "iteration 1910, dc_loss: 0.015817198902368546, tv_loss: 0.017056729644536972\n",
      "iteration 1911, dc_loss: 0.015817129984498024, tv_loss: 0.017056601122021675\n",
      "iteration 1912, dc_loss: 0.01581709273159504, tv_loss: 0.01705649308860302\n",
      "iteration 1913, dc_loss: 0.015817074105143547, tv_loss: 0.017056314274668694\n",
      "iteration 1914, dc_loss: 0.015817051753401756, tv_loss: 0.017056124284863472\n",
      "iteration 1915, dc_loss: 0.015817012637853622, tv_loss: 0.017056025564670563\n",
      "iteration 1916, dc_loss: 0.015816939994692802, tv_loss: 0.017056075856089592\n",
      "iteration 1917, dc_loss: 0.015816809609532356, tv_loss: 0.017055697739124298\n",
      "iteration 1918, dc_loss: 0.015816673636436462, tv_loss: 0.017055856063961983\n",
      "iteration 1919, dc_loss: 0.01581655815243721, tv_loss: 0.017055999487638474\n",
      "iteration 1920, dc_loss: 0.01581646129488945, tv_loss: 0.017055770382285118\n",
      "iteration 1921, dc_loss: 0.015816373750567436, tv_loss: 0.017055736854672432\n",
      "iteration 1922, dc_loss: 0.015816273167729378, tv_loss: 0.017055725678801537\n",
      "iteration 1923, dc_loss: 0.015816159546375275, tv_loss: 0.017055731266736984\n",
      "iteration 1924, dc_loss: 0.015816066414117813, tv_loss: 0.017055507749319077\n",
      "iteration 1925, dc_loss: 0.01581599935889244, tv_loss: 0.017055422067642212\n",
      "iteration 1926, dc_loss: 0.01581592671573162, tv_loss: 0.017055468633770943\n",
      "iteration 1927, dc_loss: 0.01581587642431259, tv_loss: 0.017055323347449303\n",
      "iteration 1928, dc_loss: 0.015815842896699905, tv_loss: 0.017055071890354156\n",
      "iteration 1929, dc_loss: 0.015815820544958115, tv_loss: 0.017054863274097443\n",
      "iteration 1930, dc_loss: 0.01581575907766819, tv_loss: 0.01705481857061386\n",
      "iteration 1931, dc_loss: 0.015815693885087967, tv_loss: 0.017054783180356026\n",
      "iteration 1932, dc_loss: 0.015815600752830505, tv_loss: 0.017054416239261627\n",
      "iteration 1933, dc_loss: 0.015815459191799164, tv_loss: 0.01705455780029297\n",
      "iteration 1934, dc_loss: 0.015815334394574165, tv_loss: 0.01705467700958252\n",
      "iteration 1935, dc_loss: 0.015815239399671555, tv_loss: 0.017054570838809013\n",
      "iteration 1936, dc_loss: 0.015815189108252525, tv_loss: 0.017054306343197823\n",
      "iteration 1937, dc_loss: 0.015815159305930138, tv_loss: 0.017054323107004166\n",
      "iteration 1938, dc_loss: 0.015815118327736855, tv_loss: 0.017054308205842972\n",
      "iteration 1939, dc_loss: 0.015815073624253273, tv_loss: 0.017054233700037003\n",
      "iteration 1940, dc_loss: 0.015815021470189095, tv_loss: 0.017054028809070587\n",
      "iteration 1941, dc_loss: 0.01581493206322193, tv_loss: 0.017053905874490738\n",
      "iteration 1942, dc_loss: 0.015814833343029022, tv_loss: 0.017053823918104172\n",
      "iteration 1943, dc_loss: 0.015814756974577904, tv_loss: 0.017053790390491486\n",
      "iteration 1944, dc_loss: 0.01581464149057865, tv_loss: 0.0170537568628788\n",
      "iteration 1945, dc_loss: 0.01581454649567604, tv_loss: 0.017053697258234024\n",
      "iteration 1946, dc_loss: 0.015814442187547684, tv_loss: 0.017053723335266113\n",
      "iteration 1947, dc_loss: 0.015814369544386864, tv_loss: 0.017053741961717606\n",
      "iteration 1948, dc_loss: 0.01581430621445179, tv_loss: 0.017053483054041862\n",
      "iteration 1949, dc_loss: 0.01581423357129097, tv_loss: 0.017053179442882538\n",
      "iteration 1950, dc_loss: 0.015814196318387985, tv_loss: 0.017053257673978806\n",
      "iteration 1951, dc_loss: 0.015814170241355896, tv_loss: 0.01705322414636612\n",
      "iteration 1952, dc_loss: 0.015814106911420822, tv_loss: 0.017053065821528435\n",
      "iteration 1953, dc_loss: 0.01581401377916336, tv_loss: 0.017052894458174706\n",
      "iteration 1954, dc_loss: 0.015813883394002914, tv_loss: 0.01705288514494896\n",
      "iteration 1955, dc_loss: 0.01581375114619732, tv_loss: 0.017052888870239258\n",
      "iteration 1956, dc_loss: 0.015813659876585007, tv_loss: 0.017052896320819855\n",
      "iteration 1957, dc_loss: 0.015813598409295082, tv_loss: 0.017052873969078064\n",
      "iteration 1958, dc_loss: 0.015813585370779037, tv_loss: 0.01705262064933777\n",
      "iteration 1959, dc_loss: 0.01581357978284359, tv_loss: 0.01705263741314411\n",
      "iteration 1960, dc_loss: 0.01581357978284359, tv_loss: 0.017052389681339264\n",
      "iteration 1961, dc_loss: 0.015813522040843964, tv_loss: 0.017052194103598595\n",
      "iteration 1962, dc_loss: 0.015813423320651054, tv_loss: 0.017052171751856804\n",
      "iteration 1963, dc_loss: 0.01581330969929695, tv_loss: 0.01705232635140419\n",
      "iteration 1964, dc_loss: 0.01581321284174919, tv_loss: 0.017052296549081802\n",
      "iteration 1965, dc_loss: 0.01581309176981449, tv_loss: 0.017051994800567627\n",
      "iteration 1966, dc_loss: 0.015812978148460388, tv_loss: 0.017052089795470238\n",
      "iteration 1967, dc_loss: 0.01581289991736412, tv_loss: 0.017052147537469864\n",
      "iteration 1968, dc_loss: 0.01581285521388054, tv_loss: 0.017051873728632927\n",
      "iteration 1969, dc_loss: 0.01581280678510666, tv_loss: 0.017051883041858673\n",
      "iteration 1970, dc_loss: 0.015812700614333153, tv_loss: 0.01705189235508442\n",
      "iteration 1971, dc_loss: 0.01581261120736599, tv_loss: 0.017051799222826958\n",
      "iteration 1972, dc_loss: 0.01581255905330181, tv_loss: 0.01705174334347248\n",
      "iteration 1973, dc_loss: 0.015812519937753677, tv_loss: 0.017051633447408676\n",
      "iteration 1974, dc_loss: 0.015812469646334648, tv_loss: 0.01705145090818405\n",
      "iteration 1975, dc_loss: 0.015812406316399574, tv_loss: 0.017051441594958305\n",
      "iteration 1976, dc_loss: 0.015812300145626068, tv_loss: 0.017051400616765022\n",
      "iteration 1977, dc_loss: 0.01581220142543316, tv_loss: 0.017051449045538902\n",
      "iteration 1978, dc_loss: 0.01581207849085331, tv_loss: 0.017051463946700096\n",
      "iteration 1979, dc_loss: 0.015811951830983162, tv_loss: 0.01705138012766838\n",
      "iteration 1980, dc_loss: 0.01581188291311264, tv_loss: 0.017051231116056442\n",
      "iteration 1981, dc_loss: 0.015811840072274208, tv_loss: 0.017051126807928085\n",
      "iteration 1982, dc_loss: 0.015811843797564507, tv_loss: 0.01705092005431652\n",
      "iteration 1983, dc_loss: 0.015811879187822342, tv_loss: 0.01705075427889824\n",
      "iteration 1984, dc_loss: 0.015811864286661148, tv_loss: 0.01705046184360981\n",
      "iteration 1985, dc_loss: 0.015811793506145477, tv_loss: 0.017050547525286674\n",
      "iteration 1986, dc_loss: 0.015811672434210777, tv_loss: 0.01705045811831951\n",
      "iteration 1987, dc_loss: 0.01581151969730854, tv_loss: 0.017050618305802345\n",
      "iteration 1988, dc_loss: 0.015811385586857796, tv_loss: 0.017050594091415405\n",
      "iteration 1989, dc_loss: 0.015811312943696976, tv_loss: 0.017050329595804214\n",
      "iteration 1990, dc_loss: 0.01581125147640705, tv_loss: 0.01705026440322399\n",
      "iteration 1991, dc_loss: 0.01581120304763317, tv_loss: 0.017050163820385933\n",
      "iteration 1992, dc_loss: 0.015811171382665634, tv_loss: 0.017050115391612053\n",
      "iteration 1993, dc_loss: 0.015811121091246605, tv_loss: 0.017050007358193398\n",
      "iteration 1994, dc_loss: 0.015811046585440636, tv_loss: 0.017049936577677727\n",
      "iteration 1995, dc_loss: 0.015810957178473473, tv_loss: 0.017049850896000862\n",
      "iteration 1996, dc_loss: 0.015810871496796608, tv_loss: 0.01704990863800049\n",
      "iteration 1997, dc_loss: 0.015810782089829445, tv_loss: 0.01704980991780758\n",
      "iteration 1998, dc_loss: 0.015810739248991013, tv_loss: 0.01704978756606579\n",
      "iteration 1999, dc_loss: 0.015810679644346237, tv_loss: 0.01704961434006691\n",
      "iteration 2000, dc_loss: 0.015810618177056313, tv_loss: 0.017049355432391167\n",
      "iteration 2001, dc_loss: 0.01581052877008915, tv_loss: 0.017049401998519897\n",
      "iteration 2002, dc_loss: 0.01581043004989624, tv_loss: 0.017049504444003105\n",
      "iteration 2003, dc_loss: 0.015810376033186913, tv_loss: 0.017049424350261688\n",
      "iteration 2004, dc_loss: 0.015810320153832436, tv_loss: 0.017049230635166168\n",
      "iteration 2005, dc_loss: 0.015810251235961914, tv_loss: 0.017049137502908707\n",
      "iteration 2006, dc_loss: 0.015810199081897736, tv_loss: 0.01704918034374714\n",
      "iteration 2007, dc_loss: 0.015810150653123856, tv_loss: 0.01704925112426281\n",
      "iteration 2008, dc_loss: 0.015810081735253334, tv_loss: 0.017049001529812813\n",
      "iteration 2009, dc_loss: 0.01581001468002796, tv_loss: 0.01704876683652401\n",
      "iteration 2010, dc_loss: 0.015809932723641396, tv_loss: 0.017048804089426994\n",
      "iteration 2011, dc_loss: 0.01580986939370632, tv_loss: 0.01704891212284565\n",
      "iteration 2012, dc_loss: 0.0158097967505455, tv_loss: 0.017048902809619904\n",
      "iteration 2013, dc_loss: 0.015809688717126846, tv_loss: 0.017048699781298637\n",
      "iteration 2014, dc_loss: 0.015809595584869385, tv_loss: 0.017048606649041176\n",
      "iteration 2015, dc_loss: 0.015809545293450356, tv_loss: 0.01704859733581543\n",
      "iteration 2016, dc_loss: 0.015809504315257072, tv_loss: 0.017048513516783714\n",
      "iteration 2017, dc_loss: 0.015809476375579834, tv_loss: 0.017048301175236702\n",
      "iteration 2018, dc_loss: 0.015809444710612297, tv_loss: 0.01704826019704342\n",
      "iteration 2019, dc_loss: 0.015809381380677223, tv_loss: 0.017048045992851257\n",
      "iteration 2020, dc_loss: 0.015809305012226105, tv_loss: 0.017048103734850883\n",
      "iteration 2021, dc_loss: 0.015809210017323494, tv_loss: 0.017047999426722527\n",
      "iteration 2022, dc_loss: 0.01580912061035633, tv_loss: 0.017047898843884468\n",
      "iteration 2023, dc_loss: 0.0158090703189373, tv_loss: 0.017047766596078873\n",
      "iteration 2024, dc_loss: 0.015809016302227974, tv_loss: 0.017047809436917305\n",
      "iteration 2025, dc_loss: 0.01580895110964775, tv_loss: 0.01704784668982029\n",
      "iteration 2026, dc_loss: 0.015808837488293648, tv_loss: 0.017047928646206856\n",
      "iteration 2027, dc_loss: 0.015808697789907455, tv_loss: 0.01704767346382141\n",
      "iteration 2028, dc_loss: 0.015808576717972755, tv_loss: 0.017047550529241562\n",
      "iteration 2029, dc_loss: 0.015808505937457085, tv_loss: 0.017047571018338203\n",
      "iteration 2030, dc_loss: 0.015808451920747757, tv_loss: 0.017047619447112083\n",
      "iteration 2031, dc_loss: 0.015808405354619026, tv_loss: 0.017047328874468803\n",
      "iteration 2032, dc_loss: 0.015808401629328728, tv_loss: 0.017047250643372536\n",
      "iteration 2033, dc_loss: 0.01580837368965149, tv_loss: 0.017047222703695297\n",
      "iteration 2034, dc_loss: 0.01580837182700634, tv_loss: 0.017047056928277016\n",
      "iteration 2035, dc_loss: 0.01580834947526455, tv_loss: 0.017046919092535973\n",
      "iteration 2036, dc_loss: 0.01580829918384552, tv_loss: 0.017046833410859108\n",
      "iteration 2037, dc_loss: 0.01580822467803955, tv_loss: 0.01704656332731247\n",
      "iteration 2038, dc_loss: 0.015808146446943283, tv_loss: 0.017046699300408363\n",
      "iteration 2039, dc_loss: 0.01580803468823433, tv_loss: 0.017046619206666946\n",
      "iteration 2040, dc_loss: 0.015807924792170525, tv_loss: 0.017046619206666946\n",
      "iteration 2041, dc_loss: 0.01580779254436493, tv_loss: 0.01704656518995762\n",
      "iteration 2042, dc_loss: 0.015807699412107468, tv_loss: 0.017046622931957245\n",
      "iteration 2043, dc_loss: 0.01580764539539814, tv_loss: 0.01704651303589344\n",
      "iteration 2044, dc_loss: 0.015807639807462692, tv_loss: 0.01704626902937889\n",
      "iteration 2045, dc_loss: 0.01580766960978508, tv_loss: 0.017046183347702026\n",
      "iteration 2046, dc_loss: 0.015807686373591423, tv_loss: 0.0170461256057024\n",
      "iteration 2047, dc_loss: 0.015807682648301125, tv_loss: 0.017045995220541954\n",
      "iteration 2048, dc_loss: 0.01580759882926941, tv_loss: 0.017045777291059494\n",
      "iteration 2049, dc_loss: 0.015807457268238068, tv_loss: 0.01704588159918785\n",
      "iteration 2050, dc_loss: 0.01580732688307762, tv_loss: 0.017045995220541954\n",
      "iteration 2051, dc_loss: 0.01580723375082016, tv_loss: 0.01704595237970352\n",
      "iteration 2052, dc_loss: 0.015807155519723892, tv_loss: 0.017045872285962105\n",
      "iteration 2053, dc_loss: 0.015807118266820908, tv_loss: 0.017045840620994568\n",
      "iteration 2054, dc_loss: 0.015807131305336952, tv_loss: 0.01704568788409233\n",
      "iteration 2055, dc_loss: 0.015807131305336952, tv_loss: 0.017045507207512856\n",
      "iteration 2056, dc_loss: 0.015807077288627625, tv_loss: 0.0170454028993845\n",
      "iteration 2057, dc_loss: 0.015806980431079865, tv_loss: 0.017045382410287857\n",
      "iteration 2058, dc_loss: 0.015806879848241806, tv_loss: 0.017045272514224052\n",
      "iteration 2059, dc_loss: 0.015806769952178, tv_loss: 0.01704540103673935\n",
      "iteration 2060, dc_loss: 0.01580667309463024, tv_loss: 0.017045289278030396\n",
      "iteration 2061, dc_loss: 0.01580655574798584, tv_loss: 0.017045417800545692\n",
      "iteration 2062, dc_loss: 0.01580646075308323, tv_loss: 0.0170452781021595\n",
      "iteration 2063, dc_loss: 0.01580643281340599, tv_loss: 0.017045006155967712\n",
      "iteration 2064, dc_loss: 0.015806440263986588, tv_loss: 0.01704486273229122\n",
      "iteration 2065, dc_loss: 0.01580643653869629, tv_loss: 0.017044713720679283\n",
      "iteration 2066, dc_loss: 0.015806399285793304, tv_loss: 0.01704460009932518\n",
      "iteration 2067, dc_loss: 0.015806308016180992, tv_loss: 0.01704481616616249\n",
      "iteration 2068, dc_loss: 0.015806186944246292, tv_loss: 0.017044657841324806\n",
      "iteration 2069, dc_loss: 0.015806103125214577, tv_loss: 0.01704491302371025\n",
      "iteration 2070, dc_loss: 0.015806015580892563, tv_loss: 0.01704457774758339\n",
      "iteration 2071, dc_loss: 0.015805957838892937, tv_loss: 0.017044540494680405\n",
      "iteration 2072, dc_loss: 0.015805905684828758, tv_loss: 0.01704484224319458\n",
      "iteration 2073, dc_loss: 0.015805888921022415, tv_loss: 0.017044510692358017\n",
      "iteration 2074, dc_loss: 0.015805870294570923, tv_loss: 0.017044290900230408\n",
      "iteration 2075, dc_loss: 0.015805842354893684, tv_loss: 0.01704445295035839\n",
      "iteration 2076, dc_loss: 0.01580577902495861, tv_loss: 0.0170443058013916\n",
      "iteration 2077, dc_loss: 0.015805726870894432, tv_loss: 0.017044253647327423\n",
      "iteration 2078, dc_loss: 0.01580568589270115, tv_loss: 0.017044274136424065\n",
      "iteration 2079, dc_loss: 0.015805663540959358, tv_loss: 0.0170440711081028\n",
      "iteration 2080, dc_loss: 0.015805624425411224, tv_loss: 0.017044035717844963\n",
      "iteration 2081, dc_loss: 0.01580553688108921, tv_loss: 0.017043884843587875\n",
      "iteration 2082, dc_loss: 0.015805402770638466, tv_loss: 0.017043842002749443\n",
      "iteration 2083, dc_loss: 0.015805289149284363, tv_loss: 0.017043739557266235\n",
      "iteration 2084, dc_loss: 0.015805207192897797, tv_loss: 0.017043782398104668\n",
      "iteration 2085, dc_loss: 0.01580512896180153, tv_loss: 0.017043905332684517\n",
      "iteration 2086, dc_loss: 0.015805084258317947, tv_loss: 0.017043696716427803\n",
      "iteration 2087, dc_loss: 0.01580505259335041, tv_loss: 0.017043743282556534\n",
      "iteration 2088, dc_loss: 0.015805061906576157, tv_loss: 0.017043616622686386\n",
      "iteration 2089, dc_loss: 0.015805058181285858, tv_loss: 0.017043275758624077\n",
      "iteration 2090, dc_loss: 0.01580500230193138, tv_loss: 0.01704336889088154\n",
      "iteration 2091, dc_loss: 0.015804963186383247, tv_loss: 0.017043285071849823\n",
      "iteration 2092, dc_loss: 0.01580490544438362, tv_loss: 0.01704304851591587\n",
      "iteration 2093, dc_loss: 0.01580483838915825, tv_loss: 0.017042994499206543\n",
      "iteration 2094, dc_loss: 0.015804756432771683, tv_loss: 0.01704292744398117\n",
      "iteration 2095, dc_loss: 0.015804661437869072, tv_loss: 0.017043063417077065\n",
      "iteration 2096, dc_loss: 0.015804588794708252, tv_loss: 0.01704310066998005\n",
      "iteration 2097, dc_loss: 0.015804514288902283, tv_loss: 0.01704290136694908\n",
      "iteration 2098, dc_loss: 0.015804437920451164, tv_loss: 0.01704282872378826\n",
      "iteration 2099, dc_loss: 0.015804369002580643, tv_loss: 0.017042888328433037\n",
      "iteration 2100, dc_loss: 0.015804333612322807, tv_loss: 0.01704288460314274\n",
      "iteration 2101, dc_loss: 0.015804318711161613, tv_loss: 0.01704285480082035\n",
      "iteration 2102, dc_loss: 0.015804294496774673, tv_loss: 0.01704266667366028\n",
      "iteration 2103, dc_loss: 0.015804266557097435, tv_loss: 0.017042558640241623\n",
      "iteration 2104, dc_loss: 0.015804197639226913, tv_loss: 0.01704253815114498\n",
      "iteration 2105, dc_loss: 0.015804117545485497, tv_loss: 0.017042502760887146\n",
      "iteration 2106, dc_loss: 0.01580401510000229, tv_loss: 0.017042389139533043\n",
      "iteration 2107, dc_loss: 0.015803907066583633, tv_loss: 0.01704227551817894\n",
      "iteration 2108, dc_loss: 0.015803784132003784, tv_loss: 0.017042281106114388\n",
      "iteration 2109, dc_loss: 0.015803702175617218, tv_loss: 0.01704244688153267\n",
      "iteration 2110, dc_loss: 0.015803657472133636, tv_loss: 0.017042402178049088\n",
      "iteration 2111, dc_loss: 0.015803644433617592, tv_loss: 0.017042307183146477\n",
      "iteration 2112, dc_loss: 0.015803644433617592, tv_loss: 0.01704198494553566\n",
      "iteration 2113, dc_loss: 0.015803618356585503, tv_loss: 0.01704186201095581\n",
      "iteration 2114, dc_loss: 0.015803594142198563, tv_loss: 0.01704186573624611\n",
      "iteration 2115, dc_loss: 0.01580355316400528, tv_loss: 0.017041949555277824\n",
      "iteration 2116, dc_loss: 0.015803493559360504, tv_loss: 0.017041673883795738\n",
      "iteration 2117, dc_loss: 0.01580340974032879, tv_loss: 0.017041698098182678\n",
      "iteration 2118, dc_loss: 0.0158033836632967, tv_loss: 0.017041727900505066\n",
      "iteration 2119, dc_loss: 0.01580335758626461, tv_loss: 0.017041726037859917\n",
      "iteration 2120, dc_loss: 0.015803303569555283, tv_loss: 0.017041431739926338\n",
      "iteration 2121, dc_loss: 0.015803219750523567, tv_loss: 0.017041439190506935\n",
      "iteration 2122, dc_loss: 0.015803111717104912, tv_loss: 0.017041509971022606\n",
      "iteration 2123, dc_loss: 0.0158030167222023, tv_loss: 0.017041578888893127\n",
      "iteration 2124, dc_loss: 0.015802953392267227, tv_loss: 0.017041506245732307\n",
      "iteration 2125, dc_loss: 0.015802858397364616, tv_loss: 0.01704135164618492\n",
      "iteration 2126, dc_loss: 0.015802787616848946, tv_loss: 0.017041290178894997\n",
      "iteration 2127, dc_loss: 0.015802718698978424, tv_loss: 0.017041269689798355\n",
      "iteration 2128, dc_loss: 0.015802687034010887, tv_loss: 0.01704132743179798\n",
      "iteration 2129, dc_loss: 0.015802642330527306, tv_loss: 0.017041243612766266\n",
      "iteration 2130, dc_loss: 0.015802597627043724, tv_loss: 0.017041150480508804\n",
      "iteration 2131, dc_loss: 0.0158025361597538, tv_loss: 0.017041010782122612\n",
      "iteration 2132, dc_loss: 0.01580250822007656, tv_loss: 0.017040889710187912\n",
      "iteration 2133, dc_loss: 0.015802480280399323, tv_loss: 0.01704096794128418\n",
      "iteration 2134, dc_loss: 0.01580244116485119, tv_loss: 0.017040740698575974\n",
      "iteration 2135, dc_loss: 0.015802377834916115, tv_loss: 0.017040614038705826\n",
      "iteration 2136, dc_loss: 0.015802297741174698, tv_loss: 0.0170406773686409\n",
      "iteration 2137, dc_loss: 0.015802208334207535, tv_loss: 0.017040597274899483\n",
      "iteration 2138, dc_loss: 0.015802137553691864, tv_loss: 0.017040548846125603\n",
      "iteration 2139, dc_loss: 0.015802068635821342, tv_loss: 0.017040571197867393\n",
      "iteration 2140, dc_loss: 0.01580202579498291, tv_loss: 0.01704055443406105\n",
      "iteration 2141, dc_loss: 0.01580202579498291, tv_loss: 0.01704034022986889\n",
      "iteration 2142, dc_loss: 0.01580199785530567, tv_loss: 0.017040207982063293\n",
      "iteration 2143, dc_loss: 0.01580195687711239, tv_loss: 0.017040280625224113\n",
      "iteration 2144, dc_loss: 0.01580190286040306, tv_loss: 0.017040232196450233\n",
      "iteration 2145, dc_loss: 0.015801850706338882, tv_loss: 0.01703997701406479\n",
      "iteration 2146, dc_loss: 0.015801791101694107, tv_loss: 0.01703989878296852\n",
      "iteration 2147, dc_loss: 0.01580171100795269, tv_loss: 0.017039990052580833\n",
      "iteration 2148, dc_loss: 0.015801655128598213, tv_loss: 0.017039967700839043\n",
      "iteration 2149, dc_loss: 0.015801602974534035, tv_loss: 0.017039813101291656\n",
      "iteration 2150, dc_loss: 0.015801556408405304, tv_loss: 0.017039624974131584\n",
      "iteration 2151, dc_loss: 0.015801481902599335, tv_loss: 0.017039630562067032\n",
      "iteration 2152, dc_loss: 0.01580141857266426, tv_loss: 0.017039788886904716\n",
      "iteration 2153, dc_loss: 0.0158013254404068, tv_loss: 0.01703963242471218\n",
      "iteration 2154, dc_loss: 0.01580124720931053, tv_loss: 0.01703936606645584\n",
      "iteration 2155, dc_loss: 0.015801208093762398, tv_loss: 0.017039349302649498\n",
      "iteration 2156, dc_loss: 0.015801213681697845, tv_loss: 0.017039382830262184\n",
      "iteration 2157, dc_loss: 0.015801221132278442, tv_loss: 0.017039258033037186\n",
      "iteration 2158, dc_loss: 0.015801187604665756, tv_loss: 0.01703900843858719\n",
      "iteration 2159, dc_loss: 0.015801139175891876, tv_loss: 0.017039015889167786\n",
      "iteration 2160, dc_loss: 0.015801072120666504, tv_loss: 0.01703900471329689\n",
      "iteration 2161, dc_loss: 0.015800999477505684, tv_loss: 0.017038928344845772\n",
      "iteration 2162, dc_loss: 0.015800945460796356, tv_loss: 0.01703879050910473\n",
      "iteration 2163, dc_loss: 0.01580088958144188, tv_loss: 0.01703876629471779\n",
      "iteration 2164, dc_loss: 0.015800824388861656, tv_loss: 0.01703893393278122\n",
      "iteration 2165, dc_loss: 0.015800755470991135, tv_loss: 0.01703888364136219\n",
      "iteration 2166, dc_loss: 0.015800723806023598, tv_loss: 0.017038803547620773\n",
      "iteration 2167, dc_loss: 0.015800708904862404, tv_loss: 0.01703864149749279\n",
      "iteration 2168, dc_loss: 0.015800684690475464, tv_loss: 0.01703862100839615\n",
      "iteration 2169, dc_loss: 0.015800660476088524, tv_loss: 0.017038557678461075\n",
      "iteration 2170, dc_loss: 0.01580061949789524, tv_loss: 0.017038501799106598\n",
      "iteration 2171, dc_loss: 0.015800563618540764, tv_loss: 0.017038365826010704\n",
      "iteration 2172, dc_loss: 0.01580049656331539, tv_loss: 0.017038537189364433\n",
      "iteration 2173, dc_loss: 0.015800412744283676, tv_loss: 0.017038332298398018\n",
      "iteration 2174, dc_loss: 0.01580038107931614, tv_loss: 0.017038196325302124\n",
      "iteration 2175, dc_loss: 0.01580032892525196, tv_loss: 0.017038358375430107\n",
      "iteration 2176, dc_loss: 0.01580027863383293, tv_loss: 0.017038339748978615\n",
      "iteration 2177, dc_loss: 0.01580018363893032, tv_loss: 0.01703818328678608\n",
      "iteration 2178, dc_loss: 0.01580006442964077, tv_loss: 0.01703810505568981\n",
      "iteration 2179, dc_loss: 0.015799982473254204, tv_loss: 0.017038147896528244\n",
      "iteration 2180, dc_loss: 0.01579989120364189, tv_loss: 0.017038125544786453\n",
      "iteration 2181, dc_loss: 0.01579979434609413, tv_loss: 0.01703830622136593\n",
      "iteration 2182, dc_loss: 0.015799706801772118, tv_loss: 0.01703832857310772\n",
      "iteration 2183, dc_loss: 0.01579970307648182, tv_loss: 0.017037857323884964\n",
      "iteration 2184, dc_loss: 0.015799757093191147, tv_loss: 0.017037710174918175\n",
      "iteration 2185, dc_loss: 0.015799812972545624, tv_loss: 0.01703770086169243\n",
      "iteration 2186, dc_loss: 0.01579982601106167, tv_loss: 0.017037684097886086\n",
      "iteration 2187, dc_loss: 0.015799816697835922, tv_loss: 0.017037371173501015\n",
      "iteration 2188, dc_loss: 0.015799758955836296, tv_loss: 0.017037266865372658\n",
      "iteration 2189, dc_loss: 0.015799667686223984, tv_loss: 0.017037365585565567\n",
      "iteration 2190, dc_loss: 0.015799567103385925, tv_loss: 0.017037590965628624\n",
      "iteration 2191, dc_loss: 0.01579943858087063, tv_loss: 0.01703740656375885\n",
      "iteration 2192, dc_loss: 0.015799324959516525, tv_loss: 0.017037270590662956\n",
      "iteration 2193, dc_loss: 0.01579926162958145, tv_loss: 0.017037350684404373\n",
      "iteration 2194, dc_loss: 0.015799250453710556, tv_loss: 0.017037510871887207\n",
      "iteration 2195, dc_loss: 0.01579921506345272, tv_loss: 0.017037350684404373\n",
      "iteration 2196, dc_loss: 0.015799228101968765, tv_loss: 0.0170370452105999\n",
      "iteration 2197, dc_loss: 0.015799233689904213, tv_loss: 0.01703692227602005\n",
      "iteration 2198, dc_loss: 0.01579919457435608, tv_loss: 0.017036940902471542\n",
      "iteration 2199, dc_loss: 0.015799114480614662, tv_loss: 0.017036909237504005\n",
      "iteration 2200, dc_loss: 0.01579901948571205, tv_loss: 0.017036790028214455\n",
      "iteration 2201, dc_loss: 0.015798911452293396, tv_loss: 0.01703680492937565\n",
      "iteration 2202, dc_loss: 0.01579880714416504, tv_loss: 0.017036832869052887\n",
      "iteration 2203, dc_loss: 0.01579875499010086, tv_loss: 0.017036650329828262\n",
      "iteration 2204, dc_loss: 0.015798741951584816, tv_loss: 0.017036842182278633\n",
      "iteration 2205, dc_loss: 0.015798719599843025, tv_loss: 0.01703658699989319\n",
      "iteration 2206, dc_loss: 0.01579870656132698, tv_loss: 0.017036497592926025\n",
      "iteration 2207, dc_loss: 0.01579870469868183, tv_loss: 0.01703653298318386\n",
      "iteration 2208, dc_loss: 0.015798697248101234, tv_loss: 0.01703638955950737\n",
      "iteration 2209, dc_loss: 0.01579868793487549, tv_loss: 0.01703629083931446\n",
      "iteration 2210, dc_loss: 0.015798650681972504, tv_loss: 0.017036236822605133\n",
      "iteration 2211, dc_loss: 0.01579858735203743, tv_loss: 0.017036113888025284\n",
      "iteration 2212, dc_loss: 0.01579851470887661, tv_loss: 0.017035985365509987\n",
      "iteration 2213, dc_loss: 0.015798388049006462, tv_loss: 0.01703612692654133\n",
      "iteration 2214, dc_loss: 0.015798283740878105, tv_loss: 0.017036210745573044\n",
      "iteration 2215, dc_loss: 0.015798216685652733, tv_loss: 0.017036063596606255\n",
      "iteration 2216, dc_loss: 0.01579822227358818, tv_loss: 0.017035849392414093\n",
      "iteration 2217, dc_loss: 0.01579822227358818, tv_loss: 0.017035700380802155\n",
      "iteration 2218, dc_loss: 0.01579819805920124, tv_loss: 0.017035720869898796\n",
      "iteration 2219, dc_loss: 0.0157981738448143, tv_loss: 0.01703573577105999\n",
      "iteration 2220, dc_loss: 0.015798144042491913, tv_loss: 0.017035391181707382\n",
      "iteration 2221, dc_loss: 0.015798084437847137, tv_loss: 0.01703549362719059\n",
      "iteration 2222, dc_loss: 0.01579800434410572, tv_loss: 0.01703565940260887\n",
      "iteration 2223, dc_loss: 0.015797918662428856, tv_loss: 0.01703566312789917\n",
      "iteration 2224, dc_loss: 0.015797831118106842, tv_loss: 0.017035672441124916\n",
      "iteration 2225, dc_loss: 0.015797754749655724, tv_loss: 0.01703563705086708\n",
      "iteration 2226, dc_loss: 0.015797706320881844, tv_loss: 0.017035581171512604\n",
      "iteration 2227, dc_loss: 0.0157976895570755, tv_loss: 0.01703566126525402\n",
      "iteration 2228, dc_loss: 0.01579766906797886, tv_loss: 0.01703535206615925\n",
      "iteration 2229, dc_loss: 0.015797659754753113, tv_loss: 0.01703507825732231\n",
      "iteration 2230, dc_loss: 0.01579761691391468, tv_loss: 0.01703520491719246\n",
      "iteration 2231, dc_loss: 0.015797553583979607, tv_loss: 0.01703541912138462\n",
      "iteration 2232, dc_loss: 0.015797480940818787, tv_loss: 0.017035117372870445\n",
      "iteration 2233, dc_loss: 0.015797432512044907, tv_loss: 0.017034834250807762\n",
      "iteration 2234, dc_loss: 0.015797389671206474, tv_loss: 0.01703507825732231\n",
      "iteration 2235, dc_loss: 0.015797335654497147, tv_loss: 0.017035117372870445\n",
      "iteration 2236, dc_loss: 0.015797266736626625, tv_loss: 0.017035100609064102\n",
      "iteration 2237, dc_loss: 0.015797210857272148, tv_loss: 0.017034899443387985\n",
      "iteration 2238, dc_loss: 0.01579716056585312, tv_loss: 0.01703479513525963\n",
      "iteration 2239, dc_loss: 0.01579713076353073, tv_loss: 0.017035042867064476\n",
      "iteration 2240, dc_loss: 0.015797138214111328, tv_loss: 0.01703472249209881\n",
      "iteration 2241, dc_loss: 0.015797115862369537, tv_loss: 0.017034415155649185\n",
      "iteration 2242, dc_loss: 0.015797067433595657, tv_loss: 0.017034510150551796\n",
      "iteration 2243, dc_loss: 0.01579701155424118, tv_loss: 0.017034543678164482\n",
      "iteration 2244, dc_loss: 0.015796959400177002, tv_loss: 0.017034459859132767\n",
      "iteration 2245, dc_loss: 0.015796886757016182, tv_loss: 0.017034431919455528\n",
      "iteration 2246, dc_loss: 0.01579682156443596, tv_loss: 0.017034363001585007\n",
      "iteration 2247, dc_loss: 0.015796754509210587, tv_loss: 0.01703433319926262\n",
      "iteration 2248, dc_loss: 0.015796717256307602, tv_loss: 0.017034385353326797\n",
      "iteration 2249, dc_loss: 0.015796691179275513, tv_loss: 0.017034387215971947\n",
      "iteration 2250, dc_loss: 0.015796644613146782, tv_loss: 0.0170343779027462\n",
      "iteration 2251, dc_loss: 0.015796596184372902, tv_loss: 0.01703427918255329\n",
      "iteration 2252, dc_loss: 0.015796590596437454, tv_loss: 0.017034057527780533\n",
      "iteration 2253, dc_loss: 0.01579657569527626, tv_loss: 0.017034070566296577\n",
      "iteration 2254, dc_loss: 0.015796583145856857, tv_loss: 0.017033882439136505\n",
      "iteration 2255, dc_loss: 0.015796594321727753, tv_loss: 0.017033694311976433\n",
      "iteration 2256, dc_loss: 0.015796562656760216, tv_loss: 0.01703374646604061\n",
      "iteration 2257, dc_loss: 0.015796473249793053, tv_loss: 0.01703362911939621\n",
      "iteration 2258, dc_loss: 0.01579633727669716, tv_loss: 0.017033610492944717\n",
      "iteration 2259, dc_loss: 0.015796201303601265, tv_loss: 0.01703382469713688\n",
      "iteration 2260, dc_loss: 0.015796102583408356, tv_loss: 0.017033951357007027\n",
      "iteration 2261, dc_loss: 0.015796054154634476, tv_loss: 0.017033610492944717\n",
      "iteration 2262, dc_loss: 0.015796013176441193, tv_loss: 0.017033521085977554\n",
      "iteration 2263, dc_loss: 0.01579602248966694, tv_loss: 0.017033515498042107\n",
      "iteration 2264, dc_loss: 0.015796028077602386, tv_loss: 0.017033468931913376\n",
      "iteration 2265, dc_loss: 0.015796026214957237, tv_loss: 0.017033303156495094\n",
      "iteration 2266, dc_loss: 0.015795987099409103, tv_loss: 0.01703324168920517\n",
      "iteration 2267, dc_loss: 0.015795951709151268, tv_loss: 0.01703311689198017\n",
      "iteration 2268, dc_loss: 0.01579590141773224, tv_loss: 0.017033100128173828\n",
      "iteration 2269, dc_loss: 0.015795890241861343, tv_loss: 0.01703309454023838\n",
      "iteration 2270, dc_loss: 0.01579587534070015, tv_loss: 0.017032966017723083\n",
      "iteration 2271, dc_loss: 0.015795819461345673, tv_loss: 0.01703289896249771\n",
      "iteration 2272, dc_loss: 0.015795741230249405, tv_loss: 0.01703302375972271\n",
      "iteration 2273, dc_loss: 0.015795700252056122, tv_loss: 0.017032844945788383\n",
      "iteration 2274, dc_loss: 0.015795674175024033, tv_loss: 0.01703277975320816\n",
      "iteration 2275, dc_loss: 0.01579560898244381, tv_loss: 0.017032761126756668\n",
      "iteration 2276, dc_loss: 0.015795577317476273, tv_loss: 0.01703270711004734\n",
      "iteration 2277, dc_loss: 0.01579553261399269, tv_loss: 0.017032505944371223\n",
      "iteration 2278, dc_loss: 0.015795445069670677, tv_loss: 0.017032627016305923\n",
      "iteration 2279, dc_loss: 0.015795346349477768, tv_loss: 0.017032720148563385\n",
      "iteration 2280, dc_loss: 0.015795281156897545, tv_loss: 0.017032606527209282\n",
      "iteration 2281, dc_loss: 0.015795299783349037, tv_loss: 0.0170324444770813\n",
      "iteration 2282, dc_loss: 0.015795297920703888, tv_loss: 0.01703244261443615\n",
      "iteration 2283, dc_loss: 0.015795260667800903, tv_loss: 0.017032455652952194\n",
      "iteration 2284, dc_loss: 0.015795229002833366, tv_loss: 0.01703239232301712\n",
      "iteration 2285, dc_loss: 0.015795180574059486, tv_loss: 0.017032256349921227\n",
      "iteration 2286, dc_loss: 0.015795109793543816, tv_loss: 0.01703241840004921\n",
      "iteration 2287, dc_loss: 0.01579505018889904, tv_loss: 0.017032455652952194\n",
      "iteration 2288, dc_loss: 0.015795040875673294, tv_loss: 0.017032260075211525\n",
      "iteration 2289, dc_loss: 0.015795022249221802, tv_loss: 0.017032070085406303\n",
      "iteration 2290, dc_loss: 0.01579500176012516, tv_loss: 0.017032209783792496\n",
      "iteration 2291, dc_loss: 0.015794949606060982, tv_loss: 0.01703212596476078\n",
      "iteration 2292, dc_loss: 0.015794873237609863, tv_loss: 0.01703191176056862\n",
      "iteration 2293, dc_loss: 0.015794798731803894, tv_loss: 0.01703198440372944\n",
      "iteration 2294, dc_loss: 0.015794720500707626, tv_loss: 0.017032038420438766\n",
      "iteration 2295, dc_loss: 0.015794657170772552, tv_loss: 0.017031988129019737\n",
      "iteration 2296, dc_loss: 0.015794577077031136, tv_loss: 0.017031757161021233\n",
      "iteration 2297, dc_loss: 0.015794528648257256, tv_loss: 0.01703176274895668\n",
      "iteration 2298, dc_loss: 0.015794511884450912, tv_loss: 0.017031749710440636\n",
      "iteration 2299, dc_loss: 0.01579451560974121, tv_loss: 0.017031745985150337\n",
      "iteration 2300, dc_loss: 0.015794536098837852, tv_loss: 0.01703163981437683\n",
      "iteration 2301, dc_loss: 0.01579454354941845, tv_loss: 0.01703150011599064\n",
      "iteration 2302, dc_loss: 0.015794498845934868, tv_loss: 0.017031392082571983\n",
      "iteration 2303, dc_loss: 0.01579444855451584, tv_loss: 0.017031293362379074\n",
      "iteration 2304, dc_loss: 0.015794411301612854, tv_loss: 0.017031246796250343\n",
      "iteration 2305, dc_loss: 0.015794387087225914, tv_loss: 0.01703130081295967\n",
      "iteration 2306, dc_loss: 0.015794342383742332, tv_loss: 0.01703128032386303\n",
      "iteration 2307, dc_loss: 0.01579427160322666, tv_loss: 0.017031272873282433\n",
      "iteration 2308, dc_loss: 0.015794187784194946, tv_loss: 0.017031479626893997\n",
      "iteration 2309, dc_loss: 0.01579410582780838, tv_loss: 0.0170313511043787\n",
      "iteration 2310, dc_loss: 0.01579398848116398, tv_loss: 0.017031263560056686\n",
      "iteration 2311, dc_loss: 0.01579391583800316, tv_loss: 0.017031392082571983\n",
      "iteration 2312, dc_loss: 0.01579391397535801, tv_loss: 0.017031332477927208\n",
      "iteration 2313, dc_loss: 0.01579395867884159, tv_loss: 0.017031127586960793\n",
      "iteration 2314, dc_loss: 0.015793951228260994, tv_loss: 0.017030974850058556\n",
      "iteration 2315, dc_loss: 0.01579390838742256, tv_loss: 0.017030995339155197\n",
      "iteration 2316, dc_loss: 0.015793871134519577, tv_loss: 0.01703084073960781\n",
      "iteration 2317, dc_loss: 0.0157938189804554, tv_loss: 0.017031017690896988\n",
      "iteration 2318, dc_loss: 0.01579376310110092, tv_loss: 0.017030935734510422\n",
      "iteration 2319, dc_loss: 0.015793705359101295, tv_loss: 0.01703071966767311\n",
      "iteration 2320, dc_loss: 0.0157936979085207, tv_loss: 0.01703081652522087\n",
      "iteration 2321, dc_loss: 0.015793677419424057, tv_loss: 0.017030760645866394\n",
      "iteration 2322, dc_loss: 0.015793651342391968, tv_loss: 0.01703059859573841\n",
      "iteration 2323, dc_loss: 0.015793614089488983, tv_loss: 0.01703057251870632\n",
      "iteration 2324, dc_loss: 0.015793567523360252, tv_loss: 0.017030498012900352\n",
      "iteration 2325, dc_loss: 0.015793533995747566, tv_loss: 0.01703054830431938\n",
      "iteration 2326, dc_loss: 0.015793515369296074, tv_loss: 0.01703031174838543\n",
      "iteration 2327, dc_loss: 0.01579347997903824, tv_loss: 0.0170302614569664\n",
      "iteration 2328, dc_loss: 0.015793416649103165, tv_loss: 0.017030436545610428\n",
      "iteration 2329, dc_loss: 0.015793316066265106, tv_loss: 0.017030369490385056\n",
      "iteration 2330, dc_loss: 0.01579323038458824, tv_loss: 0.017030363902449608\n",
      "iteration 2331, dc_loss: 0.015793150290846825, tv_loss: 0.017030322924256325\n",
      "iteration 2332, dc_loss: 0.015793101862072945, tv_loss: 0.01703023351728916\n",
      "iteration 2333, dc_loss: 0.015793118625879288, tv_loss: 0.0170301403850317\n",
      "iteration 2334, dc_loss: 0.015793142840266228, tv_loss: 0.01703031361103058\n",
      "iteration 2335, dc_loss: 0.015793155878782272, tv_loss: 0.017030132934451103\n",
      "iteration 2336, dc_loss: 0.01579311676323414, tv_loss: 0.01703004539012909\n",
      "iteration 2337, dc_loss: 0.01579306647181511, tv_loss: 0.0170298982411623\n",
      "iteration 2338, dc_loss: 0.015793025493621826, tv_loss: 0.017029862850904465\n",
      "iteration 2339, dc_loss: 0.01579298824071884, tv_loss: 0.017029887065291405\n",
      "iteration 2340, dc_loss: 0.015792934224009514, tv_loss: 0.017029698938131332\n",
      "iteration 2341, dc_loss: 0.015792865306138992, tv_loss: 0.01702982559800148\n",
      "iteration 2342, dc_loss: 0.01579277031123638, tv_loss: 0.017029860988259315\n",
      "iteration 2343, dc_loss: 0.015792669728398323, tv_loss: 0.017029762268066406\n",
      "iteration 2344, dc_loss: 0.01579260639846325, tv_loss: 0.017029738053679466\n",
      "iteration 2345, dc_loss: 0.01579258218407631, tv_loss: 0.017029782757163048\n",
      "iteration 2346, dc_loss: 0.015792591497302055, tv_loss: 0.017029574140906334\n",
      "iteration 2347, dc_loss: 0.0157926045358181, tv_loss: 0.017029469832777977\n",
      "iteration 2348, dc_loss: 0.01579258032143116, tv_loss: 0.01702946610748768\n",
      "iteration 2349, dc_loss: 0.015792563557624817, tv_loss: 0.017029378563165665\n",
      "iteration 2350, dc_loss: 0.015792544931173325, tv_loss: 0.017029279842972755\n",
      "iteration 2351, dc_loss: 0.015792522579431534, tv_loss: 0.01702924445271492\n",
      "iteration 2352, dc_loss: 0.015792513266205788, tv_loss: 0.01702926680445671\n",
      "iteration 2353, dc_loss: 0.015792500227689743, tv_loss: 0.017029188573360443\n",
      "iteration 2354, dc_loss: 0.015792449936270714, tv_loss: 0.01702924445271492\n",
      "iteration 2355, dc_loss: 0.01579238660633564, tv_loss: 0.017029179260134697\n",
      "iteration 2356, dc_loss: 0.015792369842529297, tv_loss: 0.01702921651303768\n",
      "iteration 2357, dc_loss: 0.015792375430464745, tv_loss: 0.017028888687491417\n",
      "iteration 2358, dc_loss: 0.015792373567819595, tv_loss: 0.01702893152832985\n",
      "iteration 2359, dc_loss: 0.01579233445227146, tv_loss: 0.017029033973813057\n",
      "iteration 2360, dc_loss: 0.015792250633239746, tv_loss: 0.017028968781232834\n",
      "iteration 2361, dc_loss: 0.015792161226272583, tv_loss: 0.01702890917658806\n",
      "iteration 2362, dc_loss: 0.015792090445756912, tv_loss: 0.017028888687491417\n",
      "iteration 2363, dc_loss: 0.01579207368195057, tv_loss: 0.017028803005814552\n",
      "iteration 2364, dc_loss: 0.015792101621627808, tv_loss: 0.017028683796525\n",
      "iteration 2365, dc_loss: 0.01579209603369236, tv_loss: 0.017028581351041794\n",
      "iteration 2366, dc_loss: 0.015792062506079674, tv_loss: 0.017028633505105972\n",
      "iteration 2367, dc_loss: 0.015792010352015495, tv_loss: 0.01702849380671978\n",
      "iteration 2368, dc_loss: 0.015791935846209526, tv_loss: 0.017028573900461197\n",
      "iteration 2369, dc_loss: 0.015791861340403557, tv_loss: 0.017028531059622765\n",
      "iteration 2370, dc_loss: 0.015791796147823334, tv_loss: 0.017028551548719406\n",
      "iteration 2371, dc_loss: 0.015791747719049454, tv_loss: 0.017028557136654854\n",
      "iteration 2372, dc_loss: 0.015791719779372215, tv_loss: 0.017028361558914185\n",
      "iteration 2373, dc_loss: 0.01579168438911438, tv_loss: 0.017028382048010826\n",
      "iteration 2374, dc_loss: 0.0157916396856308, tv_loss: 0.01702837646007538\n",
      "iteration 2375, dc_loss: 0.01579159125685692, tv_loss: 0.01702832616865635\n",
      "iteration 2376, dc_loss: 0.01579154096543789, tv_loss: 0.01702829636633396\n",
      "iteration 2377, dc_loss: 0.0157915111631155, tv_loss: 0.017028283327817917\n",
      "iteration 2378, dc_loss: 0.01579148694872856, tv_loss: 0.01702808402478695\n",
      "iteration 2379, dc_loss: 0.01579149253666401, tv_loss: 0.017027899622917175\n",
      "iteration 2380, dc_loss: 0.015791477635502815, tv_loss: 0.017028117552399635\n",
      "iteration 2381, dc_loss: 0.015791425481438637, tv_loss: 0.01702812872827053\n",
      "iteration 2382, dc_loss: 0.01579139567911625, tv_loss: 0.01702781580388546\n",
      "iteration 2383, dc_loss: 0.01579139567911625, tv_loss: 0.017027759924530983\n",
      "iteration 2384, dc_loss: 0.015791378915309906, tv_loss: 0.017027707770466805\n",
      "iteration 2385, dc_loss: 0.01579137332737446, tv_loss: 0.017027447000145912\n",
      "iteration 2386, dc_loss: 0.01579130068421364, tv_loss: 0.01702762022614479\n",
      "iteration 2387, dc_loss: 0.015791187062859535, tv_loss: 0.01702769659459591\n",
      "iteration 2388, dc_loss: 0.01579105295240879, tv_loss: 0.01702771708369255\n",
      "iteration 2389, dc_loss: 0.015790974721312523, tv_loss: 0.017027607187628746\n",
      "iteration 2390, dc_loss: 0.01579098403453827, tv_loss: 0.017027489840984344\n",
      "iteration 2391, dc_loss: 0.015791011974215508, tv_loss: 0.017027471214532852\n",
      "iteration 2392, dc_loss: 0.015790989622473717, tv_loss: 0.01702743209898472\n",
      "iteration 2393, dc_loss: 0.015790965408086777, tv_loss: 0.017027229070663452\n",
      "iteration 2394, dc_loss: 0.01579095609486103, tv_loss: 0.017027417197823524\n",
      "iteration 2395, dc_loss: 0.015790970996022224, tv_loss: 0.01702740043401718\n",
      "iteration 2396, dc_loss: 0.01579093374311924, tv_loss: 0.01702704094350338\n",
      "iteration 2397, dc_loss: 0.015790898352861404, tv_loss: 0.017026977613568306\n",
      "iteration 2398, dc_loss: 0.015790853649377823, tv_loss: 0.017027156427502632\n",
      "iteration 2399, dc_loss: 0.0157907847315073, tv_loss: 0.017027178779244423\n",
      "iteration 2400, dc_loss: 0.015790650621056557, tv_loss: 0.017027126625180244\n",
      "iteration 2401, dc_loss: 0.01579056866466999, tv_loss: 0.017027104273438454\n",
      "iteration 2402, dc_loss: 0.015790512785315514, tv_loss: 0.01702716574072838\n",
      "iteration 2403, dc_loss: 0.01579047366976738, tv_loss: 0.017027197405695915\n",
      "iteration 2404, dc_loss: 0.01579047366976738, tv_loss: 0.017027152702212334\n",
      "iteration 2405, dc_loss: 0.015790492296218872, tv_loss: 0.017027031630277634\n",
      "iteration 2406, dc_loss: 0.015790510922670364, tv_loss: 0.017026953399181366\n",
      "iteration 2407, dc_loss: 0.015790488570928574, tv_loss: 0.017026927322149277\n",
      "iteration 2408, dc_loss: 0.015790460631251335, tv_loss: 0.017026808112859726\n",
      "iteration 2409, dc_loss: 0.015790436416864395, tv_loss: 0.017026785761117935\n",
      "iteration 2410, dc_loss: 0.015790395438671112, tv_loss: 0.017026765272021294\n",
      "iteration 2411, dc_loss: 0.015790358185768127, tv_loss: 0.01702667400240898\n",
      "iteration 2412, dc_loss: 0.015790313482284546, tv_loss: 0.01702664978802204\n",
      "iteration 2413, dc_loss: 0.015790283679962158, tv_loss: 0.017026646062731743\n",
      "iteration 2414, dc_loss: 0.015790201723575592, tv_loss: 0.0170265045017004\n",
      "iteration 2415, dc_loss: 0.015790114179253578, tv_loss: 0.017026660963892937\n",
      "iteration 2416, dc_loss: 0.015790069475769997, tv_loss: 0.017026560381054878\n",
      "iteration 2417, dc_loss: 0.015790026634931564, tv_loss: 0.017026476562023163\n",
      "iteration 2418, dc_loss: 0.015789981931447983, tv_loss: 0.01702645793557167\n",
      "iteration 2419, dc_loss: 0.015789946541190147, tv_loss: 0.017026705667376518\n",
      "iteration 2420, dc_loss: 0.01578994281589985, tv_loss: 0.017026547342538834\n",
      "iteration 2421, dc_loss: 0.015789983794093132, tv_loss: 0.017026247456669807\n",
      "iteration 2422, dc_loss: 0.015790022909641266, tv_loss: 0.017026083543896675\n",
      "iteration 2423, dc_loss: 0.015790022909641266, tv_loss: 0.017026221379637718\n",
      "iteration 2424, dc_loss: 0.015789994969964027, tv_loss: 0.01702612079679966\n",
      "iteration 2425, dc_loss: 0.015789948403835297, tv_loss: 0.01702604815363884\n",
      "iteration 2426, dc_loss: 0.015789881348609924, tv_loss: 0.017026031389832497\n",
      "iteration 2427, dc_loss: 0.015789806842803955, tv_loss: 0.01702605001628399\n",
      "iteration 2428, dc_loss: 0.015789750963449478, tv_loss: 0.017026111483573914\n",
      "iteration 2429, dc_loss: 0.015789691358804703, tv_loss: 0.017026035115122795\n",
      "iteration 2430, dc_loss: 0.015789644792675972, tv_loss: 0.017025882378220558\n",
      "iteration 2431, dc_loss: 0.01578962616622448, tv_loss: 0.017025792971253395\n",
      "iteration 2432, dc_loss: 0.015789633616805077, tv_loss: 0.01702590472996235\n",
      "iteration 2433, dc_loss: 0.015789633616805077, tv_loss: 0.01702573336660862\n",
      "iteration 2434, dc_loss: 0.015789592638611794, tv_loss: 0.017025558277964592\n",
      "iteration 2435, dc_loss: 0.015789562836289406, tv_loss: 0.017025507986545563\n",
      "iteration 2436, dc_loss: 0.015789523720741272, tv_loss: 0.017025647684931755\n",
      "iteration 2437, dc_loss: 0.015789467841386795, tv_loss: 0.017025690525770187\n",
      "iteration 2438, dc_loss: 0.01578941009938717, tv_loss: 0.01702566258609295\n",
      "iteration 2439, dc_loss: 0.01578935794532299, tv_loss: 0.01702553778886795\n",
      "iteration 2440, dc_loss: 0.015789316967129707, tv_loss: 0.017025398090481758\n",
      "iteration 2441, dc_loss: 0.015789303928613663, tv_loss: 0.017025524750351906\n",
      "iteration 2442, dc_loss: 0.015789281576871872, tv_loss: 0.017025494948029518\n",
      "iteration 2443, dc_loss: 0.015789242461323738, tv_loss: 0.017025379464030266\n",
      "iteration 2444, dc_loss: 0.01578919216990471, tv_loss: 0.017025383189320564\n",
      "iteration 2445, dc_loss: 0.01578916795551777, tv_loss: 0.017025193199515343\n",
      "iteration 2446, dc_loss: 0.01578916609287262, tv_loss: 0.017025211825966835\n",
      "iteration 2447, dc_loss: 0.015789153054356575, tv_loss: 0.017025185748934746\n",
      "iteration 2448, dc_loss: 0.01578914374113083, tv_loss: 0.017025256529450417\n",
      "iteration 2449, dc_loss: 0.015789108350872993, tv_loss: 0.017025213688611984\n",
      "iteration 2450, dc_loss: 0.01578906923532486, tv_loss: 0.01702483743429184\n",
      "iteration 2451, dc_loss: 0.015789037570357323, tv_loss: 0.017024988308548927\n",
      "iteration 2452, dc_loss: 0.015789013355970383, tv_loss: 0.01702512614428997\n",
      "iteration 2453, dc_loss: 0.015788961201906204, tv_loss: 0.017025046050548553\n",
      "iteration 2454, dc_loss: 0.01578892208635807, tv_loss: 0.017024844884872437\n",
      "iteration 2455, dc_loss: 0.01578889600932598, tv_loss: 0.01702485792338848\n",
      "iteration 2456, dc_loss: 0.015788884833455086, tv_loss: 0.017024902626872063\n",
      "iteration 2457, dc_loss: 0.015788832679390907, tv_loss: 0.017024928703904152\n",
      "iteration 2458, dc_loss: 0.015788808465003967, tv_loss: 0.017024779692292213\n",
      "iteration 2459, dc_loss: 0.01578875444829464, tv_loss: 0.017024943605065346\n",
      "iteration 2460, dc_loss: 0.015788674354553223, tv_loss: 0.017024848610162735\n",
      "iteration 2461, dc_loss: 0.01578862965106964, tv_loss: 0.01702469401061535\n",
      "iteration 2462, dc_loss: 0.015788624063134193, tv_loss: 0.017024662345647812\n",
      "iteration 2463, dc_loss: 0.015788609161973, tv_loss: 0.017024775967001915\n",
      "iteration 2464, dc_loss: 0.015788609161973, tv_loss: 0.017024686560034752\n",
      "iteration 2465, dc_loss: 0.015788603574037552, tv_loss: 0.017024537548422813\n",
      "iteration 2466, dc_loss: 0.015788564458489418, tv_loss: 0.017024332657456398\n",
      "iteration 2467, dc_loss: 0.015788527205586433, tv_loss: 0.017024338245391846\n",
      "iteration 2468, dc_loss: 0.015788491815328598, tv_loss: 0.017024407163262367\n",
      "iteration 2469, dc_loss: 0.015788422897458076, tv_loss: 0.01702428236603737\n",
      "iteration 2470, dc_loss: 0.015788394957780838, tv_loss: 0.017024314031004906\n",
      "iteration 2471, dc_loss: 0.015788374468684196, tv_loss: 0.01702423207461834\n",
      "iteration 2472, dc_loss: 0.0157883670181036, tv_loss: 0.017024163156747818\n",
      "iteration 2473, dc_loss: 0.015788359567523003, tv_loss: 0.01702418364584446\n",
      "iteration 2474, dc_loss: 0.015788326039910316, tv_loss: 0.017024187371134758\n",
      "iteration 2475, dc_loss: 0.015788286924362183, tv_loss: 0.017024049535393715\n",
      "iteration 2476, dc_loss: 0.015788234770298004, tv_loss: 0.017024116590619087\n",
      "iteration 2477, dc_loss: 0.01578817516565323, tv_loss: 0.017024245113134384\n",
      "iteration 2478, dc_loss: 0.015788113698363304, tv_loss: 0.017023952677845955\n",
      "iteration 2479, dc_loss: 0.015788065269589424, tv_loss: 0.01702386513352394\n",
      "iteration 2480, dc_loss: 0.015788020566105843, tv_loss: 0.017024124041199684\n",
      "iteration 2481, dc_loss: 0.01578800566494465, tv_loss: 0.017024103552103043\n",
      "iteration 2482, dc_loss: 0.01578802429139614, tv_loss: 0.01702389121055603\n",
      "iteration 2483, dc_loss: 0.015788011252880096, tv_loss: 0.01702367328107357\n",
      "iteration 2484, dc_loss: 0.015787992626428604, tv_loss: 0.017023717984557152\n",
      "iteration 2485, dc_loss: 0.015787994489073753, tv_loss: 0.017023775726556778\n",
      "iteration 2486, dc_loss: 0.015787968412041664, tv_loss: 0.017023691907525063\n",
      "iteration 2487, dc_loss: 0.015787925571203232, tv_loss: 0.017023596912622452\n",
      "iteration 2488, dc_loss: 0.015787864103913307, tv_loss: 0.017023487016558647\n",
      "iteration 2489, dc_loss: 0.015787789598107338, tv_loss: 0.017023617401719093\n",
      "iteration 2490, dc_loss: 0.015787694603204727, tv_loss: 0.01702360436320305\n",
      "iteration 2491, dc_loss: 0.0157876405864954, tv_loss: 0.017023518681526184\n",
      "iteration 2492, dc_loss: 0.01578761637210846, tv_loss: 0.017023509368300438\n",
      "iteration 2493, dc_loss: 0.015787605196237564, tv_loss: 0.017023520544171333\n",
      "iteration 2494, dc_loss: 0.015787603333592415, tv_loss: 0.01702343113720417\n",
      "iteration 2495, dc_loss: 0.015787608921527863, tv_loss: 0.017023365944623947\n",
      "iteration 2496, dc_loss: 0.015787629410624504, tv_loss: 0.017023127526044846\n",
      "iteration 2497, dc_loss: 0.015787633135914803, tv_loss: 0.017022984102368355\n",
      "iteration 2498, dc_loss: 0.015787629410624504, tv_loss: 0.017023079097270966\n",
      "iteration 2499, dc_loss: 0.015787579119205475, tv_loss: 0.017023010179400444\n",
      "iteration 2500, dc_loss: 0.015787547454237938, tv_loss: 0.017023028805851936\n",
      "iteration 2501, dc_loss: 0.015787513926625252, tv_loss: 0.017022840678691864\n",
      "iteration 2502, dc_loss: 0.015787465497851372, tv_loss: 0.01702287793159485\n",
      "iteration 2503, dc_loss: 0.01578741893172264, tv_loss: 0.017023058608174324\n",
      "iteration 2504, dc_loss: 0.015787357464432716, tv_loss: 0.017022883519530296\n",
      "iteration 2505, dc_loss: 0.01578729972243309, tv_loss: 0.01702277548611164\n",
      "iteration 2506, dc_loss: 0.015787288546562195, tv_loss: 0.017022956162691116\n",
      "iteration 2507, dc_loss: 0.01578727923333645, tv_loss: 0.017023026943206787\n",
      "iteration 2508, dc_loss: 0.01578727550804615, tv_loss: 0.017022958025336266\n",
      "iteration 2509, dc_loss: 0.015787290409207344, tv_loss: 0.017022553831338882\n",
      "iteration 2510, dc_loss: 0.015787282958626747, tv_loss: 0.01702251471579075\n",
      "iteration 2511, dc_loss: 0.015787256881594658, tv_loss: 0.01702287420630455\n",
      "iteration 2512, dc_loss: 0.01578720286488533, tv_loss: 0.01702260598540306\n",
      "iteration 2513, dc_loss: 0.01578715071082115, tv_loss: 0.01702248863875866\n",
      "iteration 2514, dc_loss: 0.01578708551824093, tv_loss: 0.01702270656824112\n",
      "iteration 2515, dc_loss: 0.015787022188305855, tv_loss: 0.01702273264527321\n",
      "iteration 2516, dc_loss: 0.01578698121011257, tv_loss: 0.01702282391488552\n",
      "iteration 2517, dc_loss: 0.015786947682499886, tv_loss: 0.01702270284295082\n",
      "iteration 2518, dc_loss: 0.015786919742822647, tv_loss: 0.0170225128531456\n",
      "iteration 2519, dc_loss: 0.015786919742822647, tv_loss: 0.017022447660565376\n",
      "iteration 2520, dc_loss: 0.01578693650662899, tv_loss: 0.017022382467985153\n",
      "iteration 2521, dc_loss: 0.015786973759531975, tv_loss: 0.017022280022501945\n",
      "iteration 2522, dc_loss: 0.015786949545145035, tv_loss: 0.01702222228050232\n",
      "iteration 2523, dc_loss: 0.015786906704306602, tv_loss: 0.017022108659148216\n",
      "iteration 2524, dc_loss: 0.015786832198500633, tv_loss: 0.017022157087922096\n",
      "iteration 2525, dc_loss: 0.01578672230243683, tv_loss: 0.017022185027599335\n",
      "iteration 2526, dc_loss: 0.015786631032824516, tv_loss: 0.017022183164954185\n",
      "iteration 2527, dc_loss: 0.015786610543727875, tv_loss: 0.017022226005792618\n",
      "iteration 2528, dc_loss: 0.015786610543727875, tv_loss: 0.017022157087922096\n",
      "iteration 2529, dc_loss: 0.015786608681082726, tv_loss: 0.017022009938955307\n",
      "iteration 2530, dc_loss: 0.015786603093147278, tv_loss: 0.01702185347676277\n",
      "iteration 2531, dc_loss: 0.01578662358224392, tv_loss: 0.01702190935611725\n",
      "iteration 2532, dc_loss: 0.01578662544488907, tv_loss: 0.01702204719185829\n",
      "iteration 2533, dc_loss: 0.01578659936785698, tv_loss: 0.017021723091602325\n",
      "iteration 2534, dc_loss: 0.01578656956553459, tv_loss: 0.01702183485031128\n",
      "iteration 2535, dc_loss: 0.015786532312631607, tv_loss: 0.017021864652633667\n",
      "iteration 2536, dc_loss: 0.015786509960889816, tv_loss: 0.017021650448441505\n",
      "iteration 2537, dc_loss: 0.01578650064766407, tv_loss: 0.01702157035470009\n",
      "iteration 2538, dc_loss: 0.015786465257406235, tv_loss: 0.01702173799276352\n",
      "iteration 2539, dc_loss: 0.015786394476890564, tv_loss: 0.017021717503666878\n",
      "iteration 2540, dc_loss: 0.01578633114695549, tv_loss: 0.01702151820063591\n",
      "iteration 2541, dc_loss: 0.015786288306117058, tv_loss: 0.017021633684635162\n",
      "iteration 2542, dc_loss: 0.01578623242676258, tv_loss: 0.017021629959344864\n",
      "iteration 2543, dc_loss: 0.015786197036504745, tv_loss: 0.01702168397605419\n",
      "iteration 2544, dc_loss: 0.01578615792095661, tv_loss: 0.017021657899022102\n",
      "iteration 2545, dc_loss: 0.015786143019795418, tv_loss: 0.017021523788571358\n",
      "iteration 2546, dc_loss: 0.015786146745085716, tv_loss: 0.017021454870700836\n",
      "iteration 2547, dc_loss: 0.015786128118634224, tv_loss: 0.017021574079990387\n",
      "iteration 2548, dc_loss: 0.015786102041602135, tv_loss: 0.01702151447534561\n",
      "iteration 2549, dc_loss: 0.015786070376634598, tv_loss: 0.017021318897604942\n",
      "iteration 2550, dc_loss: 0.015786034986376762, tv_loss: 0.017021408304572105\n",
      "iteration 2551, dc_loss: 0.015785973519086838, tv_loss: 0.017021544277668\n",
      "iteration 2552, dc_loss: 0.015785926952958107, tv_loss: 0.017021318897604942\n",
      "iteration 2553, dc_loss: 0.01578589156270027, tv_loss: 0.017021307721734047\n",
      "iteration 2554, dc_loss: 0.015785852447152138, tv_loss: 0.01702137291431427\n",
      "iteration 2555, dc_loss: 0.015785828232765198, tv_loss: 0.017021358013153076\n",
      "iteration 2556, dc_loss: 0.01578584685921669, tv_loss: 0.017021233215928078\n",
      "iteration 2557, dc_loss: 0.015785889700055122, tv_loss: 0.017021026462316513\n",
      "iteration 2558, dc_loss: 0.015785925090312958, tv_loss: 0.01702106185257435\n",
      "iteration 2559, dc_loss: 0.01578591763973236, tv_loss: 0.01702110655605793\n",
      "iteration 2560, dc_loss: 0.01578587107360363, tv_loss: 0.017021093517541885\n",
      "iteration 2561, dc_loss: 0.01578579656779766, tv_loss: 0.017020881175994873\n",
      "iteration 2562, dc_loss: 0.015785731375217438, tv_loss: 0.017020950093865395\n",
      "iteration 2563, dc_loss: 0.015785707160830498, tv_loss: 0.017020922154188156\n",
      "iteration 2564, dc_loss: 0.01578567735850811, tv_loss: 0.017020992934703827\n",
      "iteration 2565, dc_loss: 0.01578563079237938, tv_loss: 0.017021004110574722\n",
      "iteration 2566, dc_loss: 0.015785565599799156, tv_loss: 0.017020950093865395\n",
      "iteration 2567, dc_loss: 0.015785520896315575, tv_loss: 0.017021024599671364\n",
      "iteration 2568, dc_loss: 0.015785539522767067, tv_loss: 0.0170209389179945\n",
      "iteration 2569, dc_loss: 0.015785563737154007, tv_loss: 0.017020704224705696\n",
      "iteration 2570, dc_loss: 0.015785561874508858, tv_loss: 0.017020685598254204\n",
      "iteration 2571, dc_loss: 0.015785546973347664, tv_loss: 0.01702059619128704\n",
      "iteration 2572, dc_loss: 0.015785539522767067, tv_loss: 0.017020530998706818\n",
      "iteration 2573, dc_loss: 0.015785524621605873, tv_loss: 0.01702048070728779\n",
      "iteration 2574, dc_loss: 0.01578548364341259, tv_loss: 0.01702047511935234\n",
      "iteration 2575, dc_loss: 0.01578543521463871, tv_loss: 0.017020517960190773\n",
      "iteration 2576, dc_loss: 0.015785418450832367, tv_loss: 0.017020208761096\n",
      "iteration 2577, dc_loss: 0.01578538492321968, tv_loss: 0.01702025718986988\n",
      "iteration 2578, dc_loss: 0.0157853402197361, tv_loss: 0.01702050119638443\n",
      "iteration 2579, dc_loss: 0.01578526943922043, tv_loss: 0.017020530998706818\n",
      "iteration 2580, dc_loss: 0.01578523777425289, tv_loss: 0.017020486295223236\n",
      "iteration 2581, dc_loss: 0.015785234048962593, tv_loss: 0.01702033169567585\n",
      "iteration 2582, dc_loss: 0.01578524149954319, tv_loss: 0.017020301893353462\n",
      "iteration 2583, dc_loss: 0.015785234048962593, tv_loss: 0.017020229250192642\n",
      "iteration 2584, dc_loss: 0.0157852191478014, tv_loss: 0.01702018827199936\n",
      "iteration 2585, dc_loss: 0.015785181894898415, tv_loss: 0.01702006347477436\n",
      "iteration 2586, dc_loss: 0.01578512415289879, tv_loss: 0.017020078375935555\n",
      "iteration 2587, dc_loss: 0.01578507386147976, tv_loss: 0.01702001877129078\n",
      "iteration 2588, dc_loss: 0.015785062685608864, tv_loss: 0.017019832506775856\n",
      "iteration 2589, dc_loss: 0.015785085037350655, tv_loss: 0.017019925639033318\n",
      "iteration 2590, dc_loss: 0.01578507199883461, tv_loss: 0.017020054161548615\n",
      "iteration 2591, dc_loss: 0.015785010531544685, tv_loss: 0.017019934952259064\n",
      "iteration 2592, dc_loss: 0.01578494720160961, tv_loss: 0.017019908875226974\n",
      "iteration 2593, dc_loss: 0.015784868970513344, tv_loss: 0.01701994799077511\n",
      "iteration 2594, dc_loss: 0.01578482799232006, tv_loss: 0.017019953578710556\n",
      "iteration 2595, dc_loss: 0.015784816816449165, tv_loss: 0.017019761726260185\n",
      "iteration 2596, dc_loss: 0.01578483358025551, tv_loss: 0.017019784078001976\n",
      "iteration 2597, dc_loss: 0.015784837305545807, tv_loss: 0.01701986789703369\n",
      "iteration 2598, dc_loss: 0.01578483171761036, tv_loss: 0.01701956056058407\n",
      "iteration 2599, dc_loss: 0.01578482985496521, tv_loss: 0.017019454389810562\n",
      "iteration 2600, dc_loss: 0.01578480936586857, tv_loss: 0.017019564285874367\n",
      "iteration 2601, dc_loss: 0.015784788876771927, tv_loss: 0.017019610852003098\n",
      "iteration 2602, dc_loss: 0.01578478328883648, tv_loss: 0.01701963320374489\n",
      "iteration 2603, dc_loss: 0.0157847348600626, tv_loss: 0.017019497230648994\n",
      "iteration 2604, dc_loss: 0.015784693881869316, tv_loss: 0.017019541934132576\n",
      "iteration 2605, dc_loss: 0.01578466221690178, tv_loss: 0.017019525170326233\n",
      "iteration 2606, dc_loss: 0.015784645453095436, tv_loss: 0.01701943948864937\n",
      "iteration 2607, dc_loss: 0.015784574672579765, tv_loss: 0.01701933890581131\n",
      "iteration 2608, dc_loss: 0.015784496441483498, tv_loss: 0.01701938547194004\n",
      "iteration 2609, dc_loss: 0.015784448012709618, tv_loss: 0.017019450664520264\n",
      "iteration 2610, dc_loss: 0.01578444615006447, tv_loss: 0.017019357532262802\n",
      "iteration 2611, dc_loss: 0.01578444056212902, tv_loss: 0.017019182443618774\n",
      "iteration 2612, dc_loss: 0.015784429386258125, tv_loss: 0.017019212245941162\n",
      "iteration 2613, dc_loss: 0.01578439399600029, tv_loss: 0.01701921969652176\n",
      "iteration 2614, dc_loss: 0.015784360468387604, tv_loss: 0.017019152641296387\n",
      "iteration 2615, dc_loss: 0.01578434929251671, tv_loss: 0.017019225284457207\n",
      "iteration 2616, dc_loss: 0.015784312039613724, tv_loss: 0.01701914332807064\n",
      "iteration 2617, dc_loss: 0.015784261748194695, tv_loss: 0.017019182443618774\n",
      "iteration 2618, dc_loss: 0.015784218907356262, tv_loss: 0.017019206658005714\n",
      "iteration 2619, dc_loss: 0.015784218907356262, tv_loss: 0.01701907441020012\n",
      "iteration 2620, dc_loss: 0.015784231945872307, tv_loss: 0.01701897755265236\n",
      "iteration 2621, dc_loss: 0.015784231945872307, tv_loss: 0.017018955200910568\n",
      "iteration 2622, dc_loss: 0.015784218907356262, tv_loss: 0.017019040882587433\n",
      "iteration 2623, dc_loss: 0.015784194692969322, tv_loss: 0.017019031569361687\n",
      "iteration 2624, dc_loss: 0.01578414998948574, tv_loss: 0.017019031569361687\n",
      "iteration 2625, dc_loss: 0.015784088522195816, tv_loss: 0.01701904460787773\n",
      "iteration 2626, dc_loss: 0.01578405871987343, tv_loss: 0.017019109800457954\n",
      "iteration 2627, dc_loss: 0.015784019604325294, tv_loss: 0.017019109800457954\n",
      "iteration 2628, dc_loss: 0.015783987939357758, tv_loss: 0.017018867656588554\n",
      "iteration 2629, dc_loss: 0.01578395999968052, tv_loss: 0.017018891870975494\n",
      "iteration 2630, dc_loss: 0.015783948823809624, tv_loss: 0.017018962651491165\n",
      "iteration 2631, dc_loss: 0.015783963724970818, tv_loss: 0.01701892539858818\n",
      "iteration 2632, dc_loss: 0.015783973038196564, tv_loss: 0.017018688842654228\n",
      "iteration 2633, dc_loss: 0.01578396186232567, tv_loss: 0.01701846718788147\n",
      "iteration 2634, dc_loss: 0.01578390784561634, tv_loss: 0.01701856032013893\n",
      "iteration 2635, dc_loss: 0.01578386127948761, tv_loss: 0.01701863668859005\n",
      "iteration 2636, dc_loss: 0.01578381471335888, tv_loss: 0.017018787562847137\n",
      "iteration 2637, dc_loss: 0.01578378863632679, tv_loss: 0.0170186348259449\n",
      "iteration 2638, dc_loss: 0.01578374393284321, tv_loss: 0.017018495127558708\n",
      "iteration 2639, dc_loss: 0.01578373648226261, tv_loss: 0.01701868511736393\n",
      "iteration 2640, dc_loss: 0.01578378863632679, tv_loss: 0.01701849140226841\n",
      "iteration 2641, dc_loss: 0.015783820301294327, tv_loss: 0.017018316313624382\n",
      "iteration 2642, dc_loss: 0.015783824026584625, tv_loss: 0.017018495127558708\n",
      "iteration 2643, dc_loss: 0.015783794224262238, tv_loss: 0.01701829396188259\n",
      "iteration 2644, dc_loss: 0.015783725306391716, tv_loss: 0.01701834797859192\n",
      "iteration 2645, dc_loss: 0.01578366942703724, tv_loss: 0.01701843924820423\n",
      "iteration 2646, dc_loss: 0.01578363962471485, tv_loss: 0.017018377780914307\n",
      "iteration 2647, dc_loss: 0.015783606097102165, tv_loss: 0.017018316313624382\n",
      "iteration 2648, dc_loss: 0.01578357256948948, tv_loss: 0.017018239945173264\n",
      "iteration 2649, dc_loss: 0.015783540904521942, tv_loss: 0.01701841503381729\n",
      "iteration 2650, dc_loss: 0.015783507376909256, tv_loss: 0.017018219456076622\n",
      "iteration 2651, dc_loss: 0.015783468261361122, tv_loss: 0.017017951235175133\n",
      "iteration 2652, dc_loss: 0.01578342728316784, tv_loss: 0.017018169164657593\n",
      "iteration 2653, dc_loss: 0.015783432871103287, tv_loss: 0.01701808162033558\n",
      "iteration 2654, dc_loss: 0.01578347757458687, tv_loss: 0.017017878592014313\n",
      "iteration 2655, dc_loss: 0.015783488750457764, tv_loss: 0.017017997801303864\n",
      "iteration 2656, dc_loss: 0.015783464536070824, tv_loss: 0.01701803132891655\n",
      "iteration 2657, dc_loss: 0.015783408656716347, tv_loss: 0.017017852514982224\n",
      "iteration 2658, dc_loss: 0.01578340120613575, tv_loss: 0.017017846927046776\n",
      "iteration 2659, dc_loss: 0.015783406794071198, tv_loss: 0.017017913982272148\n",
      "iteration 2660, dc_loss: 0.015783432871103287, tv_loss: 0.017017731443047523\n",
      "iteration 2661, dc_loss: 0.01578340120613575, tv_loss: 0.017017634585499763\n",
      "iteration 2662, dc_loss: 0.015783334150910378, tv_loss: 0.01701756753027439\n",
      "iteration 2663, dc_loss: 0.015783268958330154, tv_loss: 0.01701776124536991\n",
      "iteration 2664, dc_loss: 0.015783194452524185, tv_loss: 0.017017781734466553\n",
      "iteration 2665, dc_loss: 0.015783099457621574, tv_loss: 0.01701773703098297\n",
      "iteration 2666, dc_loss: 0.01578303799033165, tv_loss: 0.017017662525177002\n",
      "iteration 2667, dc_loss: 0.015782998874783516, tv_loss: 0.01701762154698372\n",
      "iteration 2668, dc_loss: 0.015782993286848068, tv_loss: 0.0170175489038229\n",
      "iteration 2669, dc_loss: 0.015783008188009262, tv_loss: 0.01701754704117775\n",
      "iteration 2670, dc_loss: 0.015783056616783142, tv_loss: 0.01701747253537178\n",
      "iteration 2671, dc_loss: 0.015783045440912247, tv_loss: 0.017017334699630737\n",
      "iteration 2672, dc_loss: 0.015782997012138367, tv_loss: 0.01701752468943596\n",
      "iteration 2673, dc_loss: 0.01578298769891262, tv_loss: 0.017017507925629616\n",
      "iteration 2674, dc_loss: 0.015782982110977173, tv_loss: 0.01701739802956581\n",
      "iteration 2675, dc_loss: 0.015782978385686874, tv_loss: 0.017017243430018425\n",
      "iteration 2676, dc_loss: 0.015782980248332024, tv_loss: 0.017017249017953873\n",
      "iteration 2677, dc_loss: 0.015782974660396576, tv_loss: 0.017017291858792305\n",
      "iteration 2678, dc_loss: 0.015782972797751427, tv_loss: 0.01701718010008335\n",
      "iteration 2679, dc_loss: 0.01578298583626747, tv_loss: 0.01701727882027626\n",
      "iteration 2680, dc_loss: 0.015783019363880157, tv_loss: 0.017017168924212456\n",
      "iteration 2681, dc_loss: 0.015783004462718964, tv_loss: 0.017017127946019173\n",
      "iteration 2682, dc_loss: 0.015782954171299934, tv_loss: 0.017017150297760963\n",
      "iteration 2683, dc_loss: 0.015782825648784637, tv_loss: 0.017017094418406487\n",
      "iteration 2684, dc_loss: 0.015782710164785385, tv_loss: 0.017017193138599396\n",
      "iteration 2685, dc_loss: 0.015782631933689117, tv_loss: 0.017017362639307976\n",
      "iteration 2686, dc_loss: 0.015782587230205536, tv_loss: 0.017017371952533722\n",
      "iteration 2687, dc_loss: 0.015782581642270088, tv_loss: 0.017017202451825142\n",
      "iteration 2688, dc_loss: 0.015782618895173073, tv_loss: 0.01701704040169716\n",
      "iteration 2689, dc_loss: 0.015782661736011505, tv_loss: 0.017016947269439697\n",
      "iteration 2690, dc_loss: 0.015782691538333893, tv_loss: 0.01701699011027813\n",
      "iteration 2691, dc_loss: 0.015782687813043594, tv_loss: 0.017016934230923653\n",
      "iteration 2692, dc_loss: 0.015782654285430908, tv_loss: 0.017016811296343803\n",
      "iteration 2693, dc_loss: 0.015782609581947327, tv_loss: 0.017016896978020668\n",
      "iteration 2694, dc_loss: 0.01578257791697979, tv_loss: 0.017016971483826637\n",
      "iteration 2695, dc_loss: 0.015782555565238, tv_loss: 0.01701699197292328\n",
      "iteration 2696, dc_loss: 0.015782542526721954, tv_loss: 0.017016859725117683\n",
      "iteration 2697, dc_loss: 0.01578255370259285, tv_loss: 0.017016828060150146\n",
      "iteration 2698, dc_loss: 0.015782544389367104, tv_loss: 0.017016775906085968\n",
      "iteration 2699, dc_loss: 0.01578250527381897, tv_loss: 0.017016857862472534\n",
      "iteration 2700, dc_loss: 0.01578240655362606, tv_loss: 0.017016775906085968\n",
      "iteration 2701, dc_loss: 0.01578233204782009, tv_loss: 0.017016924917697906\n",
      "iteration 2702, dc_loss: 0.015782317146658897, tv_loss: 0.017016779631376266\n",
      "iteration 2703, dc_loss: 0.015782294794917107, tv_loss: 0.01701674982905388\n",
      "iteration 2704, dc_loss: 0.015782233327627182, tv_loss: 0.01701662316918373\n",
      "iteration 2705, dc_loss: 0.015782177448272705, tv_loss: 0.01701667718589306\n",
      "iteration 2706, dc_loss: 0.015782158821821213, tv_loss: 0.01701684109866619\n",
      "iteration 2707, dc_loss: 0.015782155096530914, tv_loss: 0.01701674424111843\n",
      "iteration 2708, dc_loss: 0.01578221097588539, tv_loss: 0.01701667159795761\n",
      "iteration 2709, dc_loss: 0.015782248228788376, tv_loss: 0.017016468569636345\n",
      "iteration 2710, dc_loss: 0.01578226499259472, tv_loss: 0.0170163381844759\n",
      "iteration 2711, dc_loss: 0.015782278031110764, tv_loss: 0.017016293480992317\n",
      "iteration 2712, dc_loss: 0.015782248228788376, tv_loss: 0.01701623946428299\n",
      "iteration 2713, dc_loss: 0.015782203525304794, tv_loss: 0.017016327008605003\n",
      "iteration 2714, dc_loss: 0.01578216440975666, tv_loss: 0.017016209661960602\n",
      "iteration 2715, dc_loss: 0.01578211970627308, tv_loss: 0.017016170546412468\n",
      "iteration 2716, dc_loss: 0.015782104805111885, tv_loss: 0.017016222700476646\n",
      "iteration 2717, dc_loss: 0.01578209362924099, tv_loss: 0.01701623946428299\n",
      "iteration 2718, dc_loss: 0.01578209176659584, tv_loss: 0.017016112804412842\n",
      "iteration 2719, dc_loss: 0.015782061964273453, tv_loss: 0.017016133293509483\n",
      "iteration 2720, dc_loss: 0.015782013535499573, tv_loss: 0.017016250640153885\n",
      "iteration 2721, dc_loss: 0.015781978145241737, tv_loss: 0.017016205936670303\n",
      "iteration 2722, dc_loss: 0.01578197441995144, tv_loss: 0.01701614074409008\n",
      "iteration 2723, dc_loss: 0.015781987458467484, tv_loss: 0.017015917226672173\n",
      "iteration 2724, dc_loss: 0.015781981870532036, tv_loss: 0.017015891149640083\n",
      "iteration 2725, dc_loss: 0.01578196883201599, tv_loss: 0.017016075551509857\n",
      "iteration 2726, dc_loss: 0.0157819464802742, tv_loss: 0.01701596938073635\n",
      "iteration 2727, dc_loss: 0.01578192226588726, tv_loss: 0.017015766352415085\n",
      "iteration 2728, dc_loss: 0.01578189991414547, tv_loss: 0.01701575145125389\n",
      "iteration 2729, dc_loss: 0.01578187197446823, tv_loss: 0.017015818506479263\n",
      "iteration 2730, dc_loss: 0.01578184962272644, tv_loss: 0.017015859484672546\n",
      "iteration 2731, dc_loss: 0.015781834721565247, tv_loss: 0.01701575331389904\n",
      "iteration 2732, dc_loss: 0.015781830996274948, tv_loss: 0.017015811055898666\n",
      "iteration 2733, dc_loss: 0.015781842172145844, tv_loss: 0.017015885561704636\n",
      "iteration 2734, dc_loss: 0.015781806781888008, tv_loss: 0.01701582595705986\n",
      "iteration 2735, dc_loss: 0.015781749039888382, tv_loss: 0.017015784978866577\n",
      "iteration 2736, dc_loss: 0.015781700611114502, tv_loss: 0.017015762627124786\n",
      "iteration 2737, dc_loss: 0.015781687572598457, tv_loss: 0.017015786841511726\n",
      "iteration 2738, dc_loss: 0.01578165777027607, tv_loss: 0.017015665769577026\n",
      "iteration 2739, dc_loss: 0.01578160747885704, tv_loss: 0.01701560989022255\n",
      "iteration 2740, dc_loss: 0.015781523659825325, tv_loss: 0.017015814781188965\n",
      "iteration 2741, dc_loss: 0.0157814659178257, tv_loss: 0.01701580546796322\n",
      "iteration 2742, dc_loss: 0.01578143797814846, tv_loss: 0.017015622928738594\n",
      "iteration 2743, dc_loss: 0.01578143984079361, tv_loss: 0.01701565831899643\n",
      "iteration 2744, dc_loss: 0.015781452879309654, tv_loss: 0.017015695571899414\n",
      "iteration 2745, dc_loss: 0.015781478956341743, tv_loss: 0.017015540972352028\n",
      "iteration 2746, dc_loss: 0.01578151248395443, tv_loss: 0.017015375196933746\n",
      "iteration 2747, dc_loss: 0.01578153297305107, tv_loss: 0.017015349119901657\n",
      "iteration 2748, dc_loss: 0.015781499445438385, tv_loss: 0.017015352845191956\n",
      "iteration 2749, dc_loss: 0.015781467780470848, tv_loss: 0.017015332356095314\n",
      "iteration 2750, dc_loss: 0.015781406313180923, tv_loss: 0.017015226185321808\n",
      "iteration 2751, dc_loss: 0.015781348571181297, tv_loss: 0.017015060409903526\n",
      "iteration 2752, dc_loss: 0.015781307592988014, tv_loss: 0.017015371471643448\n",
      "iteration 2753, dc_loss: 0.01578129269182682, tv_loss: 0.01701534166932106\n",
      "iteration 2754, dc_loss: 0.015781309455633163, tv_loss: 0.017014972865581512\n",
      "iteration 2755, dc_loss: 0.015781372785568237, tv_loss: 0.017014948651194572\n",
      "iteration 2756, dc_loss: 0.015781396999955177, tv_loss: 0.01701509580016136\n",
      "iteration 2757, dc_loss: 0.015781372785568237, tv_loss: 0.01701487973332405\n",
      "iteration 2758, dc_loss: 0.0157813411206007, tv_loss: 0.01701476238667965\n",
      "iteration 2759, dc_loss: 0.015781287103891373, tv_loss: 0.017014868557453156\n",
      "iteration 2760, dc_loss: 0.015781225636601448, tv_loss: 0.01701502501964569\n",
      "iteration 2761, dc_loss: 0.015781186521053314, tv_loss: 0.017014995217323303\n",
      "iteration 2762, dc_loss: 0.015781154856085777, tv_loss: 0.01701497845351696\n",
      "iteration 2763, dc_loss: 0.015781115740537643, tv_loss: 0.01701507903635502\n",
      "iteration 2764, dc_loss: 0.01578109711408615, tv_loss: 0.017015090212225914\n",
      "iteration 2765, dc_loss: 0.015781095251441002, tv_loss: 0.017014959827065468\n",
      "iteration 2766, dc_loss: 0.015781113877892494, tv_loss: 0.017014889046549797\n",
      "iteration 2767, dc_loss: 0.015781136229634285, tv_loss: 0.017014771699905396\n",
      "iteration 2768, dc_loss: 0.01578114926815033, tv_loss: 0.017014719545841217\n",
      "iteration 2769, dc_loss: 0.01578112505376339, tv_loss: 0.017014706507325172\n",
      "iteration 2770, dc_loss: 0.015781080350279808, tv_loss: 0.017014771699905396\n",
      "iteration 2771, dc_loss: 0.015781009569764137, tv_loss: 0.01701470836997032\n",
      "iteration 2772, dc_loss: 0.01578097976744175, tv_loss: 0.017014844343066216\n",
      "iteration 2773, dc_loss: 0.015780946239829063, tv_loss: 0.017014753073453903\n",
      "iteration 2774, dc_loss: 0.01578092947602272, tv_loss: 0.017014533281326294\n",
      "iteration 2775, dc_loss: 0.015780936926603317, tv_loss: 0.017014656215906143\n",
      "iteration 2776, dc_loss: 0.01578092947602272, tv_loss: 0.017014743760228157\n",
      "iteration 2777, dc_loss: 0.015780923888087273, tv_loss: 0.017014672979712486\n",
      "iteration 2778, dc_loss: 0.015780922025442123, tv_loss: 0.01701444387435913\n",
      "iteration 2779, dc_loss: 0.015780895948410034, tv_loss: 0.017014484852552414\n",
      "iteration 2780, dc_loss: 0.015780843794345856, tv_loss: 0.017014523968100548\n",
      "iteration 2781, dc_loss: 0.015780791640281677, tv_loss: 0.01701468788087368\n",
      "iteration 2782, dc_loss: 0.01578075997531414, tv_loss: 0.01701461151242256\n",
      "iteration 2783, dc_loss: 0.015780741348862648, tv_loss: 0.01701454073190689\n",
      "iteration 2784, dc_loss: 0.015780745074152946, tv_loss: 0.017014548182487488\n",
      "iteration 2785, dc_loss: 0.015780745074152946, tv_loss: 0.01701437681913376\n",
      "iteration 2786, dc_loss: 0.015780771151185036, tv_loss: 0.017014307901263237\n",
      "iteration 2787, dc_loss: 0.01578080654144287, tv_loss: 0.017014319077134132\n",
      "iteration 2788, dc_loss: 0.015780825167894363, tv_loss: 0.017014184966683388\n",
      "iteration 2789, dc_loss: 0.015780827030539513, tv_loss: 0.01701407879590988\n",
      "iteration 2790, dc_loss: 0.015780795365571976, tv_loss: 0.01701413467526436\n",
      "iteration 2791, dc_loss: 0.015780718997120857, tv_loss: 0.017014237120747566\n",
      "iteration 2792, dc_loss: 0.015780601650476456, tv_loss: 0.017014293000102043\n",
      "iteration 2793, dc_loss: 0.015780512243509293, tv_loss: 0.01701420173048973\n",
      "iteration 2794, dc_loss: 0.015780404210090637, tv_loss: 0.01701429858803749\n",
      "iteration 2795, dc_loss: 0.015780365094542503, tv_loss: 0.017014309763908386\n",
      "iteration 2796, dc_loss: 0.015780381858348846, tv_loss: 0.017014319077134132\n",
      "iteration 2797, dc_loss: 0.015780417248606682, tv_loss: 0.017014190554618835\n",
      "iteration 2798, dc_loss: 0.01578046940267086, tv_loss: 0.017014089971780777\n",
      "iteration 2799, dc_loss: 0.01578049547970295, tv_loss: 0.01701408624649048\n",
      "iteration 2800, dc_loss: 0.0157804936170578, tv_loss: 0.017014067620038986\n",
      "iteration 2801, dc_loss: 0.015780486166477203, tv_loss: 0.01701400987803936\n",
      "iteration 2802, dc_loss: 0.01578044518828392, tv_loss: 0.01701393723487854\n",
      "iteration 2803, dc_loss: 0.015780428424477577, tv_loss: 0.01701398193836212\n",
      "iteration 2804, dc_loss: 0.01578040048480034, tv_loss: 0.01701393723487854\n",
      "iteration 2805, dc_loss: 0.01578040048480034, tv_loss: 0.017013976350426674\n",
      "iteration 2806, dc_loss: 0.015780417248606682, tv_loss: 0.017013758420944214\n",
      "iteration 2807, dc_loss: 0.01578041911125183, tv_loss: 0.017013760283589363\n",
      "iteration 2808, dc_loss: 0.015780407935380936, tv_loss: 0.01701381243765354\n",
      "iteration 2809, dc_loss: 0.015780391171574593, tv_loss: 0.01701360009610653\n",
      "iteration 2810, dc_loss: 0.015780342742800713, tv_loss: 0.01701369695365429\n",
      "iteration 2811, dc_loss: 0.01578027941286564, tv_loss: 0.017013682052493095\n",
      "iteration 2812, dc_loss: 0.015780221670866013, tv_loss: 0.017013581469655037\n",
      "iteration 2813, dc_loss: 0.015780147165060043, tv_loss: 0.01701379381120205\n",
      "iteration 2814, dc_loss: 0.01578010991215706, tv_loss: 0.017013676464557648\n",
      "iteration 2815, dc_loss: 0.015780141577124596, tv_loss: 0.01701361872255802\n",
      "iteration 2816, dc_loss: 0.015780195593833923, tv_loss: 0.017013484612107277\n",
      "iteration 2817, dc_loss: 0.015780217945575714, tv_loss: 0.017013346776366234\n",
      "iteration 2818, dc_loss: 0.015780197456479073, tv_loss: 0.01701333187520504\n",
      "iteration 2819, dc_loss: 0.015780167654156685, tv_loss: 0.017013458535075188\n",
      "iteration 2820, dc_loss: 0.015780171379446983, tv_loss: 0.01701335236430168\n",
      "iteration 2821, dc_loss: 0.015780143439769745, tv_loss: 0.017013199627399445\n",
      "iteration 2822, dc_loss: 0.015780121088027954, tv_loss: 0.017013277858495712\n",
      "iteration 2823, dc_loss: 0.015780098736286163, tv_loss: 0.017013367265462875\n",
      "iteration 2824, dc_loss: 0.015780050307512283, tv_loss: 0.01701313629746437\n",
      "iteration 2825, dc_loss: 0.01578003354370594, tv_loss: 0.01701332814991474\n",
      "iteration 2826, dc_loss: 0.01578000746667385, tv_loss: 0.01701318472623825\n",
      "iteration 2827, dc_loss: 0.015779970213770866, tv_loss: 0.017013130709528923\n",
      "iteration 2828, dc_loss: 0.015779945999383926, tv_loss: 0.01701328717172146\n",
      "iteration 2829, dc_loss: 0.01577993482351303, tv_loss: 0.017013205215334892\n",
      "iteration 2830, dc_loss: 0.015779899433255196, tv_loss: 0.017013221979141235\n",
      "iteration 2831, dc_loss: 0.015779869630932808, tv_loss: 0.01701316237449646\n",
      "iteration 2832, dc_loss: 0.0157798919826746, tv_loss: 0.017013100907206535\n",
      "iteration 2833, dc_loss: 0.015779942274093628, tv_loss: 0.01701304130256176\n",
      "iteration 2834, dc_loss: 0.015779955312609673, tv_loss: 0.017013072967529297\n",
      "iteration 2835, dc_loss: 0.01577994041144848, tv_loss: 0.017012983560562134\n",
      "iteration 2836, dc_loss: 0.015779925510287285, tv_loss: 0.017012862488627434\n",
      "iteration 2837, dc_loss: 0.015779877081513405, tv_loss: 0.01701285131275654\n",
      "iteration 2838, dc_loss: 0.01577986776828766, tv_loss: 0.017013084143400192\n",
      "iteration 2839, dc_loss: 0.015779878944158554, tv_loss: 0.017012910917401314\n",
      "iteration 2840, dc_loss: 0.015779878944158554, tv_loss: 0.017012720927596092\n",
      "iteration 2841, dc_loss: 0.015779860317707062, tv_loss: 0.017012735828757286\n",
      "iteration 2842, dc_loss: 0.015779847279191017, tv_loss: 0.017012910917401314\n",
      "iteration 2843, dc_loss: 0.01577981375157833, tv_loss: 0.017012881115078926\n",
      "iteration 2844, dc_loss: 0.01577978953719139, tv_loss: 0.0170125812292099\n",
      "iteration 2845, dc_loss: 0.0157797671854496, tv_loss: 0.01701277866959572\n",
      "iteration 2846, dc_loss: 0.015779731795191765, tv_loss: 0.017013004049658775\n",
      "iteration 2847, dc_loss: 0.015779679641127586, tv_loss: 0.017012830823659897\n",
      "iteration 2848, dc_loss: 0.015779655426740646, tv_loss: 0.01701272651553154\n",
      "iteration 2849, dc_loss: 0.01577964425086975, tv_loss: 0.01701277308166027\n",
      "iteration 2850, dc_loss: 0.015779627487063408, tv_loss: 0.01701291836798191\n",
      "iteration 2851, dc_loss: 0.01577962189912796, tv_loss: 0.017012786120176315\n",
      "iteration 2852, dc_loss: 0.015779608860611916, tv_loss: 0.01701272837817669\n",
      "iteration 2853, dc_loss: 0.015779590234160423, tv_loss: 0.01701280102133751\n",
      "iteration 2854, dc_loss: 0.015779562294483185, tv_loss: 0.0170127023011446\n",
      "iteration 2855, dc_loss: 0.015779515728354454, tv_loss: 0.017012692987918854\n",
      "iteration 2856, dc_loss: 0.015779510140419006, tv_loss: 0.01701263338327408\n",
      "iteration 2857, dc_loss: 0.01577947661280632, tv_loss: 0.017012640833854675\n",
      "iteration 2858, dc_loss: 0.015779459848999977, tv_loss: 0.017012566328048706\n",
      "iteration 2859, dc_loss: 0.015779435634613037, tv_loss: 0.01701267436146736\n",
      "iteration 2860, dc_loss: 0.0157794076949358, tv_loss: 0.017012644559144974\n",
      "iteration 2861, dc_loss: 0.015779409557580948, tv_loss: 0.017012616619467735\n",
      "iteration 2862, dc_loss: 0.015779415145516396, tv_loss: 0.01701251044869423\n",
      "iteration 2863, dc_loss: 0.015779415145516396, tv_loss: 0.017012493684887886\n",
      "iteration 2864, dc_loss: 0.015779420733451843, tv_loss: 0.01701241172850132\n",
      "iteration 2865, dc_loss: 0.0157794002443552, tv_loss: 0.01701245829463005\n",
      "iteration 2866, dc_loss: 0.01577932760119438, tv_loss: 0.017012450844049454\n",
      "iteration 2867, dc_loss: 0.015779288485646248, tv_loss: 0.017012447118759155\n",
      "iteration 2868, dc_loss: 0.015779266133904457, tv_loss: 0.017012471333146095\n",
      "iteration 2869, dc_loss: 0.015779243782162666, tv_loss: 0.017012374475598335\n",
      "iteration 2870, dc_loss: 0.01577921025454998, tv_loss: 0.017012398689985275\n",
      "iteration 2871, dc_loss: 0.015779217705130577, tv_loss: 0.017012352123856544\n",
      "iteration 2872, dc_loss: 0.01577928103506565, tv_loss: 0.017012212425470352\n",
      "iteration 2873, dc_loss: 0.015779346227645874, tv_loss: 0.0170120932161808\n",
      "iteration 2874, dc_loss: 0.015779389068484306, tv_loss: 0.017012156546115875\n",
      "iteration 2875, dc_loss: 0.015779370442032814, tv_loss: 0.01701207086443901\n",
      "iteration 2876, dc_loss: 0.015779290348291397, tv_loss: 0.01701190136373043\n",
      "iteration 2877, dc_loss: 0.015779221430420876, tv_loss: 0.017012063413858414\n",
      "iteration 2878, dc_loss: 0.01577918976545334, tv_loss: 0.017011933028697968\n",
      "iteration 2879, dc_loss: 0.01577916368842125, tv_loss: 0.01701182872056961\n",
      "iteration 2880, dc_loss: 0.01577908545732498, tv_loss: 0.01701197773218155\n",
      "iteration 2881, dc_loss: 0.015779046341776848, tv_loss: 0.017011873424053192\n",
      "iteration 2882, dc_loss: 0.015779051929712296, tv_loss: 0.017011817544698715\n",
      "iteration 2883, dc_loss: 0.015779051929712296, tv_loss: 0.017011960968375206\n",
      "iteration 2884, dc_loss: 0.01577906310558319, tv_loss: 0.017011897638440132\n",
      "iteration 2885, dc_loss: 0.015779094770550728, tv_loss: 0.017011607065796852\n",
      "iteration 2886, dc_loss: 0.015779156237840652, tv_loss: 0.017011774703860283\n",
      "iteration 2887, dc_loss: 0.01577916368842125, tv_loss: 0.017011897638440132\n",
      "iteration 2888, dc_loss: 0.015779148787260056, tv_loss: 0.017011743038892746\n",
      "iteration 2889, dc_loss: 0.015779057517647743, tv_loss: 0.0170118547976017\n",
      "iteration 2890, dc_loss: 0.01577898859977722, tv_loss: 0.017011838033795357\n",
      "iteration 2891, dc_loss: 0.015778925269842148, tv_loss: 0.017011910676956177\n",
      "iteration 2892, dc_loss: 0.015778860077261925, tv_loss: 0.017011966556310654\n",
      "iteration 2893, dc_loss: 0.015778841450810432, tv_loss: 0.01701180823147297\n",
      "iteration 2894, dc_loss: 0.01577884517610073, tv_loss: 0.017011819407343864\n",
      "iteration 2895, dc_loss: 0.015778860077261925, tv_loss: 0.017011871561408043\n",
      "iteration 2896, dc_loss: 0.01577884517610073, tv_loss: 0.017012013122439384\n",
      "iteration 2897, dc_loss: 0.015778854489326477, tv_loss: 0.017011940479278564\n",
      "iteration 2898, dc_loss: 0.015778804197907448, tv_loss: 0.01701175607740879\n",
      "iteration 2899, dc_loss: 0.015778779983520508, tv_loss: 0.017011763527989388\n",
      "iteration 2900, dc_loss: 0.015778798609972, tv_loss: 0.01701180264353752\n",
      "iteration 2901, dc_loss: 0.01577884703874588, tv_loss: 0.01701183244585991\n",
      "iteration 2902, dc_loss: 0.015778854489326477, tv_loss: 0.017011621966958046\n",
      "iteration 2903, dc_loss: 0.01577882282435894, tv_loss: 0.017011605203151703\n",
      "iteration 2904, dc_loss: 0.015778781846165657, tv_loss: 0.017011744901537895\n",
      "iteration 2905, dc_loss: 0.015778787434101105, tv_loss: 0.01701173186302185\n",
      "iteration 2906, dc_loss: 0.015778828412294388, tv_loss: 0.01701142080128193\n",
      "iteration 2907, dc_loss: 0.015778835862874985, tv_loss: 0.017011405900120735\n",
      "iteration 2908, dc_loss: 0.01577884331345558, tv_loss: 0.017011567950248718\n",
      "iteration 2909, dc_loss: 0.0157788023352623, tv_loss: 0.01701132208108902\n",
      "iteration 2910, dc_loss: 0.015778718516230583, tv_loss: 0.017011255025863647\n",
      "iteration 2911, dc_loss: 0.01577865146100521, tv_loss: 0.01701144129037857\n",
      "iteration 2912, dc_loss: 0.015778595581650734, tv_loss: 0.017011510208249092\n",
      "iteration 2913, dc_loss: 0.01577858068048954, tv_loss: 0.017011480405926704\n",
      "iteration 2914, dc_loss: 0.015778599306941032, tv_loss: 0.017011355608701706\n",
      "iteration 2915, dc_loss: 0.015778593719005585, tv_loss: 0.01701124757528305\n",
      "iteration 2916, dc_loss: 0.015778588131070137, tv_loss: 0.017011189833283424\n",
      "iteration 2917, dc_loss: 0.015778617933392525, tv_loss: 0.01701129414141178\n",
      "iteration 2918, dc_loss: 0.01577862538397312, tv_loss: 0.017011230811476707\n",
      "iteration 2919, dc_loss: 0.01577860862016678, tv_loss: 0.017011092975735664\n",
      "iteration 2920, dc_loss: 0.015778573229908943, tv_loss: 0.017011066898703575\n",
      "iteration 2921, dc_loss: 0.015778569504618645, tv_loss: 0.01701119914650917\n",
      "iteration 2922, dc_loss: 0.015778539702296257, tv_loss: 0.017011206597089767\n",
      "iteration 2923, dc_loss: 0.015778491273522377, tv_loss: 0.017011301591992378\n",
      "iteration 2924, dc_loss: 0.015778444707393646, tv_loss: 0.01701124757528305\n",
      "iteration 2925, dc_loss: 0.01577841490507126, tv_loss: 0.017011212185025215\n",
      "iteration 2926, dc_loss: 0.015778403729200363, tv_loss: 0.017011042684316635\n",
      "iteration 2927, dc_loss: 0.015778422355651855, tv_loss: 0.01701110601425171\n",
      "iteration 2928, dc_loss: 0.015778440982103348, tv_loss: 0.01701105386018753\n",
      "iteration 2929, dc_loss: 0.01577843725681305, tv_loss: 0.017011012881994247\n",
      "iteration 2930, dc_loss: 0.015778448432683945, tv_loss: 0.017011109739542007\n",
      "iteration 2931, dc_loss: 0.015778442844748497, tv_loss: 0.0170110072940588\n",
      "iteration 2932, dc_loss: 0.015778442844748497, tv_loss: 0.017010949552059174\n",
      "iteration 2933, dc_loss: 0.015778416767716408, tv_loss: 0.01701103337109089\n",
      "iteration 2934, dc_loss: 0.015778379514813423, tv_loss: 0.01701114885509014\n",
      "iteration 2935, dc_loss: 0.015778321772813797, tv_loss: 0.017011117190122604\n",
      "iteration 2936, dc_loss: 0.015778250992298126, tv_loss: 0.017011018469929695\n",
      "iteration 2937, dc_loss: 0.015778226777911186, tv_loss: 0.01701091043651104\n",
      "iteration 2938, dc_loss: 0.015778254717588425, tv_loss: 0.01701078936457634\n",
      "iteration 2939, dc_loss: 0.0157783105969429, tv_loss: 0.017010776326060295\n",
      "iteration 2940, dc_loss: 0.015778351575136185, tv_loss: 0.017010828480124474\n",
      "iteration 2941, dc_loss: 0.015778349712491035, tv_loss: 0.017010578885674477\n",
      "iteration 2942, dc_loss: 0.015778273344039917, tv_loss: 0.01701066456735134\n",
      "iteration 2943, dc_loss: 0.015778226777911186, tv_loss: 0.01701078936457634\n",
      "iteration 2944, dc_loss: 0.015778210014104843, tv_loss: 0.017010662704706192\n",
      "iteration 2945, dc_loss: 0.015778210014104843, tv_loss: 0.017010679468512535\n",
      "iteration 2946, dc_loss: 0.01577822118997574, tv_loss: 0.017010683193802834\n",
      "iteration 2947, dc_loss: 0.01577823981642723, tv_loss: 0.01701054349541664\n",
      "iteration 2948, dc_loss: 0.015778198838233948, tv_loss: 0.017010509967803955\n",
      "iteration 2949, dc_loss: 0.015778154134750366, tv_loss: 0.017010560259222984\n",
      "iteration 2950, dc_loss: 0.015778131783008575, tv_loss: 0.01701037771999836\n",
      "iteration 2951, dc_loss: 0.015778152272105217, tv_loss: 0.017010407522320747\n",
      "iteration 2952, dc_loss: 0.01577814482152462, tv_loss: 0.01701047271490097\n",
      "iteration 2953, dc_loss: 0.015778083354234695, tv_loss: 0.017010552808642387\n",
      "iteration 2954, dc_loss: 0.015778055414557457, tv_loss: 0.017010333016514778\n",
      "iteration 2955, dc_loss: 0.015778085216879845, tv_loss: 0.017010388895869255\n",
      "iteration 2956, dc_loss: 0.015778109431266785, tv_loss: 0.01701030693948269\n",
      "iteration 2957, dc_loss: 0.015778113156557083, tv_loss: 0.017010176554322243\n",
      "iteration 2958, dc_loss: 0.01577810011804104, tv_loss: 0.017010197043418884\n",
      "iteration 2959, dc_loss: 0.015778107568621635, tv_loss: 0.017010344192385674\n",
      "iteration 2960, dc_loss: 0.015778131783008575, tv_loss: 0.017010143026709557\n",
      "iteration 2961, dc_loss: 0.015778107568621635, tv_loss: 0.017010122537612915\n",
      "iteration 2962, dc_loss: 0.01577804982662201, tv_loss: 0.01701025851070881\n",
      "iteration 2963, dc_loss: 0.0157779473811388, tv_loss: 0.017010167241096497\n",
      "iteration 2964, dc_loss: 0.015777824446558952, tv_loss: 0.017010195180773735\n",
      "iteration 2965, dc_loss: 0.015777776017785072, tv_loss: 0.017010556533932686\n",
      "iteration 2966, dc_loss: 0.015777811408042908, tv_loss: 0.017010491341352463\n",
      "iteration 2967, dc_loss: 0.015777848660945892, tv_loss: 0.017010387033224106\n",
      "iteration 2968, dc_loss: 0.015777908265590668, tv_loss: 0.017010146751999855\n",
      "iteration 2969, dc_loss: 0.015777941793203354, tv_loss: 0.017010100185871124\n",
      "iteration 2970, dc_loss: 0.0157779548317194, tv_loss: 0.01701008714735508\n",
      "iteration 2971, dc_loss: 0.015777956694364548, tv_loss: 0.01701004058122635\n",
      "iteration 2972, dc_loss: 0.015777913853526115, tv_loss: 0.017010174691677094\n",
      "iteration 2973, dc_loss: 0.015777884051203728, tv_loss: 0.017010141164064407\n",
      "iteration 2974, dc_loss: 0.01577782817184925, tv_loss: 0.017010128125548363\n",
      "iteration 2975, dc_loss: 0.01577778346836567, tv_loss: 0.017010122537612915\n",
      "iteration 2976, dc_loss: 0.015777776017785072, tv_loss: 0.017010189592838287\n",
      "iteration 2977, dc_loss: 0.01577778346836567, tv_loss: 0.017010172829031944\n",
      "iteration 2978, dc_loss: 0.015777789056301117, tv_loss: 0.0170101560652256\n",
      "iteration 2979, dc_loss: 0.01577780395746231, tv_loss: 0.017009995877742767\n",
      "iteration 2980, dc_loss: 0.015777794644236565, tv_loss: 0.017010033130645752\n",
      "iteration 2981, dc_loss: 0.015777748078107834, tv_loss: 0.017010176554322243\n",
      "iteration 2982, dc_loss: 0.015777699649333954, tv_loss: 0.017009995877742767\n",
      "iteration 2983, dc_loss: 0.015777643769979477, tv_loss: 0.017010066658258438\n",
      "iteration 2984, dc_loss: 0.015777619555592537, tv_loss: 0.017010027542710304\n",
      "iteration 2985, dc_loss: 0.015777604654431343, tv_loss: 0.017010116949677467\n",
      "iteration 2986, dc_loss: 0.015777604654431343, tv_loss: 0.01701003685593605\n",
      "iteration 2987, dc_loss: 0.015777623280882835, tv_loss: 0.017009908333420753\n",
      "iteration 2988, dc_loss: 0.01577763631939888, tv_loss: 0.017010018229484558\n",
      "iteration 2989, dc_loss: 0.01577761210501194, tv_loss: 0.017009831964969635\n",
      "iteration 2990, dc_loss: 0.015777556225657463, tv_loss: 0.01700984686613083\n",
      "iteration 2991, dc_loss: 0.015777530148625374, tv_loss: 0.017010057345032692\n",
      "iteration 2992, dc_loss: 0.015777472406625748, tv_loss: 0.017009897157549858\n",
      "iteration 2993, dc_loss: 0.01577746868133545, tv_loss: 0.01700984127819538\n",
      "iteration 2994, dc_loss: 0.015777522698044777, tv_loss: 0.017009828239679337\n",
      "iteration 2995, dc_loss: 0.015777533873915672, tv_loss: 0.017009589821100235\n",
      "iteration 2996, dc_loss: 0.015777548775076866, tv_loss: 0.017009593546390533\n",
      "iteration 2997, dc_loss: 0.015777533873915672, tv_loss: 0.017009520903229713\n",
      "iteration 2998, dc_loss: 0.015777526423335075, tv_loss: 0.017009390518069267\n",
      "iteration 2999, dc_loss: 0.015777546912431717, tv_loss: 0.017009440809488297\n",
      "iteration 3000, dc_loss: 0.015777548775076866, tv_loss: 0.017009573057293892\n",
      "iteration 3001, dc_loss: 0.015777526423335075, tv_loss: 0.01700948365032673\n",
      "iteration 3002, dc_loss: 0.015777459368109703, tv_loss: 0.017009388655424118\n",
      "iteration 3003, dc_loss: 0.015777410939335823, tv_loss: 0.017009390518069267\n",
      "iteration 3004, dc_loss: 0.01577732525765896, tv_loss: 0.017009543254971504\n",
      "iteration 3005, dc_loss: 0.015777237713336945, tv_loss: 0.017009811475872993\n",
      "iteration 3006, dc_loss: 0.01577719859778881, tv_loss: 0.017009729519486427\n",
      "iteration 3007, dc_loss: 0.015777209773659706, tv_loss: 0.01700958050787449\n",
      "iteration 3008, dc_loss: 0.015777258202433586, tv_loss: 0.01700943522155285\n",
      "iteration 3009, dc_loss: 0.01577732339501381, tv_loss: 0.017009560018777847\n",
      "iteration 3010, dc_loss: 0.01577736996114254, tv_loss: 0.017009546980261803\n",
      "iteration 3011, dc_loss: 0.01577736996114254, tv_loss: 0.017009291797876358\n",
      "iteration 3012, dc_loss: 0.0157773420214653, tv_loss: 0.01700935885310173\n",
      "iteration 3013, dc_loss: 0.015777310356497765, tv_loss: 0.017009448260068893\n",
      "iteration 3014, dc_loss: 0.015777258202433586, tv_loss: 0.017009425908327103\n",
      "iteration 3015, dc_loss: 0.0157772284001112, tv_loss: 0.017009666189551353\n",
      "iteration 3016, dc_loss: 0.015777193009853363, tv_loss: 0.017009470611810684\n",
      "iteration 3017, dc_loss: 0.015777168795466423, tv_loss: 0.017009329050779343\n",
      "iteration 3018, dc_loss: 0.01577717438340187, tv_loss: 0.017009468749165535\n",
      "iteration 3019, dc_loss: 0.015777168795466423, tv_loss: 0.01700957491993904\n",
      "iteration 3020, dc_loss: 0.015777157619595528, tv_loss: 0.017009394243359566\n",
      "iteration 3021, dc_loss: 0.015777144581079483, tv_loss: 0.017009247094392776\n",
      "iteration 3022, dc_loss: 0.01577715203166008, tv_loss: 0.017009196802973747\n",
      "iteration 3023, dc_loss: 0.015777140855789185, tv_loss: 0.017009330913424492\n",
      "iteration 3024, dc_loss: 0.015777142718434334, tv_loss: 0.01700938306748867\n",
      "iteration 3025, dc_loss: 0.01577715575695038, tv_loss: 0.01700928993523121\n",
      "iteration 3026, dc_loss: 0.015777165070176125, tv_loss: 0.017008954659104347\n",
      "iteration 3027, dc_loss: 0.01577717624604702, tv_loss: 0.01700902357697487\n",
      "iteration 3028, dc_loss: 0.01577715203166008, tv_loss: 0.017009153962135315\n",
      "iteration 3029, dc_loss: 0.015777109190821648, tv_loss: 0.017009057104587555\n",
      "iteration 3030, dc_loss: 0.01577708125114441, tv_loss: 0.017009034752845764\n",
      "iteration 3031, dc_loss: 0.015777084976434708, tv_loss: 0.017008960247039795\n",
      "iteration 3032, dc_loss: 0.015777118504047394, tv_loss: 0.017008984461426735\n",
      "iteration 3033, dc_loss: 0.015777135267853737, tv_loss: 0.017008785158395767\n",
      "iteration 3034, dc_loss: 0.015777133405208588, tv_loss: 0.017008895054459572\n",
      "iteration 3035, dc_loss: 0.015777118504047394, tv_loss: 0.017008943483233452\n",
      "iteration 3036, dc_loss: 0.015777084976434708, tv_loss: 0.017008980736136436\n",
      "iteration 3037, dc_loss: 0.015777043998241425, tv_loss: 0.01700892113149166\n",
      "iteration 3038, dc_loss: 0.015776975080370903, tv_loss: 0.017009012401103973\n",
      "iteration 3039, dc_loss: 0.015776945278048515, tv_loss: 0.01700923964381218\n",
      "iteration 3040, dc_loss: 0.01577693223953247, tv_loss: 0.01700904779136181\n",
      "iteration 3041, dc_loss: 0.015776921063661575, tv_loss: 0.017009060829877853\n",
      "iteration 3042, dc_loss: 0.015776939690113068, tv_loss: 0.017009083181619644\n",
      "iteration 3043, dc_loss: 0.01577696204185486, tv_loss: 0.01700889877974987\n",
      "iteration 3044, dc_loss: 0.015776943415403366, tv_loss: 0.017008841037750244\n",
      "iteration 3045, dc_loss: 0.01577690802514553, tv_loss: 0.017008816823363304\n",
      "iteration 3046, dc_loss: 0.015776876360177994, tv_loss: 0.017008967697620392\n",
      "iteration 3047, dc_loss: 0.015776852145791054, tv_loss: 0.017008809372782707\n",
      "iteration 3048, dc_loss: 0.015776880085468292, tv_loss: 0.01700875349342823\n",
      "iteration 3049, dc_loss: 0.015776943415403366, tv_loss: 0.01700875163078308\n",
      "iteration 3050, dc_loss: 0.015776997432112694, tv_loss: 0.01700851321220398\n",
      "iteration 3051, dc_loss: 0.015776999294757843, tv_loss: 0.017008597031235695\n",
      "iteration 3052, dc_loss: 0.01577695459127426, tv_loss: 0.017008701339364052\n",
      "iteration 3053, dc_loss: 0.015776880085468292, tv_loss: 0.01700863242149353\n",
      "iteration 3054, dc_loss: 0.015776807442307472, tv_loss: 0.01700875535607338\n",
      "iteration 3055, dc_loss: 0.015776779502630234, tv_loss: 0.017008574679493904\n",
      "iteration 3056, dc_loss: 0.01577678509056568, tv_loss: 0.017008647322654724\n",
      "iteration 3057, dc_loss: 0.01577679254114628, tv_loss: 0.01700848713517189\n",
      "iteration 3058, dc_loss: 0.015776779502630234, tv_loss: 0.017008358612656593\n",
      "iteration 3059, dc_loss: 0.0157767441123724, tv_loss: 0.017008451744914055\n",
      "iteration 3060, dc_loss: 0.015776703134179115, tv_loss: 0.017008619382977486\n",
      "iteration 3061, dc_loss: 0.015776686370372772, tv_loss: 0.01700848527252674\n",
      "iteration 3062, dc_loss: 0.015776678919792175, tv_loss: 0.017008431255817413\n",
      "iteration 3063, dc_loss: 0.015776686370372772, tv_loss: 0.017008520662784576\n",
      "iteration 3064, dc_loss: 0.01577669009566307, tv_loss: 0.017008351162075996\n",
      "iteration 3065, dc_loss: 0.015776678919792175, tv_loss: 0.01700849086046219\n",
      "iteration 3066, dc_loss: 0.015776673331856728, tv_loss: 0.017008401453495026\n",
      "iteration 3067, dc_loss: 0.01577666774392128, tv_loss: 0.01700826734304428\n",
      "iteration 3068, dc_loss: 0.015776654705405235, tv_loss: 0.0170082189142704\n",
      "iteration 3069, dc_loss: 0.015776649117469788, tv_loss: 0.01700820028781891\n",
      "iteration 3070, dc_loss: 0.015776637941598892, tv_loss: 0.01700824312865734\n",
      "iteration 3071, dc_loss: 0.01577664539217949, tv_loss: 0.017008285969495773\n",
      "iteration 3072, dc_loss: 0.01577664725482464, tv_loss: 0.017008192837238312\n",
      "iteration 3073, dc_loss: 0.015776626765727997, tv_loss: 0.017008258029818535\n",
      "iteration 3074, dc_loss: 0.015776613727211952, tv_loss: 0.0170082189142704\n",
      "iteration 3075, dc_loss: 0.015776589512825012, tv_loss: 0.017008202150464058\n",
      "iteration 3076, dc_loss: 0.015776578336954117, tv_loss: 0.017008189111948013\n",
      "iteration 3077, dc_loss: 0.01577656716108322, tv_loss: 0.017008304595947266\n",
      "iteration 3078, dc_loss: 0.01577656716108322, tv_loss: 0.017008071765303612\n",
      "iteration 3079, dc_loss: 0.015776559710502625, tv_loss: 0.01700793392956257\n",
      "iteration 3080, dc_loss: 0.01577654294669628, tv_loss: 0.01700814999639988\n",
      "iteration 3081, dc_loss: 0.015776505693793297, tv_loss: 0.017008038237690926\n",
      "iteration 3082, dc_loss: 0.015776466578245163, tv_loss: 0.017007917165756226\n",
      "iteration 3083, dc_loss: 0.015776433050632477, tv_loss: 0.017008095979690552\n",
      "iteration 3084, dc_loss: 0.01577640324831009, tv_loss: 0.017008138820528984\n",
      "iteration 3085, dc_loss: 0.015776384621858597, tv_loss: 0.017008069902658463\n",
      "iteration 3086, dc_loss: 0.015776390209794044, tv_loss: 0.017008094117045403\n",
      "iteration 3087, dc_loss: 0.015776438638567924, tv_loss: 0.017007941380143166\n",
      "iteration 3088, dc_loss: 0.015776503831148148, tv_loss: 0.01700793020427227\n",
      "iteration 3089, dc_loss: 0.01577657274901867, tv_loss: 0.01700795814394951\n",
      "iteration 3090, dc_loss: 0.01577662117779255, tv_loss: 0.017007865011692047\n",
      "iteration 3091, dc_loss: 0.01577659137547016, tv_loss: 0.01700778491795063\n",
      "iteration 3092, dc_loss: 0.015776531770825386, tv_loss: 0.01700783148407936\n",
      "iteration 3093, dc_loss: 0.01577644608914852, tv_loss: 0.017007870599627495\n",
      "iteration 3094, dc_loss: 0.015776339918375015, tv_loss: 0.01700803078711033\n",
      "iteration 3095, dc_loss: 0.015776243060827255, tv_loss: 0.017008021473884583\n",
      "iteration 3096, dc_loss: 0.015776196494698524, tv_loss: 0.01700795814394951\n",
      "iteration 3097, dc_loss: 0.01577620767056942, tv_loss: 0.01700807362794876\n",
      "iteration 3098, dc_loss: 0.01577628031373024, tv_loss: 0.01700805313885212\n",
      "iteration 3099, dc_loss: 0.01577633246779442, tv_loss: 0.017007870599627495\n",
      "iteration 3100, dc_loss: 0.015776367858052254, tv_loss: 0.017007816582918167\n",
      "iteration 3101, dc_loss: 0.015776420012116432, tv_loss: 0.01700788550078869\n",
      "iteration 3102, dc_loss: 0.015776420012116432, tv_loss: 0.017007719725370407\n",
      "iteration 3103, dc_loss: 0.015776362270116806, tv_loss: 0.01700766570866108\n",
      "iteration 3104, dc_loss: 0.01577628403902054, tv_loss: 0.017007730901241302\n",
      "iteration 3105, dc_loss: 0.015776243060827255, tv_loss: 0.017007766291499138\n",
      "iteration 3106, dc_loss: 0.01577621139585972, tv_loss: 0.017007751390337944\n",
      "iteration 3107, dc_loss: 0.015776220709085464, tv_loss: 0.017007825896143913\n",
      "iteration 3108, dc_loss: 0.015776241198182106, tv_loss: 0.01700768619775772\n",
      "iteration 3109, dc_loss: 0.015776271000504494, tv_loss: 0.017007578164339066\n",
      "iteration 3110, dc_loss: 0.015776271000504494, tv_loss: 0.017007606104016304\n",
      "iteration 3111, dc_loss: 0.015776295214891434, tv_loss: 0.01700769178569317\n",
      "iteration 3112, dc_loss: 0.015776265412569046, tv_loss: 0.017007596790790558\n",
      "iteration 3113, dc_loss: 0.015776224434375763, tv_loss: 0.01700761541724205\n",
      "iteration 3114, dc_loss: 0.015776166692376137, tv_loss: 0.017007596790790558\n",
      "iteration 3115, dc_loss: 0.015776125714182854, tv_loss: 0.017007609829306602\n",
      "iteration 3116, dc_loss: 0.015776092186570168, tv_loss: 0.017007717862725258\n",
      "iteration 3117, dc_loss: 0.01577611267566681, tv_loss: 0.01700778864324093\n",
      "iteration 3118, dc_loss: 0.015776148065924644, tv_loss: 0.0170076135545969\n",
      "iteration 3119, dc_loss: 0.015776176005601883, tv_loss: 0.01700759120285511\n",
      "iteration 3120, dc_loss: 0.01577618531882763, tv_loss: 0.017007557675242424\n",
      "iteration 3121, dc_loss: 0.015776166692376137, tv_loss: 0.01700749807059765\n",
      "iteration 3122, dc_loss: 0.015776125714182854, tv_loss: 0.017007553949952126\n",
      "iteration 3123, dc_loss: 0.015776101499795914, tv_loss: 0.017007604241371155\n",
      "iteration 3124, dc_loss: 0.015776049345731735, tv_loss: 0.017007548362016678\n",
      "iteration 3125, dc_loss: 0.015775995329022408, tv_loss: 0.01700747385621071\n",
      "iteration 3126, dc_loss: 0.01577596925199032, tv_loss: 0.01700763963162899\n",
      "iteration 3127, dc_loss: 0.015775952488183975, tv_loss: 0.017007673159241676\n",
      "iteration 3128, dc_loss: 0.015775952488183975, tv_loss: 0.01700741797685623\n",
      "iteration 3129, dc_loss: 0.015775974839925766, tv_loss: 0.01700722984969616\n",
      "iteration 3130, dc_loss: 0.0157760139554739, tv_loss: 0.017007313668727875\n",
      "iteration 3131, dc_loss: 0.01577603630721569, tv_loss: 0.01700737327337265\n",
      "iteration 3132, dc_loss: 0.015776047483086586, tv_loss: 0.017007244750857353\n",
      "iteration 3133, dc_loss: 0.015776030719280243, tv_loss: 0.01700727827847004\n",
      "iteration 3134, dc_loss: 0.01577601209282875, tv_loss: 0.01700718328356743\n",
      "iteration 3135, dc_loss: 0.015775974839925766, tv_loss: 0.017007460817694664\n",
      "iteration 3136, dc_loss: 0.015775959938764572, tv_loss: 0.01700729690492153\n",
      "iteration 3137, dc_loss: 0.01577591896057129, tv_loss: 0.017007295042276382\n",
      "iteration 3138, dc_loss: 0.015775877982378006, tv_loss: 0.01700727827847004\n",
      "iteration 3139, dc_loss: 0.015775835141539574, tv_loss: 0.017007295042276382\n",
      "iteration 3140, dc_loss: 0.01577581651508808, tv_loss: 0.01700720191001892\n",
      "iteration 3141, dc_loss: 0.015775805339217186, tv_loss: 0.01700722798705101\n",
      "iteration 3142, dc_loss: 0.015775829553604126, tv_loss: 0.01700720563530922\n",
      "iteration 3143, dc_loss: 0.01577582024037838, tv_loss: 0.01700715906918049\n",
      "iteration 3144, dc_loss: 0.01577582024037838, tv_loss: 0.0170071329921484\n",
      "iteration 3145, dc_loss: 0.015775810927152634, tv_loss: 0.01700713112950325\n",
      "iteration 3146, dc_loss: 0.015775805339217186, tv_loss: 0.01700713112950325\n",
      "iteration 3147, dc_loss: 0.015775803476572037, tv_loss: 0.017007095739245415\n",
      "iteration 3148, dc_loss: 0.015775838866829872, tv_loss: 0.017007144168019295\n",
      "iteration 3149, dc_loss: 0.015775853767991066, tv_loss: 0.017007123678922653\n",
      "iteration 3150, dc_loss: 0.015775851905345917, tv_loss: 0.017006980255246162\n",
      "iteration 3151, dc_loss: 0.01577587239444256, tv_loss: 0.017006883397698402\n",
      "iteration 3152, dc_loss: 0.015775855630636215, tv_loss: 0.01700691320002079\n",
      "iteration 3153, dc_loss: 0.015775810927152634, tv_loss: 0.017006807029247284\n",
      "iteration 3154, dc_loss: 0.015775790438055992, tv_loss: 0.017006875947117805\n",
      "iteration 3155, dc_loss: 0.0157757755368948, tv_loss: 0.017006978392601013\n",
      "iteration 3156, dc_loss: 0.015775784850120544, tv_loss: 0.01700683869421482\n",
      "iteration 3157, dc_loss: 0.015775756910443306, tv_loss: 0.017006784677505493\n",
      "iteration 3158, dc_loss: 0.015775689855217934, tv_loss: 0.017006875947117805\n",
      "iteration 3159, dc_loss: 0.01577562652528286, tv_loss: 0.01700700633227825\n",
      "iteration 3160, dc_loss: 0.015775559470057487, tv_loss: 0.017006829380989075\n",
      "iteration 3161, dc_loss: 0.0157755259424448, tv_loss: 0.017006903886795044\n",
      "iteration 3162, dc_loss: 0.015775561332702637, tv_loss: 0.017007004469633102\n",
      "iteration 3163, dc_loss: 0.015775587409734726, tv_loss: 0.01700684241950512\n",
      "iteration 3164, dc_loss: 0.015775613486766815, tv_loss: 0.017006834968924522\n",
      "iteration 3165, dc_loss: 0.015775665640830994, tv_loss: 0.017006758600473404\n",
      "iteration 3166, dc_loss: 0.01577567309141159, tv_loss: 0.017006758600473404\n",
      "iteration 3167, dc_loss: 0.015775687992572784, tv_loss: 0.017006827518343925\n",
      "iteration 3168, dc_loss: 0.015775680541992188, tv_loss: 0.017006687819957733\n",
      "iteration 3169, dc_loss: 0.015775665640830994, tv_loss: 0.01700671575963497\n",
      "iteration 3170, dc_loss: 0.01577567867934704, tv_loss: 0.01700679212808609\n",
      "iteration 3171, dc_loss: 0.0157756507396698, tv_loss: 0.017006687819957733\n",
      "iteration 3172, dc_loss: 0.01577560044825077, tv_loss: 0.01700667478144169\n",
      "iteration 3173, dc_loss: 0.015775512903928757, tv_loss: 0.01700694113969803\n",
      "iteration 3174, dc_loss: 0.015775462612509727, tv_loss: 0.01700695790350437\n",
      "iteration 3175, dc_loss: 0.015775421634316444, tv_loss: 0.017006782814860344\n",
      "iteration 3176, dc_loss: 0.0157754048705101, tv_loss: 0.017006851732730865\n",
      "iteration 3177, dc_loss: 0.015775416046380997, tv_loss: 0.017006831243634224\n",
      "iteration 3178, dc_loss: 0.01577542908489704, tv_loss: 0.01700671948492527\n",
      "iteration 3179, dc_loss: 0.01577547751367092, tv_loss: 0.017006801441311836\n",
      "iteration 3180, dc_loss: 0.015775516629219055, tv_loss: 0.017006611451506615\n",
      "iteration 3181, dc_loss: 0.015775535255670547, tv_loss: 0.017006494104862213\n",
      "iteration 3182, dc_loss: 0.015775568783283234, tv_loss: 0.01700643077492714\n",
      "iteration 3183, dc_loss: 0.01577560044825077, tv_loss: 0.01700657419860363\n",
      "iteration 3184, dc_loss: 0.01577560044825077, tv_loss: 0.017006440088152885\n",
      "iteration 3185, dc_loss: 0.01577557995915413, tv_loss: 0.01700638420879841\n",
      "iteration 3186, dc_loss: 0.015775535255670547, tv_loss: 0.01700649969279766\n",
      "iteration 3187, dc_loss: 0.015775486826896667, tv_loss: 0.017006531357765198\n",
      "iteration 3188, dc_loss: 0.01577543281018734, tv_loss: 0.017006512731313705\n",
      "iteration 3189, dc_loss: 0.015775395557284355, tv_loss: 0.017006531357765198\n",
      "iteration 3190, dc_loss: 0.01577536202967167, tv_loss: 0.01700647734105587\n",
      "iteration 3191, dc_loss: 0.015775352716445923, tv_loss: 0.017006348818540573\n",
      "iteration 3192, dc_loss: 0.015775375068187714, tv_loss: 0.017006345093250275\n",
      "iteration 3193, dc_loss: 0.015775388106703758, tv_loss: 0.017006386071443558\n",
      "iteration 3194, dc_loss: 0.01577535830438137, tv_loss: 0.01700635254383087\n",
      "iteration 3195, dc_loss: 0.015775339677929878, tv_loss: 0.017006536945700645\n",
      "iteration 3196, dc_loss: 0.01577533781528473, tv_loss: 0.01700633019208908\n",
      "iteration 3197, dc_loss: 0.015775328502058983, tv_loss: 0.017006246373057365\n",
      "iteration 3198, dc_loss: 0.015775315463542938, tv_loss: 0.01700640469789505\n",
      "iteration 3199, dc_loss: 0.01577530987560749, tv_loss: 0.017006466165184975\n",
      "iteration 3200, dc_loss: 0.015775304287672043, tv_loss: 0.017006324604153633\n",
      "iteration 3201, dc_loss: 0.01577530801296234, tv_loss: 0.017006229609251022\n",
      "iteration 3202, dc_loss: 0.015775302425026894, tv_loss: 0.017006387934088707\n",
      "iteration 3203, dc_loss: 0.01577530801296234, tv_loss: 0.01700633205473423\n",
      "iteration 3204, dc_loss: 0.015775302425026894, tv_loss: 0.017006201669573784\n",
      "iteration 3205, dc_loss: 0.015775315463542938, tv_loss: 0.01700613833963871\n",
      "iteration 3206, dc_loss: 0.015775321051478386, tv_loss: 0.01700613833963871\n",
      "iteration 3207, dc_loss: 0.01577531173825264, tv_loss: 0.017006289213895798\n",
      "iteration 3208, dc_loss: 0.015775248408317566, tv_loss: 0.01700620912015438\n",
      "iteration 3209, dc_loss: 0.015775181353092194, tv_loss: 0.01700627990067005\n",
      "iteration 3210, dc_loss: 0.015775125473737717, tv_loss: 0.01700652949512005\n",
      "iteration 3211, dc_loss: 0.015775131061673164, tv_loss: 0.01700648106634617\n",
      "iteration 3212, dc_loss: 0.01577519252896309, tv_loss: 0.01700606942176819\n",
      "iteration 3213, dc_loss: 0.015775276347994804, tv_loss: 0.01700596697628498\n",
      "iteration 3214, dc_loss: 0.015775291249155998, tv_loss: 0.017006170004606247\n",
      "iteration 3215, dc_loss: 0.015775267034769058, tv_loss: 0.017006035894155502\n",
      "iteration 3216, dc_loss: 0.01577521674335003, tv_loss: 0.017005888745188713\n",
      "iteration 3217, dc_loss: 0.015775183215737343, tv_loss: 0.01700623147189617\n",
      "iteration 3218, dc_loss: 0.01577521488070488, tv_loss: 0.017006104812026024\n",
      "iteration 3219, dc_loss: 0.01577523723244667, tv_loss: 0.017006106674671173\n",
      "iteration 3220, dc_loss: 0.015775233507156372, tv_loss: 0.017005961388349533\n",
      "iteration 3221, dc_loss: 0.01577521488070488, tv_loss: 0.017005817964673042\n",
      "iteration 3222, dc_loss: 0.015775201842188835, tv_loss: 0.017005953937768936\n",
      "iteration 3223, dc_loss: 0.015775231644511223, tv_loss: 0.017005957663059235\n",
      "iteration 3224, dc_loss: 0.015775250270962715, tv_loss: 0.0170056764036417\n",
      "iteration 3225, dc_loss: 0.01577521488070488, tv_loss: 0.017005816102027893\n",
      "iteration 3226, dc_loss: 0.015775112435221672, tv_loss: 0.01700601913034916\n",
      "iteration 3227, dc_loss: 0.015774980187416077, tv_loss: 0.017006179317831993\n",
      "iteration 3228, dc_loss: 0.01577490009367466, tv_loss: 0.017006179317831993\n",
      "iteration 3229, dc_loss: 0.015774894505739212, tv_loss: 0.01700606569647789\n",
      "iteration 3230, dc_loss: 0.015774942934513092, tv_loss: 0.0170060396194458\n",
      "iteration 3231, dc_loss: 0.015775039792060852, tv_loss: 0.017006026580929756\n",
      "iteration 3232, dc_loss: 0.015775078907608986, tv_loss: 0.017005987465381622\n",
      "iteration 3233, dc_loss: 0.015775073319673538, tv_loss: 0.017006026580929756\n",
      "iteration 3234, dc_loss: 0.01577506773173809, tv_loss: 0.01700596883893013\n",
      "iteration 3235, dc_loss: 0.015775075182318687, tv_loss: 0.0170059185475111\n",
      "iteration 3236, dc_loss: 0.015775049105286598, tv_loss: 0.017005927860736847\n",
      "iteration 3237, dc_loss: 0.015775009989738464, tv_loss: 0.017005877569317818\n",
      "iteration 3238, dc_loss: 0.015775000676512718, tv_loss: 0.01700596697628498\n",
      "iteration 3239, dc_loss: 0.015775024890899658, tv_loss: 0.017005978152155876\n",
      "iteration 3240, dc_loss: 0.015775032341480255, tv_loss: 0.017005804926156998\n",
      "iteration 3241, dc_loss: 0.015775041654706, tv_loss: 0.017005808651447296\n",
      "iteration 3242, dc_loss: 0.01577502116560936, tv_loss: 0.017005780711770058\n",
      "iteration 3243, dc_loss: 0.015774965286254883, tv_loss: 0.017005788162350655\n",
      "iteration 3244, dc_loss: 0.01577489823102951, tv_loss: 0.017005611211061478\n",
      "iteration 3245, dc_loss: 0.015774862840771675, tv_loss: 0.01700586825609207\n",
      "iteration 3246, dc_loss: 0.015774859115481377, tv_loss: 0.01700596511363983\n",
      "iteration 3247, dc_loss: 0.01577487215399742, tv_loss: 0.017005961388349533\n",
      "iteration 3248, dc_loss: 0.015774860978126526, tv_loss: 0.01700584776699543\n",
      "iteration 3249, dc_loss: 0.015774885192513466, tv_loss: 0.01700584776699543\n",
      "iteration 3250, dc_loss: 0.01577484793961048, tv_loss: 0.017005905508995056\n",
      "iteration 3251, dc_loss: 0.01577480509877205, tv_loss: 0.017005788162350655\n",
      "iteration 3252, dc_loss: 0.01577477715909481, tv_loss: 0.01700572483241558\n",
      "iteration 3253, dc_loss: 0.01577475108206272, tv_loss: 0.017005790024995804\n",
      "iteration 3254, dc_loss: 0.015774767845869064, tv_loss: 0.017005836591124535\n",
      "iteration 3255, dc_loss: 0.015774792060256004, tv_loss: 0.017005758360028267\n",
      "iteration 3256, dc_loss: 0.015774833038449287, tv_loss: 0.01700560189783573\n",
      "iteration 3257, dc_loss: 0.015774864703416824, tv_loss: 0.01700553670525551\n",
      "iteration 3258, dc_loss: 0.015774855390191078, tv_loss: 0.01700548082590103\n",
      "iteration 3259, dc_loss: 0.015774806961417198, tv_loss: 0.01700565218925476\n",
      "iteration 3260, dc_loss: 0.01577475480735302, tv_loss: 0.01700555346906185\n",
      "iteration 3261, dc_loss: 0.015774697065353394, tv_loss: 0.017005497589707375\n",
      "iteration 3262, dc_loss: 0.0157746784389019, tv_loss: 0.017005575820803642\n",
      "iteration 3263, dc_loss: 0.01577470637857914, tv_loss: 0.017005568370223045\n",
      "iteration 3264, dc_loss: 0.015774765983223915, tv_loss: 0.017005370929837227\n",
      "iteration 3265, dc_loss: 0.015774844214320183, tv_loss: 0.01700521819293499\n",
      "iteration 3266, dc_loss: 0.01577489636838436, tv_loss: 0.017005203291773796\n",
      "iteration 3267, dc_loss: 0.01577489823102951, tv_loss: 0.017005182802677155\n",
      "iteration 3268, dc_loss: 0.01577489636838436, tv_loss: 0.01700526475906372\n",
      "iteration 3269, dc_loss: 0.015774868428707123, tv_loss: 0.01700519770383835\n",
      "iteration 3270, dc_loss: 0.015774797648191452, tv_loss: 0.017005158588290215\n",
      "iteration 3271, dc_loss: 0.015774689614772797, tv_loss: 0.017005309462547302\n",
      "iteration 3272, dc_loss: 0.015774589031934738, tv_loss: 0.017005519941449165\n",
      "iteration 3273, dc_loss: 0.01577453687787056, tv_loss: 0.01700546033680439\n",
      "iteration 3274, dc_loss: 0.015774544328451157, tv_loss: 0.01700538955628872\n",
      "iteration 3275, dc_loss: 0.015774603933095932, tv_loss: 0.017005376517772675\n",
      "iteration 3276, dc_loss: 0.01577465981245041, tv_loss: 0.01700533740222454\n",
      "iteration 3277, dc_loss: 0.015774721279740334, tv_loss: 0.017005212604999542\n",
      "iteration 3278, dc_loss: 0.015774762257933617, tv_loss: 0.017005182802677155\n",
      "iteration 3279, dc_loss: 0.015774734318256378, tv_loss: 0.01700512506067753\n",
      "iteration 3280, dc_loss: 0.01577468402683735, tv_loss: 0.017005164176225662\n",
      "iteration 3281, dc_loss: 0.015774616971611977, tv_loss: 0.017005300149321556\n",
      "iteration 3282, dc_loss: 0.015774570405483246, tv_loss: 0.017005298286676407\n",
      "iteration 3283, dc_loss: 0.015774518251419067, tv_loss: 0.01700516603887081\n",
      "iteration 3284, dc_loss: 0.015774482861161232, tv_loss: 0.0170051958411932\n",
      "iteration 3285, dc_loss: 0.015774520114064217, tv_loss: 0.01700524240732193\n",
      "iteration 3286, dc_loss: 0.015774566680192947, tv_loss: 0.017005205154418945\n",
      "iteration 3287, dc_loss: 0.015774628147482872, tv_loss: 0.017004944384098053\n",
      "iteration 3288, dc_loss: 0.0157746821641922, tv_loss: 0.017004959285259247\n",
      "iteration 3289, dc_loss: 0.01577470265328884, tv_loss: 0.017004964873194695\n",
      "iteration 3290, dc_loss: 0.015774665400385857, tv_loss: 0.017004961147904396\n",
      "iteration 3291, dc_loss: 0.015774594619870186, tv_loss: 0.017005035653710365\n",
      "iteration 3292, dc_loss: 0.015774525701999664, tv_loss: 0.017005063593387604\n",
      "iteration 3293, dc_loss: 0.015774540603160858, tv_loss: 0.017005153000354767\n",
      "iteration 3294, dc_loss: 0.015774531289935112, tv_loss: 0.017005128785967827\n",
      "iteration 3295, dc_loss: 0.015774540603160858, tv_loss: 0.01700511947274208\n",
      "iteration 3296, dc_loss: 0.015774555504322052, tv_loss: 0.017005005851387978\n",
      "iteration 3297, dc_loss: 0.015774564817547798, tv_loss: 0.017005054280161858\n",
      "iteration 3298, dc_loss: 0.015774548053741455, tv_loss: 0.017004936933517456\n",
      "iteration 3299, dc_loss: 0.015774520114064217, tv_loss: 0.017005113884806633\n",
      "iteration 3300, dc_loss: 0.015774520114064217, tv_loss: 0.017005205154418945\n",
      "iteration 3301, dc_loss: 0.015774501487612724, tv_loss: 0.01700500398874283\n",
      "iteration 3302, dc_loss: 0.015774499624967575, tv_loss: 0.017005011439323425\n",
      "iteration 3303, dc_loss: 0.01577446237206459, tv_loss: 0.017005080357193947\n",
      "iteration 3304, dc_loss: 0.01577439345419407, tv_loss: 0.01700514554977417\n",
      "iteration 3305, dc_loss: 0.01577436365187168, tv_loss: 0.017005259171128273\n",
      "iteration 3306, dc_loss: 0.015774354338645935, tv_loss: 0.01700521819293499\n",
      "iteration 3307, dc_loss: 0.015774330124258995, tv_loss: 0.017005039379000664\n",
      "iteration 3308, dc_loss: 0.015774358063936234, tv_loss: 0.017004968598484993\n",
      "iteration 3309, dc_loss: 0.015774404630064964, tv_loss: 0.017004916444420815\n",
      "iteration 3310, dc_loss: 0.015774445608258247, tv_loss: 0.017004750669002533\n",
      "iteration 3311, dc_loss: 0.015774423256516457, tv_loss: 0.017004821449518204\n",
      "iteration 3312, dc_loss: 0.015774359926581383, tv_loss: 0.01700485125184059\n",
      "iteration 3313, dc_loss: 0.015774358063936234, tv_loss: 0.01700500026345253\n",
      "iteration 3314, dc_loss: 0.015774359926581383, tv_loss: 0.017004918307065964\n",
      "iteration 3315, dc_loss: 0.015774346888065338, tv_loss: 0.01700473390519619\n",
      "iteration 3316, dc_loss: 0.015774328261613846, tv_loss: 0.017004985362291336\n",
      "iteration 3317, dc_loss: 0.01577432081103325, tv_loss: 0.017005041241645813\n",
      "iteration 3318, dc_loss: 0.015774304047226906, tv_loss: 0.017004944384098053\n",
      "iteration 3319, dc_loss: 0.015774307772517204, tv_loss: 0.017004745081067085\n",
      "iteration 3320, dc_loss: 0.015774322673678398, tv_loss: 0.01700489968061447\n",
      "iteration 3321, dc_loss: 0.015774346888065338, tv_loss: 0.01700487546622753\n",
      "iteration 3322, dc_loss: 0.015774348750710487, tv_loss: 0.01700485125184059\n",
      "iteration 3323, dc_loss: 0.015774346888065338, tv_loss: 0.01700473204255104\n",
      "iteration 3324, dc_loss: 0.01577432081103325, tv_loss: 0.017004890367388725\n",
      "iteration 3325, dc_loss: 0.01577432081103325, tv_loss: 0.017004961147904396\n",
      "iteration 3326, dc_loss: 0.015774333849549294, tv_loss: 0.01700483448803425\n",
      "iteration 3327, dc_loss: 0.015774322673678398, tv_loss: 0.01700483076274395\n",
      "iteration 3328, dc_loss: 0.015774324536323547, tv_loss: 0.01700475811958313\n",
      "iteration 3329, dc_loss: 0.015774324536323547, tv_loss: 0.01700461097061634\n",
      "iteration 3330, dc_loss: 0.015774287283420563, tv_loss: 0.0170047078281641\n",
      "iteration 3331, dc_loss: 0.015774236992001534, tv_loss: 0.017004715278744698\n",
      "iteration 3332, dc_loss: 0.01577422209084034, tv_loss: 0.017004674300551414\n",
      "iteration 3333, dc_loss: 0.015774210914969444, tv_loss: 0.017004841938614845\n",
      "iteration 3334, dc_loss: 0.01577416993677616, tv_loss: 0.01700483448803425\n",
      "iteration 3335, dc_loss: 0.01577417179942131, tv_loss: 0.017004765570163727\n",
      "iteration 3336, dc_loss: 0.015774212777614594, tv_loss: 0.017004651948809624\n",
      "iteration 3337, dc_loss: 0.015774281695485115, tv_loss: 0.01700449176132679\n",
      "iteration 3338, dc_loss: 0.015774313360452652, tv_loss: 0.017004510387778282\n",
      "iteration 3339, dc_loss: 0.015774313360452652, tv_loss: 0.017004702240228653\n",
      "iteration 3340, dc_loss: 0.015774277970194817, tv_loss: 0.017004646360874176\n",
      "iteration 3341, dc_loss: 0.015774236992001534, tv_loss: 0.017004501074552536\n",
      "iteration 3342, dc_loss: 0.0157741941511631, tv_loss: 0.017004411667585373\n",
      "iteration 3343, dc_loss: 0.015774168074131012, tv_loss: 0.017004605382680893\n",
      "iteration 3344, dc_loss: 0.01577412150800228, tv_loss: 0.01700485497713089\n",
      "iteration 3345, dc_loss: 0.015774086117744446, tv_loss: 0.017004668712615967\n",
      "iteration 3346, dc_loss: 0.015774058178067207, tv_loss: 0.017004458233714104\n",
      "iteration 3347, dc_loss: 0.01577405072748661, tv_loss: 0.01700461097061634\n",
      "iteration 3348, dc_loss: 0.015774047002196312, tv_loss: 0.017004644498229027\n",
      "iteration 3349, dc_loss: 0.015774071216583252, tv_loss: 0.017004551365971565\n",
      "iteration 3350, dc_loss: 0.01577410101890564, tv_loss: 0.017004599794745445\n",
      "iteration 3351, dc_loss: 0.015774087980389595, tv_loss: 0.017004502937197685\n",
      "iteration 3352, dc_loss: 0.015774087980389595, tv_loss: 0.0170045867562294\n",
      "iteration 3353, dc_loss: 0.015774093568325043, tv_loss: 0.017004596069455147\n",
      "iteration 3354, dc_loss: 0.01577407494187355, tv_loss: 0.017004413530230522\n",
      "iteration 3355, dc_loss: 0.01577409729361534, tv_loss: 0.01700454205274582\n",
      "iteration 3356, dc_loss: 0.015774132683873177, tv_loss: 0.01700446382164955\n",
      "iteration 3357, dc_loss: 0.015774095430970192, tv_loss: 0.017004286870360374\n",
      "iteration 3358, dc_loss: 0.015774071216583252, tv_loss: 0.01700439862906933\n",
      "iteration 3359, dc_loss: 0.015774045139551163, tv_loss: 0.01700456440448761\n",
      "iteration 3360, dc_loss: 0.01577402651309967, tv_loss: 0.017004603520035744\n",
      "iteration 3361, dc_loss: 0.015773992985486984, tv_loss: 0.017004309222102165\n",
      "iteration 3362, dc_loss: 0.015773968771100044, tv_loss: 0.017004432156682014\n",
      "iteration 3363, dc_loss: 0.015773968771100044, tv_loss: 0.017004409804940224\n",
      "iteration 3364, dc_loss: 0.01577397994697094, tv_loss: 0.01700427383184433\n",
      "iteration 3365, dc_loss: 0.01577398180961609, tv_loss: 0.017004327848553658\n",
      "iteration 3366, dc_loss: 0.01577398180961609, tv_loss: 0.017004400491714478\n",
      "iteration 3367, dc_loss: 0.015773991122841835, tv_loss: 0.017004165798425674\n",
      "iteration 3368, dc_loss: 0.015773987397551537, tv_loss: 0.017004279419779778\n",
      "iteration 3369, dc_loss: 0.015773965045809746, tv_loss: 0.017004281282424927\n",
      "iteration 3370, dc_loss: 0.01577393338084221, tv_loss: 0.01700429432094097\n",
      "iteration 3371, dc_loss: 0.015773875638842583, tv_loss: 0.017004283145070076\n",
      "iteration 3372, dc_loss: 0.01577388308942318, tv_loss: 0.017004327848553658\n",
      "iteration 3373, dc_loss: 0.01577393151819706, tv_loss: 0.01700427569448948\n",
      "iteration 3374, dc_loss: 0.0157739520072937, tv_loss: 0.017004188150167465\n",
      "iteration 3375, dc_loss: 0.015773965045809746, tv_loss: 0.01700420305132866\n",
      "iteration 3376, dc_loss: 0.015773961320519447, tv_loss: 0.017004191875457764\n",
      "iteration 3377, dc_loss: 0.015773944556713104, tv_loss: 0.01700405590236187\n",
      "iteration 3378, dc_loss: 0.01577390916645527, tv_loss: 0.01700417511165142\n",
      "iteration 3379, dc_loss: 0.015773892402648926, tv_loss: 0.01700412854552269\n",
      "iteration 3380, dc_loss: 0.015773862600326538, tv_loss: 0.017004262655973434\n",
      "iteration 3381, dc_loss: 0.015773830935359, tv_loss: 0.017004190012812614\n",
      "iteration 3382, dc_loss: 0.01577383279800415, tv_loss: 0.017004355788230896\n",
      "iteration 3383, dc_loss: 0.015773847699165344, tv_loss: 0.017004195600748062\n",
      "iteration 3384, dc_loss: 0.015773864462971687, tv_loss: 0.017004122957587242\n",
      "iteration 3385, dc_loss: 0.015773851424455643, tv_loss: 0.0170041061937809\n",
      "iteration 3386, dc_loss: 0.015773870050907135, tv_loss: 0.017004063352942467\n",
      "iteration 3387, dc_loss: 0.015773875638842583, tv_loss: 0.0170041024684906\n",
      "iteration 3388, dc_loss: 0.015773877501487732, tv_loss: 0.017004134133458138\n",
      "iteration 3389, dc_loss: 0.015773843973875046, tv_loss: 0.01700420491397381\n",
      "iteration 3390, dc_loss: 0.01577378623187542, tv_loss: 0.017004089429974556\n",
      "iteration 3391, dc_loss: 0.015773773193359375, tv_loss: 0.017004096880555153\n",
      "iteration 3392, dc_loss: 0.015773775056004524, tv_loss: 0.017004169523715973\n",
      "iteration 3393, dc_loss: 0.015773793682456017, tv_loss: 0.017003880813717842\n",
      "iteration 3394, dc_loss: 0.015773771330714226, tv_loss: 0.01700381003320217\n",
      "iteration 3395, dc_loss: 0.015773754566907883, tv_loss: 0.017003914341330528\n",
      "iteration 3396, dc_loss: 0.015773767605423927, tv_loss: 0.017003698274493217\n",
      "iteration 3397, dc_loss: 0.01577381230890751, tv_loss: 0.01700383797287941\n",
      "iteration 3398, dc_loss: 0.015773870050907135, tv_loss: 0.017003851011395454\n",
      "iteration 3399, dc_loss: 0.01577390544116497, tv_loss: 0.017003772780299187\n",
      "iteration 3400, dc_loss: 0.015773862600326538, tv_loss: 0.017003575339913368\n",
      "iteration 3401, dc_loss: 0.01577376388013363, tv_loss: 0.017003832384943962\n",
      "iteration 3402, dc_loss: 0.01577373780310154, tv_loss: 0.017003964632749557\n",
      "iteration 3403, dc_loss: 0.0157737098634243, tv_loss: 0.01700385846197605\n",
      "iteration 3404, dc_loss: 0.015773745253682137, tv_loss: 0.0170036181807518\n",
      "iteration 3405, dc_loss: 0.01577378623187542, tv_loss: 0.017003731802105904\n",
      "iteration 3406, dc_loss: 0.01577383279800415, tv_loss: 0.01700379140675068\n",
      "iteration 3407, dc_loss: 0.015773864462971687, tv_loss: 0.017003623768687248\n",
      "iteration 3408, dc_loss: 0.015773823484778404, tv_loss: 0.017003631219267845\n",
      "iteration 3409, dc_loss: 0.015773775056004524, tv_loss: 0.017003824934363365\n",
      "iteration 3410, dc_loss: 0.015773773193359375, tv_loss: 0.017003674060106277\n",
      "iteration 3411, dc_loss: 0.015773732215166092, tv_loss: 0.01700367033481598\n",
      "iteration 3412, dc_loss: 0.015773681923747063, tv_loss: 0.017003826797008514\n",
      "iteration 3413, dc_loss: 0.01577366702258587, tv_loss: 0.017003847286105156\n",
      "iteration 3414, dc_loss: 0.015773646533489227, tv_loss: 0.01700371317565441\n",
      "iteration 3415, dc_loss: 0.015773607417941093, tv_loss: 0.01700381189584732\n",
      "iteration 3416, dc_loss: 0.015773609280586243, tv_loss: 0.017003901302814484\n",
      "iteration 3417, dc_loss: 0.015773599967360497, tv_loss: 0.017003661021590233\n",
      "iteration 3418, dc_loss: 0.01577361859381199, tv_loss: 0.01700366660952568\n",
      "iteration 3419, dc_loss: 0.015773646533489227, tv_loss: 0.017003539949655533\n",
      "iteration 3420, dc_loss: 0.015773681923747063, tv_loss: 0.017003534361720085\n",
      "iteration 3421, dc_loss: 0.015773721039295197, tv_loss: 0.017003649845719337\n",
      "iteration 3422, dc_loss: 0.015773743391036987, tv_loss: 0.017003679648041725\n",
      "iteration 3423, dc_loss: 0.015773745253682137, tv_loss: 0.017003417015075684\n",
      "iteration 3424, dc_loss: 0.015773698687553406, tv_loss: 0.017003418877720833\n",
      "iteration 3425, dc_loss: 0.015773659572005272, tv_loss: 0.017003625631332397\n",
      "iteration 3426, dc_loss: 0.01577361486852169, tv_loss: 0.017003701999783516\n",
      "iteration 3427, dc_loss: 0.015773577615618706, tv_loss: 0.017003430053591728\n",
      "iteration 3428, dc_loss: 0.01577356643974781, tv_loss: 0.017003552988171577\n",
      "iteration 3429, dc_loss: 0.01577356830239296, tv_loss: 0.01700368896126747\n",
      "iteration 3430, dc_loss: 0.015773620456457138, tv_loss: 0.01700359582901001\n",
      "iteration 3431, dc_loss: 0.015773648396134377, tv_loss: 0.017003443092107773\n",
      "iteration 3432, dc_loss: 0.015773650258779526, tv_loss: 0.017003444954752922\n",
      "iteration 3433, dc_loss: 0.015773659572005272, tv_loss: 0.017003485932946205\n",
      "iteration 3434, dc_loss: 0.015773620456457138, tv_loss: 0.01700369268655777\n",
      "iteration 3435, dc_loss: 0.015773583203554153, tv_loss: 0.0170035008341074\n",
      "iteration 3436, dc_loss: 0.015773560851812363, tv_loss: 0.017003605142235756\n",
      "iteration 3437, dc_loss: 0.0157735887914896, tv_loss: 0.017003444954752922\n",
      "iteration 3438, dc_loss: 0.01577361859381199, tv_loss: 0.017003437504172325\n",
      "iteration 3439, dc_loss: 0.01577361673116684, tv_loss: 0.01700349524617195\n",
      "iteration 3440, dc_loss: 0.015773579478263855, tv_loss: 0.017003638669848442\n",
      "iteration 3441, dc_loss: 0.01577354408800602, tv_loss: 0.017003463581204414\n",
      "iteration 3442, dc_loss: 0.015773512423038483, tv_loss: 0.017003465443849564\n",
      "iteration 3443, dc_loss: 0.015773503109812737, tv_loss: 0.01700332947075367\n",
      "iteration 3444, dc_loss: 0.015773504972457886, tv_loss: 0.017003539949655533\n",
      "iteration 3445, dc_loss: 0.015773532912135124, tv_loss: 0.017003580927848816\n",
      "iteration 3446, dc_loss: 0.015773562714457512, tv_loss: 0.01700352318584919\n",
      "iteration 3447, dc_loss: 0.015773596242070198, tv_loss: 0.017003344371914864\n",
      "iteration 3448, dc_loss: 0.015773596242070198, tv_loss: 0.017003295943140984\n",
      "iteration 3449, dc_loss: 0.015773596242070198, tv_loss: 0.01700332574546337\n",
      "iteration 3450, dc_loss: 0.01577359065413475, tv_loss: 0.01700332574546337\n",
      "iteration 3451, dc_loss: 0.015773560851812363, tv_loss: 0.01700333133339882\n",
      "iteration 3452, dc_loss: 0.015773523598909378, tv_loss: 0.017003383487462997\n",
      "iteration 3453, dc_loss: 0.015773527324199677, tv_loss: 0.017003364861011505\n",
      "iteration 3454, dc_loss: 0.01577354408800602, tv_loss: 0.017003361135721207\n",
      "iteration 3455, dc_loss: 0.015773503109812737, tv_loss: 0.017003336921334267\n",
      "iteration 3456, dc_loss: 0.015773454681038857, tv_loss: 0.01700322888791561\n",
      "iteration 3457, dc_loss: 0.015773402526974678, tv_loss: 0.01700334995985031\n",
      "iteration 3458, dc_loss: 0.01577337086200714, tv_loss: 0.017003512009978294\n",
      "iteration 3459, dc_loss: 0.015773329883813858, tv_loss: 0.01700315624475479\n",
      "iteration 3460, dc_loss: 0.015773307532072067, tv_loss: 0.017003245651721954\n",
      "iteration 3461, dc_loss: 0.01577332429587841, tv_loss: 0.017003517597913742\n",
      "iteration 3462, dc_loss: 0.015773339197039604, tv_loss: 0.017003539949655533\n",
      "iteration 3463, dc_loss: 0.015773361548781395, tv_loss: 0.017003338783979416\n",
      "iteration 3464, dc_loss: 0.015773387625813484, tv_loss: 0.017003286629915237\n",
      "iteration 3465, dc_loss: 0.015773406252264977, tv_loss: 0.017003361135721207\n",
      "iteration 3466, dc_loss: 0.015773434191942215, tv_loss: 0.017003286629915237\n",
      "iteration 3467, dc_loss: 0.01577344536781311, tv_loss: 0.017003202810883522\n",
      "iteration 3468, dc_loss: 0.01577344536781311, tv_loss: 0.017003163695335388\n",
      "iteration 3469, dc_loss: 0.015773441642522812, tv_loss: 0.017003362998366356\n",
      "iteration 3470, dc_loss: 0.015773378312587738, tv_loss: 0.01700335182249546\n",
      "iteration 3471, dc_loss: 0.015773333609104156, tv_loss: 0.017003219574689865\n",
      "iteration 3472, dc_loss: 0.01577327959239483, tv_loss: 0.017003245651721954\n",
      "iteration 3473, dc_loss: 0.015773272141814232, tv_loss: 0.017003322020173073\n",
      "iteration 3474, dc_loss: 0.015773294493556023, tv_loss: 0.0170032549649477\n",
      "iteration 3475, dc_loss: 0.01577335223555565, tv_loss: 0.017003243789076805\n",
      "iteration 3476, dc_loss: 0.01577337086200714, tv_loss: 0.01700315624475479\n",
      "iteration 3477, dc_loss: 0.01577335223555565, tv_loss: 0.017003221437335014\n",
      "iteration 3478, dc_loss: 0.015773307532072067, tv_loss: 0.01700310967862606\n",
      "iteration 3479, dc_loss: 0.015773290768265724, tv_loss: 0.017003200948238373\n",
      "iteration 3480, dc_loss: 0.015773320570588112, tv_loss: 0.017003212124109268\n",
      "iteration 3481, dc_loss: 0.015773331746459007, tv_loss: 0.017002994194626808\n",
      "iteration 3482, dc_loss: 0.015773367136716843, tv_loss: 0.017002997919917107\n",
      "iteration 3483, dc_loss: 0.01577339693903923, tv_loss: 0.017003022134304047\n",
      "iteration 3484, dc_loss: 0.015773413702845573, tv_loss: 0.017002908512949944\n",
      "iteration 3485, dc_loss: 0.015773387625813484, tv_loss: 0.017002832144498825\n",
      "iteration 3486, dc_loss: 0.015773368999361992, tv_loss: 0.017002930864691734\n",
      "iteration 3487, dc_loss: 0.01577332615852356, tv_loss: 0.017002858221530914\n",
      "iteration 3488, dc_loss: 0.015773266553878784, tv_loss: 0.017002876847982407\n",
      "iteration 3489, dc_loss: 0.015773236751556396, tv_loss: 0.01700315997004509\n",
      "iteration 3490, dc_loss: 0.015773219987750053, tv_loss: 0.01700316183269024\n",
      "iteration 3491, dc_loss: 0.015773219987750053, tv_loss: 0.017003187909722328\n",
      "iteration 3492, dc_loss: 0.01577323116362095, tv_loss: 0.017003057524561882\n",
      "iteration 3493, dc_loss: 0.01577320136129856, tv_loss: 0.017002994194626808\n",
      "iteration 3494, dc_loss: 0.015773208811879158, tv_loss: 0.017003120854496956\n",
      "iteration 3495, dc_loss: 0.015773199498653412, tv_loss: 0.017003219574689865\n",
      "iteration 3496, dc_loss: 0.015773165971040726, tv_loss: 0.017003027722239494\n",
      "iteration 3497, dc_loss: 0.01577315852046013, tv_loss: 0.017003092914819717\n",
      "iteration 3498, dc_loss: 0.015773160383105278, tv_loss: 0.017003236338496208\n",
      "iteration 3499, dc_loss: 0.015773175284266472, tv_loss: 0.017003165557980537\n",
      "iteration 3500, dc_loss: 0.015773195773363113, tv_loss: 0.01700305938720703\n",
      "iteration 3501, dc_loss: 0.015773195773363113, tv_loss: 0.017003100365400314\n",
      "iteration 3502, dc_loss: 0.01577317714691162, tv_loss: 0.017003130167722702\n",
      "iteration 3503, dc_loss: 0.01577317900955677, tv_loss: 0.01700315624475479\n",
      "iteration 3504, dc_loss: 0.01577315852046013, tv_loss: 0.01700294017791748\n",
      "iteration 3505, dc_loss: 0.01577315293252468, tv_loss: 0.017003033310174942\n",
      "iteration 3506, dc_loss: 0.015773164108395576, tv_loss: 0.017002951353788376\n",
      "iteration 3507, dc_loss: 0.015773175284266472, tv_loss: 0.01700294017791748\n",
      "iteration 3508, dc_loss: 0.015773165971040726, tv_loss: 0.01700291410088539\n",
      "iteration 3509, dc_loss: 0.015773167833685875, tv_loss: 0.017002804204821587\n",
      "iteration 3510, dc_loss: 0.015773167833685875, tv_loss: 0.017002908512949944\n",
      "iteration 3511, dc_loss: 0.01577315293252468, tv_loss: 0.017002953216433525\n",
      "iteration 3512, dc_loss: 0.015773139894008636, tv_loss: 0.017002897337079048\n",
      "iteration 3513, dc_loss: 0.015773102641105652, tv_loss: 0.017002778127789497\n",
      "iteration 3514, dc_loss: 0.015773063525557518, tv_loss: 0.017002876847982407\n",
      "iteration 3515, dc_loss: 0.01577303186058998, tv_loss: 0.017003118991851807\n",
      "iteration 3516, dc_loss: 0.015773018822073936, tv_loss: 0.017003023996949196\n",
      "iteration 3517, dc_loss: 0.015772974118590355, tv_loss: 0.017002716660499573\n",
      "iteration 3518, dc_loss: 0.015772966668009758, tv_loss: 0.01700284332036972\n",
      "iteration 3519, dc_loss: 0.015773005783557892, tv_loss: 0.01700301095843315\n",
      "iteration 3520, dc_loss: 0.01577303744852543, tv_loss: 0.017002800479531288\n",
      "iteration 3521, dc_loss: 0.015773070976138115, tv_loss: 0.017002712935209274\n",
      "iteration 3522, dc_loss: 0.015773093327879906, tv_loss: 0.01700274832546711\n",
      "iteration 3523, dc_loss: 0.015773113816976547, tv_loss: 0.017002806067466736\n",
      "iteration 3524, dc_loss: 0.015773124992847443, tv_loss: 0.01700269617140293\n",
      "iteration 3525, dc_loss: 0.015773102641105652, tv_loss: 0.017002560198307037\n",
      "iteration 3526, dc_loss: 0.01577308215200901, tv_loss: 0.017002778127789497\n",
      "iteration 3527, dc_loss: 0.015773020684719086, tv_loss: 0.017002908512949944\n",
      "iteration 3528, dc_loss: 0.01577298901975155, tv_loss: 0.0170027744024992\n",
      "iteration 3529, dc_loss: 0.015773002058267593, tv_loss: 0.017002709209918976\n",
      "iteration 3530, dc_loss: 0.01577303744852543, tv_loss: 0.017002709209918976\n",
      "iteration 3531, dc_loss: 0.015773067250847816, tv_loss: 0.01700259931385517\n",
      "iteration 3532, dc_loss: 0.01577310636639595, tv_loss: 0.01700269803404808\n",
      "iteration 3533, dc_loss: 0.015773123130202293, tv_loss: 0.01700272597372532\n",
      "iteration 3534, dc_loss: 0.01577308401465416, tv_loss: 0.01700262539088726\n",
      "iteration 3535, dc_loss: 0.015772994607686996, tv_loss: 0.017002547159790993\n",
      "iteration 3536, dc_loss: 0.015772905200719833, tv_loss: 0.01700270175933838\n",
      "iteration 3537, dc_loss: 0.01577286422252655, tv_loss: 0.017002969980239868\n",
      "iteration 3538, dc_loss: 0.015772834420204163, tv_loss: 0.017002929002046585\n",
      "iteration 3539, dc_loss: 0.01577286794781685, tv_loss: 0.017002703621983528\n",
      "iteration 3540, dc_loss: 0.015772942453622818, tv_loss: 0.017002766951918602\n",
      "iteration 3541, dc_loss: 0.01577298901975155, tv_loss: 0.017002763226628304\n",
      "iteration 3542, dc_loss: 0.015773018822073936, tv_loss: 0.01700255647301674\n",
      "iteration 3543, dc_loss: 0.01577303372323513, tv_loss: 0.01700233481824398\n",
      "iteration 3544, dc_loss: 0.015773028135299683, tv_loss: 0.01700233668088913\n",
      "iteration 3545, dc_loss: 0.015772996470332146, tv_loss: 0.01700255088508129\n",
      "iteration 3546, dc_loss: 0.01577288843691349, tv_loss: 0.017002500593662262\n",
      "iteration 3547, dc_loss: 0.015772810205817223, tv_loss: 0.017002619802951813\n",
      "iteration 3548, dc_loss: 0.015772799029946327, tv_loss: 0.017002640292048454\n",
      "iteration 3549, dc_loss: 0.01577283814549446, tv_loss: 0.017002619802951813\n",
      "iteration 3550, dc_loss: 0.015772847458720207, tv_loss: 0.017002543434500694\n",
      "iteration 3551, dc_loss: 0.015772894024848938, tv_loss: 0.017002413049340248\n",
      "iteration 3552, dc_loss: 0.015772946178913116, tv_loss: 0.017002491280436516\n",
      "iteration 3553, dc_loss: 0.01577294059097767, tv_loss: 0.017002539709210396\n",
      "iteration 3554, dc_loss: 0.015772921964526176, tv_loss: 0.017002345994114876\n",
      "iteration 3555, dc_loss: 0.015772901475429535, tv_loss: 0.01700236089527607\n",
      "iteration 3556, dc_loss: 0.015772877261042595, tv_loss: 0.01700243540108204\n",
      "iteration 3557, dc_loss: 0.01577281579375267, tv_loss: 0.017002327367663383\n",
      "iteration 3558, dc_loss: 0.015772808343172073, tv_loss: 0.017002427950501442\n",
      "iteration 3559, dc_loss: 0.015772845596075058, tv_loss: 0.017002325505018234\n",
      "iteration 3560, dc_loss: 0.015772907063364983, tv_loss: 0.017002256587147713\n",
      "iteration 3561, dc_loss: 0.015772975981235504, tv_loss: 0.017002243548631668\n",
      "iteration 3562, dc_loss: 0.015772998332977295, tv_loss: 0.017002234235405922\n",
      "iteration 3563, dc_loss: 0.0157729871571064, tv_loss: 0.017002161592245102\n",
      "iteration 3564, dc_loss: 0.015772942453622818, tv_loss: 0.01700231246650219\n",
      "iteration 3565, dc_loss: 0.01577289216220379, tv_loss: 0.017002373933792114\n",
      "iteration 3566, dc_loss: 0.015772830694913864, tv_loss: 0.017002467066049576\n",
      "iteration 3567, dc_loss: 0.015772812068462372, tv_loss: 0.01700226403772831\n",
      "iteration 3568, dc_loss: 0.015772847458720207, tv_loss: 0.017002208158373833\n",
      "iteration 3569, dc_loss: 0.01577288657426834, tv_loss: 0.01700248196721077\n",
      "iteration 3570, dc_loss: 0.015772869810461998, tv_loss: 0.017002515494823456\n",
      "iteration 3571, dc_loss: 0.015772826969623566, tv_loss: 0.01700231246650219\n",
      "iteration 3572, dc_loss: 0.015772797167301178, tv_loss: 0.017002301290631294\n",
      "iteration 3573, dc_loss: 0.015772745013237, tv_loss: 0.017002394422888756\n",
      "iteration 3574, dc_loss: 0.015772711485624313, tv_loss: 0.017002318054437637\n",
      "iteration 3575, dc_loss: 0.015772705897688866, tv_loss: 0.017002301290631294\n",
      "iteration 3576, dc_loss: 0.015772705897688866, tv_loss: 0.017002495005726814\n",
      "iteration 3577, dc_loss: 0.015772700309753418, tv_loss: 0.017002439126372337\n",
      "iteration 3578, dc_loss: 0.015772730112075806, tv_loss: 0.01700236089527607\n",
      "iteration 3579, dc_loss: 0.015772739425301552, tv_loss: 0.017002267763018608\n",
      "iteration 3580, dc_loss: 0.015772735700011253, tv_loss: 0.017002426087856293\n",
      "iteration 3581, dc_loss: 0.015772733837366104, tv_loss: 0.01700238324701786\n",
      "iteration 3582, dc_loss: 0.015772756189107895, tv_loss: 0.017002319917082787\n",
      "iteration 3583, dc_loss: 0.015772812068462372, tv_loss: 0.01700221747159958\n",
      "iteration 3584, dc_loss: 0.015772854909300804, tv_loss: 0.017002210021018982\n",
      "iteration 3585, dc_loss: 0.015772894024848938, tv_loss: 0.017002137377858162\n",
      "iteration 3586, dc_loss: 0.015772895887494087, tv_loss: 0.017002306878566742\n",
      "iteration 3587, dc_loss: 0.01577286422252655, tv_loss: 0.017002198845148087\n",
      "iteration 3588, dc_loss: 0.01577281951904297, tv_loss: 0.017002062872052193\n",
      "iteration 3589, dc_loss: 0.015772761777043343, tv_loss: 0.01700202003121376\n",
      "iteration 3590, dc_loss: 0.015772739425301552, tv_loss: 0.017002202570438385\n",
      "iteration 3591, dc_loss: 0.015772705897688866, tv_loss: 0.01700223982334137\n",
      "iteration 3592, dc_loss: 0.015772689133882523, tv_loss: 0.017002297565340996\n",
      "iteration 3593, dc_loss: 0.015772681683301926, tv_loss: 0.017002122476696968\n",
      "iteration 3594, dc_loss: 0.01577266864478588, tv_loss: 0.01700226217508316\n",
      "iteration 3595, dc_loss: 0.015772661194205284, tv_loss: 0.01700250245630741\n",
      "iteration 3596, dc_loss: 0.015772655606269836, tv_loss: 0.017002351582050323\n",
      "iteration 3597, dc_loss: 0.01577264443039894, tv_loss: 0.017002249136567116\n",
      "iteration 3598, dc_loss: 0.015772612765431404, tv_loss: 0.017002232372760773\n",
      "iteration 3599, dc_loss: 0.01577259786427021, tv_loss: 0.017002223059535027\n",
      "iteration 3600, dc_loss: 0.01577260158956051, tv_loss: 0.017002223059535027\n",
      "iteration 3601, dc_loss: 0.015772610902786255, tv_loss: 0.017002301290631294\n",
      "iteration 3602, dc_loss: 0.015772657468914986, tv_loss: 0.01700223796069622\n",
      "iteration 3603, dc_loss: 0.01577267237007618, tv_loss: 0.017002033069729805\n",
      "iteration 3604, dc_loss: 0.01577267423272133, tv_loss: 0.017002124339342117\n",
      "iteration 3605, dc_loss: 0.01577266864478588, tv_loss: 0.017002103850245476\n",
      "iteration 3606, dc_loss: 0.015772687271237373, tv_loss: 0.01700194738805294\n",
      "iteration 3607, dc_loss: 0.01577269472181797, tv_loss: 0.017001919448375702\n",
      "iteration 3608, dc_loss: 0.015772733837366104, tv_loss: 0.017001906409859657\n",
      "iteration 3609, dc_loss: 0.01577279344201088, tv_loss: 0.017001861706376076\n",
      "iteration 3610, dc_loss: 0.015772821381688118, tv_loss: 0.01700170896947384\n",
      "iteration 3611, dc_loss: 0.015772830694913864, tv_loss: 0.01700177974998951\n",
      "iteration 3612, dc_loss: 0.01577279344201088, tv_loss: 0.01700177602469921\n",
      "iteration 3613, dc_loss: 0.015772739425301552, tv_loss: 0.017001884058117867\n",
      "iteration 3614, dc_loss: 0.015772683545947075, tv_loss: 0.017001934349536896\n",
      "iteration 3615, dc_loss: 0.015772614628076553, tv_loss: 0.017001977190375328\n",
      "iteration 3616, dc_loss: 0.015772564336657524, tv_loss: 0.017001930624246597\n",
      "iteration 3617, dc_loss: 0.01577254943549633, tv_loss: 0.01700197532773018\n",
      "iteration 3618, dc_loss: 0.01577254757285118, tv_loss: 0.017002040520310402\n",
      "iteration 3619, dc_loss: 0.015772594138979912, tv_loss: 0.01700206845998764\n",
      "iteration 3620, dc_loss: 0.015772635117173195, tv_loss: 0.017002027481794357\n",
      "iteration 3621, dc_loss: 0.01577269844710827, tv_loss: 0.017001977190375328\n",
      "iteration 3622, dc_loss: 0.015772705897688866, tv_loss: 0.017001761123538017\n",
      "iteration 3623, dc_loss: 0.015772705897688866, tv_loss: 0.017001911997795105\n",
      "iteration 3624, dc_loss: 0.015772687271237373, tv_loss: 0.01700192503631115\n",
      "iteration 3625, dc_loss: 0.015772642567753792, tv_loss: 0.017001943662762642\n",
      "iteration 3626, dc_loss: 0.015772592276334763, tv_loss: 0.017001910135149956\n",
      "iteration 3627, dc_loss: 0.015772569924592972, tv_loss: 0.017001837491989136\n",
      "iteration 3628, dc_loss: 0.01577252894639969, tv_loss: 0.017001934349536896\n",
      "iteration 3629, dc_loss: 0.015772469341754913, tv_loss: 0.017002129927277565\n",
      "iteration 3630, dc_loss: 0.01577243022620678, tv_loss: 0.01700216345489025\n",
      "iteration 3631, dc_loss: 0.01577243022620678, tv_loss: 0.017002100124955177\n",
      "iteration 3632, dc_loss: 0.015772415325045586, tv_loss: 0.01700214296579361\n",
      "iteration 3633, dc_loss: 0.01577245630323887, tv_loss: 0.01700199581682682\n",
      "iteration 3634, dc_loss: 0.015772538259625435, tv_loss: 0.017001952975988388\n",
      "iteration 3635, dc_loss: 0.015772610902786255, tv_loss: 0.017002079635858536\n",
      "iteration 3636, dc_loss: 0.01577267423272133, tv_loss: 0.017001956701278687\n",
      "iteration 3637, dc_loss: 0.015772651880979538, tv_loss: 0.01700160652399063\n",
      "iteration 3638, dc_loss: 0.015772631391882896, tv_loss: 0.017001694068312645\n",
      "iteration 3639, dc_loss: 0.015772609040141106, tv_loss: 0.017001960426568985\n",
      "iteration 3640, dc_loss: 0.015772590413689613, tv_loss: 0.017001841217279434\n",
      "iteration 3641, dc_loss: 0.015772562474012375, tv_loss: 0.01700177788734436\n",
      "iteration 3642, dc_loss: 0.015772564336657524, tv_loss: 0.017001895233988762\n",
      "iteration 3643, dc_loss: 0.015772581100463867, tv_loss: 0.017001887783408165\n",
      "iteration 3644, dc_loss: 0.015772603452205658, tv_loss: 0.017001718282699585\n",
      "iteration 3645, dc_loss: 0.015772607177495956, tv_loss: 0.017001720145344734\n",
      "iteration 3646, dc_loss: 0.015772592276334763, tv_loss: 0.01700175739824772\n",
      "iteration 3647, dc_loss: 0.015772556886076927, tv_loss: 0.017001967877149582\n",
      "iteration 3648, dc_loss: 0.015772491693496704, tv_loss: 0.017001939937472343\n",
      "iteration 3649, dc_loss: 0.015772445127367973, tv_loss: 0.017002012580633163\n",
      "iteration 3650, dc_loss: 0.01577243022620678, tv_loss: 0.017002040520310402\n",
      "iteration 3651, dc_loss: 0.015772437676787376, tv_loss: 0.017002016305923462\n",
      "iteration 3652, dc_loss: 0.015772443264722824, tv_loss: 0.017001980915665627\n",
      "iteration 3653, dc_loss: 0.015772437676787376, tv_loss: 0.01700194738805294\n",
      "iteration 3654, dc_loss: 0.015772458165884018, tv_loss: 0.017001884058117867\n",
      "iteration 3655, dc_loss: 0.015772510319948196, tv_loss: 0.017001893371343613\n",
      "iteration 3656, dc_loss: 0.015772532671689987, tv_loss: 0.017001742497086525\n",
      "iteration 3657, dc_loss: 0.015772536396980286, tv_loss: 0.017001867294311523\n",
      "iteration 3658, dc_loss: 0.01577252522110939, tv_loss: 0.017002033069729805\n",
      "iteration 3659, dc_loss: 0.015772497281432152, tv_loss: 0.017001967877149582\n",
      "iteration 3660, dc_loss: 0.015772473067045212, tv_loss: 0.01700202003121376\n",
      "iteration 3661, dc_loss: 0.015772437676787376, tv_loss: 0.01700187847018242\n",
      "iteration 3662, dc_loss: 0.015772420912981033, tv_loss: 0.017001960426568985\n",
      "iteration 3663, dc_loss: 0.015772422775626183, tv_loss: 0.01700199767947197\n",
      "iteration 3664, dc_loss: 0.01577238366007805, tv_loss: 0.017001863569021225\n",
      "iteration 3665, dc_loss: 0.015772344544529915, tv_loss: 0.017001857981085777\n",
      "iteration 3666, dc_loss: 0.015772327780723572, tv_loss: 0.01700202375650406\n",
      "iteration 3667, dc_loss: 0.01577233336865902, tv_loss: 0.017002016305923462\n",
      "iteration 3668, dc_loss: 0.015772327780723572, tv_loss: 0.0170019268989563\n",
      "iteration 3669, dc_loss: 0.015772337093949318, tv_loss: 0.017001915723085403\n",
      "iteration 3670, dc_loss: 0.015772338956594467, tv_loss: 0.017001990228891373\n",
      "iteration 3671, dc_loss: 0.01577233336865902, tv_loss: 0.017001954838633537\n",
      "iteration 3672, dc_loss: 0.015772327780723572, tv_loss: 0.017001861706376076\n",
      "iteration 3673, dc_loss: 0.01577233150601387, tv_loss: 0.017001861706376076\n",
      "iteration 3674, dc_loss: 0.015772366896271706, tv_loss: 0.017001811414957047\n",
      "iteration 3675, dc_loss: 0.01577235385775566, tv_loss: 0.017002014443278313\n",
      "iteration 3676, dc_loss: 0.015772340819239616, tv_loss: 0.01700204238295555\n",
      "iteration 3677, dc_loss: 0.015772351995110512, tv_loss: 0.017001798376441002\n",
      "iteration 3678, dc_loss: 0.015772350132465363, tv_loss: 0.017001761123538017\n",
      "iteration 3679, dc_loss: 0.015772376209497452, tv_loss: 0.017001815140247345\n",
      "iteration 3680, dc_loss: 0.01577240601181984, tv_loss: 0.017001725733280182\n",
      "iteration 3681, dc_loss: 0.01577242836356163, tv_loss: 0.017001571133732796\n",
      "iteration 3682, dc_loss: 0.01577245630323887, tv_loss: 0.017001649364829063\n",
      "iteration 3683, dc_loss: 0.015772465616464615, tv_loss: 0.017001649364829063\n",
      "iteration 3684, dc_loss: 0.015772461891174316, tv_loss: 0.017001517117023468\n",
      "iteration 3685, dc_loss: 0.01577240414917469, tv_loss: 0.017001435160636902\n",
      "iteration 3686, dc_loss: 0.01577235758304596, tv_loss: 0.017001591622829437\n",
      "iteration 3687, dc_loss: 0.015772322192788124, tv_loss: 0.017001718282699585\n",
      "iteration 3688, dc_loss: 0.015772337093949318, tv_loss: 0.017001615837216377\n",
      "iteration 3689, dc_loss: 0.01577235944569111, tv_loss: 0.01700150966644287\n",
      "iteration 3690, dc_loss: 0.015772387385368347, tv_loss: 0.0170016847550869\n",
      "iteration 3691, dc_loss: 0.01577240414917469, tv_loss: 0.01700146496295929\n",
      "iteration 3692, dc_loss: 0.015772392973303795, tv_loss: 0.017001401633024216\n",
      "iteration 3693, dc_loss: 0.015772391110658646, tv_loss: 0.017001470550894737\n",
      "iteration 3694, dc_loss: 0.01577238366007805, tv_loss: 0.017001420259475708\n",
      "iteration 3695, dc_loss: 0.01577237993478775, tv_loss: 0.017001410946249962\n",
      "iteration 3696, dc_loss: 0.015772387385368347, tv_loss: 0.017001550644636154\n",
      "iteration 3697, dc_loss: 0.015772415325045586, tv_loss: 0.01700136438012123\n",
      "iteration 3698, dc_loss: 0.015772396698594093, tv_loss: 0.017001193016767502\n",
      "iteration 3699, dc_loss: 0.01577235572040081, tv_loss: 0.01700134202837944\n",
      "iteration 3700, dc_loss: 0.015772322192788124, tv_loss: 0.017001435160636902\n",
      "iteration 3701, dc_loss: 0.015772303566336632, tv_loss: 0.01700136996805668\n",
      "iteration 3702, dc_loss: 0.01577226258814335, tv_loss: 0.017001336440443993\n",
      "iteration 3703, dc_loss: 0.01577226258814335, tv_loss: 0.017001304775476456\n",
      "iteration 3704, dc_loss: 0.015772279351949692, tv_loss: 0.017001304775476456\n",
      "iteration 3705, dc_loss: 0.015772255137562752, tv_loss: 0.017001288011670113\n",
      "iteration 3706, dc_loss: 0.01577220857143402, tv_loss: 0.017001328989863396\n",
      "iteration 3707, dc_loss: 0.015772201120853424, tv_loss: 0.017001306638121605\n",
      "iteration 3708, dc_loss: 0.01577221415936947, tv_loss: 0.017001301050186157\n",
      "iteration 3709, dc_loss: 0.015772197395563126, tv_loss: 0.01700129732489586\n",
      "iteration 3710, dc_loss: 0.01577218435704708, tv_loss: 0.017001420259475708\n",
      "iteration 3711, dc_loss: 0.01577218435704708, tv_loss: 0.017001425847411156\n",
      "iteration 3712, dc_loss: 0.015772176906466484, tv_loss: 0.017001407220959663\n",
      "iteration 3713, dc_loss: 0.01577216386795044, tv_loss: 0.017001338303089142\n",
      "iteration 3714, dc_loss: 0.015772156417369843, tv_loss: 0.017001347616314888\n",
      "iteration 3715, dc_loss: 0.01577218621969223, tv_loss: 0.017001314088702202\n",
      "iteration 3716, dc_loss: 0.015772216022014618, tv_loss: 0.017001431435346603\n",
      "iteration 3717, dc_loss: 0.015772266313433647, tv_loss: 0.017001377418637276\n",
      "iteration 3718, dc_loss: 0.015772273764014244, tv_loss: 0.01700115017592907\n",
      "iteration 3719, dc_loss: 0.015772273764014244, tv_loss: 0.01700134016573429\n",
      "iteration 3720, dc_loss: 0.0157722570002079, tv_loss: 0.017001401633024216\n",
      "iteration 3721, dc_loss: 0.015772221609950066, tv_loss: 0.017001254484057426\n",
      "iteration 3722, dc_loss: 0.015772154554724693, tv_loss: 0.01700141839683056\n",
      "iteration 3723, dc_loss: 0.0157721396535635, tv_loss: 0.01700136996805668\n",
      "iteration 3724, dc_loss: 0.01577216014266014, tv_loss: 0.01700119860470295\n",
      "iteration 3725, dc_loss: 0.01577221415936947, tv_loss: 0.017001261934638023\n",
      "iteration 3726, dc_loss: 0.015772245824337006, tv_loss: 0.01700127311050892\n",
      "iteration 3727, dc_loss: 0.015772275626659393, tv_loss: 0.017001185566186905\n",
      "iteration 3728, dc_loss: 0.01577230915427208, tv_loss: 0.017001204192638397\n",
      "iteration 3729, dc_loss: 0.015772312879562378, tv_loss: 0.01700115017592907\n",
      "iteration 3730, dc_loss: 0.01577228121459484, tv_loss: 0.0170010793954134\n",
      "iteration 3731, dc_loss: 0.015772253274917603, tv_loss: 0.017001232132315636\n",
      "iteration 3732, dc_loss: 0.015772195532917976, tv_loss: 0.017001235857605934\n",
      "iteration 3733, dc_loss: 0.015772145241498947, tv_loss: 0.017001360654830933\n",
      "iteration 3734, dc_loss: 0.015772124752402306, tv_loss: 0.017001299187541008\n",
      "iteration 3735, dc_loss: 0.015772130340337753, tv_loss: 0.017001086845993996\n",
      "iteration 3736, dc_loss: 0.015772147104144096, tv_loss: 0.017001116648316383\n",
      "iteration 3737, dc_loss: 0.015772175043821335, tv_loss: 0.017001191154122353\n",
      "iteration 3738, dc_loss: 0.015772216022014618, tv_loss: 0.017001181840896606\n",
      "iteration 3739, dc_loss: 0.015772202983498573, tv_loss: 0.017001105472445488\n",
      "iteration 3740, dc_loss: 0.01577220857143402, tv_loss: 0.01700103096663952\n",
      "iteration 3741, dc_loss: 0.015772191807627678, tv_loss: 0.01700117625296116\n",
      "iteration 3742, dc_loss: 0.015772175043821335, tv_loss: 0.017001334577798843\n",
      "iteration 3743, dc_loss: 0.01577218621969223, tv_loss: 0.01700127124786377\n",
      "iteration 3744, dc_loss: 0.015772173181176186, tv_loss: 0.017001215368509293\n",
      "iteration 3745, dc_loss: 0.015772173181176186, tv_loss: 0.017001133412122726\n",
      "iteration 3746, dc_loss: 0.01577218994498253, tv_loss: 0.017001206055283546\n",
      "iteration 3747, dc_loss: 0.015772173181176186, tv_loss: 0.017001405358314514\n",
      "iteration 3748, dc_loss: 0.01577216200530529, tv_loss: 0.01700119860470295\n",
      "iteration 3749, dc_loss: 0.015772122889757156, tv_loss: 0.0170012004673481\n",
      "iteration 3750, dc_loss: 0.015772106125950813, tv_loss: 0.017001468688249588\n",
      "iteration 3751, dc_loss: 0.015772107988595963, tv_loss: 0.017001301050186157\n",
      "iteration 3752, dc_loss: 0.015772072598338127, tv_loss: 0.017001133412122726\n",
      "iteration 3753, dc_loss: 0.01577202044427395, tv_loss: 0.017001185566186905\n",
      "iteration 3754, dc_loss: 0.01577197201550007, tv_loss: 0.01700136438012123\n",
      "iteration 3755, dc_loss: 0.015771912410855293, tv_loss: 0.017001410946249962\n",
      "iteration 3756, dc_loss: 0.015771901234984398, tv_loss: 0.01700124889612198\n",
      "iteration 3757, dc_loss: 0.015771934762597084, tv_loss: 0.01700127124786377\n",
      "iteration 3758, dc_loss: 0.01577199622988701, tv_loss: 0.017001327127218246\n",
      "iteration 3759, dc_loss: 0.015772053971886635, tv_loss: 0.01700119860470295\n",
      "iteration 3760, dc_loss: 0.015772094950079918, tv_loss: 0.017001153901219368\n",
      "iteration 3761, dc_loss: 0.015772132202982903, tv_loss: 0.017001096159219742\n",
      "iteration 3762, dc_loss: 0.01577216014266014, tv_loss: 0.017001096159219742\n",
      "iteration 3763, dc_loss: 0.015772143378853798, tv_loss: 0.01700109802186489\n",
      "iteration 3764, dc_loss: 0.015772072598338127, tv_loss: 0.017001045867800713\n",
      "iteration 3765, dc_loss: 0.01577199622988701, tv_loss: 0.017001010477542877\n",
      "iteration 3766, dc_loss: 0.015771957114338875, tv_loss: 0.017001129686832428\n",
      "iteration 3767, dc_loss: 0.01577194407582283, tv_loss: 0.017001328989863396\n",
      "iteration 3768, dc_loss: 0.015771953389048576, tv_loss: 0.017001181840896606\n",
      "iteration 3769, dc_loss: 0.015771979466080666, tv_loss: 0.017001163214445114\n",
      "iteration 3770, dc_loss: 0.015772009268403053, tv_loss: 0.017001314088702202\n",
      "iteration 3771, dc_loss: 0.01577203907072544, tv_loss: 0.017001232132315636\n",
      "iteration 3772, dc_loss: 0.015772037208080292, tv_loss: 0.017001131549477577\n",
      "iteration 3773, dc_loss: 0.01577204093337059, tv_loss: 0.017001096159219742\n",
      "iteration 3774, dc_loss: 0.01577206514775753, tv_loss: 0.017001163214445114\n",
      "iteration 3775, dc_loss: 0.01577208749949932, tv_loss: 0.017001034691929817\n",
      "iteration 3776, dc_loss: 0.015772059559822083, tv_loss: 0.017001109197735786\n",
      "iteration 3777, dc_loss: 0.015772053971886635, tv_loss: 0.01700117252767086\n",
      "iteration 3778, dc_loss: 0.01577202044427395, tv_loss: 0.017000945284962654\n",
      "iteration 3779, dc_loss: 0.015771979466080666, tv_loss: 0.017000949010252953\n",
      "iteration 3780, dc_loss: 0.015771958976984024, tv_loss: 0.017001155763864517\n",
      "iteration 3781, dc_loss: 0.015771932899951935, tv_loss: 0.01700124889612198\n",
      "iteration 3782, dc_loss: 0.015771882608532906, tv_loss: 0.017001090571284294\n",
      "iteration 3783, dc_loss: 0.01577184349298477, tv_loss: 0.01700112782418728\n",
      "iteration 3784, dc_loss: 0.015771832317113876, tv_loss: 0.017001274973154068\n",
      "iteration 3785, dc_loss: 0.01577187515795231, tv_loss: 0.017001189291477203\n",
      "iteration 3786, dc_loss: 0.015771878883242607, tv_loss: 0.017001178115606308\n",
      "iteration 3787, dc_loss: 0.015771852806210518, tv_loss: 0.01700115203857422\n",
      "iteration 3788, dc_loss: 0.015771865844726562, tv_loss: 0.017001112923026085\n",
      "iteration 3789, dc_loss: 0.01577194221317768, tv_loss: 0.017001179978251457\n",
      "iteration 3790, dc_loss: 0.015772027894854546, tv_loss: 0.017000960186123848\n",
      "iteration 3791, dc_loss: 0.01577211171388626, tv_loss: 0.017000798135995865\n",
      "iteration 3792, dc_loss: 0.01577208936214447, tv_loss: 0.017000805586576462\n",
      "iteration 3793, dc_loss: 0.015771999955177307, tv_loss: 0.017001012340188026\n",
      "iteration 3794, dc_loss: 0.01577189937233925, tv_loss: 0.017000989988446236\n",
      "iteration 3795, dc_loss: 0.015771817415952682, tv_loss: 0.017001261934638023\n",
      "iteration 3796, dc_loss: 0.015771813690662384, tv_loss: 0.017001386731863022\n",
      "iteration 3797, dc_loss: 0.01577186957001686, tv_loss: 0.017001217231154442\n",
      "iteration 3798, dc_loss: 0.015771903097629547, tv_loss: 0.01700103096663952\n",
      "iteration 3799, dc_loss: 0.01577194780111313, tv_loss: 0.017000988125801086\n",
      "iteration 3800, dc_loss: 0.015772005543112755, tv_loss: 0.017001034691929817\n",
      "iteration 3801, dc_loss: 0.01577204093337059, tv_loss: 0.01700110174715519\n",
      "iteration 3802, dc_loss: 0.015772048383951187, tv_loss: 0.017000874504446983\n",
      "iteration 3803, dc_loss: 0.015772001817822456, tv_loss: 0.017000820487737656\n",
      "iteration 3804, dc_loss: 0.01577194407582283, tv_loss: 0.01700100675225258\n",
      "iteration 3805, dc_loss: 0.015771862119436264, tv_loss: 0.017001032829284668\n",
      "iteration 3806, dc_loss: 0.015771841630339622, tv_loss: 0.017000963911414146\n",
      "iteration 3807, dc_loss: 0.01577187143266201, tv_loss: 0.017000870779156685\n",
      "iteration 3808, dc_loss: 0.01577187143266201, tv_loss: 0.01700097694993019\n",
      "iteration 3809, dc_loss: 0.01577187143266201, tv_loss: 0.017000997439026833\n",
      "iteration 3810, dc_loss: 0.01577192358672619, tv_loss: 0.01700088568031788\n",
      "iteration 3811, dc_loss: 0.015771955251693726, tv_loss: 0.0170008335262537\n",
      "iteration 3812, dc_loss: 0.01577199250459671, tv_loss: 0.017000848427414894\n",
      "iteration 3813, dc_loss: 0.01577199064195156, tv_loss: 0.01700083538889885\n",
      "iteration 3814, dc_loss: 0.015771960839629173, tv_loss: 0.01700066216289997\n",
      "iteration 3815, dc_loss: 0.015771910548210144, tv_loss: 0.017000827938318253\n",
      "iteration 3816, dc_loss: 0.015771865844726562, tv_loss: 0.017000827938318253\n",
      "iteration 3817, dc_loss: 0.01577182114124298, tv_loss: 0.01700095273554325\n",
      "iteration 3818, dc_loss: 0.015771811828017235, tv_loss: 0.017000937834382057\n",
      "iteration 3819, dc_loss: 0.01577182672917843, tv_loss: 0.017000805586576462\n",
      "iteration 3820, dc_loss: 0.015771877020597458, tv_loss: 0.017000645399093628\n",
      "iteration 3821, dc_loss: 0.015771878883242607, tv_loss: 0.017000792548060417\n",
      "iteration 3822, dc_loss: 0.01577185094356537, tv_loss: 0.017000887542963028\n",
      "iteration 3823, dc_loss: 0.01577182114124298, tv_loss: 0.017000753432512283\n",
      "iteration 3824, dc_loss: 0.0157717727124691, tv_loss: 0.017000827938318253\n",
      "iteration 3825, dc_loss: 0.015771765261888504, tv_loss: 0.017001092433929443\n",
      "iteration 3826, dc_loss: 0.015771793201565742, tv_loss: 0.017001036554574966\n",
      "iteration 3827, dc_loss: 0.015771817415952682, tv_loss: 0.017000960186123848\n",
      "iteration 3828, dc_loss: 0.015771860256791115, tv_loss: 0.017000848427414894\n",
      "iteration 3829, dc_loss: 0.015771914273500443, tv_loss: 0.01700076460838318\n",
      "iteration 3830, dc_loss: 0.01577191799879074, tv_loss: 0.017000891268253326\n",
      "iteration 3831, dc_loss: 0.015771884471178055, tv_loss: 0.017000941559672356\n",
      "iteration 3832, dc_loss: 0.015771862119436264, tv_loss: 0.017000917345285416\n",
      "iteration 3833, dc_loss: 0.015771834179759026, tv_loss: 0.017000937834382057\n",
      "iteration 3834, dc_loss: 0.015771806240081787, tv_loss: 0.017000943422317505\n",
      "iteration 3835, dc_loss: 0.01577177830040455, tv_loss: 0.017001086845993996\n",
      "iteration 3836, dc_loss: 0.0157717764377594, tv_loss: 0.01700098067522049\n",
      "iteration 3837, dc_loss: 0.015771789476275444, tv_loss: 0.017000872641801834\n",
      "iteration 3838, dc_loss: 0.01577180251479149, tv_loss: 0.01700092852115631\n",
      "iteration 3839, dc_loss: 0.015771782025694847, tv_loss: 0.01700090803205967\n",
      "iteration 3840, dc_loss: 0.01577175222337246, tv_loss: 0.017000924795866013\n",
      "iteration 3841, dc_loss: 0.015771718695759773, tv_loss: 0.017000911757349968\n",
      "iteration 3842, dc_loss: 0.015771714970469475, tv_loss: 0.017001088708639145\n",
      "iteration 3843, dc_loss: 0.015771759673953056, tv_loss: 0.017000896856188774\n",
      "iteration 3844, dc_loss: 0.015771787613630295, tv_loss: 0.017000814899802208\n",
      "iteration 3845, dc_loss: 0.015771785750985146, tv_loss: 0.01700075902044773\n",
      "iteration 3846, dc_loss: 0.01577179506421089, tv_loss: 0.017000604420900345\n",
      "iteration 3847, dc_loss: 0.015771804377436638, tv_loss: 0.017000675201416016\n",
      "iteration 3848, dc_loss: 0.01577180251479149, tv_loss: 0.017000699415802956\n",
      "iteration 3849, dc_loss: 0.015771791338920593, tv_loss: 0.017000731080770493\n",
      "iteration 3850, dc_loss: 0.015771783888339996, tv_loss: 0.017000695690512657\n",
      "iteration 3851, dc_loss: 0.015771793201565742, tv_loss: 0.017000606283545494\n",
      "iteration 3852, dc_loss: 0.01577182486653328, tv_loss: 0.017000528052449226\n",
      "iteration 3853, dc_loss: 0.01577181927859783, tv_loss: 0.017000675201416016\n",
      "iteration 3854, dc_loss: 0.01577180065214634, tv_loss: 0.017000572755932808\n",
      "iteration 3855, dc_loss: 0.015771793201565742, tv_loss: 0.017000585794448853\n",
      "iteration 3856, dc_loss: 0.015771793201565742, tv_loss: 0.017000705003738403\n",
      "iteration 3857, dc_loss: 0.015771759673953056, tv_loss: 0.017000582069158554\n",
      "iteration 3858, dc_loss: 0.01577172428369522, tv_loss: 0.017000636085867882\n",
      "iteration 3859, dc_loss: 0.015771713107824326, tv_loss: 0.01700064353644848\n",
      "iteration 3860, dc_loss: 0.01577175408601761, tv_loss: 0.017000708729028702\n",
      "iteration 3861, dc_loss: 0.0157717764377594, tv_loss: 0.017000678926706314\n",
      "iteration 3862, dc_loss: 0.01577172614634037, tv_loss: 0.017000628635287285\n",
      "iteration 3863, dc_loss: 0.015771683305501938, tv_loss: 0.017000803723931313\n",
      "iteration 3864, dc_loss: 0.01577162928879261, tv_loss: 0.017000703141093254\n",
      "iteration 3865, dc_loss: 0.01577160321176052, tv_loss: 0.017000742256641388\n",
      "iteration 3866, dc_loss: 0.015771659091114998, tv_loss: 0.017000852152705193\n",
      "iteration 3867, dc_loss: 0.015771713107824326, tv_loss: 0.017000824213027954\n",
      "iteration 3868, dc_loss: 0.015771688893437386, tv_loss: 0.017000900581479073\n",
      "iteration 3869, dc_loss: 0.015771640464663506, tv_loss: 0.017000755295157433\n",
      "iteration 3870, dc_loss: 0.015771595761179924, tv_loss: 0.017000775784254074\n",
      "iteration 3871, dc_loss: 0.015771599486470222, tv_loss: 0.017000878229737282\n",
      "iteration 3872, dc_loss: 0.01577162556350231, tv_loss: 0.017000814899802208\n",
      "iteration 3873, dc_loss: 0.015771659091114998, tv_loss: 0.017000678926706314\n",
      "iteration 3874, dc_loss: 0.015771666541695595, tv_loss: 0.017000703141093254\n",
      "iteration 3875, dc_loss: 0.015771687030792236, tv_loss: 0.017000719904899597\n",
      "iteration 3876, dc_loss: 0.01577170565724373, tv_loss: 0.017000630497932434\n",
      "iteration 3877, dc_loss: 0.015771696344017982, tv_loss: 0.017000410705804825\n",
      "iteration 3878, dc_loss: 0.01577172614634037, tv_loss: 0.01700056903064251\n",
      "iteration 3879, dc_loss: 0.015771761536598206, tv_loss: 0.01700073480606079\n",
      "iteration 3880, dc_loss: 0.01577177830040455, tv_loss: 0.017000535503029823\n",
      "iteration 3881, dc_loss: 0.01577177084982395, tv_loss: 0.017000513151288033\n",
      "iteration 3882, dc_loss: 0.015771731734275818, tv_loss: 0.01700069196522236\n",
      "iteration 3883, dc_loss: 0.015771711245179176, tv_loss: 0.01700064167380333\n",
      "iteration 3884, dc_loss: 0.01577170565724373, tv_loss: 0.01700066402554512\n",
      "iteration 3885, dc_loss: 0.015771694481372833, tv_loss: 0.017000772058963776\n",
      "iteration 3886, dc_loss: 0.01577169820666313, tv_loss: 0.017000656574964523\n",
      "iteration 3887, dc_loss: 0.015771711245179176, tv_loss: 0.017000526189804077\n",
      "iteration 3888, dc_loss: 0.01577167958021164, tv_loss: 0.01700061745941639\n",
      "iteration 3889, dc_loss: 0.01577160693705082, tv_loss: 0.017000854015350342\n",
      "iteration 3890, dc_loss: 0.015771588310599327, tv_loss: 0.01700081117451191\n",
      "iteration 3891, dc_loss: 0.015771573409438133, tv_loss: 0.017000745981931686\n",
      "iteration 3892, dc_loss: 0.01577155292034149, tv_loss: 0.017000829800963402\n",
      "iteration 3893, dc_loss: 0.015771590173244476, tv_loss: 0.01700078696012497\n",
      "iteration 3894, dc_loss: 0.015771683305501938, tv_loss: 0.017000602558255196\n",
      "iteration 3895, dc_loss: 0.015771755948662758, tv_loss: 0.017000501975417137\n",
      "iteration 3896, dc_loss: 0.015771759673953056, tv_loss: 0.017000628635287285\n",
      "iteration 3897, dc_loss: 0.01577175408601761, tv_loss: 0.017000649124383926\n",
      "iteration 3898, dc_loss: 0.015771714970469475, tv_loss: 0.0170007161796093\n",
      "iteration 3899, dc_loss: 0.015771685168147087, tv_loss: 0.017000846564769745\n",
      "iteration 3900, dc_loss: 0.01577163115143776, tv_loss: 0.017000753432512283\n",
      "iteration 3901, dc_loss: 0.01577163115143776, tv_loss: 0.01700061932206154\n",
      "iteration 3902, dc_loss: 0.01577162928879261, tv_loss: 0.017000416293740273\n",
      "iteration 3903, dc_loss: 0.01577164977788925, tv_loss: 0.017000596970319748\n",
      "iteration 3904, dc_loss: 0.015771688893437386, tv_loss: 0.017000555992126465\n",
      "iteration 3905, dc_loss: 0.01577169820666313, tv_loss: 0.017000606283545494\n",
      "iteration 3906, dc_loss: 0.015771672129631042, tv_loss: 0.017000481486320496\n",
      "iteration 3907, dc_loss: 0.015771614387631416, tv_loss: 0.017000418156385422\n",
      "iteration 3908, dc_loss: 0.01577157899737358, tv_loss: 0.017000434920191765\n",
      "iteration 3909, dc_loss: 0.015771549195051193, tv_loss: 0.01700049825012684\n",
      "iteration 3910, dc_loss: 0.015771521255373955, tv_loss: 0.017000650987029076\n",
      "iteration 3911, dc_loss: 0.015771498903632164, tv_loss: 0.01700056530535221\n",
      "iteration 3912, dc_loss: 0.01577152870595455, tv_loss: 0.01700044423341751\n",
      "iteration 3913, dc_loss: 0.01577158272266388, tv_loss: 0.017000451683998108\n",
      "iteration 3914, dc_loss: 0.01577160693705082, tv_loss: 0.01700051687657833\n",
      "iteration 3915, dc_loss: 0.015771619975566864, tv_loss: 0.017000354826450348\n",
      "iteration 3916, dc_loss: 0.015771634876728058, tv_loss: 0.017000311985611916\n",
      "iteration 3917, dc_loss: 0.01577162556350231, tv_loss: 0.017000412568449974\n",
      "iteration 3918, dc_loss: 0.01577158086001873, tv_loss: 0.01700054481625557\n",
      "iteration 3919, dc_loss: 0.015771470963954926, tv_loss: 0.017000576481223106\n",
      "iteration 3920, dc_loss: 0.015771398320794106, tv_loss: 0.017000563442707062\n",
      "iteration 3921, dc_loss: 0.015771403908729553, tv_loss: 0.01700058951973915\n",
      "iteration 3922, dc_loss: 0.015771448612213135, tv_loss: 0.017000535503029823\n",
      "iteration 3923, dc_loss: 0.01577148400247097, tv_loss: 0.017000645399093628\n",
      "iteration 3924, dc_loss: 0.015771517530083656, tv_loss: 0.017000511288642883\n",
      "iteration 3925, dc_loss: 0.015771517530083656, tv_loss: 0.017000460997223854\n",
      "iteration 3926, dc_loss: 0.015771564096212387, tv_loss: 0.01700044795870781\n",
      "iteration 3927, dc_loss: 0.015771593898534775, tv_loss: 0.017000405117869377\n",
      "iteration 3928, dc_loss: 0.015771599486470222, tv_loss: 0.017000259831547737\n",
      "iteration 3929, dc_loss: 0.015771612524986267, tv_loss: 0.01700027473270893\n",
      "iteration 3930, dc_loss: 0.01577162556350231, tv_loss: 0.01700042374432087\n",
      "iteration 3931, dc_loss: 0.015771614387631416, tv_loss: 0.017000406980514526\n",
      "iteration 3932, dc_loss: 0.01577157713472843, tv_loss: 0.017000334337353706\n",
      "iteration 3933, dc_loss: 0.015771523118019104, tv_loss: 0.017000341787934303\n",
      "iteration 3934, dc_loss: 0.015771524980664253, tv_loss: 0.01700042001903057\n",
      "iteration 3935, dc_loss: 0.01577151194214821, tv_loss: 0.017000438645482063\n",
      "iteration 3936, dc_loss: 0.01577151007950306, tv_loss: 0.017000367864966393\n",
      "iteration 3937, dc_loss: 0.0157715305685997, tv_loss: 0.017000341787934303\n",
      "iteration 3938, dc_loss: 0.015771521255373955, tv_loss: 0.017000410705804825\n",
      "iteration 3939, dc_loss: 0.01577153243124485, tv_loss: 0.017000583931803703\n",
      "iteration 3940, dc_loss: 0.015771515667438507, tv_loss: 0.01700044609606266\n",
      "iteration 3941, dc_loss: 0.01577148213982582, tv_loss: 0.017000434920191765\n",
      "iteration 3942, dc_loss: 0.01577145978808403, tv_loss: 0.01700064353644848\n",
      "iteration 3943, dc_loss: 0.01577145606279373, tv_loss: 0.01700054295361042\n",
      "iteration 3944, dc_loss: 0.01577146351337433, tv_loss: 0.01700044423341751\n",
      "iteration 3945, dc_loss: 0.01577148586511612, tv_loss: 0.017000405117869377\n",
      "iteration 3946, dc_loss: 0.0157715305685997, tv_loss: 0.01700056903064251\n",
      "iteration 3947, dc_loss: 0.01577156037092209, tv_loss: 0.017000533640384674\n",
      "iteration 3948, dc_loss: 0.01577155664563179, tv_loss: 0.01700061559677124\n",
      "iteration 3949, dc_loss: 0.01577155478298664, tv_loss: 0.017000382766127586\n",
      "iteration 3950, dc_loss: 0.015771564096212387, tv_loss: 0.0170003529638052\n",
      "iteration 3951, dc_loss: 0.015771523118019104, tv_loss: 0.017000487074255943\n",
      "iteration 3952, dc_loss: 0.015771465376019478, tv_loss: 0.017000585794448853\n",
      "iteration 3953, dc_loss: 0.01577143371105194, tv_loss: 0.01700049452483654\n",
      "iteration 3954, dc_loss: 0.01577138714492321, tv_loss: 0.01700056903064251\n",
      "iteration 3955, dc_loss: 0.015771398320794106, tv_loss: 0.017000580206513405\n",
      "iteration 3956, dc_loss: 0.015771424397826195, tv_loss: 0.017000460997223854\n",
      "iteration 3957, dc_loss: 0.015771429985761642, tv_loss: 0.017000317573547363\n",
      "iteration 3958, dc_loss: 0.015771429985761642, tv_loss: 0.017000345513224602\n",
      "iteration 3959, dc_loss: 0.015771443024277687, tv_loss: 0.01700039766728878\n",
      "iteration 3960, dc_loss: 0.015771454200148582, tv_loss: 0.01700025424361229\n",
      "iteration 3961, dc_loss: 0.01577143371105194, tv_loss: 0.01700012758374214\n",
      "iteration 3962, dc_loss: 0.01577143184840679, tv_loss: 0.01700027845799923\n",
      "iteration 3963, dc_loss: 0.015771424397826195, tv_loss: 0.01700013317167759\n",
      "iteration 3964, dc_loss: 0.015771426260471344, tv_loss: 0.017000071704387665\n",
      "iteration 3965, dc_loss: 0.01577143371105194, tv_loss: 0.0170001108199358\n",
      "iteration 3966, dc_loss: 0.015771428123116493, tv_loss: 0.01700013317167759\n",
      "iteration 3967, dc_loss: 0.01577141135931015, tv_loss: 0.017000187188386917\n",
      "iteration 3968, dc_loss: 0.015771420672535896, tv_loss: 0.017000121995806694\n",
      "iteration 3969, dc_loss: 0.01577143557369709, tv_loss: 0.0170002318918705\n",
      "iteration 3970, dc_loss: 0.015771450474858284, tv_loss: 0.017000149935483932\n",
      "iteration 3971, dc_loss: 0.01577146165072918, tv_loss: 0.01700010895729065\n",
      "iteration 3972, dc_loss: 0.015771469101309776, tv_loss: 0.017000112682580948\n",
      "iteration 3973, dc_loss: 0.015771470963954926, tv_loss: 0.017000047490000725\n",
      "iteration 3974, dc_loss: 0.015771476551890373, tv_loss: 0.01700025424361229\n",
      "iteration 3975, dc_loss: 0.015771450474858284, tv_loss: 0.017000121995806694\n",
      "iteration 3976, dc_loss: 0.015771426260471344, tv_loss: 0.017000095918774605\n",
      "iteration 3977, dc_loss: 0.01577138341963291, tv_loss: 0.01700020022690296\n",
      "iteration 3978, dc_loss: 0.01577134244143963, tv_loss: 0.017000289633870125\n",
      "iteration 3979, dc_loss: 0.01577136106789112, tv_loss: 0.017000248655676842\n",
      "iteration 3980, dc_loss: 0.015771400183439255, tv_loss: 0.01700015738606453\n",
      "iteration 3981, dc_loss: 0.015771443024277687, tv_loss: 0.017000144347548485\n",
      "iteration 3982, dc_loss: 0.015771493315696716, tv_loss: 0.017000136896967888\n",
      "iteration 3983, dc_loss: 0.015771465376019478, tv_loss: 0.01700008101761341\n",
      "iteration 3984, dc_loss: 0.015771470963954926, tv_loss: 0.016999950632452965\n",
      "iteration 3985, dc_loss: 0.01577145978808403, tv_loss: 0.01700015366077423\n",
      "iteration 3986, dc_loss: 0.01577143371105194, tv_loss: 0.01700032874941826\n",
      "iteration 3987, dc_loss: 0.01577138900756836, tv_loss: 0.017000358551740646\n",
      "iteration 3988, dc_loss: 0.015771331265568733, tv_loss: 0.017000241205096245\n",
      "iteration 3989, dc_loss: 0.01577131636440754, tv_loss: 0.017000088468194008\n",
      "iteration 3990, dc_loss: 0.01577134244143963, tv_loss: 0.017000265419483185\n",
      "iteration 3991, dc_loss: 0.01577139087021351, tv_loss: 0.017000297084450722\n",
      "iteration 3992, dc_loss: 0.015771446749567986, tv_loss: 0.017000257968902588\n",
      "iteration 3993, dc_loss: 0.015771443024277687, tv_loss: 0.01700017601251602\n",
      "iteration 3994, dc_loss: 0.015771426260471344, tv_loss: 0.017000116407871246\n",
      "iteration 3995, dc_loss: 0.015771370381116867, tv_loss: 0.017000099644064903\n",
      "iteration 3996, dc_loss: 0.01577131636440754, tv_loss: 0.017000390216708183\n",
      "iteration 3997, dc_loss: 0.015771258622407913, tv_loss: 0.017000479623675346\n",
      "iteration 3998, dc_loss: 0.015771223232150078, tv_loss: 0.017000483348965645\n",
      "iteration 3999, dc_loss: 0.015771236270666122, tv_loss: 0.017000362277030945\n",
      "iteration 4000, dc_loss: 0.01577131822705269, tv_loss: 0.017000414431095123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3986a077d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scheduler = lambda t: 0.8 ** (t // 400) * 1e-1\n",
    "optimizer = torchopt.adam(lr=0.1)\n",
    "kspace, (mean, std), masked_kspace, mask, csm = dataset_4[15]\n",
    "field = Grid((640, 368), mean, std)\n",
    "params, image_list_ADAM = reconstruct(\n",
    "    field,\n",
    "    torch.rand(1, 2),\n",
    "    masked_kspace,\n",
    "    csm,\n",
    "    mask,\n",
    "    alpha=0.005,\n",
    "    optimizer=optimizer,\n",
    "    iterations=4000,\n",
    "    device=torch.device(\"cuda\"),\n",
    ")\n",
    "plt.imshow(complex_abs(image_list_ADAM[-1]), cmap=\"gray\", vmax=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIREN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[6.0409e-04, 7.4484e-05]]]) tensor([[[1.2870, 1.2171]]])\n",
      "torch.Size([1, 1, 2]) torch.Size([1, 1, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| params.keys(): dict_keys(['net.0.linear.weight', 'net.0.linear.bias', 'net.1.linear.weight', 'net.1.linear.bias', 'net.2.linear.weight', 'net.2.linear.bias', 'net.3.linear.weight', 'net.3.linear.bias', 'net.4.linear.weight', 'net.4.linear.bias', 'net.5.linear.weight', 'net.5.linear.bias', 'net.6.linear.weight', 'net.6.linear.bias', 'net.7.linear.weight', 'net.7.linear.bias', 'net.8.linear.weight', 'net.8.linear.bias', 'net.9.weight', 'net.9.bias'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "iteration 1, dc_loss: 3.011019706726074, tv_loss: 0.0017327064415439963\n",
      "iteration 2, dc_loss: 2.9130382537841797, tv_loss: 0.002176767447963357\n",
      "iteration 3, dc_loss: 2.850787401199341, tv_loss: 0.004349838476628065\n",
      "iteration 4, dc_loss: 2.779738664627075, tv_loss: 0.006884200498461723\n",
      "iteration 5, dc_loss: 2.739759922027588, tv_loss: 0.008020037785172462\n",
      "iteration 6, dc_loss: 2.715916395187378, tv_loss: 0.00864491518586874\n",
      "iteration 7, dc_loss: 2.6938934326171875, tv_loss: 0.009087405167520046\n",
      "iteration 8, dc_loss: 2.675283432006836, tv_loss: 0.009270278736948967\n",
      "iteration 9, dc_loss: 2.6572728157043457, tv_loss: 0.009474550373852253\n",
      "iteration 10, dc_loss: 2.639674425125122, tv_loss: 0.009803587570786476\n",
      "iteration 11, dc_loss: 2.6227874755859375, tv_loss: 0.00976830068975687\n",
      "iteration 12, dc_loss: 2.6067543029785156, tv_loss: 0.009428935125470161\n",
      "iteration 13, dc_loss: 2.591423749923706, tv_loss: 0.009527657181024551\n",
      "iteration 14, dc_loss: 2.5770721435546875, tv_loss: 0.009319865144789219\n",
      "iteration 15, dc_loss: 2.562526226043701, tv_loss: 0.00894583947956562\n",
      "iteration 16, dc_loss: 2.5480542182922363, tv_loss: 0.008838821202516556\n",
      "iteration 17, dc_loss: 2.534332036972046, tv_loss: 0.008383606560528278\n",
      "iteration 18, dc_loss: 2.521318197250366, tv_loss: 0.008522258140146732\n",
      "iteration 19, dc_loss: 2.5145764350891113, tv_loss: 0.007000250741839409\n",
      "iteration 20, dc_loss: 2.512187957763672, tv_loss: 0.008399502374231815\n",
      "iteration 21, dc_loss: 2.483168125152588, tv_loss: 0.006769952829927206\n",
      "iteration 22, dc_loss: 2.48060941696167, tv_loss: 0.0054505025036633015\n",
      "iteration 23, dc_loss: 2.459995746612549, tv_loss: 0.006431024521589279\n",
      "iteration 24, dc_loss: 2.448087453842163, tv_loss: 0.00628888513892889\n",
      "iteration 25, dc_loss: 2.434100866317749, tv_loss: 0.00608158390969038\n",
      "iteration 26, dc_loss: 2.4207563400268555, tv_loss: 0.006221133749932051\n",
      "iteration 27, dc_loss: 2.4079482555389404, tv_loss: 0.006251935381442308\n",
      "iteration 28, dc_loss: 2.39479660987854, tv_loss: 0.006376486737281084\n",
      "iteration 29, dc_loss: 2.3818607330322266, tv_loss: 0.006391352973878384\n",
      "iteration 30, dc_loss: 2.370069980621338, tv_loss: 0.006245597265660763\n",
      "iteration 31, dc_loss: 2.3573789596557617, tv_loss: 0.006257118657231331\n",
      "iteration 32, dc_loss: 2.3451664447784424, tv_loss: 0.006444177590310574\n",
      "iteration 33, dc_loss: 2.3335039615631104, tv_loss: 0.0065000844188034534\n",
      "iteration 34, dc_loss: 2.321035623550415, tv_loss: 0.006631897762417793\n",
      "iteration 35, dc_loss: 2.3095359802246094, tv_loss: 0.006828825455158949\n",
      "iteration 36, dc_loss: 2.2977614402770996, tv_loss: 0.0069916448555886745\n",
      "iteration 37, dc_loss: 2.2860546112060547, tv_loss: 0.007108993828296661\n",
      "iteration 38, dc_loss: 2.2747445106506348, tv_loss: 0.00716735515743494\n",
      "iteration 39, dc_loss: 2.263352632522583, tv_loss: 0.007256892044097185\n",
      "iteration 40, dc_loss: 2.251694917678833, tv_loss: 0.00744652608409524\n",
      "iteration 41, dc_loss: 2.240520477294922, tv_loss: 0.007639609277248383\n",
      "iteration 42, dc_loss: 2.229175090789795, tv_loss: 0.007726793177425861\n",
      "iteration 43, dc_loss: 2.218125104904175, tv_loss: 0.007805177941918373\n",
      "iteration 44, dc_loss: 2.2070343494415283, tv_loss: 0.007929381914436817\n",
      "iteration 45, dc_loss: 2.196000099182129, tv_loss: 0.008091223426163197\n",
      "iteration 46, dc_loss: 2.1849606037139893, tv_loss: 0.008247186429798603\n",
      "iteration 47, dc_loss: 2.174225091934204, tv_loss: 0.008362289518117905\n",
      "iteration 48, dc_loss: 2.163339853286743, tv_loss: 0.008479674346745014\n",
      "iteration 49, dc_loss: 2.1526830196380615, tv_loss: 0.008605060167610645\n",
      "iteration 50, dc_loss: 2.1420536041259766, tv_loss: 0.008748704567551613\n",
      "iteration 51, dc_loss: 2.131424903869629, tv_loss: 0.00888988096266985\n",
      "iteration 52, dc_loss: 2.1209981441497803, tv_loss: 0.009017931297421455\n",
      "iteration 53, dc_loss: 2.1105711460113525, tv_loss: 0.00915809627622366\n",
      "iteration 54, dc_loss: 2.100196599960327, tv_loss: 0.009274868294596672\n",
      "iteration 55, dc_loss: 2.089926242828369, tv_loss: 0.009407572448253632\n",
      "iteration 56, dc_loss: 2.0796167850494385, tv_loss: 0.009592738933861256\n",
      "iteration 57, dc_loss: 2.069509744644165, tv_loss: 0.009703991003334522\n",
      "iteration 58, dc_loss: 2.0594263076782227, tv_loss: 0.009837288409471512\n",
      "iteration 59, dc_loss: 2.0494465827941895, tv_loss: 0.009966461919248104\n",
      "iteration 60, dc_loss: 2.039457321166992, tv_loss: 0.010238079354166985\n",
      "iteration 61, dc_loss: 2.0298478603363037, tv_loss: 0.01027714367955923\n",
      "iteration 62, dc_loss: 2.0200397968292236, tv_loss: 0.01053064689040184\n",
      "iteration 63, dc_loss: 2.010913372039795, tv_loss: 0.01047047134488821\n",
      "iteration 64, dc_loss: 2.0018556118011475, tv_loss: 0.01073442678898573\n",
      "iteration 65, dc_loss: 1.9932880401611328, tv_loss: 0.010641836561262608\n",
      "iteration 66, dc_loss: 1.9833476543426514, tv_loss: 0.010933229699730873\n",
      "iteration 67, dc_loss: 1.973144292831421, tv_loss: 0.01095948088914156\n",
      "iteration 68, dc_loss: 1.9637620449066162, tv_loss: 0.010903725400567055\n",
      "iteration 69, dc_loss: 1.954106330871582, tv_loss: 0.011782863177359104\n",
      "iteration 70, dc_loss: 1.9441252946853638, tv_loss: 0.01153433695435524\n",
      "iteration 71, dc_loss: 1.9343533515930176, tv_loss: 0.011416635476052761\n",
      "iteration 72, dc_loss: 1.925597906112671, tv_loss: 0.01190990675240755\n",
      "iteration 73, dc_loss: 1.9165468215942383, tv_loss: 0.011834762059152126\n",
      "iteration 74, dc_loss: 1.907297968864441, tv_loss: 0.011944221332669258\n",
      "iteration 75, dc_loss: 1.8982353210449219, tv_loss: 0.011823451146483421\n",
      "iteration 76, dc_loss: 1.888855218887329, tv_loss: 0.011972455307841301\n",
      "iteration 77, dc_loss: 1.880005955696106, tv_loss: 0.012394598685204983\n",
      "iteration 78, dc_loss: 1.8712501525878906, tv_loss: 0.012243404053151608\n",
      "iteration 79, dc_loss: 1.8624966144561768, tv_loss: 0.012393263168632984\n",
      "iteration 80, dc_loss: 1.8535220623016357, tv_loss: 0.012617051601409912\n",
      "iteration 81, dc_loss: 1.8442797660827637, tv_loss: 0.012477991171181202\n",
      "iteration 82, dc_loss: 1.8353976011276245, tv_loss: 0.012519684620201588\n",
      "iteration 83, dc_loss: 1.8269233703613281, tv_loss: 0.012960202060639858\n",
      "iteration 84, dc_loss: 1.8182986974716187, tv_loss: 0.012884574942290783\n",
      "iteration 85, dc_loss: 1.8092457056045532, tv_loss: 0.01289425976574421\n",
      "iteration 86, dc_loss: 1.8008874654769897, tv_loss: 0.013048390857875347\n",
      "iteration 87, dc_loss: 1.7925925254821777, tv_loss: 0.012997288256883621\n",
      "iteration 88, dc_loss: 1.7840427160263062, tv_loss: 0.013198618777096272\n",
      "iteration 89, dc_loss: 1.775824785232544, tv_loss: 0.01342096645385027\n",
      "iteration 90, dc_loss: 1.767409324645996, tv_loss: 0.013714845292270184\n",
      "iteration 91, dc_loss: 1.7589452266693115, tv_loss: 0.01359350886195898\n",
      "iteration 92, dc_loss: 1.751054286956787, tv_loss: 0.013490051962435246\n",
      "iteration 93, dc_loss: 1.7428659200668335, tv_loss: 0.014040854759514332\n",
      "iteration 94, dc_loss: 1.7354930639266968, tv_loss: 0.01360233873128891\n",
      "iteration 95, dc_loss: 1.7273004055023193, tv_loss: 0.014282937161624432\n",
      "iteration 96, dc_loss: 1.7198060750961304, tv_loss: 0.013684653677046299\n",
      "iteration 97, dc_loss: 1.7111170291900635, tv_loss: 0.01421177014708519\n",
      "iteration 98, dc_loss: 1.703086256980896, tv_loss: 0.014056084677577019\n",
      "iteration 99, dc_loss: 1.6951020956039429, tv_loss: 0.014095584861934185\n",
      "iteration 100, dc_loss: 1.6869527101516724, tv_loss: 0.014593894593417645\n",
      "iteration 101, dc_loss: 1.6796387434005737, tv_loss: 0.014375426806509495\n",
      "iteration 102, dc_loss: 1.6713546514511108, tv_loss: 0.014632614329457283\n",
      "iteration 103, dc_loss: 1.6632130146026611, tv_loss: 0.014495526440441608\n",
      "iteration 104, dc_loss: 1.6553680896759033, tv_loss: 0.014534546993672848\n",
      "iteration 105, dc_loss: 1.6480146646499634, tv_loss: 0.014842906035482883\n",
      "iteration 106, dc_loss: 1.641066551208496, tv_loss: 0.014688343740999699\n",
      "iteration 107, dc_loss: 1.632971167564392, tv_loss: 0.015268190763890743\n",
      "iteration 108, dc_loss: 1.6257578134536743, tv_loss: 0.014812679961323738\n",
      "iteration 109, dc_loss: 1.6176131963729858, tv_loss: 0.015272493474185467\n",
      "iteration 110, dc_loss: 1.6103570461273193, tv_loss: 0.015156098641455173\n",
      "iteration 111, dc_loss: 1.6036573648452759, tv_loss: 0.015125930309295654\n",
      "iteration 112, dc_loss: 1.5967851877212524, tv_loss: 0.015217483974993229\n",
      "iteration 113, dc_loss: 1.5895471572875977, tv_loss: 0.015432777814567089\n",
      "iteration 114, dc_loss: 1.5822367668151855, tv_loss: 0.015329080633819103\n",
      "iteration 115, dc_loss: 1.5741984844207764, tv_loss: 0.01576712541282177\n",
      "iteration 116, dc_loss: 1.5674625635147095, tv_loss: 0.015207015909254551\n",
      "iteration 117, dc_loss: 1.5595895051956177, tv_loss: 0.015869593247771263\n",
      "iteration 118, dc_loss: 1.5520908832550049, tv_loss: 0.015814173966646194\n",
      "iteration 119, dc_loss: 1.5448404550552368, tv_loss: 0.01576990820467472\n",
      "iteration 120, dc_loss: 1.5380386114120483, tv_loss: 0.01589057222008705\n",
      "iteration 121, dc_loss: 1.5309648513793945, tv_loss: 0.015833789482712746\n",
      "iteration 122, dc_loss: 1.5234408378601074, tv_loss: 0.01625029556453228\n",
      "iteration 123, dc_loss: 1.5168110132217407, tv_loss: 0.01600632071495056\n",
      "iteration 124, dc_loss: 1.5098496675491333, tv_loss: 0.016159961000084877\n",
      "iteration 125, dc_loss: 1.5028678178787231, tv_loss: 0.016400644555687904\n",
      "iteration 126, dc_loss: 1.4963629245758057, tv_loss: 0.0162674393504858\n",
      "iteration 127, dc_loss: 1.4897692203521729, tv_loss: 0.0166125800460577\n",
      "iteration 128, dc_loss: 1.483710527420044, tv_loss: 0.016341345384716988\n",
      "iteration 129, dc_loss: 1.4774432182312012, tv_loss: 0.01688576675951481\n",
      "iteration 130, dc_loss: 1.4719717502593994, tv_loss: 0.016138629987835884\n",
      "iteration 131, dc_loss: 1.464356541633606, tv_loss: 0.01720651425421238\n",
      "iteration 132, dc_loss: 1.456589937210083, tv_loss: 0.01646244339644909\n",
      "iteration 133, dc_loss: 1.4494327306747437, tv_loss: 0.01663806475698948\n",
      "iteration 134, dc_loss: 1.4434173107147217, tv_loss: 0.0171698946505785\n",
      "iteration 135, dc_loss: 1.4371285438537598, tv_loss: 0.016661522909998894\n",
      "iteration 136, dc_loss: 1.4297326803207397, tv_loss: 0.017015093937516212\n",
      "iteration 137, dc_loss: 1.4232522249221802, tv_loss: 0.01733454503118992\n",
      "iteration 138, dc_loss: 1.416957974433899, tv_loss: 0.01692202500998974\n",
      "iteration 139, dc_loss: 1.4099503755569458, tv_loss: 0.01728523150086403\n",
      "iteration 140, dc_loss: 1.4037013053894043, tv_loss: 0.017375482246279716\n",
      "iteration 141, dc_loss: 1.397602915763855, tv_loss: 0.017056018114089966\n",
      "iteration 142, dc_loss: 1.390821099281311, tv_loss: 0.017552170902490616\n",
      "iteration 143, dc_loss: 1.3848462104797363, tv_loss: 0.017500506713986397\n",
      "iteration 144, dc_loss: 1.3784114122390747, tv_loss: 0.017515433952212334\n",
      "iteration 145, dc_loss: 1.371962547302246, tv_loss: 0.017592577263712883\n",
      "iteration 146, dc_loss: 1.365638017654419, tv_loss: 0.017716987058520317\n",
      "iteration 147, dc_loss: 1.3594797849655151, tv_loss: 0.017571747303009033\n",
      "iteration 148, dc_loss: 1.3531615734100342, tv_loss: 0.01781764067709446\n",
      "iteration 149, dc_loss: 1.347190499305725, tv_loss: 0.017762966454029083\n",
      "iteration 150, dc_loss: 1.3409136533737183, tv_loss: 0.01788853481411934\n",
      "iteration 151, dc_loss: 1.334843397140503, tv_loss: 0.017965039238333702\n",
      "iteration 152, dc_loss: 1.3289637565612793, tv_loss: 0.018032852560281754\n",
      "iteration 153, dc_loss: 1.322910189628601, tv_loss: 0.018067974597215652\n",
      "iteration 154, dc_loss: 1.3169652223587036, tv_loss: 0.01813916116952896\n",
      "iteration 155, dc_loss: 1.3114138841629028, tv_loss: 0.018107078969478607\n",
      "iteration 156, dc_loss: 1.3057551383972168, tv_loss: 0.01831735111773014\n",
      "iteration 157, dc_loss: 1.301066279411316, tv_loss: 0.018039504066109657\n",
      "iteration 158, dc_loss: 1.296863317489624, tv_loss: 0.018574291840195656\n",
      "iteration 159, dc_loss: 1.29352605342865, tv_loss: 0.018458444625139236\n",
      "iteration 160, dc_loss: 1.289309024810791, tv_loss: 0.01890118233859539\n",
      "iteration 161, dc_loss: 1.2795450687408447, tv_loss: 0.01842590607702732\n",
      "iteration 162, dc_loss: 1.2753455638885498, tv_loss: 0.018518926575779915\n",
      "iteration 163, dc_loss: 1.267849326133728, tv_loss: 0.0191668588668108\n",
      "iteration 164, dc_loss: 1.2617968320846558, tv_loss: 0.01872328296303749\n",
      "iteration 165, dc_loss: 1.2569527626037598, tv_loss: 0.0185529924929142\n",
      "iteration 166, dc_loss: 1.250304102897644, tv_loss: 0.019215455278754234\n",
      "iteration 167, dc_loss: 1.2445017099380493, tv_loss: 0.019147181883454323\n",
      "iteration 168, dc_loss: 1.2390694618225098, tv_loss: 0.019176887348294258\n",
      "iteration 169, dc_loss: 1.2330403327941895, tv_loss: 0.019003991037607193\n",
      "iteration 170, dc_loss: 1.2275786399841309, tv_loss: 0.019440608099102974\n",
      "iteration 171, dc_loss: 1.222045660018921, tv_loss: 0.01898994855582714\n",
      "iteration 172, dc_loss: 1.2162052392959595, tv_loss: 0.019401229918003082\n",
      "iteration 173, dc_loss: 1.2107361555099487, tv_loss: 0.019105268642306328\n",
      "iteration 174, dc_loss: 1.2048803567886353, tv_loss: 0.01949140802025795\n",
      "iteration 175, dc_loss: 1.199135661125183, tv_loss: 0.019389281049370766\n",
      "iteration 176, dc_loss: 1.1942784786224365, tv_loss: 0.01935194805264473\n",
      "iteration 177, dc_loss: 1.1883212327957153, tv_loss: 0.01949344575405121\n",
      "iteration 178, dc_loss: 1.1834806203842163, tv_loss: 0.019605198875069618\n",
      "iteration 179, dc_loss: 1.1779712438583374, tv_loss: 0.019894497469067574\n",
      "iteration 180, dc_loss: 1.1733837127685547, tv_loss: 0.0195048488676548\n",
      "iteration 181, dc_loss: 1.1673109531402588, tv_loss: 0.01972588337957859\n",
      "iteration 182, dc_loss: 1.1621555089950562, tv_loss: 0.019908931106328964\n",
      "iteration 183, dc_loss: 1.1568529605865479, tv_loss: 0.019734088331460953\n",
      "iteration 184, dc_loss: 1.151707410812378, tv_loss: 0.019994206726551056\n",
      "iteration 185, dc_loss: 1.1465339660644531, tv_loss: 0.01978352665901184\n",
      "iteration 186, dc_loss: 1.1411921977996826, tv_loss: 0.02019946277141571\n",
      "iteration 187, dc_loss: 1.1361939907073975, tv_loss: 0.01988569274544716\n",
      "iteration 188, dc_loss: 1.1309291124343872, tv_loss: 0.020100869238376617\n",
      "iteration 189, dc_loss: 1.125748634338379, tv_loss: 0.020017091184854507\n",
      "iteration 190, dc_loss: 1.120712399482727, tv_loss: 0.020223330706357956\n",
      "iteration 191, dc_loss: 1.1156508922576904, tv_loss: 0.020112212747335434\n",
      "iteration 192, dc_loss: 1.1107404232025146, tv_loss: 0.02015739679336548\n",
      "iteration 193, dc_loss: 1.1055511236190796, tv_loss: 0.020268738269805908\n",
      "iteration 194, dc_loss: 1.100682258605957, tv_loss: 0.020329689607024193\n",
      "iteration 195, dc_loss: 1.0956377983093262, tv_loss: 0.02030336484313011\n",
      "iteration 196, dc_loss: 1.090827226638794, tv_loss: 0.020337942987680435\n",
      "iteration 197, dc_loss: 1.0857163667678833, tv_loss: 0.02046385034918785\n",
      "iteration 198, dc_loss: 1.0810770988464355, tv_loss: 0.020480532199144363\n",
      "iteration 199, dc_loss: 1.0761356353759766, tv_loss: 0.020685262978076935\n",
      "iteration 200, dc_loss: 1.0721232891082764, tv_loss: 0.020348798483610153\n",
      "iteration 201, dc_loss: 1.0674736499786377, tv_loss: 0.020976627245545387\n",
      "iteration 202, dc_loss: 1.0647495985031128, tv_loss: 0.020366396754980087\n",
      "iteration 203, dc_loss: 1.060471534729004, tv_loss: 0.021270882338285446\n",
      "iteration 204, dc_loss: 1.055129885673523, tv_loss: 0.020239371806383133\n",
      "iteration 205, dc_loss: 1.047720193862915, tv_loss: 0.020831050351262093\n",
      "iteration 206, dc_loss: 1.0430001020431519, tv_loss: 0.02112100087106228\n",
      "iteration 207, dc_loss: 1.0391660928726196, tv_loss: 0.020615074783563614\n",
      "iteration 208, dc_loss: 1.0337475538253784, tv_loss: 0.021127697080373764\n",
      "iteration 209, dc_loss: 1.0290502309799194, tv_loss: 0.02079201489686966\n",
      "iteration 210, dc_loss: 1.0241963863372803, tv_loss: 0.021076735109090805\n",
      "iteration 211, dc_loss: 1.0199840068817139, tv_loss: 0.020923860371112823\n",
      "iteration 212, dc_loss: 1.0147950649261475, tv_loss: 0.020963888615369797\n",
      "iteration 213, dc_loss: 1.0101213455200195, tv_loss: 0.020983746275305748\n",
      "iteration 214, dc_loss: 1.0054638385772705, tv_loss: 0.020993022248148918\n",
      "iteration 215, dc_loss: 1.0007742643356323, tv_loss: 0.02097945660352707\n",
      "iteration 216, dc_loss: 0.9959742426872253, tv_loss: 0.021100852638483047\n",
      "iteration 217, dc_loss: 0.9914951324462891, tv_loss: 0.021111559122800827\n",
      "iteration 218, dc_loss: 0.9868756532669067, tv_loss: 0.021059133112430573\n",
      "iteration 219, dc_loss: 0.9822728037834167, tv_loss: 0.02124589867889881\n",
      "iteration 220, dc_loss: 0.9779143333435059, tv_loss: 0.021190602332353592\n",
      "iteration 221, dc_loss: 0.9735457897186279, tv_loss: 0.02143860049545765\n",
      "iteration 222, dc_loss: 0.9707762002944946, tv_loss: 0.021210987120866776\n",
      "iteration 223, dc_loss: 0.9678836464881897, tv_loss: 0.021820656955242157\n",
      "iteration 224, dc_loss: 0.9658618569374084, tv_loss: 0.0209791399538517\n",
      "iteration 225, dc_loss: 0.9602729678153992, tv_loss: 0.021972736343741417\n",
      "iteration 226, dc_loss: 0.9546272158622742, tv_loss: 0.021505892276763916\n",
      "iteration 227, dc_loss: 0.9513651728630066, tv_loss: 0.021337416023015976\n",
      "iteration 228, dc_loss: 0.9461449384689331, tv_loss: 0.02190437540411949\n",
      "iteration 229, dc_loss: 0.9408878087997437, tv_loss: 0.021614184603095055\n",
      "iteration 230, dc_loss: 0.9369164109230042, tv_loss: 0.021650560200214386\n",
      "iteration 231, dc_loss: 0.9323150515556335, tv_loss: 0.021921327337622643\n",
      "iteration 232, dc_loss: 0.9277533888816833, tv_loss: 0.0217595174908638\n",
      "iteration 233, dc_loss: 0.9238521456718445, tv_loss: 0.021688103675842285\n",
      "iteration 234, dc_loss: 0.9192440509796143, tv_loss: 0.02193637005984783\n",
      "iteration 235, dc_loss: 0.9145908355712891, tv_loss: 0.022005166858434677\n",
      "iteration 236, dc_loss: 0.9106559753417969, tv_loss: 0.021879693493247032\n",
      "iteration 237, dc_loss: 0.9060510993003845, tv_loss: 0.02202676236629486\n",
      "iteration 238, dc_loss: 0.9019556045532227, tv_loss: 0.02209397219121456\n",
      "iteration 239, dc_loss: 0.8977985978126526, tv_loss: 0.021845553070306778\n",
      "iteration 240, dc_loss: 0.8931597471237183, tv_loss: 0.022248169407248497\n",
      "iteration 241, dc_loss: 0.8892063498497009, tv_loss: 0.022303206846117973\n",
      "iteration 242, dc_loss: 0.8849490880966187, tv_loss: 0.022138606756925583\n",
      "iteration 243, dc_loss: 0.8807927370071411, tv_loss: 0.02224154584109783\n",
      "iteration 244, dc_loss: 0.876815915107727, tv_loss: 0.02219301648437977\n",
      "iteration 245, dc_loss: 0.8725219964981079, tv_loss: 0.022376930341124535\n",
      "iteration 246, dc_loss: 0.8686286807060242, tv_loss: 0.02243565022945404\n",
      "iteration 247, dc_loss: 0.8644216656684875, tv_loss: 0.022481273859739304\n",
      "iteration 248, dc_loss: 0.8604620695114136, tv_loss: 0.022389326244592667\n",
      "iteration 249, dc_loss: 0.8564341068267822, tv_loss: 0.02252340503036976\n",
      "iteration 250, dc_loss: 0.8524512648582458, tv_loss: 0.022575225681066513\n",
      "iteration 251, dc_loss: 0.8484556674957275, tv_loss: 0.02262001484632492\n",
      "iteration 252, dc_loss: 0.8452060222625732, tv_loss: 0.022587386891245842\n",
      "iteration 253, dc_loss: 0.8410751819610596, tv_loss: 0.022835010662674904\n",
      "iteration 254, dc_loss: 0.8377750515937805, tv_loss: 0.02257966250181198\n",
      "iteration 255, dc_loss: 0.8348459601402283, tv_loss: 0.02287941426038742\n",
      "iteration 256, dc_loss: 0.8307036757469177, tv_loss: 0.02269037626683712\n",
      "iteration 257, dc_loss: 0.8276818990707397, tv_loss: 0.02298404835164547\n",
      "iteration 258, dc_loss: 0.8248746991157532, tv_loss: 0.022460060194134712\n",
      "iteration 259, dc_loss: 0.8215398192405701, tv_loss: 0.023428525775671005\n",
      "iteration 260, dc_loss: 0.8196427226066589, tv_loss: 0.02281644009053707\n",
      "iteration 261, dc_loss: 0.8140171766281128, tv_loss: 0.02330954186618328\n",
      "iteration 262, dc_loss: 0.8091527819633484, tv_loss: 0.023091403767466545\n",
      "iteration 263, dc_loss: 0.8042277693748474, tv_loss: 0.02313240058720112\n",
      "iteration 264, dc_loss: 0.8007887601852417, tv_loss: 0.023507684469223022\n",
      "iteration 265, dc_loss: 0.7976109385490417, tv_loss: 0.02327144518494606\n",
      "iteration 266, dc_loss: 0.7925769090652466, tv_loss: 0.0234209131449461\n",
      "iteration 267, dc_loss: 0.789035975933075, tv_loss: 0.023300055414438248\n",
      "iteration 268, dc_loss: 0.7858779430389404, tv_loss: 0.023207465186715126\n",
      "iteration 269, dc_loss: 0.7818529009819031, tv_loss: 0.02329515665769577\n",
      "iteration 270, dc_loss: 0.7776859402656555, tv_loss: 0.023424718528985977\n",
      "iteration 271, dc_loss: 0.774539589881897, tv_loss: 0.0232455562800169\n",
      "iteration 272, dc_loss: 0.7709470987319946, tv_loss: 0.023437192663550377\n",
      "iteration 273, dc_loss: 0.7667936086654663, tv_loss: 0.023472722619771957\n",
      "iteration 274, dc_loss: 0.7635302543640137, tv_loss: 0.023388514295220375\n",
      "iteration 275, dc_loss: 0.7599647045135498, tv_loss: 0.023473316803574562\n",
      "iteration 276, dc_loss: 0.7561507821083069, tv_loss: 0.023480059579014778\n",
      "iteration 277, dc_loss: 0.7525182366371155, tv_loss: 0.023516759276390076\n",
      "iteration 278, dc_loss: 0.7490644454956055, tv_loss: 0.023636572062969208\n",
      "iteration 279, dc_loss: 0.7456788420677185, tv_loss: 0.023566046729683876\n",
      "iteration 280, dc_loss: 0.7420220375061035, tv_loss: 0.023537302389740944\n",
      "iteration 281, dc_loss: 0.7385424971580505, tv_loss: 0.023866912350058556\n",
      "iteration 282, dc_loss: 0.7352681756019592, tv_loss: 0.02376345358788967\n",
      "iteration 283, dc_loss: 0.7316895723342896, tv_loss: 0.023740744218230247\n",
      "iteration 284, dc_loss: 0.7282025218009949, tv_loss: 0.0238938108086586\n",
      "iteration 285, dc_loss: 0.725135326385498, tv_loss: 0.023723972961306572\n",
      "iteration 286, dc_loss: 0.7215140461921692, tv_loss: 0.023984111845493317\n",
      "iteration 287, dc_loss: 0.718387246131897, tv_loss: 0.023976799100637436\n",
      "iteration 288, dc_loss: 0.7149777412414551, tv_loss: 0.02405373565852642\n",
      "iteration 289, dc_loss: 0.7120438814163208, tv_loss: 0.023913072422146797\n",
      "iteration 290, dc_loss: 0.7087924480438232, tv_loss: 0.024359438568353653\n",
      "iteration 291, dc_loss: 0.7063530087471008, tv_loss: 0.02379082702100277\n",
      "iteration 292, dc_loss: 0.7033364176750183, tv_loss: 0.024575646966695786\n",
      "iteration 293, dc_loss: 0.7010995745658875, tv_loss: 0.0238514244556427\n",
      "iteration 294, dc_loss: 0.6973400115966797, tv_loss: 0.024542836472392082\n",
      "iteration 295, dc_loss: 0.6935424208641052, tv_loss: 0.02392417937517166\n",
      "iteration 296, dc_loss: 0.6890805959701538, tv_loss: 0.0245329812169075\n",
      "iteration 297, dc_loss: 0.6860479116439819, tv_loss: 0.024430373683571815\n",
      "iteration 298, dc_loss: 0.6839107275009155, tv_loss: 0.024266839027404785\n",
      "iteration 299, dc_loss: 0.6801537871360779, tv_loss: 0.024669751524925232\n",
      "iteration 300, dc_loss: 0.676631510257721, tv_loss: 0.02471652440726757\n",
      "iteration 301, dc_loss: 0.6738415360450745, tv_loss: 0.02447192370891571\n",
      "iteration 302, dc_loss: 0.6706743836402893, tv_loss: 0.024830443784594536\n",
      "iteration 303, dc_loss: 0.6677002310752869, tv_loss: 0.02445227839052677\n",
      "iteration 304, dc_loss: 0.6642463207244873, tv_loss: 0.02465444803237915\n",
      "iteration 305, dc_loss: 0.6612340211868286, tv_loss: 0.02466168813407421\n",
      "iteration 306, dc_loss: 0.6584737300872803, tv_loss: 0.024684114381670952\n",
      "iteration 307, dc_loss: 0.655030369758606, tv_loss: 0.024937937036156654\n",
      "iteration 308, dc_loss: 0.6520004868507385, tv_loss: 0.024812402203679085\n",
      "iteration 309, dc_loss: 0.6490450501441956, tv_loss: 0.024723250418901443\n",
      "iteration 310, dc_loss: 0.6459577679634094, tv_loss: 0.024996642023324966\n",
      "iteration 311, dc_loss: 0.643189013004303, tv_loss: 0.024728087708353996\n",
      "iteration 312, dc_loss: 0.6400532126426697, tv_loss: 0.0249264445155859\n",
      "iteration 313, dc_loss: 0.6370348930358887, tv_loss: 0.024913795292377472\n",
      "iteration 314, dc_loss: 0.6342477798461914, tv_loss: 0.024917492642998695\n",
      "iteration 315, dc_loss: 0.6311615705490112, tv_loss: 0.025061659514904022\n",
      "iteration 316, dc_loss: 0.628387987613678, tv_loss: 0.025035912171006203\n",
      "iteration 317, dc_loss: 0.6253618597984314, tv_loss: 0.025074223056435585\n",
      "iteration 318, dc_loss: 0.6226920485496521, tv_loss: 0.02508537657558918\n",
      "iteration 319, dc_loss: 0.6200280785560608, tv_loss: 0.025170885026454926\n",
      "iteration 320, dc_loss: 0.6180676221847534, tv_loss: 0.025178423151373863\n",
      "iteration 321, dc_loss: 0.6171534061431885, tv_loss: 0.0253100898116827\n",
      "iteration 322, dc_loss: 0.6177459359169006, tv_loss: 0.024808041751384735\n",
      "iteration 323, dc_loss: 0.6141265630722046, tv_loss: 0.025900593027472496\n",
      "iteration 324, dc_loss: 0.6092331409454346, tv_loss: 0.024715188890695572\n",
      "iteration 325, dc_loss: 0.6045013666152954, tv_loss: 0.025440387427806854\n",
      "iteration 326, dc_loss: 0.6026727557182312, tv_loss: 0.02528858184814453\n",
      "iteration 327, dc_loss: 0.5999491214752197, tv_loss: 0.025052303448319435\n",
      "iteration 328, dc_loss: 0.5960761904716492, tv_loss: 0.025460604578256607\n",
      "iteration 329, dc_loss: 0.5938487648963928, tv_loss: 0.0252737607806921\n",
      "iteration 330, dc_loss: 0.5909644961357117, tv_loss: 0.025425544008612633\n",
      "iteration 331, dc_loss: 0.5883303284645081, tv_loss: 0.025424428284168243\n",
      "iteration 332, dc_loss: 0.5850141048431396, tv_loss: 0.025452010333538055\n",
      "iteration 333, dc_loss: 0.5828526616096497, tv_loss: 0.025502435863018036\n",
      "iteration 334, dc_loss: 0.5802726745605469, tv_loss: 0.025490887463092804\n",
      "iteration 335, dc_loss: 0.5768307447433472, tv_loss: 0.02566402591764927\n",
      "iteration 336, dc_loss: 0.5745633840560913, tv_loss: 0.025515185669064522\n",
      "iteration 337, dc_loss: 0.5715510249137878, tv_loss: 0.025581054389476776\n",
      "iteration 338, dc_loss: 0.5688152313232422, tv_loss: 0.025644991546869278\n",
      "iteration 339, dc_loss: 0.5659494400024414, tv_loss: 0.025693876668810844\n",
      "iteration 340, dc_loss: 0.5635992884635925, tv_loss: 0.025647133588790894\n",
      "iteration 341, dc_loss: 0.5605682730674744, tv_loss: 0.025737792253494263\n",
      "iteration 342, dc_loss: 0.558224618434906, tv_loss: 0.025706741958856583\n",
      "iteration 343, dc_loss: 0.5555000305175781, tv_loss: 0.02581275813281536\n",
      "iteration 344, dc_loss: 0.5526511073112488, tv_loss: 0.02574833855032921\n",
      "iteration 345, dc_loss: 0.5505120158195496, tv_loss: 0.025768958032131195\n",
      "iteration 346, dc_loss: 0.5474515557289124, tv_loss: 0.025910720229148865\n",
      "iteration 347, dc_loss: 0.5451563596725464, tv_loss: 0.025809066370129585\n",
      "iteration 348, dc_loss: 0.5426338911056519, tv_loss: 0.025869475677609444\n",
      "iteration 349, dc_loss: 0.5398657321929932, tv_loss: 0.02591390162706375\n",
      "iteration 350, dc_loss: 0.5375898480415344, tv_loss: 0.02592333033680916\n",
      "iteration 351, dc_loss: 0.5349280834197998, tv_loss: 0.025967227295041084\n",
      "iteration 352, dc_loss: 0.5324781537055969, tv_loss: 0.025975994765758514\n",
      "iteration 353, dc_loss: 0.5300309062004089, tv_loss: 0.026062121614813805\n",
      "iteration 354, dc_loss: 0.5274888277053833, tv_loss: 0.026070455089211464\n",
      "iteration 355, dc_loss: 0.5251215696334839, tv_loss: 0.026074325665831566\n",
      "iteration 356, dc_loss: 0.5226262807846069, tv_loss: 0.026156000792980194\n",
      "iteration 357, dc_loss: 0.520280122756958, tv_loss: 0.02611311338841915\n",
      "iteration 358, dc_loss: 0.5178447961807251, tv_loss: 0.026250628754496574\n",
      "iteration 359, dc_loss: 0.5156394839286804, tv_loss: 0.026123102754354477\n",
      "iteration 360, dc_loss: 0.5132025480270386, tv_loss: 0.026405606418848038\n",
      "iteration 361, dc_loss: 0.5114279389381409, tv_loss: 0.026070931926369667\n",
      "iteration 362, dc_loss: 0.5092847943305969, tv_loss: 0.026592334732413292\n",
      "iteration 363, dc_loss: 0.5088860392570496, tv_loss: 0.025968153029680252\n",
      "iteration 364, dc_loss: 0.5075021982192993, tv_loss: 0.026997990906238556\n",
      "iteration 365, dc_loss: 0.5063480138778687, tv_loss: 0.025837311521172523\n",
      "iteration 366, dc_loss: 0.501331090927124, tv_loss: 0.026958279311656952\n",
      "iteration 367, dc_loss: 0.4976697564125061, tv_loss: 0.0263624656945467\n",
      "iteration 368, dc_loss: 0.49654585123062134, tv_loss: 0.026132337749004364\n",
      "iteration 369, dc_loss: 0.49435752630233765, tv_loss: 0.026859737932682037\n",
      "iteration 370, dc_loss: 0.4913502037525177, tv_loss: 0.026315638795495033\n",
      "iteration 371, dc_loss: 0.4889477789402008, tv_loss: 0.026458946987986565\n",
      "iteration 372, dc_loss: 0.48689085245132446, tv_loss: 0.026734966784715652\n",
      "iteration 373, dc_loss: 0.4846254885196686, tv_loss: 0.026385512202978134\n",
      "iteration 374, dc_loss: 0.48196280002593994, tv_loss: 0.02662995271384716\n",
      "iteration 375, dc_loss: 0.47980332374572754, tv_loss: 0.026670891791582108\n",
      "iteration 376, dc_loss: 0.4778774678707123, tv_loss: 0.026454754173755646\n",
      "iteration 377, dc_loss: 0.4752452075481415, tv_loss: 0.026750583201646805\n",
      "iteration 378, dc_loss: 0.4730072319507599, tv_loss: 0.026780981570482254\n",
      "iteration 379, dc_loss: 0.47106680274009705, tv_loss: 0.026584915816783905\n",
      "iteration 380, dc_loss: 0.4686031639575958, tv_loss: 0.02684851363301277\n",
      "iteration 381, dc_loss: 0.4664468467235565, tv_loss: 0.026872817426919937\n",
      "iteration 382, dc_loss: 0.4644588232040405, tv_loss: 0.026706833392381668\n",
      "iteration 383, dc_loss: 0.46207836270332336, tv_loss: 0.026932276785373688\n",
      "iteration 384, dc_loss: 0.46003103256225586, tv_loss: 0.026914868503808975\n",
      "iteration 385, dc_loss: 0.4579950273036957, tv_loss: 0.02684572897851467\n",
      "iteration 386, dc_loss: 0.455709308385849, tv_loss: 0.027000462636351585\n",
      "iteration 387, dc_loss: 0.4537091553211212, tv_loss: 0.026964154094457626\n",
      "iteration 388, dc_loss: 0.45164039731025696, tv_loss: 0.02697701007127762\n",
      "iteration 389, dc_loss: 0.4494781494140625, tv_loss: 0.02700301818549633\n",
      "iteration 390, dc_loss: 0.4474877119064331, tv_loss: 0.027012960985302925\n",
      "iteration 391, dc_loss: 0.4453960657119751, tv_loss: 0.027106791734695435\n",
      "iteration 392, dc_loss: 0.44335678219795227, tv_loss: 0.02704339474439621\n",
      "iteration 393, dc_loss: 0.44137516617774963, tv_loss: 0.027127506211400032\n",
      "iteration 394, dc_loss: 0.43949535489082336, tv_loss: 0.027230579406023026\n",
      "iteration 395, dc_loss: 0.4377278685569763, tv_loss: 0.02712436579167843\n",
      "iteration 396, dc_loss: 0.4362144470214844, tv_loss: 0.02722473256289959\n",
      "iteration 397, dc_loss: 0.4357016682624817, tv_loss: 0.02718651480972767\n",
      "iteration 398, dc_loss: 0.43586403131484985, tv_loss: 0.027288151904940605\n",
      "iteration 399, dc_loss: 0.4360658526420593, tv_loss: 0.027343042194843292\n",
      "iteration 400, dc_loss: 0.431622713804245, tv_loss: 0.027077550068497658\n",
      "iteration 401, dc_loss: 0.42726370692253113, tv_loss: 0.02745123766362667\n",
      "iteration 402, dc_loss: 0.42548829317092896, tv_loss: 0.02722500078380108\n",
      "iteration 403, dc_loss: 0.42437222599983215, tv_loss: 0.027145549654960632\n",
      "iteration 404, dc_loss: 0.421891987323761, tv_loss: 0.027488378807902336\n",
      "iteration 405, dc_loss: 0.4206852316856384, tv_loss: 0.02737952210009098\n",
      "iteration 406, dc_loss: 0.4188956618309021, tv_loss: 0.02726033702492714\n",
      "iteration 407, dc_loss: 0.4173120856285095, tv_loss: 0.02748195268213749\n",
      "iteration 408, dc_loss: 0.41561901569366455, tv_loss: 0.027443459257483482\n",
      "iteration 409, dc_loss: 0.4140760004520416, tv_loss: 0.027400759980082512\n",
      "iteration 410, dc_loss: 0.4125848710536957, tv_loss: 0.02747621014714241\n",
      "iteration 411, dc_loss: 0.4107617139816284, tv_loss: 0.02747947722673416\n",
      "iteration 412, dc_loss: 0.4095364809036255, tv_loss: 0.02747160568833351\n",
      "iteration 413, dc_loss: 0.40766215324401855, tv_loss: 0.027564365416765213\n",
      "iteration 414, dc_loss: 0.4062911570072174, tv_loss: 0.02761998400092125\n",
      "iteration 415, dc_loss: 0.404803991317749, tv_loss: 0.02751004695892334\n",
      "iteration 416, dc_loss: 0.4031951427459717, tv_loss: 0.02772011049091816\n",
      "iteration 417, dc_loss: 0.40168285369873047, tv_loss: 0.027666375041007996\n",
      "iteration 418, dc_loss: 0.40033313632011414, tv_loss: 0.02772253006696701\n",
      "iteration 419, dc_loss: 0.3986891806125641, tv_loss: 0.02773691527545452\n",
      "iteration 420, dc_loss: 0.39736199378967285, tv_loss: 0.027758710086345673\n",
      "iteration 421, dc_loss: 0.395815372467041, tv_loss: 0.027744660153985023\n",
      "iteration 422, dc_loss: 0.3943782150745392, tv_loss: 0.027775630354881287\n",
      "iteration 423, dc_loss: 0.393009215593338, tv_loss: 0.027752449735999107\n",
      "iteration 424, dc_loss: 0.39147406816482544, tv_loss: 0.027790449559688568\n",
      "iteration 425, dc_loss: 0.3901202082633972, tv_loss: 0.02779468148946762\n",
      "iteration 426, dc_loss: 0.38862287998199463, tv_loss: 0.02784823626279831\n",
      "iteration 427, dc_loss: 0.3872813582420349, tv_loss: 0.0277776550501585\n",
      "iteration 428, dc_loss: 0.3858458995819092, tv_loss: 0.027840670198202133\n",
      "iteration 429, dc_loss: 0.38440653681755066, tv_loss: 0.02787461131811142\n",
      "iteration 430, dc_loss: 0.38300445675849915, tv_loss: 0.027878817170858383\n",
      "iteration 431, dc_loss: 0.38170960545539856, tv_loss: 0.02782491222023964\n",
      "iteration 432, dc_loss: 0.38020098209381104, tv_loss: 0.027912788093090057\n",
      "iteration 433, dc_loss: 0.37880396842956543, tv_loss: 0.027988940477371216\n",
      "iteration 434, dc_loss: 0.3775312006473541, tv_loss: 0.02787918969988823\n",
      "iteration 435, dc_loss: 0.3761177361011505, tv_loss: 0.027917122468352318\n",
      "iteration 436, dc_loss: 0.3747079372406006, tv_loss: 0.02800159901380539\n",
      "iteration 437, dc_loss: 0.37337422370910645, tv_loss: 0.027956470847129822\n",
      "iteration 438, dc_loss: 0.37206631898880005, tv_loss: 0.027966562658548355\n",
      "iteration 439, dc_loss: 0.3706514537334442, tv_loss: 0.028046755120158195\n",
      "iteration 440, dc_loss: 0.3693148195743561, tv_loss: 0.028045471757650375\n",
      "iteration 441, dc_loss: 0.36800622940063477, tv_loss: 0.02803538553416729\n",
      "iteration 442, dc_loss: 0.3666585087776184, tv_loss: 0.028064949437975883\n",
      "iteration 443, dc_loss: 0.3653097152709961, tv_loss: 0.02808588743209839\n",
      "iteration 444, dc_loss: 0.36400121450424194, tv_loss: 0.028096536174416542\n",
      "iteration 445, dc_loss: 0.36269134283065796, tv_loss: 0.028121259063482285\n",
      "iteration 446, dc_loss: 0.3613784909248352, tv_loss: 0.02814510092139244\n",
      "iteration 447, dc_loss: 0.36006519198417664, tv_loss: 0.028157014399766922\n",
      "iteration 448, dc_loss: 0.3587508201599121, tv_loss: 0.02817247249186039\n",
      "iteration 449, dc_loss: 0.3574695885181427, tv_loss: 0.028167905285954475\n",
      "iteration 450, dc_loss: 0.3561602234840393, tv_loss: 0.02820255048573017\n",
      "iteration 451, dc_loss: 0.35487455129623413, tv_loss: 0.028232192620635033\n",
      "iteration 452, dc_loss: 0.3536086082458496, tv_loss: 0.0282620657235384\n",
      "iteration 453, dc_loss: 0.3523264527320862, tv_loss: 0.028258247300982475\n",
      "iteration 454, dc_loss: 0.3510468304157257, tv_loss: 0.028265049681067467\n",
      "iteration 455, dc_loss: 0.3497636318206787, tv_loss: 0.028296181932091713\n",
      "iteration 456, dc_loss: 0.34852227568626404, tv_loss: 0.02829059585928917\n",
      "iteration 457, dc_loss: 0.3472433090209961, tv_loss: 0.028346214443445206\n",
      "iteration 458, dc_loss: 0.3460293412208557, tv_loss: 0.028345005586743355\n",
      "iteration 459, dc_loss: 0.34472930431365967, tv_loss: 0.028393911197781563\n",
      "iteration 460, dc_loss: 0.3435751497745514, tv_loss: 0.02832365408539772\n",
      "iteration 461, dc_loss: 0.3422412574291229, tv_loss: 0.028463376685976982\n",
      "iteration 462, dc_loss: 0.3411971628665924, tv_loss: 0.028349922969937325\n",
      "iteration 463, dc_loss: 0.33993011713027954, tv_loss: 0.02852569706737995\n",
      "iteration 464, dc_loss: 0.3391873240470886, tv_loss: 0.028312403708696365\n",
      "iteration 465, dc_loss: 0.3380894958972931, tv_loss: 0.028655432164669037\n",
      "iteration 466, dc_loss: 0.33825674653053284, tv_loss: 0.028246259316802025\n",
      "iteration 467, dc_loss: 0.336738646030426, tv_loss: 0.02874874509871006\n",
      "iteration 468, dc_loss: 0.3354398310184479, tv_loss: 0.028296547010540962\n",
      "iteration 469, dc_loss: 0.33275312185287476, tv_loss: 0.028640691190958023\n",
      "iteration 470, dc_loss: 0.3316890001296997, tv_loss: 0.02850119210779667\n",
      "iteration 471, dc_loss: 0.3313696086406708, tv_loss: 0.02849026769399643\n",
      "iteration 472, dc_loss: 0.32989171147346497, tv_loss: 0.028727006167173386\n",
      "iteration 473, dc_loss: 0.3284342885017395, tv_loss: 0.028447749093174934\n",
      "iteration 474, dc_loss: 0.3267876207828522, tv_loss: 0.028722841292619705\n",
      "iteration 475, dc_loss: 0.3259812891483307, tv_loss: 0.028568364679813385\n",
      "iteration 476, dc_loss: 0.3250739276409149, tv_loss: 0.028562409803271294\n",
      "iteration 477, dc_loss: 0.32350119948387146, tv_loss: 0.028753139078617096\n",
      "iteration 478, dc_loss: 0.32225292921066284, tv_loss: 0.028590364381670952\n",
      "iteration 479, dc_loss: 0.3210994005203247, tv_loss: 0.028666134923696518\n",
      "iteration 480, dc_loss: 0.31989964842796326, tv_loss: 0.028791505843400955\n",
      "iteration 481, dc_loss: 0.31897905468940735, tv_loss: 0.028568247333168983\n",
      "iteration 482, dc_loss: 0.3174822926521301, tv_loss: 0.028755025938153267\n",
      "iteration 483, dc_loss: 0.31623178720474243, tv_loss: 0.02884678542613983\n",
      "iteration 484, dc_loss: 0.3154428005218506, tv_loss: 0.02863728255033493\n",
      "iteration 485, dc_loss: 0.3139907419681549, tv_loss: 0.028914924710989\n",
      "iteration 486, dc_loss: 0.3130055367946625, tv_loss: 0.02869349718093872\n",
      "iteration 487, dc_loss: 0.31176507472991943, tv_loss: 0.028816523030400276\n",
      "iteration 488, dc_loss: 0.31055280566215515, tv_loss: 0.028913559392094612\n",
      "iteration 489, dc_loss: 0.30955833196640015, tv_loss: 0.028755666688084602\n",
      "iteration 490, dc_loss: 0.30837589502334595, tv_loss: 0.02888219803571701\n",
      "iteration 491, dc_loss: 0.3073230981826782, tv_loss: 0.028874708339571953\n",
      "iteration 492, dc_loss: 0.30612877011299133, tv_loss: 0.02890499122440815\n",
      "iteration 493, dc_loss: 0.3049871325492859, tv_loss: 0.028884725645184517\n",
      "iteration 494, dc_loss: 0.3038865327835083, tv_loss: 0.028950028121471405\n",
      "iteration 495, dc_loss: 0.30288708209991455, tv_loss: 0.028912369161844254\n",
      "iteration 496, dc_loss: 0.30169546604156494, tv_loss: 0.028995653614401817\n",
      "iteration 497, dc_loss: 0.30066871643066406, tv_loss: 0.02895720861852169\n",
      "iteration 498, dc_loss: 0.29969292879104614, tv_loss: 0.02895599789917469\n",
      "iteration 499, dc_loss: 0.29864925146102905, tv_loss: 0.02899283356964588\n",
      "iteration 500, dc_loss: 0.2975216507911682, tv_loss: 0.029128704220056534\n",
      "iteration 501, dc_loss: 0.2968041002750397, tv_loss: 0.02886652573943138\n",
      "iteration 502, dc_loss: 0.29566213488578796, tv_loss: 0.029292527586221695\n",
      "iteration 503, dc_loss: 0.29566052556037903, tv_loss: 0.028726769611239433\n",
      "iteration 504, dc_loss: 0.2947025001049042, tv_loss: 0.029613355174660683\n",
      "iteration 505, dc_loss: 0.29497456550598145, tv_loss: 0.028518494218587875\n",
      "iteration 506, dc_loss: 0.29284173250198364, tv_loss: 0.029837770387530327\n",
      "iteration 507, dc_loss: 0.29136547446250916, tv_loss: 0.02876143716275692\n",
      "iteration 508, dc_loss: 0.2890320122241974, tv_loss: 0.029256930574774742\n",
      "iteration 509, dc_loss: 0.28818508982658386, tv_loss: 0.029386525973677635\n",
      "iteration 510, dc_loss: 0.28832492232322693, tv_loss: 0.02885003201663494\n",
      "iteration 511, dc_loss: 0.28646084666252136, tv_loss: 0.029474062845110893\n",
      "iteration 512, dc_loss: 0.2851240634918213, tv_loss: 0.029191575944423676\n",
      "iteration 513, dc_loss: 0.28451675176620483, tv_loss: 0.02899988740682602\n",
      "iteration 514, dc_loss: 0.2832363545894623, tv_loss: 0.02963028848171234\n",
      "iteration 515, dc_loss: 0.282358318567276, tv_loss: 0.0290350504219532\n",
      "iteration 516, dc_loss: 0.28094133734703064, tv_loss: 0.02919527143239975\n",
      "iteration 517, dc_loss: 0.28002721071243286, tv_loss: 0.029425492510199547\n",
      "iteration 518, dc_loss: 0.2792917788028717, tv_loss: 0.02914241887629032\n",
      "iteration 519, dc_loss: 0.27782130241394043, tv_loss: 0.029354415833950043\n",
      "iteration 520, dc_loss: 0.27674785256385803, tv_loss: 0.02943406067788601\n",
      "iteration 521, dc_loss: 0.27619895339012146, tv_loss: 0.0291754137724638\n",
      "iteration 522, dc_loss: 0.274837464094162, tv_loss: 0.029495205730199814\n",
      "iteration 523, dc_loss: 0.273851215839386, tv_loss: 0.029348066076636314\n",
      "iteration 524, dc_loss: 0.2730741798877716, tv_loss: 0.02924717776477337\n",
      "iteration 525, dc_loss: 0.2718758285045624, tv_loss: 0.02957739681005478\n",
      "iteration 526, dc_loss: 0.2710382640361786, tv_loss: 0.029328396543860435\n",
      "iteration 527, dc_loss: 0.27007970213890076, tv_loss: 0.02936251275241375\n",
      "iteration 528, dc_loss: 0.26914840936660767, tv_loss: 0.029604533687233925\n",
      "iteration 529, dc_loss: 0.2690576910972595, tv_loss: 0.029222169890999794\n",
      "iteration 530, dc_loss: 0.26828673481941223, tv_loss: 0.029679706320166588\n",
      "iteration 531, dc_loss: 0.2694094479084015, tv_loss: 0.029321903362870216\n",
      "iteration 532, dc_loss: 0.2677955627441406, tv_loss: 0.02955099567770958\n",
      "iteration 533, dc_loss: 0.26581043004989624, tv_loss: 0.029475798830389977\n",
      "iteration 534, dc_loss: 0.26378700137138367, tv_loss: 0.029485762119293213\n",
      "iteration 535, dc_loss: 0.26306289434432983, tv_loss: 0.029510200023651123\n",
      "iteration 536, dc_loss: 0.2630319893360138, tv_loss: 0.029489627107977867\n",
      "iteration 537, dc_loss: 0.26158326864242554, tv_loss: 0.029622826725244522\n",
      "iteration 538, dc_loss: 0.2602016031742096, tv_loss: 0.029427876695990562\n",
      "iteration 539, dc_loss: 0.2590510845184326, tv_loss: 0.029748868197202682\n",
      "iteration 540, dc_loss: 0.25840821862220764, tv_loss: 0.02953515015542507\n",
      "iteration 541, dc_loss: 0.25757524371147156, tv_loss: 0.02949763648211956\n",
      "iteration 542, dc_loss: 0.2560797333717346, tv_loss: 0.029840558767318726\n",
      "iteration 543, dc_loss: 0.2554035186767578, tv_loss: 0.029464323073625565\n",
      "iteration 544, dc_loss: 0.25447380542755127, tv_loss: 0.029657701030373573\n",
      "iteration 545, dc_loss: 0.2533174455165863, tv_loss: 0.02981177717447281\n",
      "iteration 546, dc_loss: 0.25261950492858887, tv_loss: 0.029488738626241684\n",
      "iteration 547, dc_loss: 0.2514455020427704, tv_loss: 0.029795382171869278\n",
      "iteration 548, dc_loss: 0.2506552040576935, tv_loss: 0.029682373628020287\n",
      "iteration 549, dc_loss: 0.24972417950630188, tv_loss: 0.029662886634469032\n",
      "iteration 550, dc_loss: 0.24861732125282288, tv_loss: 0.02984185703098774\n",
      "iteration 551, dc_loss: 0.24799513816833496, tv_loss: 0.029628349468111992\n",
      "iteration 552, dc_loss: 0.24693429470062256, tv_loss: 0.029824132099747658\n",
      "iteration 553, dc_loss: 0.24600602686405182, tv_loss: 0.02976864203810692\n",
      "iteration 554, dc_loss: 0.24514397978782654, tv_loss: 0.029756130650639534\n",
      "iteration 555, dc_loss: 0.24422238767147064, tv_loss: 0.029899973422288895\n",
      "iteration 556, dc_loss: 0.24348101019859314, tv_loss: 0.029710890725255013\n",
      "iteration 557, dc_loss: 0.24237918853759766, tv_loss: 0.029867198318243027\n",
      "iteration 558, dc_loss: 0.24159330129623413, tv_loss: 0.02982904762029648\n",
      "iteration 559, dc_loss: 0.24076488614082336, tv_loss: 0.029826372861862183\n",
      "iteration 560, dc_loss: 0.23976784944534302, tv_loss: 0.029935363680124283\n",
      "iteration 561, dc_loss: 0.23908376693725586, tv_loss: 0.029784850776195526\n",
      "iteration 562, dc_loss: 0.23806533217430115, tv_loss: 0.02996119111776352\n",
      "iteration 563, dc_loss: 0.2372576743364334, tv_loss: 0.029909340664744377\n",
      "iteration 564, dc_loss: 0.23638752102851868, tv_loss: 0.029902324080467224\n",
      "iteration 565, dc_loss: 0.23551376163959503, tv_loss: 0.029965898022055626\n",
      "iteration 566, dc_loss: 0.23481565713882446, tv_loss: 0.029850179329514503\n",
      "iteration 567, dc_loss: 0.23382142186164856, tv_loss: 0.03003787063062191\n",
      "iteration 568, dc_loss: 0.23303978145122528, tv_loss: 0.029962830245494843\n",
      "iteration 569, dc_loss: 0.23216873407363892, tv_loss: 0.02996685355901718\n",
      "iteration 570, dc_loss: 0.23130536079406738, tv_loss: 0.03003082238137722\n",
      "iteration 571, dc_loss: 0.23061314225196838, tv_loss: 0.029914407059550285\n",
      "iteration 572, dc_loss: 0.22962136566638947, tv_loss: 0.030094197019934654\n",
      "iteration 573, dc_loss: 0.22897662222385406, tv_loss: 0.02995283342897892\n",
      "iteration 574, dc_loss: 0.22801198065280914, tv_loss: 0.030176321044564247\n",
      "iteration 575, dc_loss: 0.22734558582305908, tv_loss: 0.02998146414756775\n",
      "iteration 576, dc_loss: 0.22638259828090668, tv_loss: 0.03012811578810215\n",
      "iteration 577, dc_loss: 0.22572214901447296, tv_loss: 0.030004752799868584\n",
      "iteration 578, dc_loss: 0.22478508949279785, tv_loss: 0.030215809121727943\n",
      "iteration 579, dc_loss: 0.22418059408664703, tv_loss: 0.030025500804185867\n",
      "iteration 580, dc_loss: 0.22329747676849365, tv_loss: 0.030207358300685883\n",
      "iteration 581, dc_loss: 0.22298473119735718, tv_loss: 0.030001891776919365\n",
      "iteration 582, dc_loss: 0.222438782453537, tv_loss: 0.03030509315431118\n",
      "iteration 583, dc_loss: 0.2235819548368454, tv_loss: 0.0299381073564291\n",
      "iteration 584, dc_loss: 0.22328516840934753, tv_loss: 0.03052518330514431\n",
      "iteration 585, dc_loss: 0.2246800661087036, tv_loss: 0.029772309586405754\n",
      "iteration 586, dc_loss: 0.22132769227027893, tv_loss: 0.03072664700448513\n",
      "iteration 587, dc_loss: 0.21969927847385406, tv_loss: 0.029802503064274788\n",
      "iteration 588, dc_loss: 0.2186872810125351, tv_loss: 0.030397001653909683\n",
      "iteration 589, dc_loss: 0.2183583676815033, tv_loss: 0.030306944623589516\n",
      "iteration 590, dc_loss: 0.2173142284154892, tv_loss: 0.030017539858818054\n",
      "iteration 591, dc_loss: 0.21554633975028992, tv_loss: 0.03043520078063011\n",
      "iteration 592, dc_loss: 0.21538251638412476, tv_loss: 0.030111730098724365\n",
      "iteration 593, dc_loss: 0.21452780067920685, tv_loss: 0.03030688688158989\n",
      "iteration 594, dc_loss: 0.21341559290885925, tv_loss: 0.030258027836680412\n",
      "iteration 595, dc_loss: 0.21259042620658875, tv_loss: 0.03027685545384884\n",
      "iteration 596, dc_loss: 0.21176940202713013, tv_loss: 0.0302695594727993\n",
      "iteration 597, dc_loss: 0.21089817583560944, tv_loss: 0.030336380004882812\n",
      "iteration 598, dc_loss: 0.21020632982254028, tv_loss: 0.030396733433008194\n",
      "iteration 599, dc_loss: 0.20942308008670807, tv_loss: 0.030179252848029137\n",
      "iteration 600, dc_loss: 0.20836064219474792, tv_loss: 0.030485250055789948\n",
      "iteration 601, dc_loss: 0.20781230926513672, tv_loss: 0.030368126928806305\n",
      "iteration 602, dc_loss: 0.20709457993507385, tv_loss: 0.0302285123616457\n",
      "iteration 603, dc_loss: 0.2060685008764267, tv_loss: 0.030548173934221268\n",
      "iteration 604, dc_loss: 0.20548459887504578, tv_loss: 0.030312124639749527\n",
      "iteration 605, dc_loss: 0.20470872521400452, tv_loss: 0.03035045973956585\n",
      "iteration 606, dc_loss: 0.20385593175888062, tv_loss: 0.030569929629564285\n",
      "iteration 607, dc_loss: 0.20324170589447021, tv_loss: 0.030353572219610214\n",
      "iteration 608, dc_loss: 0.20241659879684448, tv_loss: 0.03041337989270687\n",
      "iteration 609, dc_loss: 0.20166991651058197, tv_loss: 0.030548805370926857\n",
      "iteration 610, dc_loss: 0.20107288658618927, tv_loss: 0.03042740933597088\n",
      "iteration 611, dc_loss: 0.20016595721244812, tv_loss: 0.030496081337332726\n",
      "iteration 612, dc_loss: 0.19953320920467377, tv_loss: 0.030486904084682465\n",
      "iteration 613, dc_loss: 0.19888252019882202, tv_loss: 0.030510416254401207\n",
      "iteration 614, dc_loss: 0.19805283844470978, tv_loss: 0.030565738677978516\n",
      "iteration 615, dc_loss: 0.19738180935382843, tv_loss: 0.030512351542711258\n",
      "iteration 616, dc_loss: 0.19671864807605743, tv_loss: 0.03052830696105957\n",
      "iteration 617, dc_loss: 0.19601529836654663, tv_loss: 0.030554350465536118\n",
      "iteration 618, dc_loss: 0.19525693356990814, tv_loss: 0.030640339478850365\n",
      "iteration 619, dc_loss: 0.19470180571079254, tv_loss: 0.030507372692227364\n",
      "iteration 620, dc_loss: 0.19389502704143524, tv_loss: 0.030674556270241737\n",
      "iteration 621, dc_loss: 0.19348610937595367, tv_loss: 0.03048432245850563\n",
      "iteration 622, dc_loss: 0.19272470474243164, tv_loss: 0.03076486475765705\n",
      "iteration 623, dc_loss: 0.19244255125522614, tv_loss: 0.030501799657940865\n",
      "iteration 624, dc_loss: 0.19185283780097961, tv_loss: 0.030772099271416664\n",
      "iteration 625, dc_loss: 0.191195547580719, tv_loss: 0.030534686520695686\n",
      "iteration 626, dc_loss: 0.1900978535413742, tv_loss: 0.03075423464179039\n",
      "iteration 627, dc_loss: 0.1892262101173401, tv_loss: 0.030685801059007645\n",
      "iteration 628, dc_loss: 0.1887962371110916, tv_loss: 0.030578043311834335\n",
      "iteration 629, dc_loss: 0.1881076544523239, tv_loss: 0.030798504129052162\n",
      "iteration 630, dc_loss: 0.18738244473934174, tv_loss: 0.03066416271030903\n",
      "iteration 631, dc_loss: 0.18668487668037415, tv_loss: 0.03067135252058506\n",
      "iteration 632, dc_loss: 0.18602043390274048, tv_loss: 0.030836183577775955\n",
      "iteration 633, dc_loss: 0.18552015721797943, tv_loss: 0.030639242380857468\n",
      "iteration 634, dc_loss: 0.184629887342453, tv_loss: 0.03078318014740944\n",
      "iteration 635, dc_loss: 0.18399307131767273, tv_loss: 0.030817529186606407\n",
      "iteration 636, dc_loss: 0.18350350856781006, tv_loss: 0.03070281259715557\n",
      "iteration 637, dc_loss: 0.18274201452732086, tv_loss: 0.030779942870140076\n",
      "iteration 638, dc_loss: 0.1820507049560547, tv_loss: 0.030822375789284706\n",
      "iteration 639, dc_loss: 0.18157565593719482, tv_loss: 0.030716117471456528\n",
      "iteration 640, dc_loss: 0.18081215023994446, tv_loss: 0.03085467591881752\n",
      "iteration 641, dc_loss: 0.18023079633712769, tv_loss: 0.030799178406596184\n",
      "iteration 642, dc_loss: 0.17957380414009094, tv_loss: 0.030871020630002022\n",
      "iteration 643, dc_loss: 0.17892929911613464, tv_loss: 0.030851220712065697\n",
      "iteration 644, dc_loss: 0.1783100962638855, tv_loss: 0.030881967395544052\n",
      "iteration 645, dc_loss: 0.17776918411254883, tv_loss: 0.030827820301055908\n",
      "iteration 646, dc_loss: 0.1769927442073822, tv_loss: 0.03100358135998249\n",
      "iteration 647, dc_loss: 0.17673642933368683, tv_loss: 0.03070811927318573\n",
      "iteration 648, dc_loss: 0.17588335275650024, tv_loss: 0.031129388138651848\n",
      "iteration 649, dc_loss: 0.17614637315273285, tv_loss: 0.03059500828385353\n",
      "iteration 650, dc_loss: 0.17552606761455536, tv_loss: 0.03143039718270302\n",
      "iteration 651, dc_loss: 0.17696572840213776, tv_loss: 0.03029615990817547\n",
      "iteration 652, dc_loss: 0.17624914646148682, tv_loss: 0.03183627128601074\n",
      "iteration 653, dc_loss: 0.1759461909532547, tv_loss: 0.030311180278658867\n",
      "iteration 654, dc_loss: 0.17272168397903442, tv_loss: 0.03123585134744644\n",
      "iteration 655, dc_loss: 0.17213168740272522, tv_loss: 0.031202444806694984\n",
      "iteration 656, dc_loss: 0.1731647253036499, tv_loss: 0.030565910041332245\n",
      "iteration 657, dc_loss: 0.17184463143348694, tv_loss: 0.031351398676633835\n",
      "iteration 658, dc_loss: 0.17057450115680695, tv_loss: 0.03099491074681282\n",
      "iteration 659, dc_loss: 0.1706753820180893, tv_loss: 0.030710382387042046\n",
      "iteration 660, dc_loss: 0.16968023777008057, tv_loss: 0.03139357641339302\n",
      "iteration 661, dc_loss: 0.16866861283779144, tv_loss: 0.030919542536139488\n",
      "iteration 662, dc_loss: 0.16834719479084015, tv_loss: 0.030817190185189247\n",
      "iteration 663, dc_loss: 0.16753533482551575, tv_loss: 0.03135169297456741\n",
      "iteration 664, dc_loss: 0.16682565212249756, tv_loss: 0.030989263206720352\n",
      "iteration 665, dc_loss: 0.16663473844528198, tv_loss: 0.030866751447319984\n",
      "iteration 666, dc_loss: 0.16578853130340576, tv_loss: 0.03134173899888992\n",
      "iteration 667, dc_loss: 0.16507001221179962, tv_loss: 0.03107476606965065\n",
      "iteration 668, dc_loss: 0.1649004966020584, tv_loss: 0.03085915930569172\n",
      "iteration 669, dc_loss: 0.16383181512355804, tv_loss: 0.031389858573675156\n",
      "iteration 670, dc_loss: 0.16324582695960999, tv_loss: 0.031068092212080956\n",
      "iteration 671, dc_loss: 0.16300733387470245, tv_loss: 0.03094862587749958\n",
      "iteration 672, dc_loss: 0.16202887892723083, tv_loss: 0.031376853585243225\n",
      "iteration 673, dc_loss: 0.16152425110340118, tv_loss: 0.0311322920024395\n",
      "iteration 674, dc_loss: 0.1612832099199295, tv_loss: 0.03104335069656372\n",
      "iteration 675, dc_loss: 0.160403773188591, tv_loss: 0.0313408225774765\n",
      "iteration 676, dc_loss: 0.15988531708717346, tv_loss: 0.031241046264767647\n",
      "iteration 677, dc_loss: 0.15968476235866547, tv_loss: 0.031025215983390808\n",
      "iteration 678, dc_loss: 0.1587696671485901, tv_loss: 0.031371600925922394\n",
      "iteration 679, dc_loss: 0.15832069516181946, tv_loss: 0.031263526529073715\n",
      "iteration 680, dc_loss: 0.15801206231117249, tv_loss: 0.031130488961935043\n",
      "iteration 681, dc_loss: 0.157246932387352, tv_loss: 0.03136080130934715\n",
      "iteration 682, dc_loss: 0.1568148136138916, tv_loss: 0.03125563636422157\n",
      "iteration 683, dc_loss: 0.15651430189609528, tv_loss: 0.031146759167313576\n",
      "iteration 684, dc_loss: 0.15575848519802094, tv_loss: 0.03139272704720497\n",
      "iteration 685, dc_loss: 0.15542958676815033, tv_loss: 0.03122822940349579\n",
      "iteration 686, dc_loss: 0.15489299595355988, tv_loss: 0.0312720350921154\n",
      "iteration 687, dc_loss: 0.15429078042507172, tv_loss: 0.03137017413973808\n",
      "iteration 688, dc_loss: 0.1537971943616867, tv_loss: 0.03127683699131012\n",
      "iteration 689, dc_loss: 0.15327292680740356, tv_loss: 0.031234657391905785\n",
      "iteration 690, dc_loss: 0.15247012674808502, tv_loss: 0.03142921254038811\n",
      "iteration 691, dc_loss: 0.1520322859287262, tv_loss: 0.03125940263271332\n",
      "iteration 692, dc_loss: 0.15137869119644165, tv_loss: 0.03140157833695412\n",
      "iteration 693, dc_loss: 0.1508864313364029, tv_loss: 0.031358085572719574\n",
      "iteration 694, dc_loss: 0.1504078358411789, tv_loss: 0.03133246675133705\n",
      "iteration 695, dc_loss: 0.1499296873807907, tv_loss: 0.031342845410108566\n",
      "iteration 696, dc_loss: 0.14939607679843903, tv_loss: 0.03141604736447334\n",
      "iteration 697, dc_loss: 0.14897534251213074, tv_loss: 0.03135641664266586\n",
      "iteration 698, dc_loss: 0.1484621912240982, tv_loss: 0.031404681503772736\n",
      "iteration 699, dc_loss: 0.14793707430362701, tv_loss: 0.03148944303393364\n",
      "iteration 700, dc_loss: 0.14758361876010895, tv_loss: 0.03134630247950554\n",
      "iteration 701, dc_loss: 0.1469370573759079, tv_loss: 0.0315154567360878\n",
      "iteration 702, dc_loss: 0.14662247896194458, tv_loss: 0.03137698024511337\n",
      "iteration 703, dc_loss: 0.1460074633359909, tv_loss: 0.031530674546957016\n",
      "iteration 704, dc_loss: 0.1456729620695114, tv_loss: 0.03139929100871086\n",
      "iteration 705, dc_loss: 0.14501844346523285, tv_loss: 0.0315810889005661\n",
      "iteration 706, dc_loss: 0.14473526179790497, tv_loss: 0.031378209590911865\n",
      "iteration 707, dc_loss: 0.14404264092445374, tv_loss: 0.03158026561141014\n",
      "iteration 708, dc_loss: 0.14365506172180176, tv_loss: 0.031452301889657974\n",
      "iteration 709, dc_loss: 0.14307713508605957, tv_loss: 0.03153001144528389\n",
      "iteration 710, dc_loss: 0.1425815224647522, tv_loss: 0.03154526278376579\n",
      "iteration 711, dc_loss: 0.1420835256576538, tv_loss: 0.03154214844107628\n",
      "iteration 712, dc_loss: 0.1415994018316269, tv_loss: 0.031547583639621735\n",
      "iteration 713, dc_loss: 0.1411844789981842, tv_loss: 0.03152916207909584\n",
      "iteration 714, dc_loss: 0.14074325561523438, tv_loss: 0.03158106282353401\n",
      "iteration 715, dc_loss: 0.14041614532470703, tv_loss: 0.031524501740932465\n",
      "iteration 716, dc_loss: 0.1399693787097931, tv_loss: 0.03166328743100166\n",
      "iteration 717, dc_loss: 0.13976053893566132, tv_loss: 0.0315299890935421\n",
      "iteration 718, dc_loss: 0.1394493281841278, tv_loss: 0.031623516231775284\n",
      "iteration 719, dc_loss: 0.13925257325172424, tv_loss: 0.03166359290480614\n",
      "iteration 720, dc_loss: 0.13968825340270996, tv_loss: 0.03147566691040993\n",
      "iteration 721, dc_loss: 0.13958631455898285, tv_loss: 0.031871311366558075\n",
      "iteration 722, dc_loss: 0.14020732045173645, tv_loss: 0.0313994437456131\n",
      "iteration 723, dc_loss: 0.13840703666210175, tv_loss: 0.031848788261413574\n",
      "iteration 724, dc_loss: 0.1369401067495346, tv_loss: 0.03158632293343544\n",
      "iteration 725, dc_loss: 0.13618707656860352, tv_loss: 0.031601183116436005\n",
      "iteration 726, dc_loss: 0.13604781031608582, tv_loss: 0.03179779276251793\n",
      "iteration 727, dc_loss: 0.13602487742900848, tv_loss: 0.03156515210866928\n",
      "iteration 728, dc_loss: 0.13491477072238922, tv_loss: 0.031749483197927475\n",
      "iteration 729, dc_loss: 0.13429512083530426, tv_loss: 0.031707845628261566\n",
      "iteration 730, dc_loss: 0.134212464094162, tv_loss: 0.03161728009581566\n",
      "iteration 731, dc_loss: 0.133583202958107, tv_loss: 0.03181393817067146\n",
      "iteration 732, dc_loss: 0.13308338820934296, tv_loss: 0.031614456325769424\n",
      "iteration 733, dc_loss: 0.13247044384479523, tv_loss: 0.03176082670688629\n",
      "iteration 734, dc_loss: 0.1320824921131134, tv_loss: 0.03180396184325218\n",
      "iteration 735, dc_loss: 0.13183145225048065, tv_loss: 0.031633663922548294\n",
      "iteration 736, dc_loss: 0.13107064366340637, tv_loss: 0.03188204765319824\n",
      "iteration 737, dc_loss: 0.13068434596061707, tv_loss: 0.03173181787133217\n",
      "iteration 738, dc_loss: 0.13039900362491608, tv_loss: 0.03169924020767212\n",
      "iteration 739, dc_loss: 0.1298539638519287, tv_loss: 0.03188308700919151\n",
      "iteration 740, dc_loss: 0.12939301133155823, tv_loss: 0.03176127001643181\n",
      "iteration 741, dc_loss: 0.12898194789886475, tv_loss: 0.03175165131688118\n",
      "iteration 742, dc_loss: 0.12853755056858063, tv_loss: 0.0319216214120388\n",
      "iteration 743, dc_loss: 0.12820936739444733, tv_loss: 0.03177153691649437\n",
      "iteration 744, dc_loss: 0.12772363424301147, tv_loss: 0.031746793538331985\n",
      "iteration 745, dc_loss: 0.1271773725748062, tv_loss: 0.03194630518555641\n",
      "iteration 746, dc_loss: 0.12704122066497803, tv_loss: 0.031727731227874756\n",
      "iteration 747, dc_loss: 0.12639951705932617, tv_loss: 0.031911496073007584\n",
      "iteration 748, dc_loss: 0.12599702179431915, tv_loss: 0.03185790777206421\n",
      "iteration 749, dc_loss: 0.1256721317768097, tv_loss: 0.03180531784892082\n",
      "iteration 750, dc_loss: 0.1251957267522812, tv_loss: 0.03194921463727951\n",
      "iteration 751, dc_loss: 0.12489406019449234, tv_loss: 0.03182978183031082\n",
      "iteration 752, dc_loss: 0.1243637427687645, tv_loss: 0.03195599094033241\n",
      "iteration 753, dc_loss: 0.12407146394252777, tv_loss: 0.03185701742768288\n",
      "iteration 754, dc_loss: 0.12360019236803055, tv_loss: 0.031943030655384064\n",
      "iteration 755, dc_loss: 0.12331242859363556, tv_loss: 0.03187187761068344\n",
      "iteration 756, dc_loss: 0.1228613555431366, tv_loss: 0.03198607265949249\n",
      "iteration 757, dc_loss: 0.12261141091585159, tv_loss: 0.03189019486308098\n",
      "iteration 758, dc_loss: 0.12213100492954254, tv_loss: 0.032018303871154785\n",
      "iteration 759, dc_loss: 0.12184601277112961, tv_loss: 0.03189484030008316\n",
      "iteration 760, dc_loss: 0.12135040760040283, tv_loss: 0.03203761205077171\n",
      "iteration 761, dc_loss: 0.1211317777633667, tv_loss: 0.03184658661484718\n",
      "iteration 762, dc_loss: 0.12049489468336105, tv_loss: 0.03209931030869484\n",
      "iteration 763, dc_loss: 0.12025757133960724, tv_loss: 0.03189373016357422\n",
      "iteration 764, dc_loss: 0.11968223750591278, tv_loss: 0.03204764798283577\n",
      "iteration 765, dc_loss: 0.11936917155981064, tv_loss: 0.031990040093660355\n",
      "iteration 766, dc_loss: 0.11902596056461334, tv_loss: 0.03199082612991333\n",
      "iteration 767, dc_loss: 0.11863335222005844, tv_loss: 0.03205277770757675\n",
      "iteration 768, dc_loss: 0.11831945180892944, tv_loss: 0.0320114865899086\n",
      "iteration 769, dc_loss: 0.11795651912689209, tv_loss: 0.0320357047021389\n",
      "iteration 770, dc_loss: 0.11752098798751831, tv_loss: 0.03207862749695778\n",
      "iteration 771, dc_loss: 0.11718432605266571, tv_loss: 0.032026831060647964\n",
      "iteration 772, dc_loss: 0.11677084863185883, tv_loss: 0.032089706510305405\n",
      "iteration 773, dc_loss: 0.11651069670915604, tv_loss: 0.032017432153224945\n",
      "iteration 774, dc_loss: 0.11603942513465881, tv_loss: 0.03215254843235016\n",
      "iteration 775, dc_loss: 0.11590807139873505, tv_loss: 0.03196191042661667\n",
      "iteration 776, dc_loss: 0.1153412014245987, tv_loss: 0.032228466123342514\n",
      "iteration 777, dc_loss: 0.11531252413988113, tv_loss: 0.03196222707629204\n",
      "iteration 778, dc_loss: 0.11470695585012436, tv_loss: 0.03226759657263756\n",
      "iteration 779, dc_loss: 0.11480366438627243, tv_loss: 0.03193262964487076\n",
      "iteration 780, dc_loss: 0.11421849578619003, tv_loss: 0.03232111036777496\n",
      "iteration 781, dc_loss: 0.11450538039207458, tv_loss: 0.03191894292831421\n",
      "iteration 782, dc_loss: 0.11386524140834808, tv_loss: 0.032391224056482315\n",
      "iteration 783, dc_loss: 0.11418962478637695, tv_loss: 0.03188437968492508\n",
      "iteration 784, dc_loss: 0.11316487193107605, tv_loss: 0.03252450004220009\n",
      "iteration 785, dc_loss: 0.11321884393692017, tv_loss: 0.03183738887310028\n",
      "iteration 786, dc_loss: 0.11202188581228256, tv_loss: 0.032541658729314804\n",
      "iteration 787, dc_loss: 0.1122753694653511, tv_loss: 0.03189953416585922\n",
      "iteration 788, dc_loss: 0.11174993216991425, tv_loss: 0.03241611272096634\n",
      "iteration 789, dc_loss: 0.11208793520927429, tv_loss: 0.03208410367369652\n",
      "iteration 790, dc_loss: 0.11177706718444824, tv_loss: 0.03228137269616127\n",
      "iteration 791, dc_loss: 0.11128779500722885, tv_loss: 0.032139621675014496\n",
      "iteration 792, dc_loss: 0.11037516593933105, tv_loss: 0.03234364092350006\n",
      "iteration 793, dc_loss: 0.10991638898849487, tv_loss: 0.03216426447033882\n",
      "iteration 794, dc_loss: 0.1093931645154953, tv_loss: 0.03227034583687782\n",
      "iteration 795, dc_loss: 0.10915617644786835, tv_loss: 0.03222939744591713\n",
      "iteration 796, dc_loss: 0.10889693349599838, tv_loss: 0.03220862150192261\n",
      "iteration 797, dc_loss: 0.10850447416305542, tv_loss: 0.03231590613722801\n",
      "iteration 798, dc_loss: 0.10825707018375397, tv_loss: 0.03219947591423988\n",
      "iteration 799, dc_loss: 0.10771350562572479, tv_loss: 0.032335661351680756\n",
      "iteration 800, dc_loss: 0.10739616304636002, tv_loss: 0.03224625438451767\n",
      "iteration 801, dc_loss: 0.10702162235975266, tv_loss: 0.032256241887807846\n",
      "iteration 802, dc_loss: 0.10669773817062378, tv_loss: 0.03229445964097977\n",
      "iteration 803, dc_loss: 0.10645413398742676, tv_loss: 0.032267533242702484\n",
      "iteration 804, dc_loss: 0.10619451850652695, tv_loss: 0.03228812292218208\n",
      "iteration 805, dc_loss: 0.1059175580739975, tv_loss: 0.032299697399139404\n",
      "iteration 806, dc_loss: 0.10566327720880508, tv_loss: 0.03229489549994469\n",
      "iteration 807, dc_loss: 0.10540328174829483, tv_loss: 0.03230717405676842\n",
      "iteration 808, dc_loss: 0.10516176372766495, tv_loss: 0.032300956547260284\n",
      "iteration 809, dc_loss: 0.10488670319318771, tv_loss: 0.03231857717037201\n",
      "iteration 810, dc_loss: 0.1046239510178566, tv_loss: 0.03233112394809723\n",
      "iteration 811, dc_loss: 0.10440830886363983, tv_loss: 0.032317664474248886\n",
      "iteration 812, dc_loss: 0.1041153073310852, tv_loss: 0.032351747155189514\n",
      "iteration 813, dc_loss: 0.10387099534273148, tv_loss: 0.03234260529279709\n",
      "iteration 814, dc_loss: 0.10368503630161285, tv_loss: 0.03230826556682587\n",
      "iteration 815, dc_loss: 0.1033586710691452, tv_loss: 0.03237578272819519\n",
      "iteration 816, dc_loss: 0.10310868173837662, tv_loss: 0.03239627182483673\n",
      "iteration 817, dc_loss: 0.10294682532548904, tv_loss: 0.03233492001891136\n",
      "iteration 818, dc_loss: 0.10262509435415268, tv_loss: 0.032381538301706314\n",
      "iteration 819, dc_loss: 0.10237663984298706, tv_loss: 0.03239283710718155\n",
      "iteration 820, dc_loss: 0.10220928490161896, tv_loss: 0.03234226256608963\n",
      "iteration 821, dc_loss: 0.10190653800964355, tv_loss: 0.032411444932222366\n",
      "iteration 822, dc_loss: 0.10166218876838684, tv_loss: 0.03241787850856781\n",
      "iteration 823, dc_loss: 0.10147738456726074, tv_loss: 0.03236435726284981\n",
      "iteration 824, dc_loss: 0.10117675364017487, tv_loss: 0.032428815960884094\n",
      "iteration 825, dc_loss: 0.10096157342195511, tv_loss: 0.03240188956260681\n",
      "iteration 826, dc_loss: 0.1007516160607338, tv_loss: 0.032398100942373276\n",
      "iteration 827, dc_loss: 0.10047635436058044, tv_loss: 0.03244694694876671\n",
      "iteration 828, dc_loss: 0.10026129335165024, tv_loss: 0.0324261337518692\n",
      "iteration 829, dc_loss: 0.10002484172582626, tv_loss: 0.032431378960609436\n",
      "iteration 830, dc_loss: 0.09977700561285019, tv_loss: 0.03246084228157997\n",
      "iteration 831, dc_loss: 0.09958092868328094, tv_loss: 0.0324290469288826\n",
      "iteration 832, dc_loss: 0.09931116551160812, tv_loss: 0.032464418560266495\n",
      "iteration 833, dc_loss: 0.099087655544281, tv_loss: 0.032463446259498596\n",
      "iteration 834, dc_loss: 0.09887587279081345, tv_loss: 0.032455895096063614\n",
      "iteration 835, dc_loss: 0.09862437099218369, tv_loss: 0.03248310461640358\n",
      "iteration 836, dc_loss: 0.0984085351228714, tv_loss: 0.03247208148241043\n",
      "iteration 837, dc_loss: 0.09818196296691895, tv_loss: 0.032472170889377594\n",
      "iteration 838, dc_loss: 0.0979451984167099, tv_loss: 0.03249496966600418\n",
      "iteration 839, dc_loss: 0.0977334976196289, tv_loss: 0.032493945211172104\n",
      "iteration 840, dc_loss: 0.09749401360750198, tv_loss: 0.03250661864876747\n",
      "iteration 841, dc_loss: 0.09728223085403442, tv_loss: 0.032497018575668335\n",
      "iteration 842, dc_loss: 0.09704624861478806, tv_loss: 0.032518528401851654\n",
      "iteration 843, dc_loss: 0.09683652222156525, tv_loss: 0.03251155465841293\n",
      "iteration 844, dc_loss: 0.09660977870225906, tv_loss: 0.032516028732061386\n",
      "iteration 845, dc_loss: 0.09638182818889618, tv_loss: 0.0325278639793396\n",
      "iteration 846, dc_loss: 0.09616618603467941, tv_loss: 0.03253275156021118\n",
      "iteration 847, dc_loss: 0.09595584869384766, tv_loss: 0.03253031149506569\n",
      "iteration 848, dc_loss: 0.09571482241153717, tv_loss: 0.032557178288698196\n",
      "iteration 849, dc_loss: 0.09553218632936478, tv_loss: 0.03252509608864784\n",
      "iteration 850, dc_loss: 0.09527262300252914, tv_loss: 0.0325724259018898\n",
      "iteration 851, dc_loss: 0.09509025514125824, tv_loss: 0.03254243731498718\n",
      "iteration 852, dc_loss: 0.09486023336648941, tv_loss: 0.03256034851074219\n",
      "iteration 853, dc_loss: 0.0946386530995369, tv_loss: 0.032570503652095795\n",
      "iteration 854, dc_loss: 0.0944366380572319, tv_loss: 0.03256597742438316\n",
      "iteration 855, dc_loss: 0.09421709924936295, tv_loss: 0.0325782336294651\n",
      "iteration 856, dc_loss: 0.094001404941082, tv_loss: 0.03258615359663963\n",
      "iteration 857, dc_loss: 0.09380627423524857, tv_loss: 0.0325738787651062\n",
      "iteration 858, dc_loss: 0.09356039762496948, tv_loss: 0.03261950984597206\n",
      "iteration 859, dc_loss: 0.09340831637382507, tv_loss: 0.032568518072366714\n",
      "iteration 860, dc_loss: 0.09313167631626129, tv_loss: 0.032642047852277756\n",
      "iteration 861, dc_loss: 0.09300297498703003, tv_loss: 0.0325661227107048\n",
      "iteration 862, dc_loss: 0.09271618723869324, tv_loss: 0.03265547379851341\n",
      "iteration 863, dc_loss: 0.09260045737028122, tv_loss: 0.03257835656404495\n",
      "iteration 864, dc_loss: 0.09233429282903671, tv_loss: 0.032665349543094635\n",
      "iteration 865, dc_loss: 0.09224548935890198, tv_loss: 0.03257709741592407\n",
      "iteration 866, dc_loss: 0.09197193384170532, tv_loss: 0.03270649164915085\n",
      "iteration 867, dc_loss: 0.09197456389665604, tv_loss: 0.03254345804452896\n",
      "iteration 868, dc_loss: 0.09163593500852585, tv_loss: 0.032771989703178406\n",
      "iteration 869, dc_loss: 0.09166897088289261, tv_loss: 0.03251458704471588\n",
      "iteration 870, dc_loss: 0.09117965400218964, tv_loss: 0.03279143199324608\n",
      "iteration 871, dc_loss: 0.09110803157091141, tv_loss: 0.03254827484488487\n",
      "iteration 872, dc_loss: 0.09065855294466019, tv_loss: 0.0327259823679924\n",
      "iteration 873, dc_loss: 0.09050807356834412, tv_loss: 0.032668616622686386\n",
      "iteration 874, dc_loss: 0.09039841592311859, tv_loss: 0.032624877989292145\n",
      "iteration 875, dc_loss: 0.09012231975793839, tv_loss: 0.03275454044342041\n",
      "iteration 876, dc_loss: 0.09005692601203918, tv_loss: 0.032606419175863266\n",
      "iteration 877, dc_loss: 0.08967413753271103, tv_loss: 0.03276413306593895\n",
      "iteration 878, dc_loss: 0.08957444876432419, tv_loss: 0.03264595568180084\n",
      "iteration 879, dc_loss: 0.08932550996541977, tv_loss: 0.03272031247615814\n",
      "iteration 880, dc_loss: 0.08917649835348129, tv_loss: 0.03269040584564209\n",
      "iteration 881, dc_loss: 0.0889621153473854, tv_loss: 0.03270687162876129\n",
      "iteration 882, dc_loss: 0.0887332409620285, tv_loss: 0.03270883858203888\n",
      "iteration 883, dc_loss: 0.088542141020298, tv_loss: 0.03270462155342102\n",
      "iteration 884, dc_loss: 0.08834027498960495, tv_loss: 0.03272286802530289\n",
      "iteration 885, dc_loss: 0.08818651735782623, tv_loss: 0.032735489308834076\n",
      "iteration 886, dc_loss: 0.0879870355129242, tv_loss: 0.03273716941475868\n",
      "iteration 887, dc_loss: 0.08781583607196808, tv_loss: 0.032715510576963425\n",
      "iteration 888, dc_loss: 0.0875493735074997, tv_loss: 0.03277331590652466\n",
      "iteration 889, dc_loss: 0.08744331449270248, tv_loss: 0.03270024061203003\n",
      "iteration 890, dc_loss: 0.0871482864022255, tv_loss: 0.03280634060502052\n",
      "iteration 891, dc_loss: 0.08708697557449341, tv_loss: 0.03270459920167923\n",
      "iteration 892, dc_loss: 0.08676852285861969, tv_loss: 0.03282417356967926\n",
      "iteration 893, dc_loss: 0.08668320626020432, tv_loss: 0.03271854668855667\n",
      "iteration 894, dc_loss: 0.08641903847455978, tv_loss: 0.03278154134750366\n",
      "iteration 895, dc_loss: 0.08624700456857681, tv_loss: 0.03278237208724022\n",
      "iteration 896, dc_loss: 0.08608207106590271, tv_loss: 0.03276495262980461\n",
      "iteration 897, dc_loss: 0.08586002886295319, tv_loss: 0.03283078595995903\n",
      "iteration 898, dc_loss: 0.08579370379447937, tv_loss: 0.03272118419408798\n",
      "iteration 899, dc_loss: 0.0854603573679924, tv_loss: 0.03288940340280533\n",
      "iteration 900, dc_loss: 0.0854685977101326, tv_loss: 0.0327107273042202\n",
      "iteration 901, dc_loss: 0.08509907126426697, tv_loss: 0.032915305346250534\n",
      "iteration 902, dc_loss: 0.08513453602790833, tv_loss: 0.032708656042814255\n",
      "iteration 903, dc_loss: 0.08473865687847137, tv_loss: 0.03291681781411171\n",
      "iteration 904, dc_loss: 0.0847608745098114, tv_loss: 0.032723791897296906\n",
      "iteration 905, dc_loss: 0.08439941704273224, tv_loss: 0.032922253012657166\n",
      "iteration 906, dc_loss: 0.08439817279577255, tv_loss: 0.032758720219135284\n",
      "iteration 907, dc_loss: 0.08407966047525406, tv_loss: 0.032904233783483505\n",
      "iteration 908, dc_loss: 0.08404237776994705, tv_loss: 0.03277307748794556\n",
      "iteration 909, dc_loss: 0.0837908536195755, tv_loss: 0.03288084268569946\n",
      "iteration 910, dc_loss: 0.08373046666383743, tv_loss: 0.032818134874105453\n",
      "iteration 911, dc_loss: 0.08364340662956238, tv_loss: 0.03285713866353035\n",
      "iteration 912, dc_loss: 0.0836295560002327, tv_loss: 0.032880764454603195\n",
      "iteration 913, dc_loss: 0.08393799513578415, tv_loss: 0.03278706222772598\n",
      "iteration 914, dc_loss: 0.08389734476804733, tv_loss: 0.03297264128923416\n",
      "iteration 915, dc_loss: 0.08444278687238693, tv_loss: 0.0326971672475338\n",
      "iteration 916, dc_loss: 0.08368236571550369, tv_loss: 0.03306978940963745\n",
      "iteration 917, dc_loss: 0.08337628096342087, tv_loss: 0.032701004296541214\n",
      "iteration 918, dc_loss: 0.08241889625787735, tv_loss: 0.03298305347561836\n",
      "iteration 919, dc_loss: 0.08234263956546783, tv_loss: 0.032894786447286606\n",
      "iteration 920, dc_loss: 0.0824505165219307, tv_loss: 0.03285786882042885\n",
      "iteration 921, dc_loss: 0.08223612606525421, tv_loss: 0.03294644132256508\n",
      "iteration 922, dc_loss: 0.08188066631555557, tv_loss: 0.03289305791258812\n",
      "iteration 923, dc_loss: 0.08144151419401169, tv_loss: 0.032930683344602585\n",
      "iteration 924, dc_loss: 0.0813576728105545, tv_loss: 0.03291409835219383\n",
      "iteration 925, dc_loss: 0.0814051553606987, tv_loss: 0.03287970647215843\n",
      "iteration 926, dc_loss: 0.08109836280345917, tv_loss: 0.03296400606632233\n",
      "iteration 927, dc_loss: 0.08083536475896835, tv_loss: 0.03288679197430611\n",
      "iteration 928, dc_loss: 0.08057420700788498, tv_loss: 0.03291776403784752\n",
      "iteration 929, dc_loss: 0.08040659874677658, tv_loss: 0.03301316499710083\n",
      "iteration 930, dc_loss: 0.08050370961427689, tv_loss: 0.0328088141977787\n",
      "iteration 931, dc_loss: 0.08004561066627502, tv_loss: 0.03301575779914856\n",
      "iteration 932, dc_loss: 0.0798826813697815, tv_loss: 0.032951414585113525\n",
      "iteration 933, dc_loss: 0.07984575629234314, tv_loss: 0.03288065642118454\n",
      "iteration 934, dc_loss: 0.07954993098974228, tv_loss: 0.03303864598274231\n",
      "iteration 935, dc_loss: 0.0795164629817009, tv_loss: 0.03284590318799019\n",
      "iteration 936, dc_loss: 0.07914622873067856, tv_loss: 0.03300962597131729\n",
      "iteration 937, dc_loss: 0.07907252758741379, tv_loss: 0.03294862061738968\n",
      "iteration 938, dc_loss: 0.07896305620670319, tv_loss: 0.032953083515167236\n",
      "iteration 939, dc_loss: 0.0787554606795311, tv_loss: 0.03297977149486542\n",
      "iteration 940, dc_loss: 0.07855413854122162, tv_loss: 0.032994888722896576\n",
      "iteration 941, dc_loss: 0.07843960076570511, tv_loss: 0.03292251005768776\n",
      "iteration 942, dc_loss: 0.07817120850086212, tv_loss: 0.03305507451295853\n",
      "iteration 943, dc_loss: 0.07815370708703995, tv_loss: 0.03292876109480858\n",
      "iteration 944, dc_loss: 0.07788821309804916, tv_loss: 0.033026572316884995\n",
      "iteration 945, dc_loss: 0.07773707062005997, tv_loss: 0.03299423307180405\n",
      "iteration 946, dc_loss: 0.07760263234376907, tv_loss: 0.03298499062657356\n",
      "iteration 947, dc_loss: 0.0774182602763176, tv_loss: 0.033033307641744614\n",
      "iteration 948, dc_loss: 0.07730350643396378, tv_loss: 0.032999880611896515\n",
      "iteration 949, dc_loss: 0.07712838798761368, tv_loss: 0.03301050141453743\n",
      "iteration 950, dc_loss: 0.07696018368005753, tv_loss: 0.03301801532506943\n",
      "iteration 951, dc_loss: 0.07679533213376999, tv_loss: 0.03302633389830589\n",
      "iteration 952, dc_loss: 0.07667722553014755, tv_loss: 0.03299552947282791\n",
      "iteration 953, dc_loss: 0.07645125687122345, tv_loss: 0.03307241201400757\n",
      "iteration 954, dc_loss: 0.07642263919115067, tv_loss: 0.032971274107694626\n",
      "iteration 955, dc_loss: 0.07612957060337067, tv_loss: 0.03313063830137253\n",
      "iteration 956, dc_loss: 0.07619422674179077, tv_loss: 0.03292197361588478\n",
      "iteration 957, dc_loss: 0.07580295205116272, tv_loss: 0.033201489597558975\n",
      "iteration 958, dc_loss: 0.07606059312820435, tv_loss: 0.032862525433301926\n",
      "iteration 959, dc_loss: 0.07556965947151184, tv_loss: 0.03333999589085579\n",
      "iteration 960, dc_loss: 0.07626353204250336, tv_loss: 0.032706599682569504\n",
      "iteration 961, dc_loss: 0.07562143355607986, tv_loss: 0.03357307240366936\n",
      "iteration 962, dc_loss: 0.07690873742103577, tv_loss: 0.03249093145132065\n",
      "iteration 963, dc_loss: 0.07589121907949448, tv_loss: 0.03379218652844429\n",
      "iteration 964, dc_loss: 0.0767575278878212, tv_loss: 0.032480306923389435\n",
      "iteration 965, dc_loss: 0.07495870441198349, tv_loss: 0.03354686498641968\n",
      "iteration 966, dc_loss: 0.07478698343038559, tv_loss: 0.0329749621450901\n",
      "iteration 967, dc_loss: 0.07470794767141342, tv_loss: 0.03291339427232742\n",
      "iteration 968, dc_loss: 0.07441096007823944, tv_loss: 0.033488716930150986\n",
      "iteration 969, dc_loss: 0.0750705748796463, tv_loss: 0.03270760551095009\n",
      "iteration 970, dc_loss: 0.07398427277803421, tv_loss: 0.03333473950624466\n",
      "iteration 971, dc_loss: 0.07387124747037888, tv_loss: 0.03312547132372856\n",
      "iteration 972, dc_loss: 0.0741272047162056, tv_loss: 0.032879725098609924\n",
      "iteration 973, dc_loss: 0.07357071340084076, tv_loss: 0.03340918570756912\n",
      "iteration 974, dc_loss: 0.07374581694602966, tv_loss: 0.032935626804828644\n",
      "iteration 975, dc_loss: 0.07336439192295074, tv_loss: 0.03312985599040985\n",
      "iteration 976, dc_loss: 0.07313656806945801, tv_loss: 0.03327910229563713\n",
      "iteration 977, dc_loss: 0.07328727096319199, tv_loss: 0.03294599428772926\n",
      "iteration 978, dc_loss: 0.07281921803951263, tv_loss: 0.03323638066649437\n",
      "iteration 979, dc_loss: 0.07273795455694199, tv_loss: 0.03317911922931671\n",
      "iteration 980, dc_loss: 0.07273081690073013, tv_loss: 0.033046476542949677\n",
      "iteration 981, dc_loss: 0.07242967188358307, tv_loss: 0.03321411460638046\n",
      "iteration 982, dc_loss: 0.07231885194778442, tv_loss: 0.03315146267414093\n",
      "iteration 983, dc_loss: 0.07220423966646194, tv_loss: 0.033112600445747375\n",
      "iteration 984, dc_loss: 0.07200929522514343, tv_loss: 0.03321141004562378\n",
      "iteration 985, dc_loss: 0.07192714512348175, tv_loss: 0.03315695375204086\n",
      "iteration 986, dc_loss: 0.07176011800765991, tv_loss: 0.03315036743879318\n",
      "iteration 987, dc_loss: 0.07156070321798325, tv_loss: 0.03319261968135834\n",
      "iteration 988, dc_loss: 0.07151513546705246, tv_loss: 0.033121414482593536\n",
      "iteration 989, dc_loss: 0.07131516188383102, tv_loss: 0.03319419547915459\n",
      "iteration 990, dc_loss: 0.07118396461009979, tv_loss: 0.03318173810839653\n",
      "iteration 991, dc_loss: 0.07110603153705597, tv_loss: 0.03312385082244873\n",
      "iteration 992, dc_loss: 0.07084266096353531, tv_loss: 0.033244404941797256\n",
      "iteration 993, dc_loss: 0.0708429366350174, tv_loss: 0.03311512991786003\n",
      "iteration 994, dc_loss: 0.07062070071697235, tv_loss: 0.033209022134542465\n",
      "iteration 995, dc_loss: 0.07047457247972488, tv_loss: 0.03320762887597084\n",
      "iteration 996, dc_loss: 0.07040306180715561, tv_loss: 0.033151764422655106\n",
      "iteration 997, dc_loss: 0.07017552107572556, tv_loss: 0.0332634337246418\n",
      "iteration 998, dc_loss: 0.07017220556735992, tv_loss: 0.03313363343477249\n",
      "iteration 999, dc_loss: 0.0699271634221077, tv_loss: 0.033260904252529144\n",
      "iteration 1000, dc_loss: 0.06991248577833176, tv_loss: 0.03315160796046257\n",
      "iteration 1001, dc_loss: 0.06966967135667801, tv_loss: 0.033276911824941635\n",
      "iteration 1002, dc_loss: 0.06966327875852585, tv_loss: 0.03316773101687431\n",
      "iteration 1003, dc_loss: 0.06950920820236206, tv_loss: 0.03324471041560173\n",
      "iteration 1004, dc_loss: 0.06955751776695251, tv_loss: 0.03318566083908081\n",
      "iteration 1005, dc_loss: 0.06953594088554382, tv_loss: 0.033261582255363464\n",
      "iteration 1006, dc_loss: 0.06987160444259644, tv_loss: 0.033173419535160065\n",
      "iteration 1007, dc_loss: 0.07003063708543777, tv_loss: 0.03328825533390045\n",
      "iteration 1008, dc_loss: 0.07064847648143768, tv_loss: 0.033166367560625076\n",
      "iteration 1009, dc_loss: 0.07027769833803177, tv_loss: 0.03329993039369583\n",
      "iteration 1010, dc_loss: 0.06976794451475143, tv_loss: 0.03320953622460365\n",
      "iteration 1011, dc_loss: 0.06889638304710388, tv_loss: 0.03320108354091644\n",
      "iteration 1012, dc_loss: 0.0684393048286438, tv_loss: 0.0333392433822155\n",
      "iteration 1013, dc_loss: 0.06881622970104218, tv_loss: 0.03312239423394203\n",
      "iteration 1014, dc_loss: 0.06860539317131042, tv_loss: 0.0333700068295002\n",
      "iteration 1015, dc_loss: 0.06846099346876144, tv_loss: 0.0331907756626606\n",
      "iteration 1016, dc_loss: 0.06800579279661179, tv_loss: 0.033247604966163635\n",
      "iteration 1017, dc_loss: 0.06778796762228012, tv_loss: 0.03335588425397873\n",
      "iteration 1018, dc_loss: 0.06799204647541046, tv_loss: 0.03315834328532219\n",
      "iteration 1019, dc_loss: 0.06767476350069046, tv_loss: 0.03339635580778122\n",
      "iteration 1020, dc_loss: 0.06754858791828156, tv_loss: 0.033207617700099945\n",
      "iteration 1021, dc_loss: 0.06726735085248947, tv_loss: 0.033274851739406586\n",
      "iteration 1022, dc_loss: 0.06718625873327255, tv_loss: 0.03336513414978981\n",
      "iteration 1023, dc_loss: 0.0672643780708313, tv_loss: 0.03319505229592323\n",
      "iteration 1024, dc_loss: 0.06684240698814392, tv_loss: 0.03336054086685181\n",
      "iteration 1025, dc_loss: 0.06676097959280014, tv_loss: 0.03325994685292244\n",
      "iteration 1026, dc_loss: 0.06670504808425903, tv_loss: 0.033269546926021576\n",
      "iteration 1027, dc_loss: 0.06652045994997025, tv_loss: 0.033373136073350906\n",
      "iteration 1028, dc_loss: 0.06648309528827667, tv_loss: 0.033228982239961624\n",
      "iteration 1029, dc_loss: 0.06619476526975632, tv_loss: 0.033337973058223724\n",
      "iteration 1030, dc_loss: 0.06612268835306168, tv_loss: 0.033331699669361115\n",
      "iteration 1031, dc_loss: 0.06610715389251709, tv_loss: 0.03326411172747612\n",
      "iteration 1032, dc_loss: 0.06585346162319183, tv_loss: 0.03335943818092346\n",
      "iteration 1033, dc_loss: 0.06577922403812408, tv_loss: 0.03327663987874985\n",
      "iteration 1034, dc_loss: 0.06561128795146942, tv_loss: 0.03333181142807007\n",
      "iteration 1035, dc_loss: 0.0655229464173317, tv_loss: 0.03331093117594719\n",
      "iteration 1036, dc_loss: 0.06540147215127945, tv_loss: 0.03332677483558655\n",
      "iteration 1037, dc_loss: 0.06528979539871216, tv_loss: 0.03331482410430908\n",
      "iteration 1038, dc_loss: 0.06512479484081268, tv_loss: 0.033355794847011566\n",
      "iteration 1039, dc_loss: 0.06505487859249115, tv_loss: 0.03330299258232117\n",
      "iteration 1040, dc_loss: 0.06487171351909637, tv_loss: 0.03336385264992714\n",
      "iteration 1041, dc_loss: 0.06482360512018204, tv_loss: 0.033317457884550095\n",
      "iteration 1042, dc_loss: 0.06467630714178085, tv_loss: 0.03335488960146904\n",
      "iteration 1043, dc_loss: 0.06458048522472382, tv_loss: 0.03332902491092682\n",
      "iteration 1044, dc_loss: 0.06443172693252563, tv_loss: 0.03335177153348923\n",
      "iteration 1045, dc_loss: 0.06432370841503143, tv_loss: 0.0333571583032608\n",
      "iteration 1046, dc_loss: 0.06422317773103714, tv_loss: 0.03335043415427208\n",
      "iteration 1047, dc_loss: 0.06415487080812454, tv_loss: 0.033343732357025146\n",
      "iteration 1048, dc_loss: 0.06408490240573883, tv_loss: 0.0333552360534668\n",
      "iteration 1049, dc_loss: 0.06407957524061203, tv_loss: 0.03334759175777435\n",
      "iteration 1050, dc_loss: 0.06403926014900208, tv_loss: 0.033410850912332535\n",
      "iteration 1051, dc_loss: 0.06435096263885498, tv_loss: 0.033275846391916275\n",
      "iteration 1052, dc_loss: 0.06417841464281082, tv_loss: 0.03358440846204758\n",
      "iteration 1053, dc_loss: 0.06491077691316605, tv_loss: 0.03307914733886719\n",
      "iteration 1054, dc_loss: 0.06419399380683899, tv_loss: 0.03389108553528786\n",
      "iteration 1055, dc_loss: 0.06494995951652527, tv_loss: 0.03283810243010521\n",
      "iteration 1056, dc_loss: 0.06339102983474731, tv_loss: 0.03386947885155678\n",
      "iteration 1057, dc_loss: 0.06359650939702988, tv_loss: 0.0331418476998806\n",
      "iteration 1058, dc_loss: 0.06317608803510666, tv_loss: 0.033392615616321564\n",
      "iteration 1059, dc_loss: 0.0629197284579277, tv_loss: 0.03364503011107445\n",
      "iteration 1060, dc_loss: 0.06356018781661987, tv_loss: 0.03305382654070854\n",
      "iteration 1061, dc_loss: 0.06269023567438126, tv_loss: 0.03376172110438347\n",
      "iteration 1062, dc_loss: 0.06281989067792892, tv_loss: 0.033231545239686966\n",
      "iteration 1063, dc_loss: 0.06255749613046646, tv_loss: 0.03333747386932373\n",
      "iteration 1064, dc_loss: 0.062241725623607635, tv_loss: 0.03370068222284317\n",
      "iteration 1065, dc_loss: 0.06277142465114594, tv_loss: 0.03308504447340965\n",
      "iteration 1066, dc_loss: 0.061996303498744965, tv_loss: 0.03360584005713463\n",
      "iteration 1067, dc_loss: 0.06197093799710274, tv_loss: 0.03343012556433678\n",
      "iteration 1068, dc_loss: 0.06215762346982956, tv_loss: 0.03323312848806381\n",
      "iteration 1069, dc_loss: 0.061652641743421555, tv_loss: 0.03369041159749031\n",
      "iteration 1070, dc_loss: 0.06189552694559097, tv_loss: 0.03325127437710762\n",
      "iteration 1071, dc_loss: 0.061525408178567886, tv_loss: 0.03345614671707153\n",
      "iteration 1072, dc_loss: 0.06130756065249443, tv_loss: 0.03356822952628136\n",
      "iteration 1073, dc_loss: 0.061574000865221024, tv_loss: 0.033261220902204514\n",
      "iteration 1074, dc_loss: 0.06111684441566467, tv_loss: 0.03358324617147446\n",
      "iteration 1075, dc_loss: 0.061136141419410706, tv_loss: 0.03341256454586983\n",
      "iteration 1076, dc_loss: 0.061074256896972656, tv_loss: 0.03339376300573349\n",
      "iteration 1077, dc_loss: 0.06079648807644844, tv_loss: 0.03356001153588295\n",
      "iteration 1078, dc_loss: 0.06086868792772293, tv_loss: 0.03335989639163017\n",
      "iteration 1079, dc_loss: 0.060647740960121155, tv_loss: 0.033509500324726105\n",
      "iteration 1080, dc_loss: 0.0606064572930336, tv_loss: 0.033451151102781296\n",
      "iteration 1081, dc_loss: 0.06046951189637184, tv_loss: 0.03342783451080322\n",
      "iteration 1082, dc_loss: 0.060294345021247864, tv_loss: 0.03350050002336502\n",
      "iteration 1083, dc_loss: 0.06030844897031784, tv_loss: 0.03342396020889282\n",
      "iteration 1084, dc_loss: 0.060134150087833405, tv_loss: 0.033499203622341156\n",
      "iteration 1085, dc_loss: 0.06008155271410942, tv_loss: 0.03342961147427559\n",
      "iteration 1086, dc_loss: 0.05989043414592743, tv_loss: 0.03350963816046715\n",
      "iteration 1087, dc_loss: 0.059857096523046494, tv_loss: 0.033448539674282074\n",
      "iteration 1088, dc_loss: 0.05973061919212341, tv_loss: 0.033486783504486084\n",
      "iteration 1089, dc_loss: 0.05960433930158615, tv_loss: 0.03350260853767395\n",
      "iteration 1090, dc_loss: 0.05959867686033249, tv_loss: 0.03341564163565636\n",
      "iteration 1091, dc_loss: 0.059394534677267075, tv_loss: 0.033551912754774094\n",
      "iteration 1092, dc_loss: 0.059395670890808105, tv_loss: 0.03343747556209564\n",
      "iteration 1093, dc_loss: 0.0592382550239563, tv_loss: 0.0334911085665226\n",
      "iteration 1094, dc_loss: 0.05913795530796051, tv_loss: 0.033529214560985565\n",
      "iteration 1095, dc_loss: 0.05921676754951477, tv_loss: 0.033424850553274155\n",
      "iteration 1096, dc_loss: 0.05903096869587898, tv_loss: 0.03359638527035713\n",
      "iteration 1097, dc_loss: 0.0593702606856823, tv_loss: 0.033364322036504745\n",
      "iteration 1098, dc_loss: 0.05922483652830124, tv_loss: 0.03365441411733627\n",
      "iteration 1099, dc_loss: 0.059894874691963196, tv_loss: 0.0333508662879467\n",
      "iteration 1100, dc_loss: 0.059666842222213745, tv_loss: 0.033631034195423126\n",
      "iteration 1101, dc_loss: 0.05983288213610649, tv_loss: 0.03347411006689072\n",
      "iteration 1102, dc_loss: 0.05931656435132027, tv_loss: 0.03345319628715515\n",
      "iteration 1103, dc_loss: 0.058577220886945724, tv_loss: 0.033694665879011154\n",
      "iteration 1104, dc_loss: 0.0586874820291996, tv_loss: 0.03329819068312645\n",
      "iteration 1105, dc_loss: 0.05822383984923363, tv_loss: 0.03374923765659332\n",
      "iteration 1106, dc_loss: 0.05863761156797409, tv_loss: 0.033367712050676346\n",
      "iteration 1107, dc_loss: 0.05824359878897667, tv_loss: 0.033568695187568665\n",
      "iteration 1108, dc_loss: 0.05791408196091652, tv_loss: 0.03361262381076813\n",
      "iteration 1109, dc_loss: 0.058061715215444565, tv_loss: 0.03333887457847595\n",
      "iteration 1110, dc_loss: 0.05769561603665352, tv_loss: 0.03375619649887085\n",
      "iteration 1111, dc_loss: 0.05807219818234444, tv_loss: 0.03331024944782257\n",
      "iteration 1112, dc_loss: 0.057479679584503174, tv_loss: 0.033642880618572235\n",
      "iteration 1113, dc_loss: 0.057349253445863724, tv_loss: 0.03354679420590401\n",
      "iteration 1114, dc_loss: 0.05745464190840721, tv_loss: 0.03341129422187805\n",
      "iteration 1115, dc_loss: 0.05716630443930626, tv_loss: 0.03371613100171089\n",
      "iteration 1116, dc_loss: 0.05741609260439873, tv_loss: 0.03337264060974121\n",
      "iteration 1117, dc_loss: 0.05693003907799721, tv_loss: 0.0336637906730175\n",
      "iteration 1118, dc_loss: 0.05690520629286766, tv_loss: 0.03352408856153488\n",
      "iteration 1119, dc_loss: 0.05688349902629852, tv_loss: 0.033489059656858444\n",
      "iteration 1120, dc_loss: 0.0566958412528038, tv_loss: 0.03365908935666084\n",
      "iteration 1121, dc_loss: 0.05680432915687561, tv_loss: 0.03345438465476036\n",
      "iteration 1122, dc_loss: 0.05645361542701721, tv_loss: 0.03365691378712654\n",
      "iteration 1123, dc_loss: 0.056480247527360916, tv_loss: 0.03349696099758148\n",
      "iteration 1124, dc_loss: 0.05633140355348587, tv_loss: 0.033579934388399124\n",
      "iteration 1125, dc_loss: 0.05628586933016777, tv_loss: 0.03356087952852249\n",
      "iteration 1126, dc_loss: 0.056221768260002136, tv_loss: 0.03353558108210564\n",
      "iteration 1127, dc_loss: 0.056020595133304596, tv_loss: 0.03361167013645172\n",
      "iteration 1128, dc_loss: 0.05601651221513748, tv_loss: 0.03350890427827835\n",
      "iteration 1129, dc_loss: 0.055819232016801834, tv_loss: 0.03362174332141876\n",
      "iteration 1130, dc_loss: 0.05584399774670601, tv_loss: 0.03352954611182213\n",
      "iteration 1131, dc_loss: 0.05571988224983215, tv_loss: 0.03357692435383797\n",
      "iteration 1132, dc_loss: 0.055618371814489365, tv_loss: 0.03358595445752144\n",
      "iteration 1133, dc_loss: 0.055547669529914856, tv_loss: 0.03355339169502258\n",
      "iteration 1134, dc_loss: 0.055396147072315216, tv_loss: 0.03360019996762276\n",
      "iteration 1135, dc_loss: 0.05534319579601288, tv_loss: 0.03356011211872101\n",
      "iteration 1136, dc_loss: 0.05524778366088867, tv_loss: 0.033577557653188705\n",
      "iteration 1137, dc_loss: 0.055144548416137695, tv_loss: 0.03360813856124878\n",
      "iteration 1138, dc_loss: 0.05511216074228287, tv_loss: 0.033570244908332825\n",
      "iteration 1139, dc_loss: 0.0549686923623085, tv_loss: 0.03362732753157616\n",
      "iteration 1140, dc_loss: 0.054949019104242325, tv_loss: 0.03356999158859253\n",
      "iteration 1141, dc_loss: 0.05480584502220154, tv_loss: 0.03362226486206055\n",
      "iteration 1142, dc_loss: 0.05473683029413223, tv_loss: 0.033603034913539886\n",
      "iteration 1143, dc_loss: 0.05467984452843666, tv_loss: 0.033557891845703125\n",
      "iteration 1144, dc_loss: 0.054497912526130676, tv_loss: 0.033675797283649445\n",
      "iteration 1145, dc_loss: 0.05459374934434891, tv_loss: 0.03351640701293945\n",
      "iteration 1146, dc_loss: 0.054303139448165894, tv_loss: 0.03374738246202469\n",
      "iteration 1147, dc_loss: 0.05454585328698158, tv_loss: 0.03344627097249031\n",
      "iteration 1148, dc_loss: 0.054145123809576035, tv_loss: 0.03382178023457527\n",
      "iteration 1149, dc_loss: 0.05455769971013069, tv_loss: 0.03338956832885742\n",
      "iteration 1150, dc_loss: 0.05403730645775795, tv_loss: 0.03392650559544563\n",
      "iteration 1151, dc_loss: 0.0546567402780056, tv_loss: 0.03328521549701691\n",
      "iteration 1152, dc_loss: 0.05397489294409752, tv_loss: 0.03401724621653557\n",
      "iteration 1153, dc_loss: 0.05469895154237747, tv_loss: 0.03322509303689003\n",
      "iteration 1154, dc_loss: 0.053859878331422806, tv_loss: 0.03404342383146286\n",
      "iteration 1155, dc_loss: 0.05448371171951294, tv_loss: 0.03327837213873863\n",
      "iteration 1156, dc_loss: 0.0537371002137661, tv_loss: 0.033959925174713135\n",
      "iteration 1157, dc_loss: 0.0539933517575264, tv_loss: 0.033462319523096085\n",
      "iteration 1158, dc_loss: 0.05355530604720116, tv_loss: 0.0337059460580349\n",
      "iteration 1159, dc_loss: 0.05344609543681145, tv_loss: 0.033690210431814194\n",
      "iteration 1160, dc_loss: 0.053519293665885925, tv_loss: 0.03349638730287552\n",
      "iteration 1161, dc_loss: 0.05308132618665695, tv_loss: 0.033866558223962784\n",
      "iteration 1162, dc_loss: 0.053508080542087555, tv_loss: 0.03342406824231148\n",
      "iteration 1163, dc_loss: 0.053097356110811234, tv_loss: 0.033840689808130264\n",
      "iteration 1164, dc_loss: 0.05346912890672684, tv_loss: 0.03350703790783882\n",
      "iteration 1165, dc_loss: 0.05311606824398041, tv_loss: 0.03373900428414345\n",
      "iteration 1166, dc_loss: 0.053122397512197495, tv_loss: 0.0336461141705513\n",
      "iteration 1167, dc_loss: 0.05301832780241966, tv_loss: 0.03364460915327072\n",
      "iteration 1168, dc_loss: 0.05300543084740639, tv_loss: 0.033679671585559845\n",
      "iteration 1169, dc_loss: 0.05305764824151993, tv_loss: 0.03363840654492378\n",
      "iteration 1170, dc_loss: 0.052957333624362946, tv_loss: 0.03371124714612961\n",
      "iteration 1171, dc_loss: 0.052768558263778687, tv_loss: 0.033662665635347366\n",
      "iteration 1172, dc_loss: 0.05250700190663338, tv_loss: 0.03366828337311745\n",
      "iteration 1173, dc_loss: 0.05225364491343498, tv_loss: 0.033696748316287994\n",
      "iteration 1174, dc_loss: 0.05223281309008598, tv_loss: 0.03363008797168732\n",
      "iteration 1175, dc_loss: 0.05211888998746872, tv_loss: 0.033708300441503525\n",
      "iteration 1176, dc_loss: 0.05214357748627663, tv_loss: 0.033651042729616165\n",
      "iteration 1177, dc_loss: 0.05201193690299988, tv_loss: 0.03371146693825722\n",
      "iteration 1178, dc_loss: 0.052002571523189545, tv_loss: 0.03365249186754227\n",
      "iteration 1179, dc_loss: 0.051893945783376694, tv_loss: 0.033686961978673935\n",
      "iteration 1180, dc_loss: 0.05179354548454285, tv_loss: 0.033694248646497726\n",
      "iteration 1181, dc_loss: 0.051724374294281006, tv_loss: 0.03364279493689537\n",
      "iteration 1182, dc_loss: 0.05149461328983307, tv_loss: 0.03375563398003578\n",
      "iteration 1183, dc_loss: 0.05155031755566597, tv_loss: 0.03360404074192047\n",
      "iteration 1184, dc_loss: 0.051327940076589584, tv_loss: 0.033777784556150436\n",
      "iteration 1185, dc_loss: 0.051417164504528046, tv_loss: 0.033627700060606\n",
      "iteration 1186, dc_loss: 0.05121292546391487, tv_loss: 0.03376186639070511\n",
      "iteration 1187, dc_loss: 0.05120713636279106, tv_loss: 0.033676229417324066\n",
      "iteration 1188, dc_loss: 0.05112634226679802, tv_loss: 0.03368770703673363\n",
      "iteration 1189, dc_loss: 0.0509892962872982, tv_loss: 0.03375128284096718\n",
      "iteration 1190, dc_loss: 0.051054660230875015, tv_loss: 0.033620260655879974\n",
      "iteration 1191, dc_loss: 0.05078847333788872, tv_loss: 0.033826131373643875\n",
      "iteration 1192, dc_loss: 0.05094929039478302, tv_loss: 0.033589787781238556\n",
      "iteration 1193, dc_loss: 0.050628673285245895, tv_loss: 0.03384164720773697\n",
      "iteration 1194, dc_loss: 0.050778500735759735, tv_loss: 0.03360464796423912\n",
      "iteration 1195, dc_loss: 0.05048740282654762, tv_loss: 0.03382324054837227\n",
      "iteration 1196, dc_loss: 0.05061839148402214, tv_loss: 0.0336211621761322\n",
      "iteration 1197, dc_loss: 0.05035560205578804, tv_loss: 0.03380584344267845\n",
      "iteration 1198, dc_loss: 0.05045587569475174, tv_loss: 0.03364170342683792\n",
      "iteration 1199, dc_loss: 0.0502057746052742, tv_loss: 0.03381749242544174\n",
      "iteration 1200, dc_loss: 0.050319671630859375, tv_loss: 0.03364378213882446\n",
      "iteration 1201, dc_loss: 0.050070878118276596, tv_loss: 0.03382352367043495\n",
      "iteration 1202, dc_loss: 0.050128329545259476, tv_loss: 0.03367502987384796\n",
      "iteration 1203, dc_loss: 0.05000435933470726, tv_loss: 0.03371074050664902\n",
      "iteration 1204, dc_loss: 0.04989572986960411, tv_loss: 0.03379378467798233\n",
      "iteration 1205, dc_loss: 0.04997831583023071, tv_loss: 0.0336691215634346\n",
      "iteration 1206, dc_loss: 0.049807868897914886, tv_loss: 0.03374868258833885\n",
      "iteration 1207, dc_loss: 0.04972991347312927, tv_loss: 0.033771105110645294\n",
      "iteration 1208, dc_loss: 0.049785781651735306, tv_loss: 0.03367645666003227\n",
      "iteration 1209, dc_loss: 0.049635060131549835, tv_loss: 0.03376953676342964\n",
      "iteration 1210, dc_loss: 0.04958602041006088, tv_loss: 0.03374619781970978\n",
      "iteration 1211, dc_loss: 0.04957311972975731, tv_loss: 0.033713892102241516\n",
      "iteration 1212, dc_loss: 0.04946177452802658, tv_loss: 0.03377801179885864\n",
      "iteration 1213, dc_loss: 0.04945111647248268, tv_loss: 0.03373374789953232\n",
      "iteration 1214, dc_loss: 0.04937759041786194, tv_loss: 0.03374066203832626\n",
      "iteration 1215, dc_loss: 0.04929444193840027, tv_loss: 0.03376859426498413\n",
      "iteration 1216, dc_loss: 0.04928414523601532, tv_loss: 0.033737942576408386\n",
      "iteration 1217, dc_loss: 0.049198854714632034, tv_loss: 0.03376533463597298\n",
      "iteration 1218, dc_loss: 0.049143001437187195, tv_loss: 0.03375009074807167\n",
      "iteration 1219, dc_loss: 0.0491020642220974, tv_loss: 0.033747732639312744\n",
      "iteration 1220, dc_loss: 0.049034927040338516, tv_loss: 0.033768072724342346\n",
      "iteration 1221, dc_loss: 0.048996131867170334, tv_loss: 0.03374994918704033\n",
      "iteration 1222, dc_loss: 0.048917073756456375, tv_loss: 0.03376490995287895\n",
      "iteration 1223, dc_loss: 0.048877108842134476, tv_loss: 0.033755186945199966\n",
      "iteration 1224, dc_loss: 0.04882355034351349, tv_loss: 0.03375719115138054\n",
      "iteration 1225, dc_loss: 0.048753682523965836, tv_loss: 0.03377360478043556\n",
      "iteration 1226, dc_loss: 0.04872947558760643, tv_loss: 0.03374093770980835\n",
      "iteration 1227, dc_loss: 0.048644233494997025, tv_loss: 0.033776912838220596\n",
      "iteration 1228, dc_loss: 0.04859777167439461, tv_loss: 0.033776696771383286\n",
      "iteration 1229, dc_loss: 0.04856467619538307, tv_loss: 0.03376876562833786\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torchopt\u001b[38;5;241m.\u001b[39madamw(lr\u001b[38;5;241m=\u001b[39mlr_scheduler)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# 1e-4 1.092077389\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# 1e-3 0.08540542\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m params, image_list_SIREN \u001b[38;5;241m=\u001b[39m \u001b[43mreconstruct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfield\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmasked_kspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcsm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43miterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(complex_abs(image_list_SIREN[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m\"\u001b[39m, vmax\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m)\n",
      "File \u001b[0;32m/bmrc-an-data/Chunxu/ese5934_project/ese5934_project/tasks/mri_reconstruction_2d.py:71\u001b[0m, in \u001b[0;36mreconstruct\u001b[0;34m(field, coordinates, kspace_masked, csm, mask, masked_forward_model, alpha, optimizer, iterations, device, params_init, kspace_normalization)\u001b[0m\n\u001b[1;32m     63\u001b[0m grads, aux \u001b[38;5;241m=\u001b[39m grad_loss_fn(\n\u001b[1;32m     64\u001b[0m     params,\n\u001b[1;32m     65\u001b[0m     coordinates,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m     mask,\n\u001b[1;32m     69\u001b[0m )\n\u001b[1;32m     70\u001b[0m dc_loss, tv_loss, image, kspace_hat \u001b[38;5;241m=\u001b[39m aux\n\u001b[0;32m---> 71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, dc_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdc_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, tv_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtv_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     72\u001b[0m updates, opt_state \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mupdate(grads, opt_state, params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m     73\u001b[0m params \u001b[38;5;241m=\u001b[39m torchopt\u001b[38;5;241m.\u001b[39mapply_updates(params, updates)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "size = (640, 368)\n",
    "from ese5934_project.models.SIREN import Siren, get_coordinates\n",
    "\n",
    "coords = get_coordinates(size)\n",
    "kspace, (mean, std), masked_kspace, mask, csm = dataset_4[15]\n",
    "field = Siren(\n",
    "    size,\n",
    "    mean.to(device),\n",
    "    std.to(device),\n",
    "    in_features=2,\n",
    "    out_features=2,\n",
    "    hidden_features=256,\n",
    "    hidden_layers=8,\n",
    "    outermost_linear=True,\n",
    "    first_omega_0=25,\n",
    "    hidden_omega_0=25,\n",
    ")\n",
    "lr_scheduler = lambda t: 0.8 ** (t // 400) * 1e-4\n",
    "optimizer = torchopt.adamw(lr=lr_scheduler)\n",
    "# 1e-4 1.092077389\n",
    "# 1e-3 0.08540542\n",
    "params, image_list_SIREN = reconstruct(\n",
    "    field,\n",
    "    coords,\n",
    "    masked_kspace,\n",
    "    csm,\n",
    "    mask,\n",
    "    alpha=0.01,\n",
    "    optimizer=optimizer,\n",
    "    iterations=4000,\n",
    "    device=device,\n",
    ")\n",
    "plt.imshow(complex_abs(image_list_SIREN[-1]), cmap=\"gray\", vmax=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DictField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| self.basis_dims: array([32, 32, 32, 16, 16, 16])\n",
      "ic| self.basis_reso: array([16, 26, 35, 45, 54, 64])\n",
      "ic| self.freq_bands: tensor([40.0000, 24.6154, 18.2857, 14.2222, 11.8519, 10.0000], device='cuda:0')\n",
      "ic| self.bbox: tensor([[  0.,   0.],\n",
      "                       [640., 368.]], device='cuda:0')\n",
      "ic| self.coeff_reso: [20, 12]\n",
      "ic| coeffs.shape: torch.Size([1, 144, 20, 12])\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 32, 16, 16]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 32, 26, 26]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 32, 35, 35]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 16, 45, 45]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 16, 54, 54]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 16, 64, 64]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====> total parameters:  257584\n",
      "tensor([[[6.0409e-04, 7.4484e-05]]]) tensor([[[1.2870, 1.2171]]])\n",
      "torch.Size([1, 1, 2]) torch.Size([1, 1, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| params.keys(): dict_keys(['coeffs', 'basises.0', 'basises.1', 'basises.2', 'basises.3', 'basises.4', 'basises.5', 'linear_mat.backbone.0.weight', 'linear_mat.backbone.0.bias', 'linear_mat.backbone.1.weight'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[38;5;241m0.8\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (t \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m400\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m5e-3\u001b[39m\n\u001b[1;32m     19\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torchopt\u001b[38;5;241m.\u001b[39madam(lr\u001b[38;5;241m=\u001b[39mscheduler)\n\u001b[0;32m---> 21\u001b[0m params, image_list \u001b[38;5;241m=\u001b[39m \u001b[43mreconstruct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmasked_kspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcsm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.005\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43miterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(complex_abs(image_list[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m\"\u001b[39m, vmax\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m)\n",
      "File \u001b[0;32m/bmrc-an-data/Chunxu/ese5934_project/ese5934_project/tasks/mri_reconstruction_2d.py:63\u001b[0m, in \u001b[0;36mreconstruct\u001b[0;34m(field, coordinates, kspace_masked, csm, mask, masked_forward_model, alpha, optimizer, iterations, device, params_init, kspace_normalization)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, iterations \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     62\u001b[0m     grad_loss_fn \u001b[38;5;241m=\u001b[39m grad(loss_fn, has_aux\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 63\u001b[0m     grads, aux \u001b[38;5;241m=\u001b[39m \u001b[43mgrad_loss_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoordinates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkspace_masked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcsm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     dc_loss, tv_loss, image, kspace_hat \u001b[38;5;241m=\u001b[39m aux\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, dc_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdc_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, tv_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtv_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/bmrc-homes/nmrgrp/nmr201/micromamba/envs/python311/lib/python3.11/site-packages/torch/_functorch/apis.py:363\u001b[0m, in \u001b[0;36mgrad.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 363\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43meager_transforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/bmrc-homes/nmrgrp/nmr201/micromamba/envs/python311/lib/python3.11/site-packages/torch/_functorch/eager_transforms.py:1295\u001b[0m, in \u001b[0;36mgrad_impl\u001b[0;34m(func, argnums, has_aux, args, kwargs)\u001b[0m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrad_impl\u001b[39m(func: Callable, argnums: argnums_t, has_aux: \u001b[38;5;28mbool\u001b[39m, args, kwargs):\n\u001b[1;32m   1294\u001b[0m     func \u001b[38;5;241m=\u001b[39m lazy_dynamo_disable(func)\n\u001b[0;32m-> 1295\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mgrad_and_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_aux\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[1;32m   1297\u001b[0m         grad, (_, aux) \u001b[38;5;241m=\u001b[39m results\n",
      "File \u001b[0;32m/bmrc-homes/nmrgrp/nmr201/micromamba/envs/python311/lib/python3.11/site-packages/torch/_functorch/vmap.py:44\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[0;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/bmrc-homes/nmrgrp/nmr201/micromamba/envs/python311/lib/python3.11/site-packages/torch/_functorch/eager_transforms.py:1256\u001b[0m, in \u001b[0;36mgrad_and_value.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1253\u001b[0m diff_args \u001b[38;5;241m=\u001b[39m _slice_argnums(args, argnums, as_tuple\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1254\u001b[0m tree_map_(partial(_create_differentiable, level\u001b[38;5;241m=\u001b[39mlevel), diff_args)\n\u001b[0;32m-> 1256\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[1;32m   1258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(output) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n",
      "File \u001b[0;32m/bmrc-homes/nmrgrp/nmr201/micromamba/envs/python311/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:489\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m     dynamo_config_ctx\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    491\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m/bmrc-an-data/Chunxu/ese5934_project/ese5934_project/tasks/mri_reconstruction_2d.py:50\u001b[0m, in \u001b[0;36mreconstruct.<locals>.loss_fn\u001b[0;34m(params, coordinates, kspace_masked, csm, mask, kspace_normalization)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_fn\u001b[39m(\n\u001b[1;32m     43\u001b[0m     params,\n\u001b[1;32m     44\u001b[0m     coordinates,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m     kspace_normalization\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     49\u001b[0m ):\n\u001b[0;32m---> 50\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mfunctional_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfield\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoordinates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     kspace_hat \u001b[38;5;241m=\u001b[39m masked_forward_model(image, csm, mask)\n\u001b[1;32m     52\u001b[0m     data_consistency_loss \u001b[38;5;241m=\u001b[39m complex_abs_sq(kspace_hat \u001b[38;5;241m-\u001b[39m kspace_masked)\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m/bmrc-homes/nmrgrp/nmr201/micromamba/envs/python311/lib/python3.11/site-packages/torch/_functorch/functional_call.py:143\u001b[0m, in \u001b[0;36mfunctional_call\u001b[0;34m(module, parameter_and_buffer_dicts, args, kwargs, tie_weights, strict)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter_and_buffer_dicts to be a dict, or a list/tuple of dicts, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(parameter_and_buffer_dicts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m     )\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstateless\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_functional_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameters_and_buffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtie_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtie_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/bmrc-homes/nmrgrp/nmr201/micromamba/envs/python311/lib/python3.11/site-packages/torch/nn/utils/stateless.py:263\u001b[0m, in \u001b[0;36m_functional_call\u001b[0;34m(module, parameters_and_buffers, args, kwargs, tie_weights, strict)\u001b[0m\n\u001b[1;32m    259\u001b[0m     args \u001b[38;5;241m=\u001b[39m (args,)\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _reparametrize_module(\n\u001b[1;32m    261\u001b[0m     module, parameters_and_buffers, tie_weights\u001b[38;5;241m=\u001b[39mtie_weights, strict\u001b[38;5;241m=\u001b[39mstrict\n\u001b[1;32m    262\u001b[0m ):\n\u001b[0;32m--> 263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/bmrc-homes/nmrgrp/nmr201/micromamba/envs/python311/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/bmrc-homes/nmrgrp/nmr201/micromamba/envs/python311/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/bmrc-an-data/Chunxu/ese5934_project/ese5934_project/models/FactorFields.py:367\u001b[0m, in \u001b[0;36mDictField.forward\u001b[0;34m(self, coordinates)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, coordinates):\n\u001b[0;32m--> 367\u001b[0m     feats, coeffs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_coding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoordinates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;66;03m# ic(feats.shape)\u001b[39;00m\n\u001b[1;32m    369\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_mat(feats)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39m[\u001b[38;5;241m640\u001b[39m, \u001b[38;5;241m368\u001b[39m], \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/bmrc-an-data/Chunxu/ese5934_project/ese5934_project/models/FactorFields.py:316\u001b[0m, in \u001b[0;36mDictField.get_coding\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_coding\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcoeff_type \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mbasis_type \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 316\u001b[0m         coeff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_coeff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m         basises \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_basis(x)\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m basises \u001b[38;5;241m*\u001b[39m coeff, coeff\n",
      "File \u001b[0;32m/bmrc-an-data/Chunxu/ese5934_project/ese5934_project/models/FactorFields.py:262\u001b[0m, in \u001b[0;36mDictField.get_coeff\u001b[0;34m(self, xyz_sampled)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_coeff\u001b[39m(\u001b[38;5;28mself\u001b[39m, xyz_sampled):\n\u001b[0;32m--> 262\u001b[0m     N_points, dim \u001b[38;5;241m=\u001b[39m xyz_sampled\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    263\u001b[0m     pts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize_coord(xyz_sampled)\u001b[38;5;241m.\u001b[39mview([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m [\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m (dim \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m [dim])\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;66;03m# normalize to -1, 1\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# from datalo\n",
    "\n",
    "from ese5934_project.models.FactorFields import DictField, get_coordinates\n",
    "\n",
    "base_conf = OmegaConf.load(\"/bmrc-an-data/Chunxu/ese5934_project/configs/defaults.yaml\")\n",
    "second_conf = OmegaConf.load(\"/bmrc-an-data/Chunxu/ese5934_project/configs/image.yaml\")\n",
    "cfg = OmegaConf.merge(\n",
    "    base_conf,\n",
    "    second_conf,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "size = (640, 368)\n",
    "model = DictField(cfg, size, device)\n",
    "coords = get_coordinates(size)\n",
    "kspace, (mean, std), masked_kspace, mask, csm = dataset_4[15]\n",
    "\n",
    "scheduler = lambda t: 0.8 ** (t // 400) * 5e-3\n",
    "optimizer = torchopt.adam(lr=scheduler)\n",
    "\n",
    "params, image_list = reconstruct(\n",
    "    model,\n",
    "    coords,\n",
    "    masked_kspace,\n",
    "    csm,\n",
    "    mask,\n",
    "    alpha=0.005,\n",
    "    optimizer=optimizer,\n",
    "    iterations=4000,\n",
    "    device=device,\n",
    ")\n",
    "plt.imshow(complex_abs(image_list[-1]), cmap=\"gray\", vmax=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR Value mt1: 35.51589318336404\n",
      "SSIM Value mt1: 0.7751314202416238\n",
      "PSNR Value mt1: 28.169658116720203\n",
      "SSIM Value mt1: 0.8022582995267628\n",
      "PSNR Value mt1: 38.49677393524084\n",
      "SSIM Value mt1: 0.9359180332422214\n",
      "PSNR Value mt1: 35.51589318336404\n",
      "SSIM Value mt1: 0.7751314202416238\n",
      "PSNR Value mt1: 28.169658116720203\n",
      "SSIM Value mt1: 0.8022582995267628\n",
      "PSNR Value mt1: 38.49677393524084\n",
      "SSIM Value mt1: 0.9359180332422214\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    \"ADAM\": image_list_ADAM[-1],\n",
    "    \"SIREN\": image_list_SIREN[-1],\n",
    "    \"DF\": image_list[-1],\n",
    "    \"gt\": image_gt.squeeze(),\n",
    "}\n",
    "torch.save(results, \"experiments/best_case_comparision/images_dict.pt\")\n",
    "psnr_dict = {\n",
    "    key: Evaluate_MT1(image_gt, value)[0]\n",
    "    for key, value in results.items()\n",
    "    if key != \"gt\"\n",
    "}\n",
    "torch.save(psnr_dict, \"experiments/best_case_comparision/psnr_dict.pt\")\n",
    "ssim_dict = {\n",
    "    key: Evaluate_MT1(image_gt, value)[1]\n",
    "    for key, value in results.items()\n",
    "    if key != \"gt\"\n",
    "}\n",
    "torch.save(ssim_dict, \"experiments/best_case_comparision/ssim_dict.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different ARs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ADAM_list = []\n",
    "SIREN_list = []\n",
    "DF_list = []\n",
    "datasets_list = [dataset_gt, dataset_2, dataset_4, dataset_8]\n",
    "# results = pd.DataFrame(\n",
    "#     columns=[\n",
    "#         \"acceration_rate\",\n",
    "#         \"psnr\",\n",
    "#         \"ssim\",\n",
    "#     ],\n",
    "#     index=[\n",
    "#         \"ADAM\",\n",
    "#         \"SIREN\",\n",
    "#         \"DF\",\n",
    "#     ],\n",
    "# )\n",
    "# results.loc[\"ADAM\", \"psnr\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[6.0409e-04, 7.4484e-05]]]) tensor([[[1.2870, 1.2171]]])\n",
      "torch.Size([1, 1, 2]) torch.Size([1, 1, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| params.keys(): dict_keys(['grid'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "iteration 1, dc_loss: 3.1375977993011475, tv_loss: 0.0\n",
      "iteration 2, dc_loss: 3.0784525871276855, tv_loss: 0.00027480581775307655\n",
      "iteration 3, dc_loss: 3.0205013751983643, tv_loss: 0.0005211131647229195\n",
      "iteration 4, dc_loss: 2.9636640548706055, tv_loss: 0.0007560004014521837\n",
      "iteration 5, dc_loss: 2.9078848361968994, tv_loss: 0.0009825904853641987\n",
      "iteration 6, dc_loss: 2.8531177043914795, tv_loss: 0.0012002057628706098\n",
      "iteration 7, dc_loss: 2.799330472946167, tv_loss: 0.0014088405296206474\n",
      "iteration 8, dc_loss: 2.746499538421631, tv_loss: 0.0016107687260955572\n",
      "iteration 9, dc_loss: 2.694606304168701, tv_loss: 0.0018039247952401638\n",
      "iteration 10, dc_loss: 2.6436359882354736, tv_loss: 0.0019906985107809305\n",
      "iteration 11, dc_loss: 2.5935754776000977, tv_loss: 0.0021706600673496723\n",
      "iteration 12, dc_loss: 2.544412136077881, tv_loss: 0.0023450437001883984\n",
      "iteration 13, dc_loss: 2.4961342811584473, tv_loss: 0.002515699714422226\n",
      "iteration 14, dc_loss: 2.4487311840057373, tv_loss: 0.002682484220713377\n",
      "iteration 15, dc_loss: 2.4021897315979004, tv_loss: 0.002846999792382121\n",
      "iteration 16, dc_loss: 2.356499433517456, tv_loss: 0.003008992876857519\n",
      "iteration 17, dc_loss: 2.311647891998291, tv_loss: 0.0031695424113422632\n",
      "iteration 18, dc_loss: 2.2676239013671875, tv_loss: 0.0033298626076430082\n",
      "iteration 19, dc_loss: 2.2244150638580322, tv_loss: 0.0034889515955001116\n",
      "iteration 20, dc_loss: 2.1820101737976074, tv_loss: 0.003647234756499529\n",
      "iteration 21, dc_loss: 2.140397071838379, tv_loss: 0.0038044657558202744\n",
      "iteration 22, dc_loss: 2.09956431388855, tv_loss: 0.00396016426384449\n",
      "iteration 23, dc_loss: 2.0595009326934814, tv_loss: 0.004115438088774681\n",
      "iteration 24, dc_loss: 2.0201947689056396, tv_loss: 0.004270249977707863\n",
      "iteration 25, dc_loss: 1.981634497642517, tv_loss: 0.0044236136600375175\n",
      "iteration 26, dc_loss: 1.9438087940216064, tv_loss: 0.004573888145387173\n",
      "iteration 27, dc_loss: 1.9067062139511108, tv_loss: 0.004723789170384407\n",
      "iteration 28, dc_loss: 1.8703147172927856, tv_loss: 0.004871686454862356\n",
      "iteration 29, dc_loss: 1.8346234560012817, tv_loss: 0.005017646588385105\n",
      "iteration 30, dc_loss: 1.7996203899383545, tv_loss: 0.005161264445632696\n",
      "iteration 31, dc_loss: 1.7652943134307861, tv_loss: 0.0053041414357721806\n",
      "iteration 32, dc_loss: 1.7316337823867798, tv_loss: 0.005445331335067749\n",
      "iteration 33, dc_loss: 1.6986273527145386, tv_loss: 0.005584857892245054\n",
      "iteration 34, dc_loss: 1.666263461112976, tv_loss: 0.005722382105886936\n",
      "iteration 35, dc_loss: 1.6345314979553223, tv_loss: 0.005858262535184622\n",
      "iteration 36, dc_loss: 1.603419542312622, tv_loss: 0.005993496626615524\n",
      "iteration 37, dc_loss: 1.5729174613952637, tv_loss: 0.006127210333943367\n",
      "iteration 38, dc_loss: 1.5430134534835815, tv_loss: 0.006258868612349033\n",
      "iteration 39, dc_loss: 1.5136972665786743, tv_loss: 0.006388839799910784\n",
      "iteration 40, dc_loss: 1.4849579334259033, tv_loss: 0.006517461966723204\n",
      "iteration 41, dc_loss: 1.4567852020263672, tv_loss: 0.006644704844802618\n",
      "iteration 42, dc_loss: 1.429168462753296, tv_loss: 0.006770486012101173\n",
      "iteration 43, dc_loss: 1.402097225189209, tv_loss: 0.006894923280924559\n",
      "iteration 44, dc_loss: 1.3755614757537842, tv_loss: 0.007018400356173515\n",
      "iteration 45, dc_loss: 1.3495513200759888, tv_loss: 0.0071403454057872295\n",
      "iteration 46, dc_loss: 1.3240565061569214, tv_loss: 0.007260772865265608\n",
      "iteration 47, dc_loss: 1.2990673780441284, tv_loss: 0.007379346992820501\n",
      "iteration 48, dc_loss: 1.2745740413665771, tv_loss: 0.007497643586248159\n",
      "iteration 49, dc_loss: 1.2505674362182617, tv_loss: 0.007614660542458296\n",
      "iteration 50, dc_loss: 1.2270382642745972, tv_loss: 0.007730293553322554\n",
      "iteration 51, dc_loss: 1.2039769887924194, tv_loss: 0.007844320498406887\n",
      "iteration 52, dc_loss: 1.1813746690750122, tv_loss: 0.007956240326166153\n",
      "iteration 53, dc_loss: 1.1592228412628174, tv_loss: 0.008068381808698177\n",
      "iteration 54, dc_loss: 1.1375123262405396, tv_loss: 0.00818017777055502\n",
      "iteration 55, dc_loss: 1.1162350177764893, tv_loss: 0.00829051062464714\n",
      "iteration 56, dc_loss: 1.0953822135925293, tv_loss: 0.00839791726320982\n",
      "iteration 57, dc_loss: 1.0749456882476807, tv_loss: 0.008504652418196201\n",
      "iteration 58, dc_loss: 1.054917335510254, tv_loss: 0.008610686287283897\n",
      "iteration 59, dc_loss: 1.0352890491485596, tv_loss: 0.00871544424444437\n",
      "iteration 60, dc_loss: 1.0160530805587769, tv_loss: 0.008818990550935268\n",
      "iteration 61, dc_loss: 0.9972018003463745, tv_loss: 0.008922014385461807\n",
      "iteration 62, dc_loss: 0.9787275791168213, tv_loss: 0.009023236110806465\n",
      "iteration 63, dc_loss: 0.9606228470802307, tv_loss: 0.00912279449403286\n",
      "iteration 64, dc_loss: 0.942880392074585, tv_loss: 0.009222462773323059\n",
      "iteration 65, dc_loss: 0.9254933595657349, tv_loss: 0.009320734068751335\n",
      "iteration 66, dc_loss: 0.9084542989730835, tv_loss: 0.009416514076292515\n",
      "iteration 67, dc_loss: 0.8917567729949951, tv_loss: 0.009512984193861485\n",
      "iteration 68, dc_loss: 0.8753938674926758, tv_loss: 0.009608385153114796\n",
      "iteration 69, dc_loss: 0.8593587279319763, tv_loss: 0.009701942093670368\n",
      "iteration 70, dc_loss: 0.8436451554298401, tv_loss: 0.009793166071176529\n",
      "iteration 71, dc_loss: 0.8282466530799866, tv_loss: 0.009885990060865879\n",
      "iteration 72, dc_loss: 0.8131569623947144, tv_loss: 0.009977074339985847\n",
      "iteration 73, dc_loss: 0.7983698844909668, tv_loss: 0.010066233575344086\n",
      "iteration 74, dc_loss: 0.7838795185089111, tv_loss: 0.010154307819902897\n",
      "iteration 75, dc_loss: 0.769679844379425, tv_loss: 0.010242382064461708\n",
      "iteration 76, dc_loss: 0.7557649612426758, tv_loss: 0.010329941287636757\n",
      "iteration 77, dc_loss: 0.7421294450759888, tv_loss: 0.01041520107537508\n",
      "iteration 78, dc_loss: 0.7287676334381104, tv_loss: 0.010499578900635242\n",
      "iteration 79, dc_loss: 0.715674102306366, tv_loss: 0.010583351366221905\n",
      "iteration 80, dc_loss: 0.7028433680534363, tv_loss: 0.010666689835488796\n",
      "iteration 81, dc_loss: 0.6902702450752258, tv_loss: 0.010749401524662971\n",
      "iteration 82, dc_loss: 0.6779494881629944, tv_loss: 0.010830438695847988\n",
      "iteration 83, dc_loss: 0.6658762693405151, tv_loss: 0.010909761302173138\n",
      "iteration 84, dc_loss: 0.6540454626083374, tv_loss: 0.010988837108016014\n",
      "iteration 85, dc_loss: 0.6424521803855896, tv_loss: 0.011069005355238914\n",
      "iteration 86, dc_loss: 0.6310917139053345, tv_loss: 0.011146328411996365\n",
      "iteration 87, dc_loss: 0.6199593544006348, tv_loss: 0.01122316811233759\n",
      "iteration 88, dc_loss: 0.609050452709198, tv_loss: 0.01129879429936409\n",
      "iteration 89, dc_loss: 0.5983607172966003, tv_loss: 0.011373200453817844\n",
      "iteration 90, dc_loss: 0.5878855586051941, tv_loss: 0.011448572389781475\n",
      "iteration 91, dc_loss: 0.5776206851005554, tv_loss: 0.011522539891302586\n",
      "iteration 92, dc_loss: 0.5675619840621948, tv_loss: 0.011594632640480995\n",
      "iteration 93, dc_loss: 0.557705283164978, tv_loss: 0.011666021309792995\n",
      "iteration 94, dc_loss: 0.5480462908744812, tv_loss: 0.011736507527530193\n",
      "iteration 95, dc_loss: 0.5385813117027283, tv_loss: 0.011807220056653023\n",
      "iteration 96, dc_loss: 0.5293062329292297, tv_loss: 0.011877868324518204\n",
      "iteration 97, dc_loss: 0.5202174186706543, tv_loss: 0.011946388520300388\n",
      "iteration 98, dc_loss: 0.5113109350204468, tv_loss: 0.012013211846351624\n",
      "iteration 99, dc_loss: 0.5025830268859863, tv_loss: 0.012081323191523552\n",
      "iteration 100, dc_loss: 0.4940303564071655, tv_loss: 0.012148382142186165\n",
      "iteration 101, dc_loss: 0.4856491684913635, tv_loss: 0.012213792651891708\n",
      "iteration 102, dc_loss: 0.4774361252784729, tv_loss: 0.012277588248252869\n",
      "iteration 103, dc_loss: 0.4693877100944519, tv_loss: 0.012342432513833046\n",
      "iteration 104, dc_loss: 0.46150070428848267, tv_loss: 0.012407013215124607\n",
      "iteration 105, dc_loss: 0.45377177000045776, tv_loss: 0.012469351291656494\n",
      "iteration 106, dc_loss: 0.4461977481842041, tv_loss: 0.01252971962094307\n",
      "iteration 107, dc_loss: 0.43877556920051575, tv_loss: 0.012591924518346786\n",
      "iteration 108, dc_loss: 0.43150201439857483, tv_loss: 0.012652918696403503\n",
      "iteration 109, dc_loss: 0.42437416315078735, tv_loss: 0.012712990865111351\n",
      "iteration 110, dc_loss: 0.4173889756202698, tv_loss: 0.012771249748766422\n",
      "iteration 111, dc_loss: 0.41054368019104004, tv_loss: 0.012830138206481934\n",
      "iteration 112, dc_loss: 0.40383538603782654, tv_loss: 0.01288851723074913\n",
      "iteration 113, dc_loss: 0.3972613513469696, tv_loss: 0.012945004738867283\n",
      "iteration 114, dc_loss: 0.3908188045024872, tv_loss: 0.013000782579183578\n",
      "iteration 115, dc_loss: 0.38450512290000916, tv_loss: 0.013057776726782322\n",
      "iteration 116, dc_loss: 0.378317654132843, tv_loss: 0.013113664463162422\n",
      "iteration 117, dc_loss: 0.3722538352012634, tv_loss: 0.01316678337752819\n",
      "iteration 118, dc_loss: 0.3663111925125122, tv_loss: 0.013218954205513\n",
      "iteration 119, dc_loss: 0.36048731207847595, tv_loss: 0.013275336474180222\n",
      "iteration 120, dc_loss: 0.3547796905040741, tv_loss: 0.013328714296221733\n",
      "iteration 121, dc_loss: 0.34918612241744995, tv_loss: 0.013378777541220188\n",
      "iteration 122, dc_loss: 0.3437040448188782, tv_loss: 0.013429426588118076\n",
      "iteration 123, dc_loss: 0.3383313715457916, tv_loss: 0.013481550849974155\n",
      "iteration 124, dc_loss: 0.33306577801704407, tv_loss: 0.01353241503238678\n",
      "iteration 125, dc_loss: 0.3279052674770355, tv_loss: 0.013581633567810059\n",
      "iteration 126, dc_loss: 0.32284751534461975, tv_loss: 0.013630062341690063\n",
      "iteration 127, dc_loss: 0.31789061427116394, tv_loss: 0.013677985407412052\n",
      "iteration 128, dc_loss: 0.3130324184894562, tv_loss: 0.013727080076932907\n",
      "iteration 129, dc_loss: 0.3082709014415741, tv_loss: 0.013774924911558628\n",
      "iteration 130, dc_loss: 0.3036040961742401, tv_loss: 0.013821575790643692\n",
      "iteration 131, dc_loss: 0.2990301251411438, tv_loss: 0.013867356814444065\n",
      "iteration 132, dc_loss: 0.29454711079597473, tv_loss: 0.013911721296608448\n",
      "iteration 133, dc_loss: 0.2901532053947449, tv_loss: 0.0139585230499506\n",
      "iteration 134, dc_loss: 0.2858465909957886, tv_loss: 0.01400312501937151\n",
      "iteration 135, dc_loss: 0.2816254794597626, tv_loss: 0.014045491814613342\n",
      "iteration 136, dc_loss: 0.2774881422519684, tv_loss: 0.014089023694396019\n",
      "iteration 137, dc_loss: 0.2734328806400299, tv_loss: 0.014132428914308548\n",
      "iteration 138, dc_loss: 0.2694580554962158, tv_loss: 0.014174088835716248\n",
      "iteration 139, dc_loss: 0.2655620276927948, tv_loss: 0.014216266572475433\n",
      "iteration 140, dc_loss: 0.2617432177066803, tv_loss: 0.014257866889238358\n",
      "iteration 141, dc_loss: 0.257999986410141, tv_loss: 0.014298913069069386\n",
      "iteration 142, dc_loss: 0.25433090329170227, tv_loss: 0.014338954351842403\n",
      "iteration 143, dc_loss: 0.2507344186306, tv_loss: 0.014378033578395844\n",
      "iteration 144, dc_loss: 0.24720904231071472, tv_loss: 0.014416972175240517\n",
      "iteration 145, dc_loss: 0.24375328421592712, tv_loss: 0.014456463977694511\n",
      "iteration 146, dc_loss: 0.24036584794521332, tv_loss: 0.014495127834379673\n",
      "iteration 147, dc_loss: 0.2370453029870987, tv_loss: 0.014532175846397877\n",
      "iteration 148, dc_loss: 0.23379027843475342, tv_loss: 0.014570589177310467\n",
      "iteration 149, dc_loss: 0.23059947788715363, tv_loss: 0.01460682600736618\n",
      "iteration 150, dc_loss: 0.22747157514095306, tv_loss: 0.014643332920968533\n",
      "iteration 151, dc_loss: 0.22440534830093384, tv_loss: 0.014679569751024246\n",
      "iteration 152, dc_loss: 0.2213994860649109, tv_loss: 0.01471512671560049\n",
      "iteration 153, dc_loss: 0.2184528112411499, tv_loss: 0.014750487171113491\n",
      "iteration 154, dc_loss: 0.2155640423297882, tv_loss: 0.014785397797822952\n",
      "iteration 155, dc_loss: 0.21273210644721985, tv_loss: 0.014819975942373276\n",
      "iteration 156, dc_loss: 0.2099558711051941, tv_loss: 0.014853348024189472\n",
      "iteration 157, dc_loss: 0.20723415911197662, tv_loss: 0.014886380173265934\n",
      "iteration 158, dc_loss: 0.2045658379793167, tv_loss: 0.014920312911272049\n",
      "iteration 159, dc_loss: 0.20194987952709198, tv_loss: 0.014953861944377422\n",
      "iteration 160, dc_loss: 0.19938519597053528, tv_loss: 0.014985661953687668\n",
      "iteration 161, dc_loss: 0.196870818734169, tv_loss: 0.015017285011708736\n",
      "iteration 162, dc_loss: 0.1944057196378708, tv_loss: 0.01504848524928093\n",
      "iteration 163, dc_loss: 0.19198885560035706, tv_loss: 0.015079928562045097\n",
      "iteration 164, dc_loss: 0.189619243144989, tv_loss: 0.015111375600099564\n",
      "iteration 165, dc_loss: 0.18729589879512787, tv_loss: 0.015140918083488941\n",
      "iteration 166, dc_loss: 0.18501798808574677, tv_loss: 0.015170694328844547\n",
      "iteration 167, dc_loss: 0.1827845722436905, tv_loss: 0.015200424939393997\n",
      "iteration 168, dc_loss: 0.1805947721004486, tv_loss: 0.015231027267873287\n",
      "iteration 169, dc_loss: 0.1784476786851883, tv_loss: 0.015258977189660072\n",
      "iteration 170, dc_loss: 0.1763424426317215, tv_loss: 0.015286507084965706\n",
      "iteration 171, dc_loss: 0.17427821457386017, tv_loss: 0.015315028838813305\n",
      "iteration 172, dc_loss: 0.17225413024425507, tv_loss: 0.015342701226472855\n",
      "iteration 173, dc_loss: 0.1702694296836853, tv_loss: 0.015370090492069721\n",
      "iteration 174, dc_loss: 0.1683233231306076, tv_loss: 0.015396982431411743\n",
      "iteration 175, dc_loss: 0.1664150506258011, tv_loss: 0.015423210337758064\n",
      "iteration 176, dc_loss: 0.16454386711120605, tv_loss: 0.015449556522071362\n",
      "iteration 177, dc_loss: 0.16270896792411804, tv_loss: 0.015475733205676079\n",
      "iteration 178, dc_loss: 0.16090965270996094, tv_loss: 0.015500249341130257\n",
      "iteration 179, dc_loss: 0.15914517641067505, tv_loss: 0.015525422990322113\n",
      "iteration 180, dc_loss: 0.15741486847400665, tv_loss: 0.01555157732218504\n",
      "iteration 181, dc_loss: 0.15571807324886322, tv_loss: 0.015576576814055443\n",
      "iteration 182, dc_loss: 0.15405410528182983, tv_loss: 0.015599814243614674\n",
      "iteration 183, dc_loss: 0.15242232382297516, tv_loss: 0.01562339998781681\n",
      "iteration 184, dc_loss: 0.1508219987154007, tv_loss: 0.015647077932953835\n",
      "iteration 185, dc_loss: 0.14925260841846466, tv_loss: 0.015670379623770714\n",
      "iteration 186, dc_loss: 0.14771349728107452, tv_loss: 0.015693791210651398\n",
      "iteration 187, dc_loss: 0.14620402455329895, tv_loss: 0.01571665331721306\n",
      "iteration 188, dc_loss: 0.14472362399101257, tv_loss: 0.01573961414396763\n",
      "iteration 189, dc_loss: 0.14327169954776764, tv_loss: 0.01576082408428192\n",
      "iteration 190, dc_loss: 0.14184772968292236, tv_loss: 0.015782851725816727\n",
      "iteration 191, dc_loss: 0.14045113325119019, tv_loss: 0.01580573432147503\n",
      "iteration 192, dc_loss: 0.13908137381076813, tv_loss: 0.015827734023332596\n",
      "iteration 193, dc_loss: 0.13773790001869202, tv_loss: 0.015847941860556602\n",
      "iteration 194, dc_loss: 0.13642019033432007, tv_loss: 0.015867702662944794\n",
      "iteration 195, dc_loss: 0.1351277232170105, tv_loss: 0.015889765694737434\n",
      "iteration 196, dc_loss: 0.13385997712612152, tv_loss: 0.01591041311621666\n",
      "iteration 197, dc_loss: 0.1326165497303009, tv_loss: 0.0159300547093153\n",
      "iteration 198, dc_loss: 0.13139687478542328, tv_loss: 0.015949418768286705\n",
      "iteration 199, dc_loss: 0.13020049035549164, tv_loss: 0.015969155356287956\n",
      "iteration 200, dc_loss: 0.12902696430683136, tv_loss: 0.01598871871829033\n",
      "iteration 201, dc_loss: 0.12787580490112305, tv_loss: 0.016007358208298683\n",
      "iteration 202, dc_loss: 0.12674660980701447, tv_loss: 0.016025688499212265\n",
      "iteration 203, dc_loss: 0.1256389319896698, tv_loss: 0.016044192016124725\n",
      "iteration 204, dc_loss: 0.12455233931541443, tv_loss: 0.016063550487160683\n",
      "iteration 205, dc_loss: 0.12348641455173492, tv_loss: 0.01608183979988098\n",
      "iteration 206, dc_loss: 0.12244074046611786, tv_loss: 0.01609877310693264\n",
      "iteration 207, dc_loss: 0.12141489237546921, tv_loss: 0.016115408390760422\n",
      "iteration 208, dc_loss: 0.12040852010250092, tv_loss: 0.016133494675159454\n",
      "iteration 209, dc_loss: 0.11942125111818314, tv_loss: 0.01615096814930439\n",
      "iteration 210, dc_loss: 0.11845270544290543, tv_loss: 0.01616623066365719\n",
      "iteration 211, dc_loss: 0.11750250309705734, tv_loss: 0.016182808205485344\n",
      "iteration 212, dc_loss: 0.11657023429870605, tv_loss: 0.016199760138988495\n",
      "iteration 213, dc_loss: 0.11565552651882172, tv_loss: 0.01621553860604763\n",
      "iteration 214, dc_loss: 0.11475808918476105, tv_loss: 0.01623036526143551\n",
      "iteration 215, dc_loss: 0.11387760192155838, tv_loss: 0.016247032210230827\n",
      "iteration 216, dc_loss: 0.11301374435424805, tv_loss: 0.016263773664832115\n",
      "iteration 217, dc_loss: 0.11216619610786438, tv_loss: 0.01627829112112522\n",
      "iteration 218, dc_loss: 0.11133461445569992, tv_loss: 0.016292084008455276\n",
      "iteration 219, dc_loss: 0.11051861941814423, tv_loss: 0.01630776934325695\n",
      "iteration 220, dc_loss: 0.10971789807081223, tv_loss: 0.016323180869221687\n",
      "iteration 221, dc_loss: 0.10893220454454422, tv_loss: 0.01633700542151928\n",
      "iteration 222, dc_loss: 0.10816124826669693, tv_loss: 0.016350949183106422\n",
      "iteration 223, dc_loss: 0.10740476101636887, tv_loss: 0.016365570947527885\n",
      "iteration 224, dc_loss: 0.10666245967149734, tv_loss: 0.016379764303565025\n",
      "iteration 225, dc_loss: 0.1059340313076973, tv_loss: 0.016393102705478668\n",
      "iteration 226, dc_loss: 0.10521920025348663, tv_loss: 0.01640690304338932\n",
      "iteration 227, dc_loss: 0.10451773554086685, tv_loss: 0.016420777887105942\n",
      "iteration 228, dc_loss: 0.10382932424545288, tv_loss: 0.016433119773864746\n",
      "iteration 229, dc_loss: 0.10315372794866562, tv_loss: 0.01644711382687092\n",
      "iteration 230, dc_loss: 0.10249070078134537, tv_loss: 0.016459917649626732\n",
      "iteration 231, dc_loss: 0.10184001922607422, tv_loss: 0.016472168266773224\n",
      "iteration 232, dc_loss: 0.10120144486427307, tv_loss: 0.01648515649139881\n",
      "iteration 233, dc_loss: 0.10057474672794342, tv_loss: 0.01649794727563858\n",
      "iteration 234, dc_loss: 0.09995963424444199, tv_loss: 0.0165102556347847\n",
      "iteration 235, dc_loss: 0.09935591369867325, tv_loss: 0.016522282734513283\n",
      "iteration 236, dc_loss: 0.0987633541226387, tv_loss: 0.01653454080224037\n",
      "iteration 237, dc_loss: 0.09818174690008163, tv_loss: 0.016545424237847328\n",
      "iteration 238, dc_loss: 0.09761091321706772, tv_loss: 0.016557680442929268\n",
      "iteration 239, dc_loss: 0.09705062955617905, tv_loss: 0.01656906120479107\n",
      "iteration 240, dc_loss: 0.09650067239999771, tv_loss: 0.016579851508140564\n",
      "iteration 241, dc_loss: 0.0959608405828476, tv_loss: 0.016591228544712067\n",
      "iteration 242, dc_loss: 0.09543094038963318, tv_loss: 0.01660327799618244\n",
      "iteration 243, dc_loss: 0.09491076320409775, tv_loss: 0.01661389134824276\n",
      "iteration 244, dc_loss: 0.09440010040998459, tv_loss: 0.01662316732108593\n",
      "iteration 245, dc_loss: 0.09389882534742355, tv_loss: 0.016634156927466393\n",
      "iteration 246, dc_loss: 0.09340677410364151, tv_loss: 0.016645977273583412\n",
      "iteration 247, dc_loss: 0.09292375296354294, tv_loss: 0.016654783859848976\n",
      "iteration 248, dc_loss: 0.09244954586029053, tv_loss: 0.016665171831846237\n",
      "iteration 249, dc_loss: 0.09198396652936935, tv_loss: 0.016676146537065506\n",
      "iteration 250, dc_loss: 0.09152688086032867, tv_loss: 0.01668586954474449\n",
      "iteration 251, dc_loss: 0.09107816964387894, tv_loss: 0.016695363447070122\n",
      "iteration 252, dc_loss: 0.09063764661550522, tv_loss: 0.016704628244042397\n",
      "iteration 253, dc_loss: 0.0902051031589508, tv_loss: 0.016714034602046013\n",
      "iteration 254, dc_loss: 0.08978040516376495, tv_loss: 0.01672336459159851\n",
      "iteration 255, dc_loss: 0.08936344087123871, tv_loss: 0.016733361408114433\n",
      "iteration 256, dc_loss: 0.08895403891801834, tv_loss: 0.01674146205186844\n",
      "iteration 257, dc_loss: 0.0885520726442337, tv_loss: 0.016751114279031754\n",
      "iteration 258, dc_loss: 0.08815735578536987, tv_loss: 0.01676027663052082\n",
      "iteration 259, dc_loss: 0.0877697691321373, tv_loss: 0.016768986359238625\n",
      "iteration 260, dc_loss: 0.08738918602466583, tv_loss: 0.016776660457253456\n",
      "iteration 261, dc_loss: 0.08701545000076294, tv_loss: 0.016785573214292526\n",
      "iteration 262, dc_loss: 0.08664846420288086, tv_loss: 0.01679440774023533\n",
      "iteration 263, dc_loss: 0.08628813177347183, tv_loss: 0.01680230163037777\n",
      "iteration 264, dc_loss: 0.08593424409627914, tv_loss: 0.01681029610335827\n",
      "iteration 265, dc_loss: 0.08558669686317444, tv_loss: 0.016818325966596603\n",
      "iteration 266, dc_loss: 0.08524539321660995, tv_loss: 0.016826922073960304\n",
      "iteration 267, dc_loss: 0.08491022139787674, tv_loss: 0.016834506765007973\n",
      "iteration 268, dc_loss: 0.08458100259304047, tv_loss: 0.016840945929288864\n",
      "iteration 269, dc_loss: 0.08425769209861755, tv_loss: 0.016848811879754066\n",
      "iteration 270, dc_loss: 0.08394018560647964, tv_loss: 0.016857409849762917\n",
      "iteration 271, dc_loss: 0.08362835645675659, tv_loss: 0.016864441335201263\n",
      "iteration 272, dc_loss: 0.08332207798957825, tv_loss: 0.01687127724289894\n",
      "iteration 273, dc_loss: 0.08302125334739685, tv_loss: 0.01687879115343094\n",
      "iteration 274, dc_loss: 0.08272575587034225, tv_loss: 0.016885818913578987\n",
      "iteration 275, dc_loss: 0.08243551105260849, tv_loss: 0.016893118619918823\n",
      "iteration 276, dc_loss: 0.082150399684906, tv_loss: 0.01689993590116501\n",
      "iteration 277, dc_loss: 0.08187035471200943, tv_loss: 0.016907377168536186\n",
      "iteration 278, dc_loss: 0.08159524202346802, tv_loss: 0.016913026571273804\n",
      "iteration 279, dc_loss: 0.0813249871134758, tv_loss: 0.01692022942006588\n",
      "iteration 280, dc_loss: 0.081059530377388, tv_loss: 0.016927160322666168\n",
      "iteration 281, dc_loss: 0.08079878985881805, tv_loss: 0.016933994367718697\n",
      "iteration 282, dc_loss: 0.0805426687002182, tv_loss: 0.01693984679877758\n",
      "iteration 283, dc_loss: 0.0802910178899765, tv_loss: 0.01694493182003498\n",
      "iteration 284, dc_loss: 0.08004378527402878, tv_loss: 0.01695219799876213\n",
      "iteration 285, dc_loss: 0.07980085909366608, tv_loss: 0.01695985719561577\n",
      "iteration 286, dc_loss: 0.0795622244477272, tv_loss: 0.01696595549583435\n",
      "iteration 287, dc_loss: 0.0793277844786644, tv_loss: 0.016969939693808556\n",
      "iteration 288, dc_loss: 0.07909747213125229, tv_loss: 0.01697666198015213\n",
      "iteration 289, dc_loss: 0.07887115329504013, tv_loss: 0.016983680427074432\n",
      "iteration 290, dc_loss: 0.07864879071712494, tv_loss: 0.016989635303616524\n",
      "iteration 291, dc_loss: 0.07843031734228134, tv_loss: 0.016994066536426544\n",
      "iteration 292, dc_loss: 0.07821562886238098, tv_loss: 0.01699909381568432\n",
      "iteration 293, dc_loss: 0.07800468057394028, tv_loss: 0.01700577884912491\n",
      "iteration 294, dc_loss: 0.07779740542173386, tv_loss: 0.01701185666024685\n",
      "iteration 295, dc_loss: 0.07759378105401993, tv_loss: 0.017016464844346046\n",
      "iteration 296, dc_loss: 0.07739366590976715, tv_loss: 0.017021186649799347\n",
      "iteration 297, dc_loss: 0.07719700038433075, tv_loss: 0.01702664978802204\n",
      "iteration 298, dc_loss: 0.07700370252132416, tv_loss: 0.01703166961669922\n",
      "iteration 299, dc_loss: 0.07681375741958618, tv_loss: 0.017036786302924156\n",
      "iteration 300, dc_loss: 0.07662710547447205, tv_loss: 0.017041385173797607\n",
      "iteration 301, dc_loss: 0.07644368708133698, tv_loss: 0.017046205699443817\n",
      "iteration 302, dc_loss: 0.07626347988843918, tv_loss: 0.017051519826054573\n",
      "iteration 303, dc_loss: 0.07608634233474731, tv_loss: 0.017056267708539963\n",
      "iteration 304, dc_loss: 0.07591221481561661, tv_loss: 0.01706053875386715\n",
      "iteration 305, dc_loss: 0.07574110478162766, tv_loss: 0.01706545427441597\n",
      "iteration 306, dc_loss: 0.0755729153752327, tv_loss: 0.01707030087709427\n",
      "iteration 307, dc_loss: 0.07540762424468994, tv_loss: 0.017074959352612495\n",
      "iteration 308, dc_loss: 0.07524516433477402, tv_loss: 0.017079180106520653\n",
      "iteration 309, dc_loss: 0.07508544623851776, tv_loss: 0.01708322949707508\n",
      "iteration 310, dc_loss: 0.07492846995592117, tv_loss: 0.01708812452852726\n",
      "iteration 311, dc_loss: 0.07477419823408127, tv_loss: 0.01709207519888878\n",
      "iteration 312, dc_loss: 0.07462257891893387, tv_loss: 0.01709616556763649\n",
      "iteration 313, dc_loss: 0.0744735449552536, tv_loss: 0.017100615426898003\n",
      "iteration 314, dc_loss: 0.07432704418897629, tv_loss: 0.017105255275964737\n",
      "iteration 315, dc_loss: 0.07418299466371536, tv_loss: 0.017109109088778496\n",
      "iteration 316, dc_loss: 0.07404134422540665, tv_loss: 0.017112981528043747\n",
      "iteration 317, dc_loss: 0.07390213012695312, tv_loss: 0.017117099836468697\n",
      "iteration 318, dc_loss: 0.07376528531312943, tv_loss: 0.017120687291026115\n",
      "iteration 319, dc_loss: 0.07363077253103256, tv_loss: 0.01712506078183651\n",
      "iteration 320, dc_loss: 0.07349855452775955, tv_loss: 0.017129361629486084\n",
      "iteration 321, dc_loss: 0.07336856424808502, tv_loss: 0.0171328354626894\n",
      "iteration 322, dc_loss: 0.07324077934026718, tv_loss: 0.017136266455054283\n",
      "iteration 323, dc_loss: 0.07311517000198364, tv_loss: 0.01714005507528782\n",
      "iteration 324, dc_loss: 0.07299160957336426, tv_loss: 0.017143694683909416\n",
      "iteration 325, dc_loss: 0.07287010550498962, tv_loss: 0.017147092148661613\n",
      "iteration 326, dc_loss: 0.07275065779685974, tv_loss: 0.017150938510894775\n",
      "iteration 327, dc_loss: 0.07263325154781342, tv_loss: 0.017154935747385025\n",
      "iteration 328, dc_loss: 0.07251779735088348, tv_loss: 0.017157720401883125\n",
      "iteration 329, dc_loss: 0.07240427285432816, tv_loss: 0.017160950228571892\n",
      "iteration 330, dc_loss: 0.07229267805814743, tv_loss: 0.017165208235383034\n",
      "iteration 331, dc_loss: 0.07218297570943832, tv_loss: 0.01716812513768673\n",
      "iteration 332, dc_loss: 0.07207507640123367, tv_loss: 0.017171088606119156\n",
      "iteration 333, dc_loss: 0.07196895778179169, tv_loss: 0.017174895852804184\n",
      "iteration 334, dc_loss: 0.07186460494995117, tv_loss: 0.017177777364850044\n",
      "iteration 335, dc_loss: 0.07176199555397034, tv_loss: 0.017180729657411575\n",
      "iteration 336, dc_loss: 0.07166111469268799, tv_loss: 0.017183996737003326\n",
      "iteration 337, dc_loss: 0.07156194001436234, tv_loss: 0.017187604680657387\n",
      "iteration 338, dc_loss: 0.0714644119143486, tv_loss: 0.017190303653478622\n",
      "iteration 339, dc_loss: 0.07136846333742142, tv_loss: 0.01719297096133232\n",
      "iteration 340, dc_loss: 0.07127409428358078, tv_loss: 0.017195450142025948\n",
      "iteration 341, dc_loss: 0.0711812824010849, tv_loss: 0.017198089510202408\n",
      "iteration 342, dc_loss: 0.07109004259109497, tv_loss: 0.017201421782374382\n",
      "iteration 343, dc_loss: 0.07100031524896622, tv_loss: 0.01720389537513256\n",
      "iteration 344, dc_loss: 0.07091209292411804, tv_loss: 0.017206793650984764\n",
      "iteration 345, dc_loss: 0.07082533091306686, tv_loss: 0.01720934547483921\n",
      "iteration 346, dc_loss: 0.0707399919629097, tv_loss: 0.017211774364113808\n",
      "iteration 347, dc_loss: 0.07065605372190475, tv_loss: 0.017214616760611534\n",
      "iteration 348, dc_loss: 0.07057345658540726, tv_loss: 0.0172176081687212\n",
      "iteration 349, dc_loss: 0.070492222905159, tv_loss: 0.017219964414834976\n",
      "iteration 350, dc_loss: 0.07041236758232117, tv_loss: 0.017222527414560318\n",
      "iteration 351, dc_loss: 0.07033383101224899, tv_loss: 0.017225366085767746\n",
      "iteration 352, dc_loss: 0.07025658339262009, tv_loss: 0.01722787506878376\n",
      "iteration 353, dc_loss: 0.07018062472343445, tv_loss: 0.017230432480573654\n",
      "iteration 354, dc_loss: 0.0701058954000473, tv_loss: 0.017232829704880714\n",
      "iteration 355, dc_loss: 0.07003235071897507, tv_loss: 0.017234833911061287\n",
      "iteration 356, dc_loss: 0.06996002793312073, tv_loss: 0.017237786203622818\n",
      "iteration 357, dc_loss: 0.0698888972401619, tv_loss: 0.017240289598703384\n",
      "iteration 358, dc_loss: 0.06981894373893738, tv_loss: 0.0172421894967556\n",
      "iteration 359, dc_loss: 0.06975014507770538, tv_loss: 0.017244337126612663\n",
      "iteration 360, dc_loss: 0.06968244910240173, tv_loss: 0.017247123643755913\n",
      "iteration 361, dc_loss: 0.06961585581302643, tv_loss: 0.017248986288905144\n",
      "iteration 362, dc_loss: 0.06955038756132126, tv_loss: 0.01725095324218273\n",
      "iteration 363, dc_loss: 0.06948597729206085, tv_loss: 0.017253531143069267\n",
      "iteration 364, dc_loss: 0.06942258030176163, tv_loss: 0.01725606806576252\n",
      "iteration 365, dc_loss: 0.06936023384332657, tv_loss: 0.017257945612072945\n",
      "iteration 366, dc_loss: 0.06929890811443329, tv_loss: 0.01725957915186882\n",
      "iteration 367, dc_loss: 0.06923859566450119, tv_loss: 0.017261968925595284\n",
      "iteration 368, dc_loss: 0.06917925179004669, tv_loss: 0.01726369932293892\n",
      "iteration 369, dc_loss: 0.0691208466887474, tv_loss: 0.01726596988737583\n",
      "iteration 370, dc_loss: 0.06906340271234512, tv_loss: 0.017267964780330658\n",
      "iteration 371, dc_loss: 0.06900689750909805, tv_loss: 0.017269810661673546\n",
      "iteration 372, dc_loss: 0.06895133852958679, tv_loss: 0.017271850258111954\n",
      "iteration 373, dc_loss: 0.06889668107032776, tv_loss: 0.017274582758545876\n",
      "iteration 374, dc_loss: 0.06884289532899857, tv_loss: 0.017276596277952194\n",
      "iteration 375, dc_loss: 0.06878997385501862, tv_loss: 0.01727713644504547\n",
      "iteration 376, dc_loss: 0.06873789429664612, tv_loss: 0.017279749736189842\n",
      "iteration 377, dc_loss: 0.06868667900562286, tv_loss: 0.01728242076933384\n",
      "iteration 378, dc_loss: 0.06863629072904587, tv_loss: 0.017283441498875618\n",
      "iteration 379, dc_loss: 0.06858672201633453, tv_loss: 0.01728411763906479\n",
      "iteration 380, dc_loss: 0.06853795051574707, tv_loss: 0.017286228016018867\n",
      "iteration 381, dc_loss: 0.06848996877670288, tv_loss: 0.017288630828261375\n",
      "iteration 382, dc_loss: 0.06844277679920197, tv_loss: 0.017289672046899796\n",
      "iteration 383, dc_loss: 0.06839634478092194, tv_loss: 0.017291085794568062\n",
      "iteration 384, dc_loss: 0.06835063546895981, tv_loss: 0.017293104901909828\n",
      "iteration 385, dc_loss: 0.06830566376447678, tv_loss: 0.017294351011514664\n",
      "iteration 386, dc_loss: 0.06826145201921463, tv_loss: 0.01729632541537285\n",
      "iteration 387, dc_loss: 0.06821796298027039, tv_loss: 0.017297951504588127\n",
      "iteration 388, dc_loss: 0.06817512214183807, tv_loss: 0.017299633473157883\n",
      "iteration 389, dc_loss: 0.06813298165798187, tv_loss: 0.01730087213218212\n",
      "iteration 390, dc_loss: 0.06809154152870178, tv_loss: 0.01730315014719963\n",
      "iteration 391, dc_loss: 0.06805075705051422, tv_loss: 0.017304345965385437\n",
      "iteration 392, dc_loss: 0.06801065057516098, tv_loss: 0.017305176705121994\n",
      "iteration 393, dc_loss: 0.06797119975090027, tv_loss: 0.01730700023472309\n",
      "iteration 394, dc_loss: 0.06793235242366791, tv_loss: 0.01730874367058277\n",
      "iteration 395, dc_loss: 0.06789413094520569, tv_loss: 0.01730993390083313\n",
      "iteration 396, dc_loss: 0.06785652041435242, tv_loss: 0.017311474308371544\n",
      "iteration 397, dc_loss: 0.06781953573226929, tv_loss: 0.017312780022621155\n",
      "iteration 398, dc_loss: 0.0677831619977951, tv_loss: 0.01731429249048233\n",
      "iteration 399, dc_loss: 0.06774735450744629, tv_loss: 0.017315492033958435\n",
      "iteration 400, dc_loss: 0.06771212071180344, tv_loss: 0.017316559329628944\n",
      "iteration 401, dc_loss: 0.06767746806144714, tv_loss: 0.01731785573065281\n",
      "iteration 402, dc_loss: 0.06764336675405502, tv_loss: 0.01731928065419197\n",
      "iteration 403, dc_loss: 0.06760981678962708, tv_loss: 0.01732075959444046\n",
      "iteration 404, dc_loss: 0.06757676601409912, tv_loss: 0.017321636900305748\n",
      "iteration 405, dc_loss: 0.06754424422979355, tv_loss: 0.017323032021522522\n",
      "iteration 406, dc_loss: 0.06751225143671036, tv_loss: 0.017324641346931458\n",
      "iteration 407, dc_loss: 0.06748082488775253, tv_loss: 0.017325669527053833\n",
      "iteration 408, dc_loss: 0.0674499124288559, tv_loss: 0.017326483502984047\n",
      "iteration 409, dc_loss: 0.06741947680711746, tv_loss: 0.017327748239040375\n",
      "iteration 410, dc_loss: 0.06738952547311783, tv_loss: 0.0173292625695467\n",
      "iteration 411, dc_loss: 0.06736002117395401, tv_loss: 0.017330223694443703\n",
      "iteration 412, dc_loss: 0.0673309713602066, tv_loss: 0.017331210896372795\n",
      "iteration 413, dc_loss: 0.0673024132847786, tv_loss: 0.0173322930932045\n",
      "iteration 414, dc_loss: 0.06727434694766998, tv_loss: 0.017333555966615677\n",
      "iteration 415, dc_loss: 0.06724672019481659, tv_loss: 0.01733502559363842\n",
      "iteration 416, dc_loss: 0.0672195628285408, tv_loss: 0.01733611524105072\n",
      "iteration 417, dc_loss: 0.06719283759593964, tv_loss: 0.01733684353530407\n",
      "iteration 418, dc_loss: 0.06716648489236832, tv_loss: 0.017337989062070847\n",
      "iteration 419, dc_loss: 0.06714058667421341, tv_loss: 0.017339307814836502\n",
      "iteration 420, dc_loss: 0.06711508333683014, tv_loss: 0.01734067127108574\n",
      "iteration 421, dc_loss: 0.06709001213312149, tv_loss: 0.017341744154691696\n",
      "iteration 422, dc_loss: 0.06706535071134567, tv_loss: 0.017342643812298775\n",
      "iteration 423, dc_loss: 0.06704108417034149, tv_loss: 0.017343824729323387\n",
      "iteration 424, dc_loss: 0.06701718270778656, tv_loss: 0.01734505593776703\n",
      "iteration 425, dc_loss: 0.06699366867542267, tv_loss: 0.017345815896987915\n",
      "iteration 426, dc_loss: 0.06697056442499161, tv_loss: 0.017346233129501343\n",
      "iteration 427, dc_loss: 0.06694786995649338, tv_loss: 0.017347274348139763\n",
      "iteration 428, dc_loss: 0.06692551076412201, tv_loss: 0.01734856143593788\n",
      "iteration 429, dc_loss: 0.06690345704555511, tv_loss: 0.01734910160303116\n",
      "iteration 430, dc_loss: 0.06688173115253448, tv_loss: 0.017349686473608017\n",
      "iteration 431, dc_loss: 0.06686040014028549, tv_loss: 0.017351316288113594\n",
      "iteration 432, dc_loss: 0.06683943420648575, tv_loss: 0.01735248975455761\n",
      "iteration 433, dc_loss: 0.06681881099939346, tv_loss: 0.017352739349007607\n",
      "iteration 434, dc_loss: 0.06679850816726685, tv_loss: 0.017353586852550507\n",
      "iteration 435, dc_loss: 0.06677854061126709, tv_loss: 0.017354877665638924\n",
      "iteration 436, dc_loss: 0.06675886362791061, tv_loss: 0.01735573634505272\n",
      "iteration 437, dc_loss: 0.066739521920681, tv_loss: 0.017356419935822487\n",
      "iteration 438, dc_loss: 0.06672047823667526, tv_loss: 0.017357386648654938\n",
      "iteration 439, dc_loss: 0.0667017251253128, tv_loss: 0.01735841855406761\n",
      "iteration 440, dc_loss: 0.06668329983949661, tv_loss: 0.017359403893351555\n",
      "iteration 441, dc_loss: 0.0666651576757431, tv_loss: 0.017359893769025803\n",
      "iteration 442, dc_loss: 0.06664735078811646, tv_loss: 0.017360391095280647\n",
      "iteration 443, dc_loss: 0.06662984192371368, tv_loss: 0.017361322417855263\n",
      "iteration 444, dc_loss: 0.06661256402730942, tv_loss: 0.01736202836036682\n",
      "iteration 445, dc_loss: 0.06659555435180664, tv_loss: 0.01736290566623211\n",
      "iteration 446, dc_loss: 0.06657884269952774, tv_loss: 0.017363406717777252\n",
      "iteration 447, dc_loss: 0.06656237691640854, tv_loss: 0.017363814637064934\n",
      "iteration 448, dc_loss: 0.06654618680477142, tv_loss: 0.01736515201628208\n",
      "iteration 449, dc_loss: 0.06653027981519699, tv_loss: 0.01736621744930744\n",
      "iteration 450, dc_loss: 0.06651463359594345, tv_loss: 0.017366617918014526\n",
      "iteration 451, dc_loss: 0.0664992406964302, tv_loss: 0.017366880550980568\n",
      "iteration 452, dc_loss: 0.06648408621549606, tv_loss: 0.017367562279105186\n",
      "iteration 453, dc_loss: 0.06646917760372162, tv_loss: 0.017368493601679802\n",
      "iteration 454, dc_loss: 0.06645447015762329, tv_loss: 0.01736929826438427\n",
      "iteration 455, dc_loss: 0.06644003093242645, tv_loss: 0.01736958883702755\n",
      "iteration 456, dc_loss: 0.06642583757638931, tv_loss: 0.01737031154334545\n",
      "iteration 457, dc_loss: 0.06641189008951187, tv_loss: 0.017370734363794327\n",
      "iteration 458, dc_loss: 0.06639814376831055, tv_loss: 0.017371365800499916\n",
      "iteration 459, dc_loss: 0.06638462096452713, tv_loss: 0.01737215742468834\n",
      "iteration 460, dc_loss: 0.06637129187583923, tv_loss: 0.01737247407436371\n",
      "iteration 461, dc_loss: 0.06635818630456924, tv_loss: 0.017373187467455864\n",
      "iteration 462, dc_loss: 0.06634532660245895, tv_loss: 0.017373833805322647\n",
      "iteration 463, dc_loss: 0.06633269786834717, tv_loss: 0.017374258488416672\n",
      "iteration 464, dc_loss: 0.0663202553987503, tv_loss: 0.017374776303768158\n",
      "iteration 465, dc_loss: 0.06630797684192657, tv_loss: 0.017375757917761803\n",
      "iteration 466, dc_loss: 0.06629592180252075, tv_loss: 0.01737644337117672\n",
      "iteration 467, dc_loss: 0.06628406047821045, tv_loss: 0.01737673208117485\n",
      "iteration 468, dc_loss: 0.06627237051725388, tv_loss: 0.01737728714942932\n",
      "iteration 469, dc_loss: 0.06626087427139282, tv_loss: 0.017377909272909164\n",
      "iteration 470, dc_loss: 0.06624958664178848, tv_loss: 0.017378496006131172\n",
      "iteration 471, dc_loss: 0.06623850017786026, tv_loss: 0.017379362136125565\n",
      "iteration 472, dc_loss: 0.06622759252786636, tv_loss: 0.017379971221089363\n",
      "iteration 473, dc_loss: 0.0662168636918068, tv_loss: 0.017379822209477425\n",
      "iteration 474, dc_loss: 0.06620626151561737, tv_loss: 0.017380323261022568\n",
      "iteration 475, dc_loss: 0.06619584560394287, tv_loss: 0.017381593585014343\n",
      "iteration 476, dc_loss: 0.0661856085062027, tv_loss: 0.017382310703396797\n",
      "iteration 477, dc_loss: 0.06617553532123566, tv_loss: 0.017382364720106125\n",
      "iteration 478, dc_loss: 0.06616561859846115, tv_loss: 0.01738244853913784\n",
      "iteration 479, dc_loss: 0.06615587323904037, tv_loss: 0.01738305762410164\n",
      "iteration 480, dc_loss: 0.06614631414413452, tv_loss: 0.01738419197499752\n",
      "iteration 481, dc_loss: 0.06613689661026001, tv_loss: 0.017384497448801994\n",
      "iteration 482, dc_loss: 0.06612767279148102, tv_loss: 0.017384303733706474\n",
      "iteration 483, dc_loss: 0.06611855328083038, tv_loss: 0.017385195940732956\n",
      "iteration 484, dc_loss: 0.0661095678806305, tv_loss: 0.017386076971888542\n",
      "iteration 485, dc_loss: 0.06610072404146194, tv_loss: 0.017386537045240402\n",
      "iteration 486, dc_loss: 0.06609202921390533, tv_loss: 0.017386676743626595\n",
      "iteration 487, dc_loss: 0.06608350574970245, tv_loss: 0.017386937513947487\n",
      "iteration 488, dc_loss: 0.06607510149478912, tv_loss: 0.01738794706761837\n",
      "iteration 489, dc_loss: 0.06606683880090714, tv_loss: 0.01738809049129486\n",
      "iteration 490, dc_loss: 0.06605872511863708, tv_loss: 0.017388271167874336\n",
      "iteration 491, dc_loss: 0.06605078279972076, tv_loss: 0.01738876849412918\n",
      "iteration 492, dc_loss: 0.0660429447889328, tv_loss: 0.0173894464969635\n",
      "iteration 493, dc_loss: 0.0660352036356926, tv_loss: 0.01738937757909298\n",
      "iteration 494, dc_loss: 0.06602759659290314, tv_loss: 0.017389804124832153\n",
      "iteration 495, dc_loss: 0.06602008640766144, tv_loss: 0.017390387132763863\n",
      "iteration 496, dc_loss: 0.06601271033287048, tv_loss: 0.01739090122282505\n",
      "iteration 497, dc_loss: 0.06600548326969147, tv_loss: 0.01739116944372654\n",
      "iteration 498, dc_loss: 0.06599842011928558, tv_loss: 0.017391329631209373\n",
      "iteration 499, dc_loss: 0.06599145382642746, tv_loss: 0.017391828820109367\n",
      "iteration 500, dc_loss: 0.0659845769405365, tv_loss: 0.017392337322235107\n",
      "iteration 501, dc_loss: 0.0659778043627739, tv_loss: 0.017393171787261963\n",
      "iteration 502, dc_loss: 0.06597114354372025, tv_loss: 0.017392894253134727\n",
      "iteration 503, dc_loss: 0.06596460193395615, tv_loss: 0.017393074929714203\n",
      "iteration 504, dc_loss: 0.06595815718173981, tv_loss: 0.017393821850419044\n",
      "iteration 505, dc_loss: 0.06595183163881302, tv_loss: 0.017394330352544785\n",
      "iteration 506, dc_loss: 0.06594562530517578, tv_loss: 0.017394477501511574\n",
      "iteration 507, dc_loss: 0.0659395307302475, tv_loss: 0.017394710332155228\n",
      "iteration 508, dc_loss: 0.06593349575996399, tv_loss: 0.017395222559571266\n",
      "iteration 509, dc_loss: 0.06592755764722824, tv_loss: 0.017395704984664917\n",
      "iteration 510, dc_loss: 0.06592170149087906, tv_loss: 0.017396043986082077\n",
      "iteration 511, dc_loss: 0.06591598689556122, tv_loss: 0.017396055161952972\n",
      "iteration 512, dc_loss: 0.06591040641069412, tv_loss: 0.017396479845046997\n",
      "iteration 513, dc_loss: 0.06590490788221359, tv_loss: 0.01739707589149475\n",
      "iteration 514, dc_loss: 0.06589945405721664, tv_loss: 0.017396926879882812\n",
      "iteration 515, dc_loss: 0.06589408963918686, tv_loss: 0.01739736832678318\n",
      "iteration 516, dc_loss: 0.06588882207870483, tv_loss: 0.017398076131939888\n",
      "iteration 517, dc_loss: 0.06588365882635117, tv_loss: 0.017398260533809662\n",
      "iteration 518, dc_loss: 0.06587859243154526, tv_loss: 0.017398390918970108\n",
      "iteration 519, dc_loss: 0.06587357074022293, tv_loss: 0.017398597672581673\n",
      "iteration 520, dc_loss: 0.06586863845586777, tv_loss: 0.01739940419793129\n",
      "iteration 521, dc_loss: 0.06586381047964096, tv_loss: 0.017399771139025688\n",
      "iteration 522, dc_loss: 0.06585909426212311, tv_loss: 0.017399637028574944\n",
      "iteration 523, dc_loss: 0.06585443764925003, tv_loss: 0.017399756237864494\n",
      "iteration 524, dc_loss: 0.06584982573986053, tv_loss: 0.01740003563463688\n",
      "iteration 525, dc_loss: 0.06584527343511581, tv_loss: 0.017400674521923065\n",
      "iteration 526, dc_loss: 0.06584082543849945, tv_loss: 0.01740107126533985\n",
      "iteration 527, dc_loss: 0.06583648175001144, tv_loss: 0.01740134134888649\n",
      "iteration 528, dc_loss: 0.06583216786384583, tv_loss: 0.017401495948433876\n",
      "iteration 529, dc_loss: 0.06582795083522797, tv_loss: 0.017401693388819695\n",
      "iteration 530, dc_loss: 0.06582380086183548, tv_loss: 0.017401982098817825\n",
      "iteration 531, dc_loss: 0.06581972539424896, tv_loss: 0.017402205616235733\n",
      "iteration 532, dc_loss: 0.06581570208072662, tv_loss: 0.017402350902557373\n",
      "iteration 533, dc_loss: 0.06581177562475204, tv_loss: 0.01740248315036297\n",
      "iteration 534, dc_loss: 0.06580789387226105, tv_loss: 0.01740281656384468\n",
      "iteration 535, dc_loss: 0.06580407172441483, tv_loss: 0.017403030768036842\n",
      "iteration 536, dc_loss: 0.06580033898353577, tv_loss: 0.01740335114300251\n",
      "iteration 537, dc_loss: 0.06579665094614029, tv_loss: 0.017403483390808105\n",
      "iteration 538, dc_loss: 0.06579302251338959, tv_loss: 0.01740366406738758\n",
      "iteration 539, dc_loss: 0.06578947603702545, tv_loss: 0.017403924837708473\n",
      "iteration 540, dc_loss: 0.06578598916530609, tv_loss: 0.01740446500480175\n",
      "iteration 541, dc_loss: 0.06578254699707031, tv_loss: 0.017404483631253242\n",
      "iteration 542, dc_loss: 0.06577916443347931, tv_loss: 0.01740446500480175\n",
      "iteration 543, dc_loss: 0.06577584147453308, tv_loss: 0.017404913902282715\n",
      "iteration 544, dc_loss: 0.06577253341674805, tv_loss: 0.017405275255441666\n",
      "iteration 545, dc_loss: 0.06576931476593018, tv_loss: 0.017405636608600616\n",
      "iteration 546, dc_loss: 0.06576617062091827, tv_loss: 0.017406096681952477\n",
      "iteration 547, dc_loss: 0.06576313823461533, tv_loss: 0.017406266182661057\n",
      "iteration 548, dc_loss: 0.06576009839773178, tv_loss: 0.017406003549695015\n",
      "iteration 549, dc_loss: 0.06575706601142883, tv_loss: 0.01740643009543419\n",
      "iteration 550, dc_loss: 0.06575407087802887, tv_loss: 0.01740708015859127\n",
      "iteration 551, dc_loss: 0.06575114279985428, tv_loss: 0.017407190054655075\n",
      "iteration 552, dc_loss: 0.06574831157922745, tv_loss: 0.017407070845365524\n",
      "iteration 553, dc_loss: 0.06574553996324539, tv_loss: 0.017407381907105446\n",
      "iteration 554, dc_loss: 0.06574282050132751, tv_loss: 0.0174074936658144\n",
      "iteration 555, dc_loss: 0.06574013084173203, tv_loss: 0.017407745122909546\n",
      "iteration 556, dc_loss: 0.06573746353387833, tv_loss: 0.01740795001387596\n",
      "iteration 557, dc_loss: 0.0657348483800888, tv_loss: 0.017408082261681557\n",
      "iteration 558, dc_loss: 0.06573225557804108, tv_loss: 0.017408281564712524\n",
      "iteration 559, dc_loss: 0.06572970002889633, tv_loss: 0.017408831045031548\n",
      "iteration 560, dc_loss: 0.06572721153497696, tv_loss: 0.017408933490514755\n",
      "iteration 561, dc_loss: 0.06572480499744415, tv_loss: 0.01740860380232334\n",
      "iteration 562, dc_loss: 0.06572244316339493, tv_loss: 0.017408957704901695\n",
      "iteration 563, dc_loss: 0.06572011113166809, tv_loss: 0.017409153282642365\n",
      "iteration 564, dc_loss: 0.06571778655052185, tv_loss: 0.017409583553671837\n",
      "iteration 565, dc_loss: 0.0657154768705368, tv_loss: 0.017409978434443474\n",
      "iteration 566, dc_loss: 0.06571324169635773, tv_loss: 0.0174096766859293\n",
      "iteration 567, dc_loss: 0.06571106612682343, tv_loss: 0.01740972325205803\n",
      "iteration 568, dc_loss: 0.06570892781019211, tv_loss: 0.0174101535230875\n",
      "iteration 569, dc_loss: 0.06570680439472198, tv_loss: 0.01741057261824608\n",
      "iteration 570, dc_loss: 0.06570472568273544, tv_loss: 0.017410507425665855\n",
      "iteration 571, dc_loss: 0.06570268422365189, tv_loss: 0.01741059310734272\n",
      "iteration 572, dc_loss: 0.06570066511631012, tv_loss: 0.017411060631275177\n",
      "iteration 573, dc_loss: 0.06569866091012955, tv_loss: 0.01741119660437107\n",
      "iteration 574, dc_loss: 0.06569670140743256, tv_loss: 0.017411313951015472\n",
      "iteration 575, dc_loss: 0.06569478660821915, tv_loss: 0.017411375418305397\n",
      "iteration 576, dc_loss: 0.06569292396306992, tv_loss: 0.017411790788173676\n",
      "iteration 577, dc_loss: 0.06569109857082367, tv_loss: 0.017412113025784492\n",
      "iteration 578, dc_loss: 0.06568930298089981, tv_loss: 0.017412152141332626\n",
      "iteration 579, dc_loss: 0.06568752974271774, tv_loss: 0.017412004992365837\n",
      "iteration 580, dc_loss: 0.06568574905395508, tv_loss: 0.01741236448287964\n",
      "iteration 581, dc_loss: 0.06568402051925659, tv_loss: 0.017412612214684486\n",
      "iteration 582, dc_loss: 0.06568232923746109, tv_loss: 0.017412636429071426\n",
      "iteration 583, dc_loss: 0.06568069010972977, tv_loss: 0.017412489280104637\n",
      "iteration 584, dc_loss: 0.06567905843257904, tv_loss: 0.017412761226296425\n",
      "iteration 585, dc_loss: 0.0656774714589119, tv_loss: 0.0174128245562315\n",
      "iteration 586, dc_loss: 0.06567588448524475, tv_loss: 0.017412886023521423\n",
      "iteration 587, dc_loss: 0.06567434966564178, tv_loss: 0.017413360998034477\n",
      "iteration 588, dc_loss: 0.06567282229661942, tv_loss: 0.017413269728422165\n",
      "iteration 589, dc_loss: 0.06567132472991943, tv_loss: 0.01741335354745388\n",
      "iteration 590, dc_loss: 0.06566984951496124, tv_loss: 0.017413465306162834\n",
      "iteration 591, dc_loss: 0.06566839665174484, tv_loss: 0.017413614317774773\n",
      "iteration 592, dc_loss: 0.06566698104143143, tv_loss: 0.01741376705467701\n",
      "iteration 593, dc_loss: 0.06566561013460159, tv_loss: 0.017414003610610962\n",
      "iteration 594, dc_loss: 0.06566426157951355, tv_loss: 0.017414120957255363\n",
      "iteration 595, dc_loss: 0.0656629353761673, tv_loss: 0.017414003610610962\n",
      "iteration 596, dc_loss: 0.06566161662340164, tv_loss: 0.017414232715964317\n",
      "iteration 597, dc_loss: 0.06566032022237778, tv_loss: 0.017414458096027374\n",
      "iteration 598, dc_loss: 0.06565901637077332, tv_loss: 0.017414063215255737\n",
      "iteration 599, dc_loss: 0.06565776467323303, tv_loss: 0.0174140352755785\n",
      "iteration 600, dc_loss: 0.06565651297569275, tv_loss: 0.01741465926170349\n",
      "iteration 601, dc_loss: 0.06565529853105545, tv_loss: 0.017414866015315056\n",
      "iteration 602, dc_loss: 0.06565415114164352, tv_loss: 0.017414743080735207\n",
      "iteration 603, dc_loss: 0.06565302610397339, tv_loss: 0.017415007576346397\n",
      "iteration 604, dc_loss: 0.06565187871456146, tv_loss: 0.017414823174476624\n",
      "iteration 605, dc_loss: 0.06565073132514954, tv_loss: 0.01741519570350647\n",
      "iteration 606, dc_loss: 0.0656496211886406, tv_loss: 0.017415424808859825\n",
      "iteration 607, dc_loss: 0.06564851850271225, tv_loss: 0.0174154844135046\n",
      "iteration 608, dc_loss: 0.06564745306968689, tv_loss: 0.01741553470492363\n",
      "iteration 609, dc_loss: 0.06564640253782272, tv_loss: 0.017415586858987808\n",
      "iteration 610, dc_loss: 0.06564538180828094, tv_loss: 0.01741575077176094\n",
      "iteration 611, dc_loss: 0.06564441323280334, tv_loss: 0.017415739595890045\n",
      "iteration 612, dc_loss: 0.06564345955848694, tv_loss: 0.017415648326277733\n",
      "iteration 613, dc_loss: 0.06564248353242874, tv_loss: 0.017416037619113922\n",
      "iteration 614, dc_loss: 0.06564147770404816, tv_loss: 0.01741599477827549\n",
      "iteration 615, dc_loss: 0.06564049422740936, tv_loss: 0.01741611212491989\n",
      "iteration 616, dc_loss: 0.06563955545425415, tv_loss: 0.017416290938854218\n",
      "iteration 617, dc_loss: 0.06563869118690491, tv_loss: 0.01741640828549862\n",
      "iteration 618, dc_loss: 0.06563784182071686, tv_loss: 0.017416473478078842\n",
      "iteration 619, dc_loss: 0.0656369999051094, tv_loss: 0.017416667193174362\n",
      "iteration 620, dc_loss: 0.06563612818717957, tv_loss: 0.017416616901755333\n",
      "iteration 621, dc_loss: 0.06563524156808853, tv_loss: 0.017416570335626602\n",
      "iteration 622, dc_loss: 0.06563439220190048, tv_loss: 0.01741735264658928\n",
      "iteration 623, dc_loss: 0.06563358753919601, tv_loss: 0.017417289316654205\n",
      "iteration 624, dc_loss: 0.06563280522823334, tv_loss: 0.01741698570549488\n",
      "iteration 625, dc_loss: 0.06563203036785126, tv_loss: 0.017416995018720627\n",
      "iteration 626, dc_loss: 0.06563129276037216, tv_loss: 0.017417630180716515\n",
      "iteration 627, dc_loss: 0.06563055515289307, tv_loss: 0.017417704686522484\n",
      "iteration 628, dc_loss: 0.06562978029251099, tv_loss: 0.017417430877685547\n",
      "iteration 629, dc_loss: 0.06562899798154831, tv_loss: 0.01741756685078144\n",
      "iteration 630, dc_loss: 0.06562826037406921, tv_loss: 0.01741820015013218\n",
      "iteration 631, dc_loss: 0.0656275674700737, tv_loss: 0.01741815358400345\n",
      "iteration 632, dc_loss: 0.06562691926956177, tv_loss: 0.01741783134639263\n",
      "iteration 633, dc_loss: 0.06562625616788864, tv_loss: 0.017417987808585167\n",
      "iteration 634, dc_loss: 0.06562559306621552, tv_loss: 0.017418358474969864\n",
      "iteration 635, dc_loss: 0.06562492996454239, tv_loss: 0.017418546602129936\n",
      "iteration 636, dc_loss: 0.06562427431344986, tv_loss: 0.01741827465593815\n",
      "iteration 637, dc_loss: 0.06562365591526031, tv_loss: 0.017418302595615387\n",
      "iteration 638, dc_loss: 0.06562303006649017, tv_loss: 0.01741856150329113\n",
      "iteration 639, dc_loss: 0.06562240421772003, tv_loss: 0.01741868071258068\n",
      "iteration 640, dc_loss: 0.06562177836894989, tv_loss: 0.01741846278309822\n",
      "iteration 641, dc_loss: 0.06562118977308273, tv_loss: 0.017418768256902695\n",
      "iteration 642, dc_loss: 0.06562064588069916, tv_loss: 0.017418786883354187\n",
      "iteration 643, dc_loss: 0.06562012434005737, tv_loss: 0.017418965697288513\n",
      "iteration 644, dc_loss: 0.0656195804476738, tv_loss: 0.017419051378965378\n",
      "iteration 645, dc_loss: 0.06561901420354843, tv_loss: 0.01741914264857769\n",
      "iteration 646, dc_loss: 0.06561844795942307, tv_loss: 0.017419064417481422\n",
      "iteration 647, dc_loss: 0.06561792641878128, tv_loss: 0.0174191202968359\n",
      "iteration 648, dc_loss: 0.0656173974275589, tv_loss: 0.017419395968317986\n",
      "iteration 649, dc_loss: 0.06561686098575592, tv_loss: 0.017419438809156418\n",
      "iteration 650, dc_loss: 0.06561636924743652, tv_loss: 0.01741943135857582\n",
      "iteration 651, dc_loss: 0.0656159296631813, tv_loss: 0.017419693991541862\n",
      "iteration 652, dc_loss: 0.06561548262834549, tv_loss: 0.017419563606381416\n",
      "iteration 653, dc_loss: 0.06561500579118729, tv_loss: 0.017419397830963135\n",
      "iteration 654, dc_loss: 0.06561452150344849, tv_loss: 0.01741967350244522\n",
      "iteration 655, dc_loss: 0.06561406701803207, tv_loss: 0.017419926822185516\n",
      "iteration 656, dc_loss: 0.06561359763145447, tv_loss: 0.01741955056786537\n",
      "iteration 657, dc_loss: 0.06561313569545746, tv_loss: 0.017419802024960518\n",
      "iteration 658, dc_loss: 0.06561270356178284, tv_loss: 0.017420044168829918\n",
      "iteration 659, dc_loss: 0.0656123086810112, tv_loss: 0.017419669777154922\n",
      "iteration 660, dc_loss: 0.06561192125082016, tv_loss: 0.017419788986444473\n",
      "iteration 661, dc_loss: 0.06561152637004852, tv_loss: 0.017420202493667603\n",
      "iteration 662, dc_loss: 0.06561112403869629, tv_loss: 0.01742016337811947\n",
      "iteration 663, dc_loss: 0.06561070680618286, tv_loss: 0.017420168966054916\n",
      "iteration 664, dc_loss: 0.06561029702425003, tv_loss: 0.017420215532183647\n",
      "iteration 665, dc_loss: 0.06560990959405899, tv_loss: 0.0174204520881176\n",
      "iteration 666, dc_loss: 0.06560956686735153, tv_loss: 0.017420589923858643\n",
      "iteration 667, dc_loss: 0.06560924649238586, tv_loss: 0.01742037758231163\n",
      "iteration 668, dc_loss: 0.06560888141393661, tv_loss: 0.017420347779989243\n",
      "iteration 669, dc_loss: 0.06560850143432617, tv_loss: 0.017420612275600433\n",
      "iteration 670, dc_loss: 0.06560809910297394, tv_loss: 0.017420697957277298\n",
      "iteration 671, dc_loss: 0.06560775637626648, tv_loss: 0.01742083951830864\n",
      "iteration 672, dc_loss: 0.06560743600130081, tv_loss: 0.017420632764697075\n",
      "iteration 673, dc_loss: 0.06560716778039932, tv_loss: 0.01742078922688961\n",
      "iteration 674, dc_loss: 0.06560688465833664, tv_loss: 0.017420466989278793\n",
      "iteration 675, dc_loss: 0.06560656428337097, tv_loss: 0.017420528456568718\n",
      "iteration 676, dc_loss: 0.06560622900724411, tv_loss: 0.01742107793688774\n",
      "iteration 677, dc_loss: 0.06560589373111725, tv_loss: 0.017421042546629906\n",
      "iteration 678, dc_loss: 0.06560558825731277, tv_loss: 0.017421167343854904\n",
      "iteration 679, dc_loss: 0.0656052827835083, tv_loss: 0.017421279102563858\n",
      "iteration 680, dc_loss: 0.06560500711202621, tv_loss: 0.017421461641788483\n",
      "iteration 681, dc_loss: 0.06560473889112473, tv_loss: 0.017421169206500053\n",
      "iteration 682, dc_loss: 0.06560448557138443, tv_loss: 0.017421092838048935\n",
      "iteration 683, dc_loss: 0.06560425460338593, tv_loss: 0.017421111464500427\n",
      "iteration 684, dc_loss: 0.06560397893190384, tv_loss: 0.017421333119273186\n",
      "iteration 685, dc_loss: 0.06560368835926056, tv_loss: 0.017421647906303406\n",
      "iteration 686, dc_loss: 0.06560340523719788, tv_loss: 0.01742147095501423\n",
      "iteration 687, dc_loss: 0.06560314446687698, tv_loss: 0.01742132194340229\n",
      "iteration 688, dc_loss: 0.06560289114713669, tv_loss: 0.017421625554561615\n",
      "iteration 689, dc_loss: 0.06560265272855759, tv_loss: 0.017421841621398926\n",
      "iteration 690, dc_loss: 0.06560244411230087, tv_loss: 0.01742207072675228\n",
      "iteration 691, dc_loss: 0.06560222059488297, tv_loss: 0.017421869561076164\n",
      "iteration 692, dc_loss: 0.06560198962688446, tv_loss: 0.017421528697013855\n",
      "iteration 693, dc_loss: 0.06560174375772476, tv_loss: 0.01742178574204445\n",
      "iteration 694, dc_loss: 0.06560149043798447, tv_loss: 0.017422091215848923\n",
      "iteration 695, dc_loss: 0.06560128182172775, tv_loss: 0.01742173358798027\n",
      "iteration 696, dc_loss: 0.06560107320547104, tv_loss: 0.017421627417206764\n",
      "iteration 697, dc_loss: 0.06560089439153671, tv_loss: 0.01742202788591385\n",
      "iteration 698, dc_loss: 0.06560070812702179, tv_loss: 0.01742205210030079\n",
      "iteration 699, dc_loss: 0.06560050696134567, tv_loss: 0.017421916127204895\n",
      "iteration 700, dc_loss: 0.06560029089450836, tv_loss: 0.01742193102836609\n",
      "iteration 701, dc_loss: 0.06560008972883224, tv_loss: 0.017422059550881386\n",
      "iteration 702, dc_loss: 0.06559991836547852, tv_loss: 0.017422111704945564\n",
      "iteration 703, dc_loss: 0.06559973955154419, tv_loss: 0.017421981319785118\n",
      "iteration 704, dc_loss: 0.06559953838586807, tv_loss: 0.017421798780560493\n",
      "iteration 705, dc_loss: 0.06559931486845016, tv_loss: 0.01742226630449295\n",
      "iteration 706, dc_loss: 0.06559912115335464, tv_loss: 0.017422322183847427\n",
      "iteration 707, dc_loss: 0.0655989870429039, tv_loss: 0.01742228865623474\n",
      "iteration 708, dc_loss: 0.06559882313013077, tv_loss: 0.017422130331397057\n",
      "iteration 709, dc_loss: 0.06559866666793823, tv_loss: 0.0174221433699131\n",
      "iteration 710, dc_loss: 0.0655984953045845, tv_loss: 0.01742219552397728\n",
      "iteration 711, dc_loss: 0.06559833884239197, tv_loss: 0.017422344535589218\n",
      "iteration 712, dc_loss: 0.06559817492961884, tv_loss: 0.017422644421458244\n",
      "iteration 713, dc_loss: 0.0655980110168457, tv_loss: 0.01742234267294407\n",
      "iteration 714, dc_loss: 0.06559786200523376, tv_loss: 0.017422355711460114\n",
      "iteration 715, dc_loss: 0.06559772044420242, tv_loss: 0.01742267608642578\n",
      "iteration 716, dc_loss: 0.06559757888317108, tv_loss: 0.017422521486878395\n",
      "iteration 717, dc_loss: 0.06559742242097855, tv_loss: 0.017422406002879143\n",
      "iteration 718, dc_loss: 0.06559726595878601, tv_loss: 0.017422594130039215\n",
      "iteration 719, dc_loss: 0.06559710949659348, tv_loss: 0.0174228698015213\n",
      "iteration 720, dc_loss: 0.06559696793556213, tv_loss: 0.017422884702682495\n",
      "iteration 721, dc_loss: 0.06559684872627258, tv_loss: 0.01742260344326496\n",
      "iteration 722, dc_loss: 0.06559676676988602, tv_loss: 0.017422430217266083\n",
      "iteration 723, dc_loss: 0.06559666246175766, tv_loss: 0.017422376200556755\n",
      "iteration 724, dc_loss: 0.06559650599956512, tv_loss: 0.017422551289200783\n",
      "iteration 725, dc_loss: 0.06559635698795319, tv_loss: 0.017422739416360855\n",
      "iteration 726, dc_loss: 0.06559619307518005, tv_loss: 0.01742260530591011\n",
      "iteration 727, dc_loss: 0.0655960813164711, tv_loss: 0.017422618344426155\n",
      "iteration 728, dc_loss: 0.06559597700834274, tv_loss: 0.017422515898942947\n",
      "iteration 729, dc_loss: 0.06559588015079498, tv_loss: 0.017422696575522423\n",
      "iteration 730, dc_loss: 0.06559576839208603, tv_loss: 0.01742260344326496\n",
      "iteration 731, dc_loss: 0.06559568643569946, tv_loss: 0.01742250844836235\n",
      "iteration 732, dc_loss: 0.0655956044793129, tv_loss: 0.01742256060242653\n",
      "iteration 733, dc_loss: 0.06559548527002335, tv_loss: 0.017422685399651527\n",
      "iteration 734, dc_loss: 0.065595343708992, tv_loss: 0.017423052340745926\n",
      "iteration 735, dc_loss: 0.06559519469738007, tv_loss: 0.017423009499907494\n",
      "iteration 736, dc_loss: 0.06559508293867111, tv_loss: 0.017422908917069435\n",
      "iteration 737, dc_loss: 0.06559500843286514, tv_loss: 0.017422867938876152\n",
      "iteration 738, dc_loss: 0.06559495627880096, tv_loss: 0.017423061653971672\n",
      "iteration 739, dc_loss: 0.06559488922357559, tv_loss: 0.01742313988506794\n",
      "iteration 740, dc_loss: 0.06559477746486664, tv_loss: 0.017422806471586227\n",
      "iteration 741, dc_loss: 0.06559465080499649, tv_loss: 0.017422819510102272\n",
      "iteration 742, dc_loss: 0.06559453159570694, tv_loss: 0.017423098906874657\n",
      "iteration 743, dc_loss: 0.06559446454048157, tv_loss: 0.01742304116487503\n",
      "iteration 744, dc_loss: 0.0655943751335144, tv_loss: 0.01742279715836048\n",
      "iteration 745, dc_loss: 0.06559427827596664, tv_loss: 0.017422793433070183\n",
      "iteration 746, dc_loss: 0.06559422612190247, tv_loss: 0.017423180863261223\n",
      "iteration 747, dc_loss: 0.0655941590666771, tv_loss: 0.017422670498490334\n",
      "iteration 748, dc_loss: 0.06559406220912933, tv_loss: 0.017422616481781006\n",
      "iteration 749, dc_loss: 0.06559395045042038, tv_loss: 0.01742308773100376\n",
      "iteration 750, dc_loss: 0.06559387594461441, tv_loss: 0.01742318645119667\n",
      "iteration 751, dc_loss: 0.06559381633996964, tv_loss: 0.017422914505004883\n",
      "iteration 752, dc_loss: 0.06559376418590546, tv_loss: 0.017423219978809357\n",
      "iteration 753, dc_loss: 0.06559368222951889, tv_loss: 0.017423482611775398\n",
      "iteration 754, dc_loss: 0.06559358537197113, tv_loss: 0.017423385754227638\n",
      "iteration 755, dc_loss: 0.06559350341558456, tv_loss: 0.017423266544938087\n",
      "iteration 756, dc_loss: 0.0655934289097786, tv_loss: 0.017423102632164955\n",
      "iteration 757, dc_loss: 0.06559338420629501, tv_loss: 0.01742350123822689\n",
      "iteration 758, dc_loss: 0.06559330970048904, tv_loss: 0.017423391342163086\n",
      "iteration 759, dc_loss: 0.06559324264526367, tv_loss: 0.01742318831384182\n",
      "iteration 760, dc_loss: 0.0655931681394577, tv_loss: 0.017423449084162712\n",
      "iteration 761, dc_loss: 0.06559311598539352, tv_loss: 0.017423590645194054\n",
      "iteration 762, dc_loss: 0.06559307128190994, tv_loss: 0.017423391342163086\n",
      "iteration 763, dc_loss: 0.06559303402900696, tv_loss: 0.01742335595190525\n",
      "iteration 764, dc_loss: 0.06559295952320099, tv_loss: 0.017423832789063454\n",
      "iteration 765, dc_loss: 0.06559287011623383, tv_loss: 0.017423486337065697\n",
      "iteration 766, dc_loss: 0.06559278815984726, tv_loss: 0.017423484474420547\n",
      "iteration 767, dc_loss: 0.06559272855520248, tv_loss: 0.017423752695322037\n",
      "iteration 768, dc_loss: 0.06559266895055771, tv_loss: 0.017423909157514572\n",
      "iteration 769, dc_loss: 0.06559263914823532, tv_loss: 0.01742371916770935\n",
      "iteration 770, dc_loss: 0.06559262424707413, tv_loss: 0.017423737794160843\n",
      "iteration 771, dc_loss: 0.06559259444475174, tv_loss: 0.017423968762159348\n",
      "iteration 772, dc_loss: 0.06559252738952637, tv_loss: 0.017423825338482857\n",
      "iteration 773, dc_loss: 0.065592460334301, tv_loss: 0.017423531040549278\n",
      "iteration 774, dc_loss: 0.06559238582849503, tv_loss: 0.0174238458275795\n",
      "iteration 775, dc_loss: 0.06559229642152786, tv_loss: 0.017424365505576134\n",
      "iteration 776, dc_loss: 0.06559222936630249, tv_loss: 0.017424067482352257\n",
      "iteration 777, dc_loss: 0.0655922144651413, tv_loss: 0.01742362417280674\n",
      "iteration 778, dc_loss: 0.0655921995639801, tv_loss: 0.017423825338482857\n",
      "iteration 779, dc_loss: 0.0655921995639801, tv_loss: 0.017424020916223526\n",
      "iteration 780, dc_loss: 0.06559213250875473, tv_loss: 0.01742393895983696\n",
      "iteration 781, dc_loss: 0.06559208035469055, tv_loss: 0.01742376945912838\n",
      "iteration 782, dc_loss: 0.06559202820062637, tv_loss: 0.017423830926418304\n",
      "iteration 783, dc_loss: 0.065591961145401, tv_loss: 0.017423924058675766\n",
      "iteration 784, dc_loss: 0.06559187918901443, tv_loss: 0.0174242053180933\n",
      "iteration 785, dc_loss: 0.06559184193611145, tv_loss: 0.017424285411834717\n",
      "iteration 786, dc_loss: 0.06559183448553085, tv_loss: 0.01742387004196644\n",
      "iteration 787, dc_loss: 0.06559183448553085, tv_loss: 0.01742366887629032\n",
      "iteration 788, dc_loss: 0.06559182703495026, tv_loss: 0.017424283549189568\n",
      "iteration 789, dc_loss: 0.06559175252914429, tv_loss: 0.01742396503686905\n",
      "iteration 790, dc_loss: 0.06559166312217712, tv_loss: 0.017423950135707855\n",
      "iteration 791, dc_loss: 0.06559161096811295, tv_loss: 0.017424004152417183\n",
      "iteration 792, dc_loss: 0.06559160351753235, tv_loss: 0.017424363642930984\n",
      "iteration 793, dc_loss: 0.06559160351753235, tv_loss: 0.017424557358026505\n",
      "iteration 794, dc_loss: 0.06559158861637115, tv_loss: 0.01742413081228733\n",
      "iteration 795, dc_loss: 0.06559157371520996, tv_loss: 0.0174242090433836\n",
      "iteration 796, dc_loss: 0.06559149920940399, tv_loss: 0.0174243226647377\n",
      "iteration 797, dc_loss: 0.06559144705533981, tv_loss: 0.017424417659640312\n",
      "iteration 798, dc_loss: 0.06559139490127563, tv_loss: 0.017424216493964195\n",
      "iteration 799, dc_loss: 0.06559135019779205, tv_loss: 0.01742394082248211\n",
      "iteration 800, dc_loss: 0.06559131294488907, tv_loss: 0.017424236983060837\n",
      "iteration 801, dc_loss: 0.06559130549430847, tv_loss: 0.01742432452738285\n",
      "iteration 802, dc_loss: 0.06559130549430847, tv_loss: 0.017424097284674644\n",
      "iteration 803, dc_loss: 0.06559130549430847, tv_loss: 0.017423659563064575\n",
      "iteration 804, dc_loss: 0.0655912458896637, tv_loss: 0.017424168065190315\n",
      "iteration 805, dc_loss: 0.06559117883443832, tv_loss: 0.017424382269382477\n",
      "iteration 806, dc_loss: 0.06559117138385773, tv_loss: 0.017424259334802628\n",
      "iteration 807, dc_loss: 0.06559116393327713, tv_loss: 0.017423780634999275\n",
      "iteration 808, dc_loss: 0.06559111922979355, tv_loss: 0.017423978075385094\n",
      "iteration 809, dc_loss: 0.06559108197689056, tv_loss: 0.017424408346414566\n",
      "iteration 810, dc_loss: 0.06559103727340698, tv_loss: 0.01742434687912464\n",
      "iteration 811, dc_loss: 0.06559103727340698, tv_loss: 0.017424078658223152\n",
      "iteration 812, dc_loss: 0.06559101492166519, tv_loss: 0.017424143850803375\n",
      "iteration 813, dc_loss: 0.065591000020504, tv_loss: 0.017424196004867554\n",
      "iteration 814, dc_loss: 0.06559097021818161, tv_loss: 0.017423946410417557\n",
      "iteration 815, dc_loss: 0.06559094041585922, tv_loss: 0.017423829063773155\n",
      "iteration 816, dc_loss: 0.06559092551469803, tv_loss: 0.017424220219254494\n",
      "iteration 817, dc_loss: 0.06559090316295624, tv_loss: 0.017424030229449272\n",
      "iteration 818, dc_loss: 0.06559088081121445, tv_loss: 0.017423680052161217\n",
      "iteration 819, dc_loss: 0.06559085100889206, tv_loss: 0.01742413640022278\n",
      "iteration 820, dc_loss: 0.06559079885482788, tv_loss: 0.017423834651708603\n",
      "iteration 821, dc_loss: 0.06559079885482788, tv_loss: 0.017424073070287704\n",
      "iteration 822, dc_loss: 0.06559081375598907, tv_loss: 0.01742417737841606\n",
      "iteration 823, dc_loss: 0.06559080630540848, tv_loss: 0.017423909157514572\n",
      "iteration 824, dc_loss: 0.0655907616019249, tv_loss: 0.017423927783966064\n",
      "iteration 825, dc_loss: 0.06559070944786072, tv_loss: 0.01742403395473957\n",
      "iteration 826, dc_loss: 0.06559066474437714, tv_loss: 0.017424212768673897\n",
      "iteration 827, dc_loss: 0.06559068709611893, tv_loss: 0.0174240805208683\n",
      "iteration 828, dc_loss: 0.06559067219495773, tv_loss: 0.017424270510673523\n",
      "iteration 829, dc_loss: 0.06559064984321594, tv_loss: 0.017424380406737328\n",
      "iteration 830, dc_loss: 0.06559061259031296, tv_loss: 0.01742427609860897\n",
      "iteration 831, dc_loss: 0.06559059023857117, tv_loss: 0.01742454059422016\n",
      "iteration 832, dc_loss: 0.06559057533740997, tv_loss: 0.017424451187253\n",
      "iteration 833, dc_loss: 0.06559058278799057, tv_loss: 0.017424151301383972\n",
      "iteration 834, dc_loss: 0.06559058278799057, tv_loss: 0.01742430217564106\n",
      "iteration 835, dc_loss: 0.06559057533740997, tv_loss: 0.017424458637833595\n",
      "iteration 836, dc_loss: 0.06559055298566818, tv_loss: 0.01742447167634964\n",
      "iteration 837, dc_loss: 0.0655905157327652, tv_loss: 0.017424628138542175\n",
      "iteration 838, dc_loss: 0.06559047102928162, tv_loss: 0.017424680292606354\n",
      "iteration 839, dc_loss: 0.06559044867753983, tv_loss: 0.017424656078219414\n",
      "iteration 840, dc_loss: 0.06559043377637863, tv_loss: 0.017424989491701126\n",
      "iteration 841, dc_loss: 0.06559043377637863, tv_loss: 0.01742476224899292\n",
      "iteration 842, dc_loss: 0.06559045612812042, tv_loss: 0.017424583435058594\n",
      "iteration 843, dc_loss: 0.06559045612812042, tv_loss: 0.017424670979380608\n",
      "iteration 844, dc_loss: 0.06559041887521744, tv_loss: 0.01742478273808956\n",
      "iteration 845, dc_loss: 0.06559037417173386, tv_loss: 0.017424819990992546\n",
      "iteration 846, dc_loss: 0.06559035927057266, tv_loss: 0.017424505203962326\n",
      "iteration 847, dc_loss: 0.06559035927057266, tv_loss: 0.01742452383041382\n",
      "iteration 848, dc_loss: 0.06559034436941147, tv_loss: 0.01742481254041195\n",
      "iteration 849, dc_loss: 0.06559032201766968, tv_loss: 0.017424695193767548\n",
      "iteration 850, dc_loss: 0.06559030711650848, tv_loss: 0.017424212768673897\n",
      "iteration 851, dc_loss: 0.06559032946825027, tv_loss: 0.01742497831583023\n",
      "iteration 852, dc_loss: 0.06559030711650848, tv_loss: 0.017425034195184708\n",
      "iteration 853, dc_loss: 0.0655902773141861, tv_loss: 0.01742456667125225\n",
      "iteration 854, dc_loss: 0.06559024751186371, tv_loss: 0.01742464490234852\n",
      "iteration 855, dc_loss: 0.06559023261070251, tv_loss: 0.01742492988705635\n",
      "iteration 856, dc_loss: 0.06559023261070251, tv_loss: 0.01742490567266941\n",
      "iteration 857, dc_loss: 0.06559024006128311, tv_loss: 0.017424535006284714\n",
      "iteration 858, dc_loss: 0.0655902698636055, tv_loss: 0.017424795776605606\n",
      "iteration 859, dc_loss: 0.06559024751186371, tv_loss: 0.017424805089831352\n",
      "iteration 860, dc_loss: 0.06559019535779953, tv_loss: 0.017424767836928368\n",
      "iteration 861, dc_loss: 0.06559014320373535, tv_loss: 0.017424583435058594\n",
      "iteration 862, dc_loss: 0.06559013575315475, tv_loss: 0.01742488518357277\n",
      "iteration 863, dc_loss: 0.06559014320373535, tv_loss: 0.017424805089831352\n",
      "iteration 864, dc_loss: 0.06559016555547714, tv_loss: 0.0174244474619627\n",
      "iteration 865, dc_loss: 0.06559017300605774, tv_loss: 0.017424680292606354\n",
      "iteration 866, dc_loss: 0.06559017300605774, tv_loss: 0.01742488145828247\n",
      "iteration 867, dc_loss: 0.06559017300605774, tv_loss: 0.017424849793314934\n",
      "iteration 868, dc_loss: 0.06559012830257416, tv_loss: 0.017424505203962326\n",
      "iteration 869, dc_loss: 0.06559011340141296, tv_loss: 0.017424462363123894\n",
      "iteration 870, dc_loss: 0.06559009104967117, tv_loss: 0.01742500439286232\n",
      "iteration 871, dc_loss: 0.06559006869792938, tv_loss: 0.017425047233700752\n",
      "iteration 872, dc_loss: 0.06559006869792938, tv_loss: 0.01742471009492874\n",
      "iteration 873, dc_loss: 0.06559006869792938, tv_loss: 0.0174244474619627\n",
      "iteration 874, dc_loss: 0.06559004634618759, tv_loss: 0.01742459461092949\n",
      "iteration 875, dc_loss: 0.06559005379676819, tv_loss: 0.017424581572413445\n",
      "iteration 876, dc_loss: 0.06559007614850998, tv_loss: 0.01742451824247837\n",
      "iteration 877, dc_loss: 0.06559006869792938, tv_loss: 0.01742447167634964\n",
      "iteration 878, dc_loss: 0.06559004634618759, tv_loss: 0.017424385994672775\n",
      "iteration 879, dc_loss: 0.0655900090932846, tv_loss: 0.0174246933311224\n",
      "iteration 880, dc_loss: 0.06558998674154282, tv_loss: 0.01742469146847725\n",
      "iteration 881, dc_loss: 0.06558997184038162, tv_loss: 0.01742422953248024\n",
      "iteration 882, dc_loss: 0.0655900090932846, tv_loss: 0.01742435060441494\n",
      "iteration 883, dc_loss: 0.06559006124734879, tv_loss: 0.017424510791897774\n",
      "iteration 884, dc_loss: 0.065590038895607, tv_loss: 0.01742434874176979\n",
      "iteration 885, dc_loss: 0.06558999419212341, tv_loss: 0.017424415796995163\n",
      "iteration 886, dc_loss: 0.06558995693922043, tv_loss: 0.017424512654542923\n",
      "iteration 887, dc_loss: 0.06558995693922043, tv_loss: 0.017424480989575386\n",
      "iteration 888, dc_loss: 0.06558994203805923, tv_loss: 0.0174243301153183\n",
      "iteration 889, dc_loss: 0.06558993458747864, tv_loss: 0.01742435432970524\n",
      "iteration 890, dc_loss: 0.06558994948863983, tv_loss: 0.017424488440155983\n",
      "iteration 891, dc_loss: 0.06558997184038162, tv_loss: 0.017424680292606354\n",
      "iteration 892, dc_loss: 0.06558995693922043, tv_loss: 0.017424803227186203\n",
      "iteration 893, dc_loss: 0.06558995693922043, tv_loss: 0.017424916848540306\n",
      "iteration 894, dc_loss: 0.06558993458747864, tv_loss: 0.01742473989725113\n",
      "iteration 895, dc_loss: 0.06558989733457565, tv_loss: 0.017424551770091057\n",
      "iteration 896, dc_loss: 0.06558988243341446, tv_loss: 0.01742471568286419\n",
      "iteration 897, dc_loss: 0.06558988988399506, tv_loss: 0.017424829304218292\n",
      "iteration 898, dc_loss: 0.06558989733457565, tv_loss: 0.017424551770091057\n",
      "iteration 899, dc_loss: 0.06558988988399506, tv_loss: 0.01742454245686531\n",
      "iteration 900, dc_loss: 0.06558988988399506, tv_loss: 0.017424650490283966\n",
      "iteration 901, dc_loss: 0.06558988243341446, tv_loss: 0.017424659803509712\n",
      "iteration 902, dc_loss: 0.06558986753225327, tv_loss: 0.017424460500478745\n",
      "iteration 903, dc_loss: 0.06558986008167267, tv_loss: 0.01742442511022091\n",
      "iteration 904, dc_loss: 0.06558984518051147, tv_loss: 0.017424535006284714\n",
      "iteration 905, dc_loss: 0.06558986008167267, tv_loss: 0.01742471009492874\n",
      "iteration 906, dc_loss: 0.06558986753225327, tv_loss: 0.017424505203962326\n",
      "iteration 907, dc_loss: 0.06558990478515625, tv_loss: 0.017424017190933228\n",
      "iteration 908, dc_loss: 0.06558988988399506, tv_loss: 0.01742415316402912\n",
      "iteration 909, dc_loss: 0.06558983772993088, tv_loss: 0.017424726858735085\n",
      "iteration 910, dc_loss: 0.0655897855758667, tv_loss: 0.017424380406737328\n",
      "iteration 911, dc_loss: 0.0655897706747055, tv_loss: 0.017423948273062706\n",
      "iteration 912, dc_loss: 0.0655898004770279, tv_loss: 0.0174243226647377\n",
      "iteration 913, dc_loss: 0.06558983772993088, tv_loss: 0.017424382269382477\n",
      "iteration 914, dc_loss: 0.06558986753225327, tv_loss: 0.017424188554286957\n",
      "iteration 915, dc_loss: 0.06558986008167267, tv_loss: 0.017423825338482857\n",
      "iteration 916, dc_loss: 0.06558983027935028, tv_loss: 0.01742386631667614\n",
      "iteration 917, dc_loss: 0.0655897930264473, tv_loss: 0.01742420718073845\n",
      "iteration 918, dc_loss: 0.0655897855758667, tv_loss: 0.017424557358026505\n",
      "iteration 919, dc_loss: 0.0655897706747055, tv_loss: 0.01742446795105934\n",
      "iteration 920, dc_loss: 0.06558976322412491, tv_loss: 0.017424272373318672\n",
      "iteration 921, dc_loss: 0.0655897855758667, tv_loss: 0.017424140125513077\n",
      "iteration 922, dc_loss: 0.0655898004770279, tv_loss: 0.01742427609860897\n",
      "iteration 923, dc_loss: 0.06558983027935028, tv_loss: 0.017424454912543297\n",
      "iteration 924, dc_loss: 0.06558980792760849, tv_loss: 0.017424505203962326\n",
      "iteration 925, dc_loss: 0.0655897706747055, tv_loss: 0.017424456775188446\n",
      "iteration 926, dc_loss: 0.06558975577354431, tv_loss: 0.01742462068796158\n",
      "iteration 927, dc_loss: 0.06558974087238312, tv_loss: 0.017424529418349266\n",
      "iteration 928, dc_loss: 0.06558974832296371, tv_loss: 0.017424343153834343\n",
      "iteration 929, dc_loss: 0.06558974087238312, tv_loss: 0.017424415796995163\n",
      "iteration 930, dc_loss: 0.06558976322412491, tv_loss: 0.01742449216544628\n",
      "iteration 931, dc_loss: 0.0655897781252861, tv_loss: 0.017424581572413445\n",
      "iteration 932, dc_loss: 0.06558980792760849, tv_loss: 0.01742430217564106\n",
      "iteration 933, dc_loss: 0.0655897855758667, tv_loss: 0.017424440011382103\n",
      "iteration 934, dc_loss: 0.06558974087238312, tv_loss: 0.01742441952228546\n",
      "iteration 935, dc_loss: 0.06558970361948013, tv_loss: 0.017424624413251877\n",
      "iteration 936, dc_loss: 0.06558968126773834, tv_loss: 0.017424723133444786\n",
      "iteration 937, dc_loss: 0.06558969616889954, tv_loss: 0.01742435060441494\n",
      "iteration 938, dc_loss: 0.06558971852064133, tv_loss: 0.01742459461092949\n",
      "iteration 939, dc_loss: 0.06558974832296371, tv_loss: 0.0174244474619627\n",
      "iteration 940, dc_loss: 0.06558974087238312, tv_loss: 0.01742417924106121\n",
      "iteration 941, dc_loss: 0.06558974087238312, tv_loss: 0.01742439717054367\n",
      "iteration 942, dc_loss: 0.06558974832296371, tv_loss: 0.01742473617196083\n",
      "iteration 943, dc_loss: 0.06558971852064133, tv_loss: 0.017424821853637695\n",
      "iteration 944, dc_loss: 0.06558967381715775, tv_loss: 0.017424358054995537\n",
      "iteration 945, dc_loss: 0.06558965146541595, tv_loss: 0.01742422580718994\n",
      "iteration 946, dc_loss: 0.06558968126773834, tv_loss: 0.017424924299120903\n",
      "iteration 947, dc_loss: 0.06558972597122192, tv_loss: 0.01742476597428322\n",
      "iteration 948, dc_loss: 0.06558974087238312, tv_loss: 0.01742420345544815\n",
      "iteration 949, dc_loss: 0.06558971852064133, tv_loss: 0.01742430031299591\n",
      "iteration 950, dc_loss: 0.06558968871831894, tv_loss: 0.01742452196776867\n",
      "iteration 951, dc_loss: 0.06558966636657715, tv_loss: 0.01742454059422016\n",
      "iteration 952, dc_loss: 0.06558965891599655, tv_loss: 0.01742463931441307\n",
      "iteration 953, dc_loss: 0.06558967381715775, tv_loss: 0.017424382269382477\n",
      "iteration 954, dc_loss: 0.06558970361948013, tv_loss: 0.01742406003177166\n",
      "iteration 955, dc_loss: 0.06558971852064133, tv_loss: 0.017424630001187325\n",
      "iteration 956, dc_loss: 0.06558970361948013, tv_loss: 0.017424557358026505\n",
      "iteration 957, dc_loss: 0.06558968871831894, tv_loss: 0.017424283549189568\n",
      "iteration 958, dc_loss: 0.06558966636657715, tv_loss: 0.017424415796995163\n",
      "iteration 959, dc_loss: 0.06558964401483536, tv_loss: 0.01742480881512165\n",
      "iteration 960, dc_loss: 0.06558965146541595, tv_loss: 0.01742476038634777\n",
      "iteration 961, dc_loss: 0.06558966636657715, tv_loss: 0.017424479126930237\n",
      "iteration 962, dc_loss: 0.06558970361948013, tv_loss: 0.017424682155251503\n",
      "iteration 963, dc_loss: 0.06558971107006073, tv_loss: 0.0174249280244112\n",
      "iteration 964, dc_loss: 0.06558966636657715, tv_loss: 0.017424780875444412\n",
      "iteration 965, dc_loss: 0.06558962911367416, tv_loss: 0.017424585297703743\n",
      "iteration 966, dc_loss: 0.06558962911367416, tv_loss: 0.01742471568286419\n",
      "iteration 967, dc_loss: 0.06558965146541595, tv_loss: 0.01742488332092762\n",
      "iteration 968, dc_loss: 0.06558967381715775, tv_loss: 0.017424875870347023\n",
      "iteration 969, dc_loss: 0.06558965146541595, tv_loss: 0.017424769699573517\n",
      "iteration 970, dc_loss: 0.06558965146541595, tv_loss: 0.017424875870347023\n",
      "iteration 971, dc_loss: 0.06558964401483536, tv_loss: 0.01742483302950859\n",
      "iteration 972, dc_loss: 0.06558962911367416, tv_loss: 0.01742473803460598\n",
      "iteration 973, dc_loss: 0.06558963656425476, tv_loss: 0.017425043508410454\n",
      "iteration 974, dc_loss: 0.06558964401483536, tv_loss: 0.017424901947379112\n",
      "iteration 975, dc_loss: 0.06558962166309357, tv_loss: 0.017424754798412323\n",
      "iteration 976, dc_loss: 0.06558962166309357, tv_loss: 0.017424840480089188\n",
      "iteration 977, dc_loss: 0.06558965146541595, tv_loss: 0.01742488145828247\n",
      "iteration 978, dc_loss: 0.06558966636657715, tv_loss: 0.017424998804926872\n",
      "iteration 979, dc_loss: 0.06558967381715775, tv_loss: 0.017424490302801132\n",
      "iteration 980, dc_loss: 0.06558964401483536, tv_loss: 0.01742442138493061\n",
      "iteration 981, dc_loss: 0.06558960676193237, tv_loss: 0.01742488145828247\n",
      "iteration 982, dc_loss: 0.06558959931135178, tv_loss: 0.01742485538125038\n",
      "iteration 983, dc_loss: 0.06558959931135178, tv_loss: 0.01742461323738098\n",
      "iteration 984, dc_loss: 0.06558960676193237, tv_loss: 0.017424842342734337\n",
      "iteration 985, dc_loss: 0.06558960676193237, tv_loss: 0.017424756661057472\n",
      "iteration 986, dc_loss: 0.06558965146541595, tv_loss: 0.017424823716282845\n",
      "iteration 987, dc_loss: 0.06558966636657715, tv_loss: 0.01742488145828247\n",
      "iteration 988, dc_loss: 0.06558965891599655, tv_loss: 0.017424888908863068\n",
      "iteration 989, dc_loss: 0.06558963656425476, tv_loss: 0.017424484714865685\n",
      "iteration 990, dc_loss: 0.06558960676193237, tv_loss: 0.017424600198864937\n",
      "iteration 991, dc_loss: 0.06558959931135178, tv_loss: 0.017424874007701874\n",
      "iteration 992, dc_loss: 0.06558960676193237, tv_loss: 0.017424805089831352\n",
      "iteration 993, dc_loss: 0.06558960676193237, tv_loss: 0.017424700781702995\n",
      "iteration 994, dc_loss: 0.06558960676193237, tv_loss: 0.017424577847123146\n",
      "iteration 995, dc_loss: 0.06558962166309357, tv_loss: 0.01742440089583397\n",
      "iteration 996, dc_loss: 0.06558962166309357, tv_loss: 0.01742476597428322\n",
      "iteration 997, dc_loss: 0.06558962911367416, tv_loss: 0.017424849793314934\n",
      "iteration 998, dc_loss: 0.06558961421251297, tv_loss: 0.017424369230866432\n",
      "iteration 999, dc_loss: 0.06558959186077118, tv_loss: 0.01742485538125038\n",
      "iteration 1000, dc_loss: 0.0655895546078682, tv_loss: 0.017424961552023888\n",
      "iteration 1001, dc_loss: 0.06558956205844879, tv_loss: 0.01742451824247837\n",
      "iteration 1002, dc_loss: 0.06558959931135178, tv_loss: 0.017424406483769417\n",
      "iteration 1003, dc_loss: 0.06558962911367416, tv_loss: 0.017424559220671654\n",
      "iteration 1004, dc_loss: 0.06558962911367416, tv_loss: 0.017424775287508965\n",
      "iteration 1005, dc_loss: 0.06558960676193237, tv_loss: 0.0174244474619627\n",
      "iteration 1006, dc_loss: 0.06558957695960999, tv_loss: 0.01742430590093136\n",
      "iteration 1007, dc_loss: 0.06558956950902939, tv_loss: 0.017424670979380608\n",
      "iteration 1008, dc_loss: 0.06558956950902939, tv_loss: 0.017424821853637695\n",
      "iteration 1009, dc_loss: 0.06558958441019058, tv_loss: 0.017424585297703743\n",
      "iteration 1010, dc_loss: 0.06558958441019058, tv_loss: 0.017424507066607475\n",
      "iteration 1011, dc_loss: 0.06558959931135178, tv_loss: 0.017424633726477623\n",
      "iteration 1012, dc_loss: 0.06558961421251297, tv_loss: 0.01742478273808956\n",
      "iteration 1013, dc_loss: 0.06558962166309357, tv_loss: 0.017424795776605606\n",
      "iteration 1014, dc_loss: 0.06558958441019058, tv_loss: 0.01742481254041195\n",
      "iteration 1015, dc_loss: 0.0655895546078682, tv_loss: 0.017424708232283592\n",
      "iteration 1016, dc_loss: 0.06558956950902939, tv_loss: 0.017424888908863068\n",
      "iteration 1017, dc_loss: 0.06558959186077118, tv_loss: 0.01742478273808956\n",
      "iteration 1018, dc_loss: 0.06558959931135178, tv_loss: 0.017424775287508965\n",
      "iteration 1019, dc_loss: 0.06558962166309357, tv_loss: 0.01742466539144516\n",
      "iteration 1020, dc_loss: 0.06558960676193237, tv_loss: 0.01742497831583023\n",
      "iteration 1021, dc_loss: 0.06558956205844879, tv_loss: 0.017424775287508965\n",
      "iteration 1022, dc_loss: 0.06558956205844879, tv_loss: 0.017424969002604485\n",
      "iteration 1023, dc_loss: 0.06558956950902939, tv_loss: 0.017424961552023888\n",
      "iteration 1024, dc_loss: 0.06558957695960999, tv_loss: 0.017424842342734337\n",
      "iteration 1025, dc_loss: 0.06558957695960999, tv_loss: 0.017424779012799263\n",
      "iteration 1026, dc_loss: 0.06558960676193237, tv_loss: 0.01742483302950859\n",
      "iteration 1027, dc_loss: 0.06558962911367416, tv_loss: 0.017424974590539932\n",
      "iteration 1028, dc_loss: 0.06558959186077118, tv_loss: 0.017424726858735085\n",
      "iteration 1029, dc_loss: 0.0655895471572876, tv_loss: 0.017424577847123146\n",
      "iteration 1030, dc_loss: 0.0655895248055458, tv_loss: 0.01742476411163807\n",
      "iteration 1031, dc_loss: 0.0655895471572876, tv_loss: 0.017424918711185455\n",
      "iteration 1032, dc_loss: 0.06558956205844879, tv_loss: 0.017425043508410454\n",
      "iteration 1033, dc_loss: 0.06558960676193237, tv_loss: 0.017424581572413445\n",
      "iteration 1034, dc_loss: 0.06558961421251297, tv_loss: 0.017424697056412697\n",
      "iteration 1035, dc_loss: 0.06558961421251297, tv_loss: 0.017424415796995163\n",
      "iteration 1036, dc_loss: 0.06558957695960999, tv_loss: 0.0174246933311224\n",
      "iteration 1037, dc_loss: 0.06558957695960999, tv_loss: 0.017424730584025383\n",
      "iteration 1038, dc_loss: 0.0655895546078682, tv_loss: 0.01742483489215374\n",
      "iteration 1039, dc_loss: 0.0655895322561264, tv_loss: 0.01742454245686531\n",
      "iteration 1040, dc_loss: 0.0655895546078682, tv_loss: 0.01742442511022091\n",
      "iteration 1041, dc_loss: 0.0655895471572876, tv_loss: 0.017424650490283966\n",
      "iteration 1042, dc_loss: 0.06558956205844879, tv_loss: 0.01742485724389553\n",
      "iteration 1043, dc_loss: 0.06558958441019058, tv_loss: 0.017424795776605606\n",
      "iteration 1044, dc_loss: 0.06558959931135178, tv_loss: 0.01742456667125225\n",
      "iteration 1045, dc_loss: 0.06558958441019058, tv_loss: 0.017424602061510086\n",
      "iteration 1046, dc_loss: 0.06558956205844879, tv_loss: 0.01742471382021904\n",
      "iteration 1047, dc_loss: 0.06558956950902939, tv_loss: 0.01742462068796158\n",
      "iteration 1048, dc_loss: 0.06558956205844879, tv_loss: 0.017424508929252625\n",
      "iteration 1049, dc_loss: 0.0655895546078682, tv_loss: 0.017424562945961952\n",
      "iteration 1050, dc_loss: 0.065589539706707, tv_loss: 0.01742454059422016\n",
      "iteration 1051, dc_loss: 0.06558956950902939, tv_loss: 0.0174246858805418\n",
      "iteration 1052, dc_loss: 0.06558958441019058, tv_loss: 0.01742446795105934\n",
      "iteration 1053, dc_loss: 0.06558957695960999, tv_loss: 0.01742440275847912\n",
      "iteration 1054, dc_loss: 0.06558956205844879, tv_loss: 0.017424684017896652\n",
      "iteration 1055, dc_loss: 0.0655895471572876, tv_loss: 0.017424851655960083\n",
      "iteration 1056, dc_loss: 0.0655895546078682, tv_loss: 0.017424536868929863\n",
      "iteration 1057, dc_loss: 0.0655895546078682, tv_loss: 0.01742418296635151\n",
      "iteration 1058, dc_loss: 0.06558957695960999, tv_loss: 0.01742449775338173\n",
      "iteration 1059, dc_loss: 0.06558956205844879, tv_loss: 0.017424767836928368\n",
      "iteration 1060, dc_loss: 0.0655895471572876, tv_loss: 0.01742473803460598\n",
      "iteration 1061, dc_loss: 0.0655895471572876, tv_loss: 0.0174245685338974\n",
      "iteration 1062, dc_loss: 0.0655895546078682, tv_loss: 0.017424805089831352\n",
      "iteration 1063, dc_loss: 0.0655895546078682, tv_loss: 0.017424749210476875\n",
      "iteration 1064, dc_loss: 0.0655895322561264, tv_loss: 0.0174244474619627\n",
      "iteration 1065, dc_loss: 0.0655895322561264, tv_loss: 0.017424749210476875\n",
      "iteration 1066, dc_loss: 0.0655895471572876, tv_loss: 0.0174246933311224\n",
      "iteration 1067, dc_loss: 0.06558959931135178, tv_loss: 0.01742427796125412\n",
      "iteration 1068, dc_loss: 0.06558959931135178, tv_loss: 0.01742422953248024\n",
      "iteration 1069, dc_loss: 0.06558958441019058, tv_loss: 0.0174246896058321\n",
      "iteration 1070, dc_loss: 0.0655895546078682, tv_loss: 0.0174246933311224\n",
      "iteration 1071, dc_loss: 0.06558951735496521, tv_loss: 0.017424434423446655\n",
      "iteration 1072, dc_loss: 0.06558951735496521, tv_loss: 0.017424270510673523\n",
      "iteration 1073, dc_loss: 0.0655895471572876, tv_loss: 0.017424432560801506\n",
      "iteration 1074, dc_loss: 0.06558957695960999, tv_loss: 0.01742457039654255\n",
      "iteration 1075, dc_loss: 0.06558959931135178, tv_loss: 0.017424531280994415\n",
      "iteration 1076, dc_loss: 0.06558957695960999, tv_loss: 0.01742437668144703\n",
      "iteration 1077, dc_loss: 0.06558956205844879, tv_loss: 0.017424391582608223\n",
      "iteration 1078, dc_loss: 0.06558950990438461, tv_loss: 0.017424453049898148\n",
      "iteration 1079, dc_loss: 0.06558950245380402, tv_loss: 0.017424587160348892\n",
      "iteration 1080, dc_loss: 0.06558951735496521, tv_loss: 0.017424630001187325\n",
      "iteration 1081, dc_loss: 0.0655895471572876, tv_loss: 0.01742459088563919\n",
      "iteration 1082, dc_loss: 0.06558958441019058, tv_loss: 0.017424417659640312\n",
      "iteration 1083, dc_loss: 0.06558959931135178, tv_loss: 0.017424464225769043\n",
      "iteration 1084, dc_loss: 0.06558956205844879, tv_loss: 0.01742437295615673\n",
      "iteration 1085, dc_loss: 0.0655895248055458, tv_loss: 0.017424283549189568\n",
      "iteration 1086, dc_loss: 0.06558950990438461, tv_loss: 0.017424866557121277\n",
      "iteration 1087, dc_loss: 0.0655895248055458, tv_loss: 0.017424458637833595\n",
      "iteration 1088, dc_loss: 0.0655895322561264, tv_loss: 0.01742411032319069\n",
      "iteration 1089, dc_loss: 0.0655895248055458, tv_loss: 0.017424635589122772\n",
      "iteration 1090, dc_loss: 0.06558950245380402, tv_loss: 0.017424866557121277\n",
      "iteration 1091, dc_loss: 0.06558950990438461, tv_loss: 0.0174246896058321\n",
      "iteration 1092, dc_loss: 0.0655895471572876, tv_loss: 0.017424054443836212\n",
      "iteration 1093, dc_loss: 0.06558958441019058, tv_loss: 0.01742415316402912\n",
      "iteration 1094, dc_loss: 0.06558959931135178, tv_loss: 0.01742452196776867\n",
      "iteration 1095, dc_loss: 0.0655895546078682, tv_loss: 0.01742447540163994\n",
      "iteration 1096, dc_loss: 0.0655895248055458, tv_loss: 0.017424169927835464\n",
      "iteration 1097, dc_loss: 0.0655895322561264, tv_loss: 0.017424266785383224\n",
      "iteration 1098, dc_loss: 0.065589539706707, tv_loss: 0.017424626275897026\n",
      "iteration 1099, dc_loss: 0.0655895322561264, tv_loss: 0.01742427423596382\n",
      "iteration 1100, dc_loss: 0.0655895248055458, tv_loss: 0.01742429845035076\n",
      "iteration 1101, dc_loss: 0.0655895322561264, tv_loss: 0.01742461696267128\n",
      "iteration 1102, dc_loss: 0.0655895471572876, tv_loss: 0.017424434423446655\n",
      "iteration 1103, dc_loss: 0.06558956205844879, tv_loss: 0.017424412071704865\n",
      "iteration 1104, dc_loss: 0.0655895471572876, tv_loss: 0.017424708232283592\n",
      "iteration 1105, dc_loss: 0.0655895248055458, tv_loss: 0.01742459461092949\n",
      "iteration 1106, dc_loss: 0.0655895248055458, tv_loss: 0.017424561083316803\n",
      "iteration 1107, dc_loss: 0.06558956205844879, tv_loss: 0.01742452196776867\n",
      "iteration 1108, dc_loss: 0.06558956205844879, tv_loss: 0.01742430590093136\n",
      "iteration 1109, dc_loss: 0.0655895248055458, tv_loss: 0.01742466166615486\n",
      "iteration 1110, dc_loss: 0.06558950990438461, tv_loss: 0.017424670979380608\n",
      "iteration 1111, dc_loss: 0.06558950990438461, tv_loss: 0.017424480989575386\n",
      "iteration 1112, dc_loss: 0.0655895322561264, tv_loss: 0.01742437295615673\n",
      "iteration 1113, dc_loss: 0.06558950990438461, tv_loss: 0.017424514517188072\n",
      "iteration 1114, dc_loss: 0.0655895248055458, tv_loss: 0.01742464490234852\n",
      "iteration 1115, dc_loss: 0.06558956205844879, tv_loss: 0.01742488518357277\n",
      "iteration 1116, dc_loss: 0.06558956950902939, tv_loss: 0.01742476597428322\n",
      "iteration 1117, dc_loss: 0.06558957695960999, tv_loss: 0.017424385994672775\n",
      "iteration 1118, dc_loss: 0.0655895546078682, tv_loss: 0.01742447167634964\n",
      "iteration 1119, dc_loss: 0.0655895248055458, tv_loss: 0.017424985766410828\n",
      "iteration 1120, dc_loss: 0.06558948010206223, tv_loss: 0.01742476038634777\n",
      "iteration 1121, dc_loss: 0.06558950245380402, tv_loss: 0.01742471568286419\n",
      "iteration 1122, dc_loss: 0.0655895322561264, tv_loss: 0.017424536868929863\n",
      "iteration 1123, dc_loss: 0.06558956950902939, tv_loss: 0.017424700781702995\n",
      "iteration 1124, dc_loss: 0.06558957695960999, tv_loss: 0.01742483861744404\n",
      "iteration 1125, dc_loss: 0.0655895322561264, tv_loss: 0.017424944788217545\n",
      "iteration 1126, dc_loss: 0.06558950245380402, tv_loss: 0.01742473617196083\n",
      "iteration 1127, dc_loss: 0.06558950245380402, tv_loss: 0.017424659803509712\n",
      "iteration 1128, dc_loss: 0.0655895322561264, tv_loss: 0.01742490939795971\n",
      "iteration 1129, dc_loss: 0.0655895546078682, tv_loss: 0.017424337565898895\n",
      "iteration 1130, dc_loss: 0.06558957695960999, tv_loss: 0.017424361780285835\n",
      "iteration 1131, dc_loss: 0.06558957695960999, tv_loss: 0.01742462068796158\n",
      "iteration 1132, dc_loss: 0.0655895546078682, tv_loss: 0.017424454912543297\n",
      "iteration 1133, dc_loss: 0.0655895322561264, tv_loss: 0.017424600198864937\n",
      "iteration 1134, dc_loss: 0.06558950990438461, tv_loss: 0.017424868419766426\n",
      "iteration 1135, dc_loss: 0.06558950990438461, tv_loss: 0.017424743622541428\n",
      "iteration 1136, dc_loss: 0.0655895248055458, tv_loss: 0.017424415796995163\n",
      "iteration 1137, dc_loss: 0.0655895471572876, tv_loss: 0.017424684017896652\n",
      "iteration 1138, dc_loss: 0.06558956205844879, tv_loss: 0.017424892634153366\n",
      "iteration 1139, dc_loss: 0.06558956205844879, tv_loss: 0.017424674704670906\n",
      "iteration 1140, dc_loss: 0.0655895248055458, tv_loss: 0.017424805089831352\n",
      "iteration 1141, dc_loss: 0.06558949500322342, tv_loss: 0.0174248144030571\n",
      "iteration 1142, dc_loss: 0.06558950245380402, tv_loss: 0.017424695193767548\n",
      "iteration 1143, dc_loss: 0.0655895322561264, tv_loss: 0.017424823716282845\n",
      "iteration 1144, dc_loss: 0.0655895248055458, tv_loss: 0.017424389719963074\n",
      "iteration 1145, dc_loss: 0.06558951735496521, tv_loss: 0.017424626275897026\n",
      "iteration 1146, dc_loss: 0.0655895248055458, tv_loss: 0.017424894496798515\n",
      "iteration 1147, dc_loss: 0.0655895471572876, tv_loss: 0.017424920573830605\n",
      "iteration 1148, dc_loss: 0.0655895322561264, tv_loss: 0.01742459647357464\n",
      "iteration 1149, dc_loss: 0.06558950990438461, tv_loss: 0.01742459088563919\n",
      "iteration 1150, dc_loss: 0.06558950990438461, tv_loss: 0.01742473430931568\n",
      "iteration 1151, dc_loss: 0.06558950990438461, tv_loss: 0.017424583435058594\n",
      "iteration 1152, dc_loss: 0.0655895322561264, tv_loss: 0.01742471568286419\n",
      "iteration 1153, dc_loss: 0.0655895471572876, tv_loss: 0.01742485538125038\n",
      "iteration 1154, dc_loss: 0.0655895471572876, tv_loss: 0.017424792051315308\n",
      "iteration 1155, dc_loss: 0.0655895248055458, tv_loss: 0.017424514517188072\n",
      "iteration 1156, dc_loss: 0.0655895248055458, tv_loss: 0.01742452196776867\n",
      "iteration 1157, dc_loss: 0.0655895546078682, tv_loss: 0.01742454804480076\n",
      "iteration 1158, dc_loss: 0.0655895546078682, tv_loss: 0.017424743622541428\n",
      "iteration 1159, dc_loss: 0.0655895471572876, tv_loss: 0.017424851655960083\n",
      "iteration 1160, dc_loss: 0.06558950990438461, tv_loss: 0.017424704506993294\n",
      "iteration 1161, dc_loss: 0.06558947265148163, tv_loss: 0.017424389719963074\n",
      "iteration 1162, dc_loss: 0.06558948010206223, tv_loss: 0.017424603924155235\n",
      "iteration 1163, dc_loss: 0.06558951735496521, tv_loss: 0.01742483302950859\n",
      "iteration 1164, dc_loss: 0.0655895546078682, tv_loss: 0.017424695193767548\n",
      "iteration 1165, dc_loss: 0.06558956205844879, tv_loss: 0.017424417659640312\n",
      "iteration 1166, dc_loss: 0.06558957695960999, tv_loss: 0.017424626275897026\n",
      "iteration 1167, dc_loss: 0.0655895546078682, tv_loss: 0.017424646764993668\n",
      "iteration 1168, dc_loss: 0.06558951735496521, tv_loss: 0.01742449961602688\n",
      "iteration 1169, dc_loss: 0.06558948755264282, tv_loss: 0.01742476224899292\n",
      "iteration 1170, dc_loss: 0.06558950245380402, tv_loss: 0.01742487959563732\n",
      "iteration 1171, dc_loss: 0.0655895471572876, tv_loss: 0.01742430403828621\n",
      "iteration 1172, dc_loss: 0.06558957695960999, tv_loss: 0.01742422580718994\n",
      "iteration 1173, dc_loss: 0.06558957695960999, tv_loss: 0.01742473989725113\n",
      "iteration 1174, dc_loss: 0.0655895248055458, tv_loss: 0.017424382269382477\n",
      "iteration 1175, dc_loss: 0.06558949500322342, tv_loss: 0.017424413934350014\n",
      "iteration 1176, dc_loss: 0.06558948010206223, tv_loss: 0.01742459647357464\n",
      "iteration 1177, dc_loss: 0.0655895248055458, tv_loss: 0.017424602061510086\n",
      "iteration 1178, dc_loss: 0.06558957695960999, tv_loss: 0.017424201592803\n",
      "iteration 1179, dc_loss: 0.06558959931135178, tv_loss: 0.017424337565898895\n",
      "iteration 1180, dc_loss: 0.06558956205844879, tv_loss: 0.017424246296286583\n",
      "iteration 1181, dc_loss: 0.06558951735496521, tv_loss: 0.017424462363123894\n",
      "iteration 1182, dc_loss: 0.06558950245380402, tv_loss: 0.01742422953248024\n",
      "iteration 1183, dc_loss: 0.06558950990438461, tv_loss: 0.017424216493964195\n",
      "iteration 1184, dc_loss: 0.0655895248055458, tv_loss: 0.01742442138493061\n",
      "iteration 1185, dc_loss: 0.0655895248055458, tv_loss: 0.01742435246706009\n",
      "iteration 1186, dc_loss: 0.0655895322561264, tv_loss: 0.017424143850803375\n",
      "iteration 1187, dc_loss: 0.06558956950902939, tv_loss: 0.01742417924106121\n",
      "iteration 1188, dc_loss: 0.0655895546078682, tv_loss: 0.0174242090433836\n",
      "iteration 1189, dc_loss: 0.0655895322561264, tv_loss: 0.01742415688931942\n",
      "iteration 1190, dc_loss: 0.06558950245380402, tv_loss: 0.017424410209059715\n",
      "iteration 1191, dc_loss: 0.06558951735496521, tv_loss: 0.01742439530789852\n",
      "iteration 1192, dc_loss: 0.065589539706707, tv_loss: 0.01742423139512539\n",
      "iteration 1193, dc_loss: 0.06558956205844879, tv_loss: 0.01742442138493061\n",
      "iteration 1194, dc_loss: 0.06558956205844879, tv_loss: 0.017424345016479492\n",
      "iteration 1195, dc_loss: 0.0655895471572876, tv_loss: 0.01742415688931942\n",
      "iteration 1196, dc_loss: 0.06558950990438461, tv_loss: 0.017424415796995163\n",
      "iteration 1197, dc_loss: 0.06558948755264282, tv_loss: 0.017424695193767548\n",
      "iteration 1198, dc_loss: 0.06558950990438461, tv_loss: 0.017424337565898895\n",
      "iteration 1199, dc_loss: 0.0655895248055458, tv_loss: 0.017424345016479492\n",
      "iteration 1200, dc_loss: 0.0655895471572876, tv_loss: 0.017424438148736954\n",
      "iteration 1201, dc_loss: 0.06558956205844879, tv_loss: 0.017424285411834717\n",
      "iteration 1202, dc_loss: 0.06558957695960999, tv_loss: 0.017423992976546288\n",
      "iteration 1203, dc_loss: 0.0655895546078682, tv_loss: 0.01742442697286606\n",
      "iteration 1204, dc_loss: 0.06558951735496521, tv_loss: 0.01742483861744404\n",
      "iteration 1205, dc_loss: 0.06558950990438461, tv_loss: 0.017424581572413445\n",
      "iteration 1206, dc_loss: 0.0655895248055458, tv_loss: 0.017424242570996284\n",
      "iteration 1207, dc_loss: 0.0655895546078682, tv_loss: 0.01742425374686718\n",
      "iteration 1208, dc_loss: 0.0655895471572876, tv_loss: 0.01742449402809143\n",
      "iteration 1209, dc_loss: 0.06558950245380402, tv_loss: 0.01742452383041382\n",
      "iteration 1210, dc_loss: 0.06558948755264282, tv_loss: 0.017424220219254494\n",
      "iteration 1211, dc_loss: 0.06558950245380402, tv_loss: 0.017424095422029495\n",
      "iteration 1212, dc_loss: 0.0655895248055458, tv_loss: 0.017424141988158226\n",
      "iteration 1213, dc_loss: 0.06558956205844879, tv_loss: 0.01742430031299591\n",
      "iteration 1214, dc_loss: 0.06558956205844879, tv_loss: 0.017424140125513077\n",
      "iteration 1215, dc_loss: 0.065589539706707, tv_loss: 0.017424313351511955\n",
      "iteration 1216, dc_loss: 0.06558950245380402, tv_loss: 0.017424393445253372\n",
      "iteration 1217, dc_loss: 0.06558950245380402, tv_loss: 0.017424097284674644\n",
      "iteration 1218, dc_loss: 0.0655895248055458, tv_loss: 0.01742410659790039\n",
      "iteration 1219, dc_loss: 0.0655895546078682, tv_loss: 0.017424067482352257\n",
      "iteration 1220, dc_loss: 0.06558956950902939, tv_loss: 0.01742437295615673\n",
      "iteration 1221, dc_loss: 0.0655895471572876, tv_loss: 0.01742427609860897\n",
      "iteration 1222, dc_loss: 0.06558950990438461, tv_loss: 0.017424101009964943\n",
      "iteration 1223, dc_loss: 0.06558950245380402, tv_loss: 0.017424127086997032\n",
      "iteration 1224, dc_loss: 0.06558948755264282, tv_loss: 0.017424367368221283\n",
      "iteration 1225, dc_loss: 0.06558950990438461, tv_loss: 0.017424283549189568\n",
      "iteration 1226, dc_loss: 0.0655895546078682, tv_loss: 0.01742412894964218\n",
      "iteration 1227, dc_loss: 0.06558958441019058, tv_loss: 0.017424095422029495\n",
      "iteration 1228, dc_loss: 0.06558956205844879, tv_loss: 0.0174240842461586\n",
      "iteration 1229, dc_loss: 0.0655895322561264, tv_loss: 0.01742386631667614\n",
      "iteration 1230, dc_loss: 0.06558950245380402, tv_loss: 0.017424240708351135\n",
      "iteration 1231, dc_loss: 0.06558948755264282, tv_loss: 0.01742449775338173\n",
      "iteration 1232, dc_loss: 0.06558950245380402, tv_loss: 0.017424296587705612\n",
      "iteration 1233, dc_loss: 0.0655895322561264, tv_loss: 0.017424067482352257\n",
      "iteration 1234, dc_loss: 0.0655895471572876, tv_loss: 0.017424119636416435\n",
      "iteration 1235, dc_loss: 0.0655895546078682, tv_loss: 0.017424315214157104\n",
      "iteration 1236, dc_loss: 0.065589539706707, tv_loss: 0.017424501478672028\n",
      "iteration 1237, dc_loss: 0.0655895248055458, tv_loss: 0.017424574121832848\n",
      "iteration 1238, dc_loss: 0.06558950990438461, tv_loss: 0.017424296587705612\n",
      "iteration 1239, dc_loss: 0.06558950245380402, tv_loss: 0.017424404621124268\n",
      "iteration 1240, dc_loss: 0.06558948755264282, tv_loss: 0.017424574121832848\n",
      "iteration 1241, dc_loss: 0.06558950990438461, tv_loss: 0.017424805089831352\n",
      "iteration 1242, dc_loss: 0.0655895471572876, tv_loss: 0.017424609512090683\n",
      "iteration 1243, dc_loss: 0.06558957695960999, tv_loss: 0.017424531280994415\n",
      "iteration 1244, dc_loss: 0.0655895546078682, tv_loss: 0.017424818128347397\n",
      "iteration 1245, dc_loss: 0.06558951735496521, tv_loss: 0.017424892634153366\n",
      "iteration 1246, dc_loss: 0.06558950245380402, tv_loss: 0.017424477264285088\n",
      "iteration 1247, dc_loss: 0.06558950990438461, tv_loss: 0.017424553632736206\n",
      "iteration 1248, dc_loss: 0.06558950245380402, tv_loss: 0.017424719408154488\n",
      "iteration 1249, dc_loss: 0.06558950990438461, tv_loss: 0.017424579709768295\n",
      "iteration 1250, dc_loss: 0.0655895322561264, tv_loss: 0.017424464225769043\n",
      "iteration 1251, dc_loss: 0.0655895322561264, tv_loss: 0.017424151301383972\n",
      "iteration 1252, dc_loss: 0.0655895248055458, tv_loss: 0.017424624413251877\n",
      "iteration 1253, dc_loss: 0.0655895248055458, tv_loss: 0.01742461323738098\n",
      "iteration 1254, dc_loss: 0.0655895471572876, tv_loss: 0.017424337565898895\n",
      "iteration 1255, dc_loss: 0.0655895546078682, tv_loss: 0.017424434423446655\n",
      "iteration 1256, dc_loss: 0.0655895546078682, tv_loss: 0.017424408346414566\n",
      "iteration 1257, dc_loss: 0.0655895322561264, tv_loss: 0.017424125224351883\n",
      "iteration 1258, dc_loss: 0.06558950990438461, tv_loss: 0.01742413453757763\n",
      "iteration 1259, dc_loss: 0.06558950990438461, tv_loss: 0.0174243226647377\n",
      "iteration 1260, dc_loss: 0.06558948010206223, tv_loss: 0.01742437295615673\n",
      "iteration 1261, dc_loss: 0.06558949500322342, tv_loss: 0.017424127086997032\n",
      "iteration 1262, dc_loss: 0.0655895322561264, tv_loss: 0.017424024641513824\n",
      "iteration 1263, dc_loss: 0.0655895546078682, tv_loss: 0.01742381602525711\n",
      "iteration 1264, dc_loss: 0.0655895471572876, tv_loss: 0.01742406189441681\n",
      "iteration 1265, dc_loss: 0.0655895322561264, tv_loss: 0.01742403581738472\n",
      "iteration 1266, dc_loss: 0.0655895248055458, tv_loss: 0.017423931509256363\n",
      "iteration 1267, dc_loss: 0.0655895322561264, tv_loss: 0.0174242053180933\n",
      "iteration 1268, dc_loss: 0.0655895471572876, tv_loss: 0.017424218356609344\n",
      "iteration 1269, dc_loss: 0.06558950990438461, tv_loss: 0.01742391660809517\n",
      "iteration 1270, dc_loss: 0.06558946520090103, tv_loss: 0.017423948273062706\n",
      "iteration 1271, dc_loss: 0.06558946520090103, tv_loss: 0.017424020916223526\n",
      "iteration 1272, dc_loss: 0.06558950245380402, tv_loss: 0.017424045130610466\n",
      "iteration 1273, dc_loss: 0.0655895471572876, tv_loss: 0.01742388866841793\n",
      "iteration 1274, dc_loss: 0.06558956205844879, tv_loss: 0.0174238383769989\n",
      "iteration 1275, dc_loss: 0.0655895546078682, tv_loss: 0.01742413640022278\n",
      "iteration 1276, dc_loss: 0.0655895322561264, tv_loss: 0.017424220219254494\n",
      "iteration 1277, dc_loss: 0.06558951735496521, tv_loss: 0.01742405630648136\n",
      "iteration 1278, dc_loss: 0.06558950990438461, tv_loss: 0.017423924058675766\n",
      "iteration 1279, dc_loss: 0.06558950990438461, tv_loss: 0.017424119636416435\n",
      "iteration 1280, dc_loss: 0.06558948755264282, tv_loss: 0.017424391582608223\n",
      "iteration 1281, dc_loss: 0.06558948755264282, tv_loss: 0.017424408346414566\n",
      "iteration 1282, dc_loss: 0.0655895248055458, tv_loss: 0.01742440275847912\n",
      "iteration 1283, dc_loss: 0.06558956950902939, tv_loss: 0.017424317076802254\n",
      "iteration 1284, dc_loss: 0.0655895471572876, tv_loss: 0.017424428835511208\n",
      "iteration 1285, dc_loss: 0.0655895322561264, tv_loss: 0.017424441874027252\n",
      "iteration 1286, dc_loss: 0.06558950990438461, tv_loss: 0.017424650490283966\n",
      "iteration 1287, dc_loss: 0.06558948755264282, tv_loss: 0.017424581572413445\n",
      "iteration 1288, dc_loss: 0.06558948010206223, tv_loss: 0.01742476411163807\n",
      "iteration 1289, dc_loss: 0.06558950245380402, tv_loss: 0.01742461696267128\n",
      "iteration 1290, dc_loss: 0.0655895322561264, tv_loss: 0.0174245648086071\n",
      "iteration 1291, dc_loss: 0.06558956950902939, tv_loss: 0.0174245722591877\n",
      "iteration 1292, dc_loss: 0.06558956950902939, tv_loss: 0.01742459461092949\n",
      "iteration 1293, dc_loss: 0.0655895471572876, tv_loss: 0.017424603924155235\n",
      "iteration 1294, dc_loss: 0.06558948010206223, tv_loss: 0.017424562945961952\n",
      "iteration 1295, dc_loss: 0.06558945775032043, tv_loss: 0.0174245648086071\n",
      "iteration 1296, dc_loss: 0.06558947265148163, tv_loss: 0.01742449961602688\n",
      "iteration 1297, dc_loss: 0.0655895248055458, tv_loss: 0.017424417659640312\n",
      "iteration 1298, dc_loss: 0.0655895471572876, tv_loss: 0.017424672842025757\n",
      "iteration 1299, dc_loss: 0.0655895546078682, tv_loss: 0.017424670979380608\n",
      "iteration 1300, dc_loss: 0.0655895248055458, tv_loss: 0.017424415796995163\n",
      "iteration 1301, dc_loss: 0.06558951735496521, tv_loss: 0.017424488440155983\n",
      "iteration 1302, dc_loss: 0.0655895248055458, tv_loss: 0.017424756661057472\n",
      "iteration 1303, dc_loss: 0.0655895471572876, tv_loss: 0.017424723133444786\n",
      "iteration 1304, dc_loss: 0.065589539706707, tv_loss: 0.017424313351511955\n",
      "iteration 1305, dc_loss: 0.06558950990438461, tv_loss: 0.017424356192350388\n",
      "iteration 1306, dc_loss: 0.06558950990438461, tv_loss: 0.017424676567316055\n",
      "iteration 1307, dc_loss: 0.0655895322561264, tv_loss: 0.017424220219254494\n",
      "iteration 1308, dc_loss: 0.0655895322561264, tv_loss: 0.017424194142222404\n",
      "iteration 1309, dc_loss: 0.06558950990438461, tv_loss: 0.017424317076802254\n",
      "iteration 1310, dc_loss: 0.06558950245380402, tv_loss: 0.017424557358026505\n",
      "iteration 1311, dc_loss: 0.06558950990438461, tv_loss: 0.017424507066607475\n",
      "iteration 1312, dc_loss: 0.06558951735496521, tv_loss: 0.01742439903318882\n",
      "iteration 1313, dc_loss: 0.0655895322561264, tv_loss: 0.0174243301153183\n",
      "iteration 1314, dc_loss: 0.0655895471572876, tv_loss: 0.017424028366804123\n",
      "iteration 1315, dc_loss: 0.0655895546078682, tv_loss: 0.017424259334802628\n",
      "iteration 1316, dc_loss: 0.06558951735496521, tv_loss: 0.017424296587705612\n",
      "iteration 1317, dc_loss: 0.06558950245380402, tv_loss: 0.017424283549189568\n",
      "iteration 1318, dc_loss: 0.06558948755264282, tv_loss: 0.01742425560951233\n",
      "iteration 1319, dc_loss: 0.06558950245380402, tv_loss: 0.01742427982389927\n",
      "iteration 1320, dc_loss: 0.06558950990438461, tv_loss: 0.01742437668144703\n",
      "iteration 1321, dc_loss: 0.0655895322561264, tv_loss: 0.01742413640022278\n",
      "iteration 1322, dc_loss: 0.06558956950902939, tv_loss: 0.017424194142222404\n",
      "iteration 1323, dc_loss: 0.06558956205844879, tv_loss: 0.017424119636416435\n",
      "iteration 1324, dc_loss: 0.06558951735496521, tv_loss: 0.017424210906028748\n",
      "iteration 1325, dc_loss: 0.06558948010206223, tv_loss: 0.017424531280994415\n",
      "iteration 1326, dc_loss: 0.06558948755264282, tv_loss: 0.017424335703253746\n",
      "iteration 1327, dc_loss: 0.0655895248055458, tv_loss: 0.01742403768002987\n",
      "iteration 1328, dc_loss: 0.065589539706707, tv_loss: 0.017424294725060463\n",
      "iteration 1329, dc_loss: 0.0655895248055458, tv_loss: 0.01742435246706009\n",
      "iteration 1330, dc_loss: 0.06558950245380402, tv_loss: 0.017424197867512703\n",
      "iteration 1331, dc_loss: 0.06558950990438461, tv_loss: 0.017424123361706734\n",
      "iteration 1332, dc_loss: 0.0655895322561264, tv_loss: 0.017423909157514572\n",
      "iteration 1333, dc_loss: 0.0655895471572876, tv_loss: 0.017424043267965317\n",
      "iteration 1334, dc_loss: 0.0655895322561264, tv_loss: 0.01742461696267128\n",
      "iteration 1335, dc_loss: 0.06558950990438461, tv_loss: 0.01742437109351158\n",
      "iteration 1336, dc_loss: 0.06558950245380402, tv_loss: 0.0174234751611948\n",
      "iteration 1337, dc_loss: 0.06558950990438461, tv_loss: 0.017423953860998154\n",
      "iteration 1338, dc_loss: 0.06558951735496521, tv_loss: 0.017424458637833595\n",
      "iteration 1339, dc_loss: 0.0655895322561264, tv_loss: 0.0174240805208683\n",
      "iteration 1340, dc_loss: 0.0655895322561264, tv_loss: 0.017423655837774277\n",
      "iteration 1341, dc_loss: 0.0655895248055458, tv_loss: 0.017424050718545914\n",
      "iteration 1342, dc_loss: 0.06558951735496521, tv_loss: 0.017424000427126884\n",
      "iteration 1343, dc_loss: 0.06558950990438461, tv_loss: 0.017423972487449646\n",
      "iteration 1344, dc_loss: 0.06558950245380402, tv_loss: 0.0174242053180933\n",
      "iteration 1345, dc_loss: 0.06558950990438461, tv_loss: 0.01742391288280487\n",
      "iteration 1346, dc_loss: 0.0655895471572876, tv_loss: 0.017423784360289574\n",
      "iteration 1347, dc_loss: 0.06558956205844879, tv_loss: 0.01742393709719181\n",
      "iteration 1348, dc_loss: 0.0655895546078682, tv_loss: 0.017423994839191437\n",
      "iteration 1349, dc_loss: 0.0655895322561264, tv_loss: 0.017424019053578377\n",
      "iteration 1350, dc_loss: 0.06558949500322342, tv_loss: 0.017424337565898895\n",
      "iteration 1351, dc_loss: 0.06558948010206223, tv_loss: 0.017424283549189568\n",
      "iteration 1352, dc_loss: 0.06558950245380402, tv_loss: 0.017424043267965317\n",
      "iteration 1353, dc_loss: 0.06558950990438461, tv_loss: 0.017424041405320168\n",
      "iteration 1354, dc_loss: 0.06558951735496521, tv_loss: 0.017424210906028748\n",
      "iteration 1355, dc_loss: 0.0655895471572876, tv_loss: 0.01742425374686718\n",
      "iteration 1356, dc_loss: 0.0655895322561264, tv_loss: 0.017424313351511955\n",
      "iteration 1357, dc_loss: 0.0655895322561264, tv_loss: 0.017424223944544792\n",
      "iteration 1358, dc_loss: 0.0655895248055458, tv_loss: 0.017424006015062332\n",
      "iteration 1359, dc_loss: 0.06558950245380402, tv_loss: 0.01742418296635151\n",
      "iteration 1360, dc_loss: 0.06558947265148163, tv_loss: 0.01742466166615486\n",
      "iteration 1361, dc_loss: 0.06558948010206223, tv_loss: 0.01742464303970337\n",
      "iteration 1362, dc_loss: 0.0655895322561264, tv_loss: 0.017424363642930984\n",
      "iteration 1363, dc_loss: 0.06558957695960999, tv_loss: 0.0174240842461586\n",
      "iteration 1364, dc_loss: 0.06558957695960999, tv_loss: 0.01742406375706196\n",
      "iteration 1365, dc_loss: 0.0655895248055458, tv_loss: 0.017424441874027252\n",
      "iteration 1366, dc_loss: 0.06558950245380402, tv_loss: 0.017424561083316803\n",
      "iteration 1367, dc_loss: 0.06558950990438461, tv_loss: 0.0174243226647377\n",
      "iteration 1368, dc_loss: 0.06558950990438461, tv_loss: 0.01742425560951233\n",
      "iteration 1369, dc_loss: 0.06558951735496521, tv_loss: 0.01742447167634964\n",
      "iteration 1370, dc_loss: 0.06558951735496521, tv_loss: 0.01742454431951046\n",
      "iteration 1371, dc_loss: 0.06558950990438461, tv_loss: 0.017424006015062332\n",
      "iteration 1372, dc_loss: 0.06558950245380402, tv_loss: 0.017424028366804123\n",
      "iteration 1373, dc_loss: 0.06558951735496521, tv_loss: 0.017424151301383972\n",
      "iteration 1374, dc_loss: 0.06558950990438461, tv_loss: 0.017424091696739197\n",
      "iteration 1375, dc_loss: 0.06558950990438461, tv_loss: 0.017423978075385094\n",
      "iteration 1376, dc_loss: 0.0655895322561264, tv_loss: 0.017424052581191063\n",
      "iteration 1377, dc_loss: 0.0655895471572876, tv_loss: 0.017424093559384346\n",
      "iteration 1378, dc_loss: 0.0655895322561264, tv_loss: 0.017424404621124268\n",
      "iteration 1379, dc_loss: 0.0655895322561264, tv_loss: 0.017424030229449272\n",
      "iteration 1380, dc_loss: 0.06558951735496521, tv_loss: 0.017423855140805244\n",
      "iteration 1381, dc_loss: 0.0655895248055458, tv_loss: 0.017424238845705986\n",
      "iteration 1382, dc_loss: 0.0655895471572876, tv_loss: 0.017424119636416435\n",
      "iteration 1383, dc_loss: 0.0655895322561264, tv_loss: 0.017424266785383224\n",
      "iteration 1384, dc_loss: 0.06558950990438461, tv_loss: 0.01742430217564106\n",
      "iteration 1385, dc_loss: 0.06558948010206223, tv_loss: 0.017424192279577255\n",
      "iteration 1386, dc_loss: 0.06558948755264282, tv_loss: 0.01742420345544815\n",
      "iteration 1387, dc_loss: 0.065589539706707, tv_loss: 0.017424365505576134\n",
      "iteration 1388, dc_loss: 0.0655895546078682, tv_loss: 0.017424415796995163\n",
      "iteration 1389, dc_loss: 0.0655895471572876, tv_loss: 0.017424222081899643\n",
      "iteration 1390, dc_loss: 0.0655895248055458, tv_loss: 0.017424078658223152\n",
      "iteration 1391, dc_loss: 0.06558950245380402, tv_loss: 0.017424270510673523\n",
      "iteration 1392, dc_loss: 0.06558948010206223, tv_loss: 0.017424069344997406\n",
      "iteration 1393, dc_loss: 0.06558950245380402, tv_loss: 0.017423979938030243\n",
      "iteration 1394, dc_loss: 0.0655895248055458, tv_loss: 0.017423970624804497\n",
      "iteration 1395, dc_loss: 0.0655895322561264, tv_loss: 0.017424508929252625\n",
      "iteration 1396, dc_loss: 0.0655895471572876, tv_loss: 0.01742417924106121\n",
      "iteration 1397, dc_loss: 0.0655895546078682, tv_loss: 0.017423801124095917\n",
      "iteration 1398, dc_loss: 0.0655895471572876, tv_loss: 0.01742398738861084\n",
      "iteration 1399, dc_loss: 0.0655895248055458, tv_loss: 0.017424190416932106\n",
      "iteration 1400, dc_loss: 0.06558948755264282, tv_loss: 0.017424238845705986\n",
      "iteration 1401, dc_loss: 0.06558948010206223, tv_loss: 0.017424261197447777\n",
      "iteration 1402, dc_loss: 0.06558950245380402, tv_loss: 0.017424117773771286\n",
      "iteration 1403, dc_loss: 0.0655895322561264, tv_loss: 0.017423873767256737\n",
      "iteration 1404, dc_loss: 0.0655895471572876, tv_loss: 0.017424356192350388\n",
      "iteration 1405, dc_loss: 0.0655895322561264, tv_loss: 0.017424201592803\n",
      "iteration 1406, dc_loss: 0.06558951735496521, tv_loss: 0.017423944547772408\n",
      "iteration 1407, dc_loss: 0.06558950245380402, tv_loss: 0.017424356192350388\n",
      "iteration 1408, dc_loss: 0.06558949500322342, tv_loss: 0.017424289137125015\n",
      "iteration 1409, dc_loss: 0.06558950245380402, tv_loss: 0.017424296587705612\n",
      "iteration 1410, dc_loss: 0.06558950245380402, tv_loss: 0.017424363642930984\n",
      "iteration 1411, dc_loss: 0.0655895322561264, tv_loss: 0.017424296587705612\n",
      "iteration 1412, dc_loss: 0.0655895546078682, tv_loss: 0.01742425188422203\n",
      "iteration 1413, dc_loss: 0.0655895322561264, tv_loss: 0.01742432825267315\n",
      "iteration 1414, dc_loss: 0.06558948755264282, tv_loss: 0.01742422953248024\n",
      "iteration 1415, dc_loss: 0.06558950245380402, tv_loss: 0.01742435246706009\n",
      "iteration 1416, dc_loss: 0.0655895248055458, tv_loss: 0.01742452383041382\n",
      "iteration 1417, dc_loss: 0.0655895471572876, tv_loss: 0.017424097284674644\n",
      "iteration 1418, dc_loss: 0.06558956205844879, tv_loss: 0.01742422580718994\n",
      "iteration 1419, dc_loss: 0.0655895471572876, tv_loss: 0.017424428835511208\n",
      "iteration 1420, dc_loss: 0.06558950245380402, tv_loss: 0.01742430590093136\n",
      "iteration 1421, dc_loss: 0.06558948755264282, tv_loss: 0.017424196004867554\n",
      "iteration 1422, dc_loss: 0.06558950990438461, tv_loss: 0.01742410846054554\n",
      "iteration 1423, dc_loss: 0.0655895248055458, tv_loss: 0.01742398552596569\n",
      "iteration 1424, dc_loss: 0.0655895322561264, tv_loss: 0.017423927783966064\n",
      "iteration 1425, dc_loss: 0.0655895322561264, tv_loss: 0.0174240879714489\n",
      "iteration 1426, dc_loss: 0.0655895471572876, tv_loss: 0.017424283549189568\n",
      "iteration 1427, dc_loss: 0.06558956205844879, tv_loss: 0.01742403395473957\n",
      "iteration 1428, dc_loss: 0.0655895248055458, tv_loss: 0.017424115911126137\n",
      "iteration 1429, dc_loss: 0.06558950245380402, tv_loss: 0.017424268648028374\n",
      "iteration 1430, dc_loss: 0.06558950245380402, tv_loss: 0.017424143850803375\n",
      "iteration 1431, dc_loss: 0.06558949500322342, tv_loss: 0.017424246296286583\n",
      "iteration 1432, dc_loss: 0.06558950990438461, tv_loss: 0.017424194142222404\n",
      "iteration 1433, dc_loss: 0.0655895248055458, tv_loss: 0.017424270510673523\n",
      "iteration 1434, dc_loss: 0.0655895322561264, tv_loss: 0.01742427609860897\n",
      "iteration 1435, dc_loss: 0.065589539706707, tv_loss: 0.017424125224351883\n",
      "iteration 1436, dc_loss: 0.0655895546078682, tv_loss: 0.017424140125513077\n",
      "iteration 1437, dc_loss: 0.0655895322561264, tv_loss: 0.017424367368221283\n",
      "iteration 1438, dc_loss: 0.06558950245380402, tv_loss: 0.01742437854409218\n",
      "iteration 1439, dc_loss: 0.06558948010206223, tv_loss: 0.017424117773771286\n",
      "iteration 1440, dc_loss: 0.06558948010206223, tv_loss: 0.0174238421022892\n",
      "iteration 1441, dc_loss: 0.06558950990438461, tv_loss: 0.01742394082248211\n",
      "iteration 1442, dc_loss: 0.065589539706707, tv_loss: 0.017424212768673897\n",
      "iteration 1443, dc_loss: 0.06558956205844879, tv_loss: 0.017423884943127632\n",
      "iteration 1444, dc_loss: 0.0655895546078682, tv_loss: 0.017423896118998528\n",
      "iteration 1445, dc_loss: 0.0655895322561264, tv_loss: 0.017424164339900017\n",
      "iteration 1446, dc_loss: 0.06558950990438461, tv_loss: 0.017424091696739197\n",
      "iteration 1447, dc_loss: 0.06558950245380402, tv_loss: 0.017423782497644424\n",
      "iteration 1448, dc_loss: 0.06558950245380402, tv_loss: 0.017424050718545914\n",
      "iteration 1449, dc_loss: 0.06558948010206223, tv_loss: 0.01742429845035076\n",
      "iteration 1450, dc_loss: 0.06558948755264282, tv_loss: 0.017424054443836212\n",
      "iteration 1451, dc_loss: 0.06558950990438461, tv_loss: 0.01742394268512726\n",
      "iteration 1452, dc_loss: 0.0655895471572876, tv_loss: 0.017424117773771286\n",
      "iteration 1453, dc_loss: 0.06558957695960999, tv_loss: 0.017424020916223526\n",
      "iteration 1454, dc_loss: 0.06558956950902939, tv_loss: 0.017423804849386215\n",
      "iteration 1455, dc_loss: 0.0655895248055458, tv_loss: 0.017424149438738823\n",
      "iteration 1456, dc_loss: 0.06558948010206223, tv_loss: 0.01742420718073845\n",
      "iteration 1457, dc_loss: 0.06558948755264282, tv_loss: 0.017423832789063454\n",
      "iteration 1458, dc_loss: 0.06558950990438461, tv_loss: 0.017423946410417557\n",
      "iteration 1459, dc_loss: 0.0655895248055458, tv_loss: 0.017424140125513077\n",
      "iteration 1460, dc_loss: 0.0655895322561264, tv_loss: 0.017424236983060837\n",
      "iteration 1461, dc_loss: 0.0655895248055458, tv_loss: 0.017423732206225395\n",
      "iteration 1462, dc_loss: 0.0655895248055458, tv_loss: 0.017424089834094048\n",
      "iteration 1463, dc_loss: 0.0655895248055458, tv_loss: 0.017424307763576508\n",
      "iteration 1464, dc_loss: 0.06558950990438461, tv_loss: 0.01742423139512539\n",
      "iteration 1465, dc_loss: 0.06558950990438461, tv_loss: 0.017423951998353004\n",
      "iteration 1466, dc_loss: 0.06558950990438461, tv_loss: 0.017424097284674644\n",
      "iteration 1467, dc_loss: 0.0655895248055458, tv_loss: 0.01742434874176979\n",
      "iteration 1468, dc_loss: 0.0655895248055458, tv_loss: 0.01742405816912651\n",
      "iteration 1469, dc_loss: 0.06558950245380402, tv_loss: 0.017424020916223526\n",
      "iteration 1470, dc_loss: 0.06558948010206223, tv_loss: 0.01742422766983509\n",
      "iteration 1471, dc_loss: 0.06558948755264282, tv_loss: 0.01742432825267315\n",
      "iteration 1472, dc_loss: 0.0655895248055458, tv_loss: 0.01742418296635151\n",
      "iteration 1473, dc_loss: 0.0655895322561264, tv_loss: 0.01742422953248024\n",
      "iteration 1474, dc_loss: 0.0655895546078682, tv_loss: 0.017424536868929863\n",
      "iteration 1475, dc_loss: 0.0655895471572876, tv_loss: 0.017424365505576134\n",
      "iteration 1476, dc_loss: 0.06558951735496521, tv_loss: 0.01742396503686905\n",
      "iteration 1477, dc_loss: 0.06558948755264282, tv_loss: 0.01742427796125412\n",
      "iteration 1478, dc_loss: 0.06558948010206223, tv_loss: 0.01742468774318695\n",
      "iteration 1479, dc_loss: 0.06558946520090103, tv_loss: 0.017424779012799263\n",
      "iteration 1480, dc_loss: 0.06558945775032043, tv_loss: 0.01742444932460785\n",
      "iteration 1481, dc_loss: 0.06558950990438461, tv_loss: 0.017424242570996284\n",
      "iteration 1482, dc_loss: 0.06558956950902939, tv_loss: 0.0174242090433836\n",
      "iteration 1483, dc_loss: 0.06558959931135178, tv_loss: 0.01742425560951233\n",
      "iteration 1484, dc_loss: 0.06558956205844879, tv_loss: 0.0174240805208683\n",
      "iteration 1485, dc_loss: 0.06558950990438461, tv_loss: 0.01742447540163994\n",
      "iteration 1486, dc_loss: 0.06558947265148163, tv_loss: 0.017424406483769417\n",
      "iteration 1487, dc_loss: 0.06558946520090103, tv_loss: 0.017424194142222404\n",
      "iteration 1488, dc_loss: 0.06558947265148163, tv_loss: 0.017424358054995537\n",
      "iteration 1489, dc_loss: 0.06558950245380402, tv_loss: 0.017424194142222404\n",
      "iteration 1490, dc_loss: 0.0655895322561264, tv_loss: 0.01742417924106121\n",
      "iteration 1491, dc_loss: 0.0655895471572876, tv_loss: 0.01742405630648136\n",
      "iteration 1492, dc_loss: 0.06558956205844879, tv_loss: 0.01742427609860897\n",
      "iteration 1493, dc_loss: 0.06558956205844879, tv_loss: 0.017424391582608223\n",
      "iteration 1494, dc_loss: 0.065589539706707, tv_loss: 0.017424089834094048\n",
      "iteration 1495, dc_loss: 0.06558951735496521, tv_loss: 0.017424236983060837\n",
      "iteration 1496, dc_loss: 0.06558948010206223, tv_loss: 0.017424773424863815\n",
      "iteration 1497, dc_loss: 0.06558948755264282, tv_loss: 0.01742459647357464\n",
      "iteration 1498, dc_loss: 0.06558950245380402, tv_loss: 0.01742406003177166\n",
      "iteration 1499, dc_loss: 0.0655895322561264, tv_loss: 0.01742406003177166\n",
      "iteration 1500, dc_loss: 0.0655895471572876, tv_loss: 0.01742435246706009\n",
      "iteration 1501, dc_loss: 0.0655895322561264, tv_loss: 0.017424659803509712\n",
      "iteration 1502, dc_loss: 0.0655895471572876, tv_loss: 0.017424186691641808\n",
      "iteration 1503, dc_loss: 0.06558951735496521, tv_loss: 0.01742379181087017\n",
      "iteration 1504, dc_loss: 0.06558948010206223, tv_loss: 0.017424216493964195\n",
      "iteration 1505, dc_loss: 0.06558945775032043, tv_loss: 0.017424561083316803\n",
      "iteration 1506, dc_loss: 0.06558947265148163, tv_loss: 0.017424404621124268\n",
      "iteration 1507, dc_loss: 0.065589539706707, tv_loss: 0.01742410473525524\n",
      "iteration 1508, dc_loss: 0.06558958441019058, tv_loss: 0.017424065619707108\n",
      "iteration 1509, dc_loss: 0.06558956205844879, tv_loss: 0.017424166202545166\n",
      "iteration 1510, dc_loss: 0.06558951735496521, tv_loss: 0.017424460500478745\n",
      "iteration 1511, dc_loss: 0.06558948010206223, tv_loss: 0.017424127086997032\n",
      "iteration 1512, dc_loss: 0.06558948010206223, tv_loss: 0.017424076795578003\n",
      "iteration 1513, dc_loss: 0.06558950990438461, tv_loss: 0.017424296587705612\n",
      "iteration 1514, dc_loss: 0.0655895248055458, tv_loss: 0.017424339428544044\n",
      "iteration 1515, dc_loss: 0.065589539706707, tv_loss: 0.01742394268512726\n",
      "iteration 1516, dc_loss: 0.065589539706707, tv_loss: 0.017423735931515694\n",
      "iteration 1517, dc_loss: 0.0655895471572876, tv_loss: 0.017423884943127632\n",
      "iteration 1518, dc_loss: 0.06558950990438461, tv_loss: 0.017424127086997032\n",
      "iteration 1519, dc_loss: 0.06558948755264282, tv_loss: 0.0174240805208683\n",
      "iteration 1520, dc_loss: 0.06558948010206223, tv_loss: 0.017424026504158974\n",
      "iteration 1521, dc_loss: 0.06558950990438461, tv_loss: 0.017424004152417183\n",
      "iteration 1522, dc_loss: 0.06558951735496521, tv_loss: 0.017424210906028748\n",
      "iteration 1523, dc_loss: 0.0655895546078682, tv_loss: 0.017424175515770912\n",
      "iteration 1524, dc_loss: 0.065589539706707, tv_loss: 0.017423929646611214\n",
      "iteration 1525, dc_loss: 0.06558951735496521, tv_loss: 0.017424045130610466\n",
      "iteration 1526, dc_loss: 0.06558950245380402, tv_loss: 0.01742454431951046\n",
      "iteration 1527, dc_loss: 0.06558950245380402, tv_loss: 0.017424220219254494\n",
      "iteration 1528, dc_loss: 0.06558950245380402, tv_loss: 0.017423871904611588\n",
      "iteration 1529, dc_loss: 0.06558950990438461, tv_loss: 0.017424065619707108\n",
      "iteration 1530, dc_loss: 0.06558951735496521, tv_loss: 0.01742374524474144\n",
      "iteration 1531, dc_loss: 0.0655895248055458, tv_loss: 0.017423799261450768\n",
      "iteration 1532, dc_loss: 0.0655895322561264, tv_loss: 0.01742400787770748\n",
      "iteration 1533, dc_loss: 0.06558951735496521, tv_loss: 0.017423871904611588\n",
      "iteration 1534, dc_loss: 0.06558950245380402, tv_loss: 0.017424004152417183\n",
      "iteration 1535, dc_loss: 0.06558950245380402, tv_loss: 0.017424315214157104\n",
      "iteration 1536, dc_loss: 0.06558951735496521, tv_loss: 0.017424147576093674\n",
      "iteration 1537, dc_loss: 0.0655895322561264, tv_loss: 0.017423739656805992\n",
      "iteration 1538, dc_loss: 0.0655895471572876, tv_loss: 0.017424201592803\n",
      "iteration 1539, dc_loss: 0.0655895322561264, tv_loss: 0.01742463745176792\n",
      "iteration 1540, dc_loss: 0.06558948010206223, tv_loss: 0.017424164339900017\n",
      "iteration 1541, dc_loss: 0.06558948010206223, tv_loss: 0.017424391582608223\n",
      "iteration 1542, dc_loss: 0.06558950990438461, tv_loss: 0.017424149438738823\n",
      "iteration 1543, dc_loss: 0.0655895322561264, tv_loss: 0.01742427423596382\n",
      "iteration 1544, dc_loss: 0.0655895471572876, tv_loss: 0.01742464117705822\n",
      "iteration 1545, dc_loss: 0.0655895248055458, tv_loss: 0.017424611374735832\n",
      "iteration 1546, dc_loss: 0.06558950245380402, tv_loss: 0.01742420718073845\n",
      "iteration 1547, dc_loss: 0.06558951735496521, tv_loss: 0.01742422580718994\n",
      "iteration 1548, dc_loss: 0.065589539706707, tv_loss: 0.01742457039654255\n",
      "iteration 1549, dc_loss: 0.0655895471572876, tv_loss: 0.017424583435058594\n",
      "iteration 1550, dc_loss: 0.0655895248055458, tv_loss: 0.017424311488866806\n",
      "iteration 1551, dc_loss: 0.06558950245380402, tv_loss: 0.017424531280994415\n",
      "iteration 1552, dc_loss: 0.06558950990438461, tv_loss: 0.01742437854409218\n",
      "iteration 1553, dc_loss: 0.0655895248055458, tv_loss: 0.01742434874176979\n",
      "iteration 1554, dc_loss: 0.06558951735496521, tv_loss: 0.017424553632736206\n",
      "iteration 1555, dc_loss: 0.0655895248055458, tv_loss: 0.01742452010512352\n",
      "iteration 1556, dc_loss: 0.0655895322561264, tv_loss: 0.017424246296286583\n",
      "iteration 1557, dc_loss: 0.0655895248055458, tv_loss: 0.017424242570996284\n",
      "iteration 1558, dc_loss: 0.06558950990438461, tv_loss: 0.01742432825267315\n",
      "iteration 1559, dc_loss: 0.06558949500322342, tv_loss: 0.017424285411834717\n",
      "iteration 1560, dc_loss: 0.06558948010206223, tv_loss: 0.017423970624804497\n",
      "iteration 1561, dc_loss: 0.06558948010206223, tv_loss: 0.017424123361706734\n",
      "iteration 1562, dc_loss: 0.065589539706707, tv_loss: 0.01742408238351345\n",
      "iteration 1563, dc_loss: 0.06558956205844879, tv_loss: 0.01742415316402912\n",
      "iteration 1564, dc_loss: 0.06558956205844879, tv_loss: 0.0174240879714489\n",
      "iteration 1565, dc_loss: 0.0655895471572876, tv_loss: 0.01742394082248211\n",
      "iteration 1566, dc_loss: 0.0655895322561264, tv_loss: 0.017424093559384346\n",
      "iteration 1567, dc_loss: 0.06558950990438461, tv_loss: 0.017424119636416435\n",
      "iteration 1568, dc_loss: 0.06558948755264282, tv_loss: 0.017424162477254868\n",
      "iteration 1569, dc_loss: 0.06558948755264282, tv_loss: 0.017424186691641808\n",
      "iteration 1570, dc_loss: 0.06558948755264282, tv_loss: 0.01742430590093136\n",
      "iteration 1571, dc_loss: 0.06558951735496521, tv_loss: 0.017424123361706734\n",
      "iteration 1572, dc_loss: 0.0655895322561264, tv_loss: 0.017424190416932106\n",
      "iteration 1573, dc_loss: 0.0655895248055458, tv_loss: 0.017424115911126137\n",
      "iteration 1574, dc_loss: 0.06558950245380402, tv_loss: 0.017424259334802628\n",
      "iteration 1575, dc_loss: 0.06558947265148163, tv_loss: 0.017424296587705612\n",
      "iteration 1576, dc_loss: 0.06558948010206223, tv_loss: 0.017424441874027252\n",
      "iteration 1577, dc_loss: 0.06558950990438461, tv_loss: 0.017424508929252625\n",
      "iteration 1578, dc_loss: 0.0655895322561264, tv_loss: 0.017424361780285835\n",
      "iteration 1579, dc_loss: 0.0655895546078682, tv_loss: 0.017424138262867928\n",
      "iteration 1580, dc_loss: 0.0655895471572876, tv_loss: 0.01742425747215748\n",
      "iteration 1581, dc_loss: 0.06558950245380402, tv_loss: 0.017424356192350388\n",
      "iteration 1582, dc_loss: 0.06558948010206223, tv_loss: 0.017424246296286583\n",
      "iteration 1583, dc_loss: 0.06558950245380402, tv_loss: 0.017424345016479492\n",
      "iteration 1584, dc_loss: 0.06558950245380402, tv_loss: 0.017424210906028748\n",
      "iteration 1585, dc_loss: 0.06558949500322342, tv_loss: 0.017424464225769043\n",
      "iteration 1586, dc_loss: 0.06558951735496521, tv_loss: 0.017424287274479866\n",
      "iteration 1587, dc_loss: 0.0655895546078682, tv_loss: 0.01742367073893547\n",
      "iteration 1588, dc_loss: 0.065589539706707, tv_loss: 0.017423739656805992\n",
      "iteration 1589, dc_loss: 0.06558950245380402, tv_loss: 0.017424117773771286\n",
      "iteration 1590, dc_loss: 0.06558948755264282, tv_loss: 0.017424074932932854\n",
      "iteration 1591, dc_loss: 0.06558950245380402, tv_loss: 0.017423685640096664\n",
      "iteration 1592, dc_loss: 0.0655895248055458, tv_loss: 0.017423689365386963\n",
      "iteration 1593, dc_loss: 0.0655895322561264, tv_loss: 0.017424071207642555\n",
      "iteration 1594, dc_loss: 0.06558951735496521, tv_loss: 0.017424149438738823\n",
      "iteration 1595, dc_loss: 0.06558950990438461, tv_loss: 0.01742386445403099\n",
      "iteration 1596, dc_loss: 0.06558950990438461, tv_loss: 0.017423782497644424\n",
      "iteration 1597, dc_loss: 0.0655895322561264, tv_loss: 0.017423951998353004\n",
      "iteration 1598, dc_loss: 0.0655895322561264, tv_loss: 0.017424030229449272\n",
      "iteration 1599, dc_loss: 0.06558950990438461, tv_loss: 0.017423760145902634\n",
      "iteration 1600, dc_loss: 0.06558948755264282, tv_loss: 0.01742362044751644\n",
      "iteration 1601, dc_loss: 0.06558950245380402, tv_loss: 0.017423931509256363\n",
      "iteration 1602, dc_loss: 0.0655895322561264, tv_loss: 0.017424004152417183\n",
      "iteration 1603, dc_loss: 0.0655895471572876, tv_loss: 0.017423560842871666\n",
      "iteration 1604, dc_loss: 0.0655895322561264, tv_loss: 0.01742362231016159\n",
      "iteration 1605, dc_loss: 0.06558950990438461, tv_loss: 0.017424089834094048\n",
      "iteration 1606, dc_loss: 0.06558948010206223, tv_loss: 0.01742417924106121\n",
      "iteration 1607, dc_loss: 0.06558950245380402, tv_loss: 0.017424238845705986\n",
      "iteration 1608, dc_loss: 0.06558950245380402, tv_loss: 0.017424043267965317\n",
      "iteration 1609, dc_loss: 0.06558950990438461, tv_loss: 0.017424093559384346\n",
      "iteration 1610, dc_loss: 0.06558950990438461, tv_loss: 0.017424114048480988\n",
      "iteration 1611, dc_loss: 0.0655895322561264, tv_loss: 0.017424315214157104\n",
      "iteration 1612, dc_loss: 0.0655895471572876, tv_loss: 0.017424138262867928\n",
      "iteration 1613, dc_loss: 0.065589539706707, tv_loss: 0.01742401160299778\n",
      "iteration 1614, dc_loss: 0.0655895248055458, tv_loss: 0.017424296587705612\n",
      "iteration 1615, dc_loss: 0.06558948755264282, tv_loss: 0.01742442324757576\n",
      "iteration 1616, dc_loss: 0.06558948755264282, tv_loss: 0.0174240842461586\n",
      "iteration 1617, dc_loss: 0.06558948755264282, tv_loss: 0.0174237173050642\n",
      "iteration 1618, dc_loss: 0.06558950990438461, tv_loss: 0.01742415316402912\n",
      "iteration 1619, dc_loss: 0.0655895248055458, tv_loss: 0.017424369230866432\n",
      "iteration 1620, dc_loss: 0.065589539706707, tv_loss: 0.017424050718545914\n",
      "iteration 1621, dc_loss: 0.0655895471572876, tv_loss: 0.017423653975129128\n",
      "iteration 1622, dc_loss: 0.0655895322561264, tv_loss: 0.017424004152417183\n",
      "iteration 1623, dc_loss: 0.06558950990438461, tv_loss: 0.017424458637833595\n",
      "iteration 1624, dc_loss: 0.06558948755264282, tv_loss: 0.01742437481880188\n",
      "iteration 1625, dc_loss: 0.06558950245380402, tv_loss: 0.017424151301383972\n",
      "iteration 1626, dc_loss: 0.06558951735496521, tv_loss: 0.01742398925125599\n",
      "iteration 1627, dc_loss: 0.0655895546078682, tv_loss: 0.017424074932932854\n",
      "iteration 1628, dc_loss: 0.06558956205844879, tv_loss: 0.017424218356609344\n",
      "iteration 1629, dc_loss: 0.06558951735496521, tv_loss: 0.017424127086997032\n",
      "iteration 1630, dc_loss: 0.06558945775032043, tv_loss: 0.017424101009964943\n",
      "iteration 1631, dc_loss: 0.06558942049741745, tv_loss: 0.017424166202545166\n",
      "iteration 1632, dc_loss: 0.06558945775032043, tv_loss: 0.017424454912543297\n",
      "iteration 1633, dc_loss: 0.06558951735496521, tv_loss: 0.017424149438738823\n",
      "iteration 1634, dc_loss: 0.06558957695960999, tv_loss: 0.01742399111390114\n",
      "iteration 1635, dc_loss: 0.06558959186077118, tv_loss: 0.017423871904611588\n",
      "iteration 1636, dc_loss: 0.06558956950902939, tv_loss: 0.01742396503686905\n",
      "iteration 1637, dc_loss: 0.0655895322561264, tv_loss: 0.017424046993255615\n",
      "iteration 1638, dc_loss: 0.06558948755264282, tv_loss: 0.0174244437366724\n",
      "iteration 1639, dc_loss: 0.06558947265148163, tv_loss: 0.017424248158931732\n",
      "iteration 1640, dc_loss: 0.06558947265148163, tv_loss: 0.01742413081228733\n",
      "iteration 1641, dc_loss: 0.06558948010206223, tv_loss: 0.017423953860998154\n",
      "iteration 1642, dc_loss: 0.0655895322561264, tv_loss: 0.017424101009964943\n",
      "iteration 1643, dc_loss: 0.06558957695960999, tv_loss: 0.017424125224351883\n",
      "iteration 1644, dc_loss: 0.06558956205844879, tv_loss: 0.017424201592803\n",
      "iteration 1645, dc_loss: 0.0655895248055458, tv_loss: 0.01742415316402912\n",
      "iteration 1646, dc_loss: 0.06558950245380402, tv_loss: 0.01742420345544815\n",
      "iteration 1647, dc_loss: 0.06558948755264282, tv_loss: 0.017424270510673523\n",
      "iteration 1648, dc_loss: 0.06558948010206223, tv_loss: 0.017424095422029495\n",
      "iteration 1649, dc_loss: 0.06558948010206223, tv_loss: 0.017424019053578377\n",
      "iteration 1650, dc_loss: 0.06558950245380402, tv_loss: 0.01742440089583397\n",
      "iteration 1651, dc_loss: 0.0655895248055458, tv_loss: 0.017424078658223152\n",
      "iteration 1652, dc_loss: 0.0655895471572876, tv_loss: 0.01742391660809517\n",
      "iteration 1653, dc_loss: 0.0655895322561264, tv_loss: 0.01742418482899666\n",
      "iteration 1654, dc_loss: 0.06558950990438461, tv_loss: 0.017423884943127632\n",
      "iteration 1655, dc_loss: 0.06558950245380402, tv_loss: 0.017423849552869797\n",
      "iteration 1656, dc_loss: 0.06558950990438461, tv_loss: 0.01742413453757763\n",
      "iteration 1657, dc_loss: 0.0655895248055458, tv_loss: 0.017424114048480988\n",
      "iteration 1658, dc_loss: 0.0655895248055458, tv_loss: 0.017423929646611214\n",
      "iteration 1659, dc_loss: 0.06558950245380402, tv_loss: 0.01742389425635338\n",
      "iteration 1660, dc_loss: 0.06558949500322342, tv_loss: 0.0174240805208683\n",
      "iteration 1661, dc_loss: 0.06558950245380402, tv_loss: 0.01742391847074032\n",
      "iteration 1662, dc_loss: 0.0655895248055458, tv_loss: 0.017423653975129128\n",
      "iteration 1663, dc_loss: 0.0655895471572876, tv_loss: 0.017423713579773903\n",
      "iteration 1664, dc_loss: 0.0655895471572876, tv_loss: 0.017424272373318672\n",
      "iteration 1665, dc_loss: 0.06558950245380402, tv_loss: 0.017424173653125763\n",
      "iteration 1666, dc_loss: 0.06558946520090103, tv_loss: 0.017424022778868675\n",
      "iteration 1667, dc_loss: 0.06558950990438461, tv_loss: 0.01742430590093136\n",
      "iteration 1668, dc_loss: 0.0655895322561264, tv_loss: 0.017423897981643677\n",
      "iteration 1669, dc_loss: 0.065589539706707, tv_loss: 0.0174238421022892\n",
      "iteration 1670, dc_loss: 0.06558951735496521, tv_loss: 0.017424097284674644\n",
      "iteration 1671, dc_loss: 0.06558949500322342, tv_loss: 0.017424441874027252\n",
      "iteration 1672, dc_loss: 0.06558950245380402, tv_loss: 0.017424317076802254\n",
      "iteration 1673, dc_loss: 0.06558951735496521, tv_loss: 0.01742398738861084\n",
      "iteration 1674, dc_loss: 0.065589539706707, tv_loss: 0.01742396131157875\n",
      "iteration 1675, dc_loss: 0.0655895546078682, tv_loss: 0.01742447353899479\n",
      "iteration 1676, dc_loss: 0.0655895248055458, tv_loss: 0.017424263060092926\n",
      "iteration 1677, dc_loss: 0.06558951735496521, tv_loss: 0.01742391474545002\n",
      "iteration 1678, dc_loss: 0.06558948755264282, tv_loss: 0.0174240805208683\n",
      "iteration 1679, dc_loss: 0.06558946520090103, tv_loss: 0.017424028366804123\n",
      "iteration 1680, dc_loss: 0.06558945775032043, tv_loss: 0.017424143850803375\n",
      "iteration 1681, dc_loss: 0.06558950990438461, tv_loss: 0.017424074932932854\n",
      "iteration 1682, dc_loss: 0.06558957695960999, tv_loss: 0.017423732206225395\n",
      "iteration 1683, dc_loss: 0.06558958441019058, tv_loss: 0.017423590645194054\n",
      "iteration 1684, dc_loss: 0.0655895471572876, tv_loss: 0.017424210906028748\n",
      "iteration 1685, dc_loss: 0.06558948755264282, tv_loss: 0.017424335703253746\n",
      "iteration 1686, dc_loss: 0.06558945775032043, tv_loss: 0.017423812299966812\n",
      "iteration 1687, dc_loss: 0.06558946520090103, tv_loss: 0.017423810437321663\n",
      "iteration 1688, dc_loss: 0.06558949500322342, tv_loss: 0.017424186691641808\n",
      "iteration 1689, dc_loss: 0.06558950245380402, tv_loss: 0.01742425747215748\n",
      "iteration 1690, dc_loss: 0.0655895248055458, tv_loss: 0.017424169927835464\n",
      "iteration 1691, dc_loss: 0.06558956205844879, tv_loss: 0.01742403768002987\n",
      "iteration 1692, dc_loss: 0.06558956205844879, tv_loss: 0.01742406003177166\n",
      "iteration 1693, dc_loss: 0.0655895471572876, tv_loss: 0.017424220219254494\n",
      "iteration 1694, dc_loss: 0.06558948010206223, tv_loss: 0.017424147576093674\n",
      "iteration 1695, dc_loss: 0.06558945775032043, tv_loss: 0.017424140125513077\n",
      "iteration 1696, dc_loss: 0.06558946520090103, tv_loss: 0.017423981800675392\n",
      "iteration 1697, dc_loss: 0.0655895248055458, tv_loss: 0.017424067482352257\n",
      "iteration 1698, dc_loss: 0.0655895546078682, tv_loss: 0.017424089834094048\n",
      "iteration 1699, dc_loss: 0.06558956205844879, tv_loss: 0.017424050718545914\n",
      "iteration 1700, dc_loss: 0.0655895322561264, tv_loss: 0.017423950135707855\n",
      "iteration 1701, dc_loss: 0.06558950245380402, tv_loss: 0.017424190416932106\n",
      "iteration 1702, dc_loss: 0.06558949500322342, tv_loss: 0.01742415316402912\n",
      "iteration 1703, dc_loss: 0.06558950245380402, tv_loss: 0.017424199730157852\n",
      "iteration 1704, dc_loss: 0.06558949500322342, tv_loss: 0.01742406189441681\n",
      "iteration 1705, dc_loss: 0.06558951735496521, tv_loss: 0.017424076795578003\n",
      "iteration 1706, dc_loss: 0.0655895322561264, tv_loss: 0.017424065619707108\n",
      "iteration 1707, dc_loss: 0.065589539706707, tv_loss: 0.01742398552596569\n",
      "iteration 1708, dc_loss: 0.06558951735496521, tv_loss: 0.0174240842461586\n",
      "iteration 1709, dc_loss: 0.06558950990438461, tv_loss: 0.017423976212739944\n",
      "iteration 1710, dc_loss: 0.06558950245380402, tv_loss: 0.01742355339229107\n",
      "iteration 1711, dc_loss: 0.06558950245380402, tv_loss: 0.017423812299966812\n",
      "iteration 1712, dc_loss: 0.06558950990438461, tv_loss: 0.017424480989575386\n",
      "iteration 1713, dc_loss: 0.06558951735496521, tv_loss: 0.017424119636416435\n",
      "iteration 1714, dc_loss: 0.06558950990438461, tv_loss: 0.017423823475837708\n",
      "iteration 1715, dc_loss: 0.06558950990438461, tv_loss: 0.01742413453757763\n",
      "iteration 1716, dc_loss: 0.06558950245380402, tv_loss: 0.017424454912543297\n",
      "iteration 1717, dc_loss: 0.0655895248055458, tv_loss: 0.01742439903318882\n",
      "iteration 1718, dc_loss: 0.0655895471572876, tv_loss: 0.01742405630648136\n",
      "iteration 1719, dc_loss: 0.0655895248055458, tv_loss: 0.017424101009964943\n",
      "iteration 1720, dc_loss: 0.06558950245380402, tv_loss: 0.01742439530789852\n",
      "iteration 1721, dc_loss: 0.06558950245380402, tv_loss: 0.017424676567316055\n",
      "iteration 1722, dc_loss: 0.06558951735496521, tv_loss: 0.017424451187253\n",
      "iteration 1723, dc_loss: 0.0655895546078682, tv_loss: 0.01742406003177166\n",
      "iteration 1724, dc_loss: 0.06558956205844879, tv_loss: 0.01742410846054554\n",
      "iteration 1725, dc_loss: 0.06558951735496521, tv_loss: 0.017424140125513077\n",
      "iteration 1726, dc_loss: 0.06558950245380402, tv_loss: 0.017424315214157104\n",
      "iteration 1727, dc_loss: 0.06558950245380402, tv_loss: 0.01742410659790039\n",
      "iteration 1728, dc_loss: 0.06558950990438461, tv_loss: 0.01742415688931942\n",
      "iteration 1729, dc_loss: 0.06558950245380402, tv_loss: 0.01742396131157875\n",
      "iteration 1730, dc_loss: 0.0655895322561264, tv_loss: 0.017423784360289574\n",
      "iteration 1731, dc_loss: 0.06558956205844879, tv_loss: 0.01742379367351532\n",
      "iteration 1732, dc_loss: 0.0655895546078682, tv_loss: 0.01742393523454666\n",
      "iteration 1733, dc_loss: 0.0655895546078682, tv_loss: 0.01742396131157875\n",
      "iteration 1734, dc_loss: 0.06558951735496521, tv_loss: 0.01742386259138584\n",
      "iteration 1735, dc_loss: 0.06558946520090103, tv_loss: 0.01742400787770748\n",
      "iteration 1736, dc_loss: 0.06558944284915924, tv_loss: 0.017423920333385468\n",
      "iteration 1737, dc_loss: 0.06558948010206223, tv_loss: 0.017423925921320915\n",
      "iteration 1738, dc_loss: 0.06558951735496521, tv_loss: 0.017424000427126884\n",
      "iteration 1739, dc_loss: 0.0655895471572876, tv_loss: 0.01742388866841793\n",
      "iteration 1740, dc_loss: 0.06558956205844879, tv_loss: 0.017423957586288452\n",
      "iteration 1741, dc_loss: 0.0655895546078682, tv_loss: 0.017423944547772408\n",
      "iteration 1742, dc_loss: 0.0655895471572876, tv_loss: 0.017423946410417557\n",
      "iteration 1743, dc_loss: 0.06558948755264282, tv_loss: 0.017424050718545914\n",
      "iteration 1744, dc_loss: 0.06558945775032043, tv_loss: 0.017424168065190315\n",
      "iteration 1745, dc_loss: 0.06558947265148163, tv_loss: 0.01742413453757763\n",
      "iteration 1746, dc_loss: 0.06558950245380402, tv_loss: 0.017424045130610466\n",
      "iteration 1747, dc_loss: 0.06558956205844879, tv_loss: 0.01742405816912651\n",
      "iteration 1748, dc_loss: 0.06558956205844879, tv_loss: 0.017423931509256363\n",
      "iteration 1749, dc_loss: 0.0655895248055458, tv_loss: 0.017423970624804497\n",
      "iteration 1750, dc_loss: 0.06558948010206223, tv_loss: 0.01742403395473957\n",
      "iteration 1751, dc_loss: 0.06558948010206223, tv_loss: 0.01742403209209442\n",
      "iteration 1752, dc_loss: 0.06558950245380402, tv_loss: 0.017424199730157852\n",
      "iteration 1753, dc_loss: 0.0655895322561264, tv_loss: 0.017424266785383224\n",
      "iteration 1754, dc_loss: 0.0655895546078682, tv_loss: 0.017424043267965317\n",
      "iteration 1755, dc_loss: 0.0655895322561264, tv_loss: 0.01742408238351345\n",
      "iteration 1756, dc_loss: 0.06558950990438461, tv_loss: 0.01742410659790039\n",
      "iteration 1757, dc_loss: 0.06558950245380402, tv_loss: 0.017423707991838455\n",
      "iteration 1758, dc_loss: 0.06558950990438461, tv_loss: 0.01742372289299965\n",
      "iteration 1759, dc_loss: 0.06558950990438461, tv_loss: 0.01742422580718994\n",
      "iteration 1760, dc_loss: 0.06558950245380402, tv_loss: 0.01742417924106121\n",
      "iteration 1761, dc_loss: 0.06558948755264282, tv_loss: 0.01742388680577278\n",
      "iteration 1762, dc_loss: 0.06558951735496521, tv_loss: 0.017423588782548904\n",
      "iteration 1763, dc_loss: 0.0655895471572876, tv_loss: 0.01742435246706009\n",
      "iteration 1764, dc_loss: 0.0655895322561264, tv_loss: 0.017424331977963448\n",
      "iteration 1765, dc_loss: 0.06558951735496521, tv_loss: 0.017423981800675392\n",
      "iteration 1766, dc_loss: 0.06558951735496521, tv_loss: 0.017424212768673897\n",
      "iteration 1767, dc_loss: 0.06558951735496521, tv_loss: 0.0174240805208683\n",
      "iteration 1768, dc_loss: 0.06558950990438461, tv_loss: 0.017423957586288452\n",
      "iteration 1769, dc_loss: 0.06558948755264282, tv_loss: 0.01742403395473957\n",
      "iteration 1770, dc_loss: 0.06558948755264282, tv_loss: 0.017424238845705986\n",
      "iteration 1771, dc_loss: 0.06558950990438461, tv_loss: 0.017424382269382477\n",
      "iteration 1772, dc_loss: 0.0655895322561264, tv_loss: 0.017424097284674644\n",
      "iteration 1773, dc_loss: 0.0655895322561264, tv_loss: 0.01742406375706196\n",
      "iteration 1774, dc_loss: 0.06558950990438461, tv_loss: 0.017423974350094795\n",
      "iteration 1775, dc_loss: 0.06558948755264282, tv_loss: 0.017423909157514572\n",
      "iteration 1776, dc_loss: 0.06558949500322342, tv_loss: 0.01742406375706196\n",
      "iteration 1777, dc_loss: 0.06558951735496521, tv_loss: 0.01742396503686905\n",
      "iteration 1778, dc_loss: 0.06558951735496521, tv_loss: 0.017424190416932106\n",
      "iteration 1779, dc_loss: 0.0655895248055458, tv_loss: 0.0174243301153183\n",
      "iteration 1780, dc_loss: 0.06558951735496521, tv_loss: 0.017423951998353004\n",
      "iteration 1781, dc_loss: 0.0655895248055458, tv_loss: 0.01742387004196644\n",
      "iteration 1782, dc_loss: 0.0655895471572876, tv_loss: 0.01742401160299778\n",
      "iteration 1783, dc_loss: 0.0655895471572876, tv_loss: 0.017424149438738823\n",
      "iteration 1784, dc_loss: 0.06558951735496521, tv_loss: 0.01742413453757763\n",
      "iteration 1785, dc_loss: 0.06558948010206223, tv_loss: 0.01742432825267315\n",
      "iteration 1786, dc_loss: 0.06558948755264282, tv_loss: 0.017424263060092926\n",
      "iteration 1787, dc_loss: 0.0655895471572876, tv_loss: 0.01742403395473957\n",
      "iteration 1788, dc_loss: 0.06558956950902939, tv_loss: 0.017423735931515694\n",
      "iteration 1789, dc_loss: 0.0655895546078682, tv_loss: 0.01742406375706196\n",
      "iteration 1790, dc_loss: 0.06558951735496521, tv_loss: 0.017424199730157852\n",
      "iteration 1791, dc_loss: 0.06558947265148163, tv_loss: 0.017424115911126137\n",
      "iteration 1792, dc_loss: 0.06558946520090103, tv_loss: 0.017424270510673523\n",
      "iteration 1793, dc_loss: 0.06558949500322342, tv_loss: 0.017424140125513077\n",
      "iteration 1794, dc_loss: 0.06558951735496521, tv_loss: 0.017423946410417557\n",
      "iteration 1795, dc_loss: 0.0655895322561264, tv_loss: 0.01742405630648136\n",
      "iteration 1796, dc_loss: 0.0655895471572876, tv_loss: 0.017424117773771286\n",
      "iteration 1797, dc_loss: 0.0655895248055458, tv_loss: 0.01742410659790039\n",
      "iteration 1798, dc_loss: 0.06558950245380402, tv_loss: 0.017424190416932106\n",
      "iteration 1799, dc_loss: 0.06558947265148163, tv_loss: 0.01742417737841606\n",
      "iteration 1800, dc_loss: 0.06558948755264282, tv_loss: 0.01742423139512539\n",
      "iteration 1801, dc_loss: 0.0655895248055458, tv_loss: 0.01742437668144703\n",
      "iteration 1802, dc_loss: 0.06558956205844879, tv_loss: 0.017424076795578003\n",
      "iteration 1803, dc_loss: 0.0655895546078682, tv_loss: 0.017424054443836212\n",
      "iteration 1804, dc_loss: 0.06558951735496521, tv_loss: 0.017424432560801506\n",
      "iteration 1805, dc_loss: 0.06558950245380402, tv_loss: 0.017424551770091057\n",
      "iteration 1806, dc_loss: 0.06558948010206223, tv_loss: 0.01742427982389927\n",
      "iteration 1807, dc_loss: 0.06558951735496521, tv_loss: 0.01742406375706196\n",
      "iteration 1808, dc_loss: 0.0655895546078682, tv_loss: 0.017424326390028\n",
      "iteration 1809, dc_loss: 0.065589539706707, tv_loss: 0.017424076795578003\n",
      "iteration 1810, dc_loss: 0.06558951735496521, tv_loss: 0.017423981800675392\n",
      "iteration 1811, dc_loss: 0.06558950990438461, tv_loss: 0.01742425747215748\n",
      "iteration 1812, dc_loss: 0.06558950990438461, tv_loss: 0.017424238845705986\n",
      "iteration 1813, dc_loss: 0.06558950990438461, tv_loss: 0.017423978075385094\n",
      "iteration 1814, dc_loss: 0.06558950990438461, tv_loss: 0.017423953860998154\n",
      "iteration 1815, dc_loss: 0.06558950245380402, tv_loss: 0.01742406003177166\n",
      "iteration 1816, dc_loss: 0.06558950990438461, tv_loss: 0.01742425560951233\n",
      "iteration 1817, dc_loss: 0.0655895471572876, tv_loss: 0.017424101009964943\n",
      "iteration 1818, dc_loss: 0.065589539706707, tv_loss: 0.017423762008547783\n",
      "iteration 1819, dc_loss: 0.0655895471572876, tv_loss: 0.017424026504158974\n",
      "iteration 1820, dc_loss: 0.06558950990438461, tv_loss: 0.01742410846054554\n",
      "iteration 1821, dc_loss: 0.06558948755264282, tv_loss: 0.01742391847074032\n",
      "iteration 1822, dc_loss: 0.06558950990438461, tv_loss: 0.01742400787770748\n",
      "iteration 1823, dc_loss: 0.0655895322561264, tv_loss: 0.017424190416932106\n",
      "iteration 1824, dc_loss: 0.0655895322561264, tv_loss: 0.017424296587705612\n",
      "iteration 1825, dc_loss: 0.06558951735496521, tv_loss: 0.017424117773771286\n",
      "iteration 1826, dc_loss: 0.06558951735496521, tv_loss: 0.017423929646611214\n",
      "iteration 1827, dc_loss: 0.0655895248055458, tv_loss: 0.017424454912543297\n",
      "iteration 1828, dc_loss: 0.06558951735496521, tv_loss: 0.017424272373318672\n",
      "iteration 1829, dc_loss: 0.06558951735496521, tv_loss: 0.017424041405320168\n",
      "iteration 1830, dc_loss: 0.06558950990438461, tv_loss: 0.01742408238351345\n",
      "iteration 1831, dc_loss: 0.06558950245380402, tv_loss: 0.017424436286091805\n",
      "iteration 1832, dc_loss: 0.0655895248055458, tv_loss: 0.017424220219254494\n",
      "iteration 1833, dc_loss: 0.0655895248055458, tv_loss: 0.017424169927835464\n",
      "iteration 1834, dc_loss: 0.06558950990438461, tv_loss: 0.017424030229449272\n",
      "iteration 1835, dc_loss: 0.06558951735496521, tv_loss: 0.01742405816912651\n",
      "iteration 1836, dc_loss: 0.06558951735496521, tv_loss: 0.017424006015062332\n",
      "iteration 1837, dc_loss: 0.06558951735496521, tv_loss: 0.017423903569579124\n",
      "iteration 1838, dc_loss: 0.06558950990438461, tv_loss: 0.01742386259138584\n",
      "iteration 1839, dc_loss: 0.06558948755264282, tv_loss: 0.017424259334802628\n",
      "iteration 1840, dc_loss: 0.06558948010206223, tv_loss: 0.017424218356609344\n",
      "iteration 1841, dc_loss: 0.06558948010206223, tv_loss: 0.017423853278160095\n",
      "iteration 1842, dc_loss: 0.06558951735496521, tv_loss: 0.01742427609860897\n",
      "iteration 1843, dc_loss: 0.06558957695960999, tv_loss: 0.017424480989575386\n",
      "iteration 1844, dc_loss: 0.06558957695960999, tv_loss: 0.017424115911126137\n",
      "iteration 1845, dc_loss: 0.0655895322561264, tv_loss: 0.017423825338482857\n",
      "iteration 1846, dc_loss: 0.06558948010206223, tv_loss: 0.017424296587705612\n",
      "iteration 1847, dc_loss: 0.06558946520090103, tv_loss: 0.017424562945961952\n",
      "iteration 1848, dc_loss: 0.06558947265148163, tv_loss: 0.017424315214157104\n",
      "iteration 1849, dc_loss: 0.06558948010206223, tv_loss: 0.017424326390028\n",
      "iteration 1850, dc_loss: 0.0655895248055458, tv_loss: 0.017424296587705612\n",
      "iteration 1851, dc_loss: 0.0655895546078682, tv_loss: 0.01742410659790039\n",
      "iteration 1852, dc_loss: 0.06558956950902939, tv_loss: 0.0174240842461586\n",
      "iteration 1853, dc_loss: 0.06558956950902939, tv_loss: 0.017424441874027252\n",
      "iteration 1854, dc_loss: 0.0655895322561264, tv_loss: 0.017424538731575012\n",
      "iteration 1855, dc_loss: 0.06558948755264282, tv_loss: 0.017424201592803\n",
      "iteration 1856, dc_loss: 0.06558947265148163, tv_loss: 0.017424413934350014\n",
      "iteration 1857, dc_loss: 0.06558948755264282, tv_loss: 0.017424415796995163\n",
      "iteration 1858, dc_loss: 0.06558950990438461, tv_loss: 0.017424285411834717\n",
      "iteration 1859, dc_loss: 0.0655895322561264, tv_loss: 0.017424197867512703\n",
      "iteration 1860, dc_loss: 0.06558951735496521, tv_loss: 0.017424117773771286\n",
      "iteration 1861, dc_loss: 0.06558950990438461, tv_loss: 0.01742435246706009\n",
      "iteration 1862, dc_loss: 0.06558951735496521, tv_loss: 0.01742432825267315\n",
      "iteration 1863, dc_loss: 0.06558950990438461, tv_loss: 0.01742415316402912\n",
      "iteration 1864, dc_loss: 0.06558950245380402, tv_loss: 0.01742410846054554\n",
      "iteration 1865, dc_loss: 0.06558948755264282, tv_loss: 0.017424361780285835\n",
      "iteration 1866, dc_loss: 0.0655895248055458, tv_loss: 0.017423925921320915\n",
      "iteration 1867, dc_loss: 0.0655895546078682, tv_loss: 0.01742417737841606\n",
      "iteration 1868, dc_loss: 0.0655895471572876, tv_loss: 0.017424387857317924\n",
      "iteration 1869, dc_loss: 0.06558950245380402, tv_loss: 0.01742400787770748\n",
      "iteration 1870, dc_loss: 0.06558945775032043, tv_loss: 0.0174238383769989\n",
      "iteration 1871, dc_loss: 0.06558946520090103, tv_loss: 0.017424046993255615\n",
      "iteration 1872, dc_loss: 0.06558950990438461, tv_loss: 0.01742423139512539\n",
      "iteration 1873, dc_loss: 0.0655895471572876, tv_loss: 0.01742393709719181\n",
      "iteration 1874, dc_loss: 0.0655895546078682, tv_loss: 0.01742393523454666\n",
      "iteration 1875, dc_loss: 0.065589539706707, tv_loss: 0.017424119636416435\n",
      "iteration 1876, dc_loss: 0.06558950990438461, tv_loss: 0.01742381416261196\n",
      "iteration 1877, dc_loss: 0.06558950990438461, tv_loss: 0.017423957586288452\n",
      "iteration 1878, dc_loss: 0.06558950245380402, tv_loss: 0.01742391288280487\n",
      "iteration 1879, dc_loss: 0.06558948755264282, tv_loss: 0.017423948273062706\n",
      "iteration 1880, dc_loss: 0.06558950245380402, tv_loss: 0.017424238845705986\n",
      "iteration 1881, dc_loss: 0.06558950990438461, tv_loss: 0.0174242090433836\n",
      "iteration 1882, dc_loss: 0.0655895248055458, tv_loss: 0.017424076795578003\n",
      "iteration 1883, dc_loss: 0.0655895322561264, tv_loss: 0.017423752695322037\n",
      "iteration 1884, dc_loss: 0.0655895322561264, tv_loss: 0.017423944547772408\n",
      "iteration 1885, dc_loss: 0.0655895248055458, tv_loss: 0.017423955723643303\n",
      "iteration 1886, dc_loss: 0.06558951735496521, tv_loss: 0.01742405630648136\n",
      "iteration 1887, dc_loss: 0.06558950990438461, tv_loss: 0.01742388866841793\n",
      "iteration 1888, dc_loss: 0.06558950245380402, tv_loss: 0.017424019053578377\n",
      "iteration 1889, dc_loss: 0.06558948755264282, tv_loss: 0.017423927783966064\n",
      "iteration 1890, dc_loss: 0.06558950245380402, tv_loss: 0.01742370054125786\n",
      "iteration 1891, dc_loss: 0.0655895471572876, tv_loss: 0.01742378994822502\n",
      "iteration 1892, dc_loss: 0.06558956950902939, tv_loss: 0.017423946410417557\n",
      "iteration 1893, dc_loss: 0.0655895546078682, tv_loss: 0.017423931509256363\n",
      "iteration 1894, dc_loss: 0.06558951735496521, tv_loss: 0.01742367073893547\n",
      "iteration 1895, dc_loss: 0.06558948755264282, tv_loss: 0.017423681914806366\n",
      "iteration 1896, dc_loss: 0.06558947265148163, tv_loss: 0.017424164339900017\n",
      "iteration 1897, dc_loss: 0.06558948010206223, tv_loss: 0.01742410659790039\n",
      "iteration 1898, dc_loss: 0.06558951735496521, tv_loss: 0.017423683777451515\n",
      "iteration 1899, dc_loss: 0.0655895546078682, tv_loss: 0.017423860728740692\n",
      "iteration 1900, dc_loss: 0.0655895471572876, tv_loss: 0.017423903569579124\n",
      "iteration 1901, dc_loss: 0.0655895322561264, tv_loss: 0.01742391102015972\n",
      "iteration 1902, dc_loss: 0.0655895322561264, tv_loss: 0.017423836514353752\n",
      "iteration 1903, dc_loss: 0.06558951735496521, tv_loss: 0.01742371916770935\n",
      "iteration 1904, dc_loss: 0.06558948755264282, tv_loss: 0.01742389425635338\n",
      "iteration 1905, dc_loss: 0.06558946520090103, tv_loss: 0.01742403768002987\n",
      "iteration 1906, dc_loss: 0.06558948755264282, tv_loss: 0.01742391102015972\n",
      "iteration 1907, dc_loss: 0.0655895248055458, tv_loss: 0.017423786222934723\n",
      "iteration 1908, dc_loss: 0.0655895546078682, tv_loss: 0.01742400787770748\n",
      "iteration 1909, dc_loss: 0.0655895471572876, tv_loss: 0.017424054443836212\n",
      "iteration 1910, dc_loss: 0.06558951735496521, tv_loss: 0.01742366887629032\n",
      "iteration 1911, dc_loss: 0.06558946520090103, tv_loss: 0.01742376945912838\n",
      "iteration 1912, dc_loss: 0.06558947265148163, tv_loss: 0.017424043267965317\n",
      "iteration 1913, dc_loss: 0.06558950990438461, tv_loss: 0.017423655837774277\n",
      "iteration 1914, dc_loss: 0.0655895248055458, tv_loss: 0.017423398792743683\n",
      "iteration 1915, dc_loss: 0.0655895471572876, tv_loss: 0.01742379367351532\n",
      "iteration 1916, dc_loss: 0.0655895322561264, tv_loss: 0.01742386631667614\n",
      "iteration 1917, dc_loss: 0.06558950990438461, tv_loss: 0.017423367127776146\n",
      "iteration 1918, dc_loss: 0.06558948755264282, tv_loss: 0.017423667013645172\n",
      "iteration 1919, dc_loss: 0.06558948010206223, tv_loss: 0.017424024641513824\n",
      "iteration 1920, dc_loss: 0.06558948755264282, tv_loss: 0.017423726618289948\n",
      "iteration 1921, dc_loss: 0.06558950990438461, tv_loss: 0.01742357760667801\n",
      "iteration 1922, dc_loss: 0.06558956205844879, tv_loss: 0.017423612996935844\n",
      "iteration 1923, dc_loss: 0.06558957695960999, tv_loss: 0.017423922196030617\n",
      "iteration 1924, dc_loss: 0.065589539706707, tv_loss: 0.01742364838719368\n",
      "iteration 1925, dc_loss: 0.06558950245380402, tv_loss: 0.01742352731525898\n",
      "iteration 1926, dc_loss: 0.06558949500322342, tv_loss: 0.017423901706933975\n",
      "iteration 1927, dc_loss: 0.06558951735496521, tv_loss: 0.017423884943127632\n",
      "iteration 1928, dc_loss: 0.0655895322561264, tv_loss: 0.0174236036837101\n",
      "iteration 1929, dc_loss: 0.065589539706707, tv_loss: 0.017423616722226143\n",
      "iteration 1930, dc_loss: 0.06558951735496521, tv_loss: 0.01742377318441868\n",
      "iteration 1931, dc_loss: 0.06558950990438461, tv_loss: 0.017423992976546288\n",
      "iteration 1932, dc_loss: 0.06558949500322342, tv_loss: 0.01742374524474144\n",
      "iteration 1933, dc_loss: 0.06558950990438461, tv_loss: 0.01742376945912838\n",
      "iteration 1934, dc_loss: 0.0655895322561264, tv_loss: 0.01742413640022278\n",
      "iteration 1935, dc_loss: 0.0655895322561264, tv_loss: 0.017423922196030617\n",
      "iteration 1936, dc_loss: 0.06558951735496521, tv_loss: 0.01742355339229107\n",
      "iteration 1937, dc_loss: 0.06558950990438461, tv_loss: 0.017423400655388832\n",
      "iteration 1938, dc_loss: 0.06558950990438461, tv_loss: 0.017423996701836586\n",
      "iteration 1939, dc_loss: 0.06558951735496521, tv_loss: 0.017424266785383224\n",
      "iteration 1940, dc_loss: 0.06558951735496521, tv_loss: 0.01742352731525898\n",
      "iteration 1941, dc_loss: 0.06558950245380402, tv_loss: 0.017423834651708603\n",
      "iteration 1942, dc_loss: 0.06558950245380402, tv_loss: 0.017424000427126884\n",
      "iteration 1943, dc_loss: 0.0655895248055458, tv_loss: 0.017423858866095543\n",
      "iteration 1944, dc_loss: 0.0655895322561264, tv_loss: 0.01742376945912838\n",
      "iteration 1945, dc_loss: 0.0655895248055458, tv_loss: 0.017423873767256737\n",
      "iteration 1946, dc_loss: 0.06558950990438461, tv_loss: 0.01742417924106121\n",
      "iteration 1947, dc_loss: 0.06558951735496521, tv_loss: 0.017423905432224274\n",
      "iteration 1948, dc_loss: 0.06558950990438461, tv_loss: 0.01742396503686905\n",
      "iteration 1949, dc_loss: 0.06558950245380402, tv_loss: 0.017423927783966064\n",
      "iteration 1950, dc_loss: 0.06558949500322342, tv_loss: 0.017424065619707108\n",
      "iteration 1951, dc_loss: 0.06558950245380402, tv_loss: 0.01742420718073845\n",
      "iteration 1952, dc_loss: 0.0655895248055458, tv_loss: 0.01742413267493248\n",
      "iteration 1953, dc_loss: 0.0655895248055458, tv_loss: 0.01742403209209442\n",
      "iteration 1954, dc_loss: 0.06558951735496521, tv_loss: 0.017423860728740692\n",
      "iteration 1955, dc_loss: 0.06558951735496521, tv_loss: 0.017423896118998528\n",
      "iteration 1956, dc_loss: 0.06558950990438461, tv_loss: 0.01742386631667614\n",
      "iteration 1957, dc_loss: 0.0655895322561264, tv_loss: 0.017424127086997032\n",
      "iteration 1958, dc_loss: 0.0655895248055458, tv_loss: 0.0174238383769989\n",
      "iteration 1959, dc_loss: 0.06558948755264282, tv_loss: 0.01742391474545002\n",
      "iteration 1960, dc_loss: 0.06558945775032043, tv_loss: 0.01742415316402912\n",
      "iteration 1961, dc_loss: 0.06558947265148163, tv_loss: 0.017424115911126137\n",
      "iteration 1962, dc_loss: 0.0655895322561264, tv_loss: 0.01742389053106308\n",
      "iteration 1963, dc_loss: 0.06558957695960999, tv_loss: 0.017423933371901512\n",
      "iteration 1964, dc_loss: 0.06558957695960999, tv_loss: 0.01742384396493435\n",
      "iteration 1965, dc_loss: 0.065589539706707, tv_loss: 0.017423568293452263\n",
      "iteration 1966, dc_loss: 0.06558950245380402, tv_loss: 0.01742391102015972\n",
      "iteration 1967, dc_loss: 0.06558950245380402, tv_loss: 0.017424004152417183\n",
      "iteration 1968, dc_loss: 0.06558950245380402, tv_loss: 0.017424166202545166\n",
      "iteration 1969, dc_loss: 0.06558950245380402, tv_loss: 0.01742378994822502\n",
      "iteration 1970, dc_loss: 0.06558951735496521, tv_loss: 0.017424030229449272\n",
      "iteration 1971, dc_loss: 0.0655895248055458, tv_loss: 0.01742393709719181\n",
      "iteration 1972, dc_loss: 0.06558951735496521, tv_loss: 0.017423858866095543\n",
      "iteration 1973, dc_loss: 0.06558950990438461, tv_loss: 0.017423924058675766\n",
      "iteration 1974, dc_loss: 0.06558950245380402, tv_loss: 0.01742418296635151\n",
      "iteration 1975, dc_loss: 0.06558950245380402, tv_loss: 0.01742381788790226\n",
      "iteration 1976, dc_loss: 0.0655895248055458, tv_loss: 0.017423612996935844\n",
      "iteration 1977, dc_loss: 0.0655895248055458, tv_loss: 0.01742386631667614\n",
      "iteration 1978, dc_loss: 0.065589539706707, tv_loss: 0.01742389425635338\n",
      "iteration 1979, dc_loss: 0.065589539706707, tv_loss: 0.017423681914806366\n",
      "iteration 1980, dc_loss: 0.06558950245380402, tv_loss: 0.01742370054125786\n",
      "iteration 1981, dc_loss: 0.06558948010206223, tv_loss: 0.017423491925001144\n",
      "iteration 1982, dc_loss: 0.06558950245380402, tv_loss: 0.0174239594489336\n",
      "iteration 1983, dc_loss: 0.06558951735496521, tv_loss: 0.017423871904611588\n",
      "iteration 1984, dc_loss: 0.06558950990438461, tv_loss: 0.01742367073893547\n",
      "iteration 1985, dc_loss: 0.06558951735496521, tv_loss: 0.01742362603545189\n",
      "iteration 1986, dc_loss: 0.06558951735496521, tv_loss: 0.01742386631667614\n",
      "iteration 1987, dc_loss: 0.065589539706707, tv_loss: 0.017423998564481735\n",
      "iteration 1988, dc_loss: 0.0655895248055458, tv_loss: 0.017423834651708603\n",
      "iteration 1989, dc_loss: 0.06558950245380402, tv_loss: 0.01742389239370823\n",
      "iteration 1990, dc_loss: 0.06558948755264282, tv_loss: 0.01742372289299965\n",
      "iteration 1991, dc_loss: 0.06558948010206223, tv_loss: 0.01742410846054554\n",
      "iteration 1992, dc_loss: 0.06558951735496521, tv_loss: 0.0174238383769989\n",
      "iteration 1993, dc_loss: 0.0655895322561264, tv_loss: 0.01742379181087017\n",
      "iteration 1994, dc_loss: 0.0655895322561264, tv_loss: 0.017423948273062706\n",
      "iteration 1995, dc_loss: 0.06558950990438461, tv_loss: 0.017424093559384346\n",
      "iteration 1996, dc_loss: 0.06558951735496521, tv_loss: 0.0174240805208683\n",
      "iteration 1997, dc_loss: 0.0655895546078682, tv_loss: 0.01742391102015972\n",
      "iteration 1998, dc_loss: 0.0655895471572876, tv_loss: 0.017424097284674644\n",
      "iteration 1999, dc_loss: 0.06558951735496521, tv_loss: 0.017424285411834717\n",
      "iteration 2000, dc_loss: 0.06558948755264282, tv_loss: 0.01742434874176979\n",
      "iteration 2001, dc_loss: 0.06558946520090103, tv_loss: 0.0174239594489336\n",
      "iteration 2002, dc_loss: 0.06558947265148163, tv_loss: 0.01742403209209442\n",
      "iteration 2003, dc_loss: 0.0655895248055458, tv_loss: 0.017424315214157104\n",
      "iteration 2004, dc_loss: 0.0655895546078682, tv_loss: 0.017423953860998154\n",
      "iteration 2005, dc_loss: 0.06558956205844879, tv_loss: 0.01742401160299778\n",
      "iteration 2006, dc_loss: 0.06558956205844879, tv_loss: 0.01742410473525524\n",
      "iteration 2007, dc_loss: 0.0655895248055458, tv_loss: 0.0174243226647377\n",
      "iteration 2008, dc_loss: 0.06558947265148163, tv_loss: 0.017424119636416435\n",
      "iteration 2009, dc_loss: 0.06558944284915924, tv_loss: 0.017424164339900017\n",
      "iteration 2010, dc_loss: 0.06558946520090103, tv_loss: 0.01742413640022278\n",
      "iteration 2011, dc_loss: 0.0655895322561264, tv_loss: 0.01742405816912651\n",
      "iteration 2012, dc_loss: 0.06558957695960999, tv_loss: 0.01742401160299778\n",
      "iteration 2013, dc_loss: 0.06558957695960999, tv_loss: 0.01742413453757763\n",
      "iteration 2014, dc_loss: 0.0655895471572876, tv_loss: 0.01742387004196644\n",
      "iteration 2015, dc_loss: 0.06558950990438461, tv_loss: 0.017423860728740692\n",
      "iteration 2016, dc_loss: 0.06558948755264282, tv_loss: 0.017424266785383224\n",
      "iteration 2017, dc_loss: 0.06558950245380402, tv_loss: 0.017424074932932854\n",
      "iteration 2018, dc_loss: 0.06558951735496521, tv_loss: 0.01742394268512726\n",
      "iteration 2019, dc_loss: 0.06558951735496521, tv_loss: 0.017424101009964943\n",
      "iteration 2020, dc_loss: 0.06558950990438461, tv_loss: 0.017424242570996284\n",
      "iteration 2021, dc_loss: 0.06558951735496521, tv_loss: 0.017424190416932106\n",
      "iteration 2022, dc_loss: 0.0655895248055458, tv_loss: 0.017424261197447777\n",
      "iteration 2023, dc_loss: 0.0655895248055458, tv_loss: 0.01742425747215748\n",
      "iteration 2024, dc_loss: 0.0655895471572876, tv_loss: 0.0174238458275795\n",
      "iteration 2025, dc_loss: 0.0655895322561264, tv_loss: 0.01742393523454666\n",
      "iteration 2026, dc_loss: 0.06558951735496521, tv_loss: 0.017424052581191063\n",
      "iteration 2027, dc_loss: 0.0655895248055458, tv_loss: 0.01742420718073845\n",
      "iteration 2028, dc_loss: 0.06558950990438461, tv_loss: 0.01742364838719368\n",
      "iteration 2029, dc_loss: 0.06558948755264282, tv_loss: 0.01742357388138771\n",
      "iteration 2030, dc_loss: 0.06558946520090103, tv_loss: 0.01742427982389927\n",
      "iteration 2031, dc_loss: 0.06558947265148163, tv_loss: 0.017424192279577255\n",
      "iteration 2032, dc_loss: 0.06558950245380402, tv_loss: 0.017423776909708977\n",
      "iteration 2033, dc_loss: 0.0655895471572876, tv_loss: 0.017423633486032486\n",
      "iteration 2034, dc_loss: 0.06558956950902939, tv_loss: 0.017423931509256363\n",
      "iteration 2035, dc_loss: 0.0655895546078682, tv_loss: 0.017423883080482483\n",
      "iteration 2036, dc_loss: 0.06558951735496521, tv_loss: 0.017423855140805244\n",
      "iteration 2037, dc_loss: 0.06558948010206223, tv_loss: 0.01742369309067726\n",
      "iteration 2038, dc_loss: 0.06558948010206223, tv_loss: 0.017423957586288452\n",
      "iteration 2039, dc_loss: 0.06558950245380402, tv_loss: 0.017424358054995537\n",
      "iteration 2040, dc_loss: 0.06558950245380402, tv_loss: 0.017423897981643677\n",
      "iteration 2041, dc_loss: 0.06558950990438461, tv_loss: 0.017423514276742935\n",
      "iteration 2042, dc_loss: 0.0655895471572876, tv_loss: 0.01742403395473957\n",
      "iteration 2043, dc_loss: 0.0655895546078682, tv_loss: 0.017424050718545914\n",
      "iteration 2044, dc_loss: 0.06558950990438461, tv_loss: 0.017423732206225395\n",
      "iteration 2045, dc_loss: 0.06558949500322342, tv_loss: 0.017423762008547783\n",
      "iteration 2046, dc_loss: 0.06558950245380402, tv_loss: 0.01742410473525524\n",
      "iteration 2047, dc_loss: 0.06558951735496521, tv_loss: 0.017423998564481735\n",
      "iteration 2048, dc_loss: 0.0655895248055458, tv_loss: 0.017423495650291443\n",
      "iteration 2049, dc_loss: 0.06558950245380402, tv_loss: 0.017423739656805992\n",
      "iteration 2050, dc_loss: 0.06558950245380402, tv_loss: 0.01742384023964405\n",
      "iteration 2051, dc_loss: 0.06558951735496521, tv_loss: 0.01742374710738659\n",
      "iteration 2052, dc_loss: 0.065589539706707, tv_loss: 0.01742391847074032\n",
      "iteration 2053, dc_loss: 0.0655895471572876, tv_loss: 0.017423782497644424\n",
      "iteration 2054, dc_loss: 0.0655895248055458, tv_loss: 0.01742376759648323\n",
      "iteration 2055, dc_loss: 0.06558950245380402, tv_loss: 0.01742362789809704\n",
      "iteration 2056, dc_loss: 0.06558948010206223, tv_loss: 0.017423875629901886\n",
      "iteration 2057, dc_loss: 0.06558948755264282, tv_loss: 0.017423689365386963\n",
      "iteration 2058, dc_loss: 0.06558950245380402, tv_loss: 0.017423653975129128\n",
      "iteration 2059, dc_loss: 0.0655895322561264, tv_loss: 0.017423968762159348\n",
      "iteration 2060, dc_loss: 0.0655895322561264, tv_loss: 0.01742362789809704\n",
      "iteration 2061, dc_loss: 0.06558951735496521, tv_loss: 0.01742345653474331\n",
      "iteration 2062, dc_loss: 0.065589539706707, tv_loss: 0.017423417419195175\n",
      "iteration 2063, dc_loss: 0.0655895471572876, tv_loss: 0.01742369681596756\n",
      "iteration 2064, dc_loss: 0.0655895248055458, tv_loss: 0.017423806712031364\n",
      "iteration 2065, dc_loss: 0.06558948755264282, tv_loss: 0.01742367073893547\n",
      "iteration 2066, dc_loss: 0.06558950245380402, tv_loss: 0.01742362417280674\n",
      "iteration 2067, dc_loss: 0.0655895322561264, tv_loss: 0.017423558980226517\n",
      "iteration 2068, dc_loss: 0.0655895322561264, tv_loss: 0.01742384023964405\n",
      "iteration 2069, dc_loss: 0.065589539706707, tv_loss: 0.017424093559384346\n",
      "iteration 2070, dc_loss: 0.06558951735496521, tv_loss: 0.01742345280945301\n",
      "iteration 2071, dc_loss: 0.06558950245380402, tv_loss: 0.01742340438067913\n",
      "iteration 2072, dc_loss: 0.06558948755264282, tv_loss: 0.017424000427126884\n",
      "iteration 2073, dc_loss: 0.06558950245380402, tv_loss: 0.017423875629901886\n",
      "iteration 2074, dc_loss: 0.06558950990438461, tv_loss: 0.017423667013645172\n",
      "iteration 2075, dc_loss: 0.0655895471572876, tv_loss: 0.017423616722226143\n",
      "iteration 2076, dc_loss: 0.0655895546078682, tv_loss: 0.017423702403903008\n",
      "iteration 2077, dc_loss: 0.0655895546078682, tv_loss: 0.01742384396493435\n",
      "iteration 2078, dc_loss: 0.0655895322561264, tv_loss: 0.017423970624804497\n",
      "iteration 2079, dc_loss: 0.06558948010206223, tv_loss: 0.01742408610880375\n",
      "iteration 2080, dc_loss: 0.06558946520090103, tv_loss: 0.01742413267493248\n",
      "iteration 2081, dc_loss: 0.06558948755264282, tv_loss: 0.017424073070287704\n",
      "iteration 2082, dc_loss: 0.0655895248055458, tv_loss: 0.017424020916223526\n",
      "iteration 2083, dc_loss: 0.06558956205844879, tv_loss: 0.0174238421022892\n",
      "iteration 2084, dc_loss: 0.0655895546078682, tv_loss: 0.01742379553616047\n",
      "iteration 2085, dc_loss: 0.06558950990438461, tv_loss: 0.017424238845705986\n",
      "iteration 2086, dc_loss: 0.06558946520090103, tv_loss: 0.01742410659790039\n",
      "iteration 2087, dc_loss: 0.06558948755264282, tv_loss: 0.017424052581191063\n",
      "iteration 2088, dc_loss: 0.06558951735496521, tv_loss: 0.017424073070287704\n",
      "iteration 2089, dc_loss: 0.06558951735496521, tv_loss: 0.01742398738861084\n",
      "iteration 2090, dc_loss: 0.0655895248055458, tv_loss: 0.017424067482352257\n",
      "iteration 2091, dc_loss: 0.0655895322561264, tv_loss: 0.017423786222934723\n",
      "iteration 2092, dc_loss: 0.06558950990438461, tv_loss: 0.01742415875196457\n",
      "iteration 2093, dc_loss: 0.06558950245380402, tv_loss: 0.01742413640022278\n",
      "iteration 2094, dc_loss: 0.06558948755264282, tv_loss: 0.017423853278160095\n",
      "iteration 2095, dc_loss: 0.06558948755264282, tv_loss: 0.017423909157514572\n",
      "iteration 2096, dc_loss: 0.06558950990438461, tv_loss: 0.01742412894964218\n",
      "iteration 2097, dc_loss: 0.0655895322561264, tv_loss: 0.017423948273062706\n",
      "iteration 2098, dc_loss: 0.065589539706707, tv_loss: 0.017423974350094795\n",
      "iteration 2099, dc_loss: 0.0655895322561264, tv_loss: 0.01742403954267502\n",
      "iteration 2100, dc_loss: 0.06558950990438461, tv_loss: 0.01742425188422203\n",
      "iteration 2101, dc_loss: 0.06558950245380402, tv_loss: 0.017424361780285835\n",
      "iteration 2102, dc_loss: 0.06558950990438461, tv_loss: 0.017424190416932106\n",
      "iteration 2103, dc_loss: 0.06558950990438461, tv_loss: 0.0174240842461586\n",
      "iteration 2104, dc_loss: 0.06558950245380402, tv_loss: 0.01742401532828808\n",
      "iteration 2105, dc_loss: 0.06558951735496521, tv_loss: 0.01742382161319256\n",
      "iteration 2106, dc_loss: 0.0655895322561264, tv_loss: 0.01742367073893547\n",
      "iteration 2107, dc_loss: 0.0655895471572876, tv_loss: 0.017423851415514946\n",
      "iteration 2108, dc_loss: 0.0655895471572876, tv_loss: 0.017423909157514572\n",
      "iteration 2109, dc_loss: 0.06558950245380402, tv_loss: 0.01742379553616047\n",
      "iteration 2110, dc_loss: 0.06558948010206223, tv_loss: 0.017423570156097412\n",
      "iteration 2111, dc_loss: 0.06558949500322342, tv_loss: 0.017423875629901886\n",
      "iteration 2112, dc_loss: 0.06558950990438461, tv_loss: 0.017423944547772408\n",
      "iteration 2113, dc_loss: 0.0655895248055458, tv_loss: 0.01742364652454853\n",
      "iteration 2114, dc_loss: 0.0655895248055458, tv_loss: 0.017423585057258606\n",
      "iteration 2115, dc_loss: 0.0655895248055458, tv_loss: 0.017423907294869423\n",
      "iteration 2116, dc_loss: 0.06558950990438461, tv_loss: 0.017423830926418304\n",
      "iteration 2117, dc_loss: 0.06558950245380402, tv_loss: 0.017423659563064575\n",
      "iteration 2118, dc_loss: 0.06558950990438461, tv_loss: 0.017423778772354126\n",
      "iteration 2119, dc_loss: 0.06558950990438461, tv_loss: 0.017423739656805992\n",
      "iteration 2120, dc_loss: 0.06558951735496521, tv_loss: 0.017423726618289948\n",
      "iteration 2121, dc_loss: 0.06558948755264282, tv_loss: 0.01742357760667801\n",
      "iteration 2122, dc_loss: 0.06558950990438461, tv_loss: 0.017423830926418304\n",
      "iteration 2123, dc_loss: 0.0655895471572876, tv_loss: 0.01742422766983509\n",
      "iteration 2124, dc_loss: 0.0655895322561264, tv_loss: 0.017424050718545914\n",
      "iteration 2125, dc_loss: 0.06558951735496521, tv_loss: 0.01742384396493435\n",
      "iteration 2126, dc_loss: 0.06558948755264282, tv_loss: 0.01742386817932129\n",
      "iteration 2127, dc_loss: 0.06558948010206223, tv_loss: 0.01742415316402912\n",
      "iteration 2128, dc_loss: 0.06558948755264282, tv_loss: 0.017424562945961952\n",
      "iteration 2129, dc_loss: 0.06558951735496521, tv_loss: 0.017424054443836212\n",
      "iteration 2130, dc_loss: 0.0655895322561264, tv_loss: 0.0174236036837101\n",
      "iteration 2131, dc_loss: 0.0655895471572876, tv_loss: 0.01742374524474144\n",
      "iteration 2132, dc_loss: 0.0655895471572876, tv_loss: 0.017424017190933228\n",
      "iteration 2133, dc_loss: 0.0655895248055458, tv_loss: 0.017423786222934723\n",
      "iteration 2134, dc_loss: 0.06558947265148163, tv_loss: 0.01742374710738659\n",
      "iteration 2135, dc_loss: 0.06558942794799805, tv_loss: 0.017424054443836212\n",
      "iteration 2136, dc_loss: 0.06558946520090103, tv_loss: 0.017424065619707108\n",
      "iteration 2137, dc_loss: 0.0655895248055458, tv_loss: 0.0174238458275795\n",
      "iteration 2138, dc_loss: 0.06558957695960999, tv_loss: 0.01742378994822502\n",
      "iteration 2139, dc_loss: 0.06558957695960999, tv_loss: 0.017424030229449272\n",
      "iteration 2140, dc_loss: 0.0655895322561264, tv_loss: 0.01742425188422203\n",
      "iteration 2141, dc_loss: 0.06558948755264282, tv_loss: 0.017424272373318672\n",
      "iteration 2142, dc_loss: 0.06558948010206223, tv_loss: 0.01742403395473957\n",
      "iteration 2143, dc_loss: 0.06558948755264282, tv_loss: 0.01742400974035263\n",
      "iteration 2144, dc_loss: 0.06558950245380402, tv_loss: 0.017424186691641808\n",
      "iteration 2145, dc_loss: 0.06558950245380402, tv_loss: 0.017424283549189568\n",
      "iteration 2146, dc_loss: 0.06558951735496521, tv_loss: 0.01742403581738472\n",
      "iteration 2147, dc_loss: 0.0655895546078682, tv_loss: 0.017424114048480988\n",
      "iteration 2148, dc_loss: 0.0655895546078682, tv_loss: 0.01742410846054554\n",
      "iteration 2149, dc_loss: 0.06558951735496521, tv_loss: 0.01742377318441868\n",
      "iteration 2150, dc_loss: 0.06558950245380402, tv_loss: 0.01742391102015972\n",
      "iteration 2151, dc_loss: 0.06558948010206223, tv_loss: 0.017424488440155983\n",
      "iteration 2152, dc_loss: 0.06558948010206223, tv_loss: 0.01742410659790039\n",
      "iteration 2153, dc_loss: 0.06558948755264282, tv_loss: 0.017423424869775772\n",
      "iteration 2154, dc_loss: 0.06558951735496521, tv_loss: 0.017423579469323158\n",
      "iteration 2155, dc_loss: 0.0655895471572876, tv_loss: 0.01742415316402912\n",
      "iteration 2156, dc_loss: 0.065589539706707, tv_loss: 0.01742378994822502\n",
      "iteration 2157, dc_loss: 0.065589539706707, tv_loss: 0.01742333360016346\n",
      "iteration 2158, dc_loss: 0.0655895322561264, tv_loss: 0.01742364838719368\n",
      "iteration 2159, dc_loss: 0.06558951735496521, tv_loss: 0.017423903569579124\n",
      "iteration 2160, dc_loss: 0.06558950245380402, tv_loss: 0.017424194142222404\n",
      "iteration 2161, dc_loss: 0.06558948755264282, tv_loss: 0.017423713579773903\n",
      "iteration 2162, dc_loss: 0.06558950990438461, tv_loss: 0.017423471435904503\n",
      "iteration 2163, dc_loss: 0.06558951735496521, tv_loss: 0.017423922196030617\n",
      "iteration 2164, dc_loss: 0.06558950245380402, tv_loss: 0.017424030229449272\n",
      "iteration 2165, dc_loss: 0.06558950245380402, tv_loss: 0.017424045130610466\n",
      "iteration 2166, dc_loss: 0.06558950245380402, tv_loss: 0.01742410846054554\n",
      "iteration 2167, dc_loss: 0.06558950990438461, tv_loss: 0.01742394082248211\n",
      "iteration 2168, dc_loss: 0.0655895248055458, tv_loss: 0.01742376573383808\n",
      "iteration 2169, dc_loss: 0.06558951735496521, tv_loss: 0.01742374151945114\n",
      "iteration 2170, dc_loss: 0.06558949500322342, tv_loss: 0.017424069344997406\n",
      "iteration 2171, dc_loss: 0.06558950990438461, tv_loss: 0.017424149438738823\n",
      "iteration 2172, dc_loss: 0.0655895322561264, tv_loss: 0.017424169927835464\n",
      "iteration 2173, dc_loss: 0.0655895471572876, tv_loss: 0.017423462122678757\n",
      "iteration 2174, dc_loss: 0.065589539706707, tv_loss: 0.017423544079065323\n",
      "iteration 2175, dc_loss: 0.06558948010206223, tv_loss: 0.01742410846054554\n",
      "iteration 2176, dc_loss: 0.06558945775032043, tv_loss: 0.017424078658223152\n",
      "iteration 2177, dc_loss: 0.06558947265148163, tv_loss: 0.017423683777451515\n",
      "iteration 2178, dc_loss: 0.06558951735496521, tv_loss: 0.01742369681596756\n",
      "iteration 2179, dc_loss: 0.06558956205844879, tv_loss: 0.01742374710738659\n",
      "iteration 2180, dc_loss: 0.0655895546078682, tv_loss: 0.017423735931515694\n",
      "iteration 2181, dc_loss: 0.06558951735496521, tv_loss: 0.017423667013645172\n",
      "iteration 2182, dc_loss: 0.06558948755264282, tv_loss: 0.017423566430807114\n",
      "iteration 2183, dc_loss: 0.06558948010206223, tv_loss: 0.01742337830364704\n",
      "iteration 2184, dc_loss: 0.06558948010206223, tv_loss: 0.017423583194613457\n",
      "iteration 2185, dc_loss: 0.06558949500322342, tv_loss: 0.017423531040549278\n",
      "iteration 2186, dc_loss: 0.0655895471572876, tv_loss: 0.01742350496351719\n",
      "iteration 2187, dc_loss: 0.06558957695960999, tv_loss: 0.017423491925001144\n",
      "iteration 2188, dc_loss: 0.065589539706707, tv_loss: 0.01742333360016346\n",
      "iteration 2189, dc_loss: 0.06558948755264282, tv_loss: 0.01742345467209816\n",
      "iteration 2190, dc_loss: 0.06558947265148163, tv_loss: 0.017423847690224648\n",
      "iteration 2191, dc_loss: 0.06558946520090103, tv_loss: 0.01742352917790413\n",
      "iteration 2192, dc_loss: 0.06558950245380402, tv_loss: 0.017423391342163086\n",
      "iteration 2193, dc_loss: 0.06558950990438461, tv_loss: 0.01742326281964779\n",
      "iteration 2194, dc_loss: 0.06558951735496521, tv_loss: 0.017423361539840698\n",
      "iteration 2195, dc_loss: 0.0655895471572876, tv_loss: 0.017423655837774277\n",
      "iteration 2196, dc_loss: 0.0655895471572876, tv_loss: 0.01742367073893547\n",
      "iteration 2197, dc_loss: 0.0655895248055458, tv_loss: 0.01742337830364704\n",
      "iteration 2198, dc_loss: 0.06558949500322342, tv_loss: 0.017423590645194054\n",
      "iteration 2199, dc_loss: 0.06558948755264282, tv_loss: 0.017423780634999275\n",
      "iteration 2200, dc_loss: 0.06558948755264282, tv_loss: 0.017423948273062706\n",
      "iteration 2201, dc_loss: 0.06558951735496521, tv_loss: 0.017423583194613457\n",
      "iteration 2202, dc_loss: 0.0655895471572876, tv_loss: 0.01742364652454853\n",
      "iteration 2203, dc_loss: 0.06558956205844879, tv_loss: 0.01742384396493435\n",
      "iteration 2204, dc_loss: 0.065589539706707, tv_loss: 0.017423879355192184\n",
      "iteration 2205, dc_loss: 0.06558948755264282, tv_loss: 0.01742381602525711\n",
      "iteration 2206, dc_loss: 0.06558947265148163, tv_loss: 0.017423901706933975\n",
      "iteration 2207, dc_loss: 0.06558948755264282, tv_loss: 0.017424022778868675\n",
      "iteration 2208, dc_loss: 0.06558950990438461, tv_loss: 0.017423635348677635\n",
      "iteration 2209, dc_loss: 0.0655895322561264, tv_loss: 0.01742391102015972\n",
      "iteration 2210, dc_loss: 0.0655895322561264, tv_loss: 0.017424171790480614\n",
      "iteration 2211, dc_loss: 0.0655895248055458, tv_loss: 0.0174238458275795\n",
      "iteration 2212, dc_loss: 0.06558950990438461, tv_loss: 0.017423616722226143\n",
      "iteration 2213, dc_loss: 0.0655895248055458, tv_loss: 0.017424071207642555\n",
      "iteration 2214, dc_loss: 0.0655895248055458, tv_loss: 0.017423931509256363\n",
      "iteration 2215, dc_loss: 0.06558951735496521, tv_loss: 0.017423752695322037\n",
      "iteration 2216, dc_loss: 0.06558950990438461, tv_loss: 0.017423920333385468\n",
      "iteration 2217, dc_loss: 0.06558948755264282, tv_loss: 0.017423849552869797\n",
      "iteration 2218, dc_loss: 0.06558949500322342, tv_loss: 0.01742388680577278\n",
      "iteration 2219, dc_loss: 0.0655895248055458, tv_loss: 0.017423909157514572\n",
      "iteration 2220, dc_loss: 0.0655895322561264, tv_loss: 0.01742376945912838\n",
      "iteration 2221, dc_loss: 0.0655895471572876, tv_loss: 0.0174238383769989\n",
      "iteration 2222, dc_loss: 0.0655895546078682, tv_loss: 0.01742378994822502\n",
      "iteration 2223, dc_loss: 0.0655895248055458, tv_loss: 0.017423799261450768\n",
      "iteration 2224, dc_loss: 0.06558948755264282, tv_loss: 0.017423784360289574\n",
      "iteration 2225, dc_loss: 0.06558948755264282, tv_loss: 0.01742386817932129\n",
      "iteration 2226, dc_loss: 0.06558948755264282, tv_loss: 0.017423631623387337\n",
      "iteration 2227, dc_loss: 0.0655895322561264, tv_loss: 0.017423760145902634\n",
      "iteration 2228, dc_loss: 0.06558956205844879, tv_loss: 0.01742403768002987\n",
      "iteration 2229, dc_loss: 0.065589539706707, tv_loss: 0.017423762008547783\n",
      "iteration 2230, dc_loss: 0.06558948010206223, tv_loss: 0.017423303797841072\n",
      "iteration 2231, dc_loss: 0.06558945775032043, tv_loss: 0.017423687502741814\n",
      "iteration 2232, dc_loss: 0.06558948755264282, tv_loss: 0.01742406003177166\n",
      "iteration 2233, dc_loss: 0.0655895248055458, tv_loss: 0.01742374897003174\n",
      "iteration 2234, dc_loss: 0.0655895546078682, tv_loss: 0.01742382161319256\n",
      "iteration 2235, dc_loss: 0.0655895471572876, tv_loss: 0.01742403581738472\n",
      "iteration 2236, dc_loss: 0.0655895248055458, tv_loss: 0.017423788085579872\n",
      "iteration 2237, dc_loss: 0.0655895248055458, tv_loss: 0.01742386259138584\n",
      "iteration 2238, dc_loss: 0.06558950245380402, tv_loss: 0.017423946410417557\n",
      "iteration 2239, dc_loss: 0.06558948755264282, tv_loss: 0.017423981800675392\n",
      "iteration 2240, dc_loss: 0.06558947265148163, tv_loss: 0.01742425747215748\n",
      "iteration 2241, dc_loss: 0.06558948010206223, tv_loss: 0.017423810437321663\n",
      "iteration 2242, dc_loss: 0.06558950990438461, tv_loss: 0.017423836514353752\n",
      "iteration 2243, dc_loss: 0.0655895546078682, tv_loss: 0.017424244433641434\n",
      "iteration 2244, dc_loss: 0.0655895471572876, tv_loss: 0.017423922196030617\n",
      "iteration 2245, dc_loss: 0.06558950990438461, tv_loss: 0.017423922196030617\n",
      "iteration 2246, dc_loss: 0.06558948010206223, tv_loss: 0.01742382161319256\n",
      "iteration 2247, dc_loss: 0.06558948010206223, tv_loss: 0.017424123361706734\n",
      "iteration 2248, dc_loss: 0.06558950245380402, tv_loss: 0.017424089834094048\n",
      "iteration 2249, dc_loss: 0.0655895248055458, tv_loss: 0.01742382161319256\n",
      "iteration 2250, dc_loss: 0.0655895248055458, tv_loss: 0.017423970624804497\n",
      "iteration 2251, dc_loss: 0.0655895471572876, tv_loss: 0.01742384396493435\n",
      "iteration 2252, dc_loss: 0.0655895471572876, tv_loss: 0.017423707991838455\n",
      "iteration 2253, dc_loss: 0.0655895248055458, tv_loss: 0.01742391847074032\n",
      "iteration 2254, dc_loss: 0.06558948755264282, tv_loss: 0.017423786222934723\n",
      "iteration 2255, dc_loss: 0.06558948755264282, tv_loss: 0.01742367632687092\n",
      "iteration 2256, dc_loss: 0.06558950990438461, tv_loss: 0.017423951998353004\n",
      "iteration 2257, dc_loss: 0.0655895248055458, tv_loss: 0.017423780634999275\n",
      "iteration 2258, dc_loss: 0.0655895471572876, tv_loss: 0.01742357388138771\n",
      "iteration 2259, dc_loss: 0.0655895322561264, tv_loss: 0.01742387004196644\n",
      "iteration 2260, dc_loss: 0.06558950990438461, tv_loss: 0.017423788085579872\n",
      "iteration 2261, dc_loss: 0.06558948755264282, tv_loss: 0.017423806712031364\n",
      "iteration 2262, dc_loss: 0.06558950245380402, tv_loss: 0.017423788085579872\n",
      "iteration 2263, dc_loss: 0.0655895248055458, tv_loss: 0.017423735931515694\n",
      "iteration 2264, dc_loss: 0.0655895322561264, tv_loss: 0.017423704266548157\n",
      "iteration 2265, dc_loss: 0.0655895248055458, tv_loss: 0.017423968762159348\n",
      "iteration 2266, dc_loss: 0.06558950990438461, tv_loss: 0.017423594370484352\n",
      "iteration 2267, dc_loss: 0.06558950990438461, tv_loss: 0.017423531040549278\n",
      "iteration 2268, dc_loss: 0.06558950245380402, tv_loss: 0.01742384396493435\n",
      "iteration 2269, dc_loss: 0.06558950245380402, tv_loss: 0.017423775047063828\n",
      "iteration 2270, dc_loss: 0.06558950990438461, tv_loss: 0.01742371916770935\n",
      "iteration 2271, dc_loss: 0.06558950990438461, tv_loss: 0.017423730343580246\n",
      "iteration 2272, dc_loss: 0.0655895248055458, tv_loss: 0.01742391288280487\n",
      "iteration 2273, dc_loss: 0.0655895471572876, tv_loss: 0.017423758283257484\n",
      "iteration 2274, dc_loss: 0.065589539706707, tv_loss: 0.017423998564481735\n",
      "iteration 2275, dc_loss: 0.06558951735496521, tv_loss: 0.017424117773771286\n",
      "iteration 2276, dc_loss: 0.06558950990438461, tv_loss: 0.017423713579773903\n",
      "iteration 2277, dc_loss: 0.06558949500322342, tv_loss: 0.017423875629901886\n",
      "iteration 2278, dc_loss: 0.06558947265148163, tv_loss: 0.017423996701836586\n",
      "iteration 2279, dc_loss: 0.06558948010206223, tv_loss: 0.017424192279577255\n",
      "iteration 2280, dc_loss: 0.06558948010206223, tv_loss: 0.017424235120415688\n",
      "iteration 2281, dc_loss: 0.06558950990438461, tv_loss: 0.017423762008547783\n",
      "iteration 2282, dc_loss: 0.0655895322561264, tv_loss: 0.017424006015062332\n",
      "iteration 2283, dc_loss: 0.0655895471572876, tv_loss: 0.01742405816912651\n",
      "iteration 2284, dc_loss: 0.0655895322561264, tv_loss: 0.01742400787770748\n",
      "iteration 2285, dc_loss: 0.06558950245380402, tv_loss: 0.017423851415514946\n",
      "iteration 2286, dc_loss: 0.06558948010206223, tv_loss: 0.017424097284674644\n",
      "iteration 2287, dc_loss: 0.06558948010206223, tv_loss: 0.01742425002157688\n",
      "iteration 2288, dc_loss: 0.06558948010206223, tv_loss: 0.017424097284674644\n",
      "iteration 2289, dc_loss: 0.06558948010206223, tv_loss: 0.017423825338482857\n",
      "iteration 2290, dc_loss: 0.06558950245380402, tv_loss: 0.017424017190933228\n",
      "iteration 2291, dc_loss: 0.0655895471572876, tv_loss: 0.017424149438738823\n",
      "iteration 2292, dc_loss: 0.06558956205844879, tv_loss: 0.01742411218583584\n",
      "iteration 2293, dc_loss: 0.0655895546078682, tv_loss: 0.017423951998353004\n",
      "iteration 2294, dc_loss: 0.06558950990438461, tv_loss: 0.0174242090433836\n",
      "iteration 2295, dc_loss: 0.06558945775032043, tv_loss: 0.017424119636416435\n",
      "iteration 2296, dc_loss: 0.06558942794799805, tv_loss: 0.01742429845035076\n",
      "iteration 2297, dc_loss: 0.06558947265148163, tv_loss: 0.01742425188422203\n",
      "iteration 2298, dc_loss: 0.0655895322561264, tv_loss: 0.017423855140805244\n",
      "iteration 2299, dc_loss: 0.06558956205844879, tv_loss: 0.017423812299966812\n",
      "iteration 2300, dc_loss: 0.06558956205844879, tv_loss: 0.01742417924106121\n",
      "iteration 2301, dc_loss: 0.0655895248055458, tv_loss: 0.0174242053180933\n",
      "iteration 2302, dc_loss: 0.06558948010206223, tv_loss: 0.017423851415514946\n",
      "iteration 2303, dc_loss: 0.06558946520090103, tv_loss: 0.01742386259138584\n",
      "iteration 2304, dc_loss: 0.06558948010206223, tv_loss: 0.017423879355192184\n",
      "iteration 2305, dc_loss: 0.06558950990438461, tv_loss: 0.01742384396493435\n",
      "iteration 2306, dc_loss: 0.0655895322561264, tv_loss: 0.01742391102015972\n",
      "iteration 2307, dc_loss: 0.0655895546078682, tv_loss: 0.01742376573383808\n",
      "iteration 2308, dc_loss: 0.0655895322561264, tv_loss: 0.0174237173050642\n",
      "iteration 2309, dc_loss: 0.06558950990438461, tv_loss: 0.017423564568161964\n",
      "iteration 2310, dc_loss: 0.06558946520090103, tv_loss: 0.017423685640096664\n",
      "iteration 2311, dc_loss: 0.06558945775032043, tv_loss: 0.017423901706933975\n",
      "iteration 2312, dc_loss: 0.06558950245380402, tv_loss: 0.017423713579773903\n",
      "iteration 2313, dc_loss: 0.0655895322561264, tv_loss: 0.017423680052161217\n",
      "iteration 2314, dc_loss: 0.0655895471572876, tv_loss: 0.017423564568161964\n",
      "iteration 2315, dc_loss: 0.0655895322561264, tv_loss: 0.017423423007130623\n",
      "iteration 2316, dc_loss: 0.06558951735496521, tv_loss: 0.017423374578356743\n",
      "iteration 2317, dc_loss: 0.06558950990438461, tv_loss: 0.017423709854483604\n",
      "iteration 2318, dc_loss: 0.0655895248055458, tv_loss: 0.01742406003177166\n",
      "iteration 2319, dc_loss: 0.06558950245380402, tv_loss: 0.01742386817932129\n",
      "iteration 2320, dc_loss: 0.06558947265148163, tv_loss: 0.017423471435904503\n",
      "iteration 2321, dc_loss: 0.06558948755264282, tv_loss: 0.017423680052161217\n",
      "iteration 2322, dc_loss: 0.0655895248055458, tv_loss: 0.017424019053578377\n",
      "iteration 2323, dc_loss: 0.065589539706707, tv_loss: 0.017423901706933975\n",
      "iteration 2324, dc_loss: 0.0655895248055458, tv_loss: 0.017423897981643677\n",
      "iteration 2325, dc_loss: 0.06558950245380402, tv_loss: 0.01742372289299965\n",
      "iteration 2326, dc_loss: 0.06558948755264282, tv_loss: 0.01742387004196644\n",
      "iteration 2327, dc_loss: 0.06558950245380402, tv_loss: 0.017424020916223526\n",
      "iteration 2328, dc_loss: 0.06558948755264282, tv_loss: 0.017424210906028748\n",
      "iteration 2329, dc_loss: 0.06558950245380402, tv_loss: 0.017424046993255615\n",
      "iteration 2330, dc_loss: 0.06558951735496521, tv_loss: 0.017423836514353752\n",
      "iteration 2331, dc_loss: 0.06558950245380402, tv_loss: 0.01742388866841793\n",
      "iteration 2332, dc_loss: 0.06558950245380402, tv_loss: 0.017423968762159348\n",
      "iteration 2333, dc_loss: 0.06558950990438461, tv_loss: 0.017423979938030243\n",
      "iteration 2334, dc_loss: 0.06558950990438461, tv_loss: 0.01742371916770935\n",
      "iteration 2335, dc_loss: 0.06558949500322342, tv_loss: 0.017423758283257484\n",
      "iteration 2336, dc_loss: 0.06558950245380402, tv_loss: 0.01742393523454666\n",
      "iteration 2337, dc_loss: 0.06558951735496521, tv_loss: 0.01742352731525898\n",
      "iteration 2338, dc_loss: 0.0655895248055458, tv_loss: 0.017423488199710846\n",
      "iteration 2339, dc_loss: 0.0655895322561264, tv_loss: 0.01742352731525898\n",
      "iteration 2340, dc_loss: 0.06558950990438461, tv_loss: 0.01742352358996868\n",
      "iteration 2341, dc_loss: 0.06558949500322342, tv_loss: 0.017423709854483604\n",
      "iteration 2342, dc_loss: 0.06558950245380402, tv_loss: 0.01742379181087017\n",
      "iteration 2343, dc_loss: 0.06558950245380402, tv_loss: 0.017423493787646294\n",
      "iteration 2344, dc_loss: 0.06558948755264282, tv_loss: 0.0174235999584198\n",
      "iteration 2345, dc_loss: 0.06558948755264282, tv_loss: 0.01742369867861271\n",
      "iteration 2346, dc_loss: 0.06558951735496521, tv_loss: 0.017423678189516068\n",
      "iteration 2347, dc_loss: 0.06558956205844879, tv_loss: 0.01742374338209629\n",
      "iteration 2348, dc_loss: 0.0655895546078682, tv_loss: 0.017423568293452263\n",
      "iteration 2349, dc_loss: 0.06558950990438461, tv_loss: 0.017423706129193306\n",
      "iteration 2350, dc_loss: 0.06558945775032043, tv_loss: 0.017423786222934723\n",
      "iteration 2351, dc_loss: 0.06558944284915924, tv_loss: 0.017423557117581367\n",
      "iteration 2352, dc_loss: 0.06558945775032043, tv_loss: 0.01742396503686905\n",
      "iteration 2353, dc_loss: 0.06558950245380402, tv_loss: 0.01742400787770748\n",
      "iteration 2354, dc_loss: 0.0655895471572876, tv_loss: 0.01742377318441868\n",
      "iteration 2355, dc_loss: 0.0655895471572876, tv_loss: 0.017423713579773903\n",
      "iteration 2356, dc_loss: 0.0655895322561264, tv_loss: 0.01742391847074032\n",
      "iteration 2357, dc_loss: 0.065589539706707, tv_loss: 0.017423957586288452\n",
      "iteration 2358, dc_loss: 0.06558951735496521, tv_loss: 0.017423784360289574\n",
      "iteration 2359, dc_loss: 0.06558948010206223, tv_loss: 0.01742350123822689\n",
      "iteration 2360, dc_loss: 0.06558946520090103, tv_loss: 0.01742379181087017\n",
      "iteration 2361, dc_loss: 0.06558947265148163, tv_loss: 0.017424216493964195\n",
      "iteration 2362, dc_loss: 0.06558950990438461, tv_loss: 0.01742384023964405\n",
      "iteration 2363, dc_loss: 0.0655895546078682, tv_loss: 0.0174233578145504\n",
      "iteration 2364, dc_loss: 0.0655895546078682, tv_loss: 0.01742376759648323\n",
      "iteration 2365, dc_loss: 0.0655895322561264, tv_loss: 0.017424050718545914\n",
      "iteration 2366, dc_loss: 0.06558950245380402, tv_loss: 0.0174238458275795\n",
      "iteration 2367, dc_loss: 0.06558948755264282, tv_loss: 0.017423691228032112\n",
      "iteration 2368, dc_loss: 0.06558948010206223, tv_loss: 0.017423713579773903\n",
      "iteration 2369, dc_loss: 0.06558948755264282, tv_loss: 0.017423775047063828\n",
      "iteration 2370, dc_loss: 0.06558950990438461, tv_loss: 0.017423875629901886\n",
      "iteration 2371, dc_loss: 0.0655895248055458, tv_loss: 0.017423704266548157\n",
      "iteration 2372, dc_loss: 0.0655895322561264, tv_loss: 0.01742367446422577\n",
      "iteration 2373, dc_loss: 0.06558950245380402, tv_loss: 0.017423667013645172\n",
      "iteration 2374, dc_loss: 0.06558948010206223, tv_loss: 0.017423812299966812\n",
      "iteration 2375, dc_loss: 0.06558948010206223, tv_loss: 0.01742403768002987\n",
      "iteration 2376, dc_loss: 0.06558950245380402, tv_loss: 0.017424050718545914\n",
      "iteration 2377, dc_loss: 0.0655895248055458, tv_loss: 0.0174237247556448\n",
      "iteration 2378, dc_loss: 0.06558951735496521, tv_loss: 0.017423570156097412\n",
      "iteration 2379, dc_loss: 0.06558950990438461, tv_loss: 0.01742405816912651\n",
      "iteration 2380, dc_loss: 0.06558948010206223, tv_loss: 0.01742430031299591\n",
      "iteration 2381, dc_loss: 0.06558948755264282, tv_loss: 0.01742396131157875\n",
      "iteration 2382, dc_loss: 0.06558950990438461, tv_loss: 0.017423776909708977\n",
      "iteration 2383, dc_loss: 0.0655895248055458, tv_loss: 0.01742401532828808\n",
      "iteration 2384, dc_loss: 0.06558950990438461, tv_loss: 0.01742403581738472\n",
      "iteration 2385, dc_loss: 0.06558948755264282, tv_loss: 0.01742388680577278\n",
      "iteration 2386, dc_loss: 0.06558948755264282, tv_loss: 0.017423801124095917\n",
      "iteration 2387, dc_loss: 0.06558950990438461, tv_loss: 0.017423564568161964\n",
      "iteration 2388, dc_loss: 0.0655895248055458, tv_loss: 0.01742393895983696\n",
      "iteration 2389, dc_loss: 0.0655895248055458, tv_loss: 0.01742398552596569\n",
      "iteration 2390, dc_loss: 0.06558950990438461, tv_loss: 0.017423706129193306\n",
      "iteration 2391, dc_loss: 0.06558948010206223, tv_loss: 0.017423812299966812\n",
      "iteration 2392, dc_loss: 0.06558948010206223, tv_loss: 0.01742381602525711\n",
      "iteration 2393, dc_loss: 0.06558950990438461, tv_loss: 0.017423678189516068\n",
      "iteration 2394, dc_loss: 0.06558951735496521, tv_loss: 0.01742391474545002\n",
      "iteration 2395, dc_loss: 0.0655895322561264, tv_loss: 0.017423877492547035\n",
      "iteration 2396, dc_loss: 0.0655895248055458, tv_loss: 0.017423594370484352\n",
      "iteration 2397, dc_loss: 0.06558951735496521, tv_loss: 0.01742367073893547\n",
      "iteration 2398, dc_loss: 0.06558949500322342, tv_loss: 0.017423896118998528\n",
      "iteration 2399, dc_loss: 0.06558947265148163, tv_loss: 0.017423905432224274\n",
      "iteration 2400, dc_loss: 0.06558946520090103, tv_loss: 0.017423726618289948\n",
      "iteration 2401, dc_loss: 0.06558948010206223, tv_loss: 0.017423732206225395\n",
      "iteration 2402, dc_loss: 0.0655895322561264, tv_loss: 0.017423586919903755\n",
      "iteration 2403, dc_loss: 0.06558956205844879, tv_loss: 0.017423594370484352\n",
      "iteration 2404, dc_loss: 0.0655895322561264, tv_loss: 0.017423709854483604\n",
      "iteration 2405, dc_loss: 0.06558950990438461, tv_loss: 0.017423829063773155\n",
      "iteration 2406, dc_loss: 0.06558948755264282, tv_loss: 0.01742386631667614\n",
      "iteration 2407, dc_loss: 0.06558948755264282, tv_loss: 0.017423691228032112\n",
      "iteration 2408, dc_loss: 0.06558949500322342, tv_loss: 0.017423786222934723\n",
      "iteration 2409, dc_loss: 0.06558949500322342, tv_loss: 0.017423812299966812\n",
      "iteration 2410, dc_loss: 0.06558950990438461, tv_loss: 0.017423734068870544\n",
      "iteration 2411, dc_loss: 0.065589539706707, tv_loss: 0.01742369681596756\n",
      "iteration 2412, dc_loss: 0.0655895546078682, tv_loss: 0.01742382161319256\n",
      "iteration 2413, dc_loss: 0.0655895471572876, tv_loss: 0.01742354966700077\n",
      "iteration 2414, dc_loss: 0.06558951735496521, tv_loss: 0.017423545941710472\n",
      "iteration 2415, dc_loss: 0.06558946520090103, tv_loss: 0.01742398738861084\n",
      "iteration 2416, dc_loss: 0.06558948010206223, tv_loss: 0.017423847690224648\n",
      "iteration 2417, dc_loss: 0.06558950990438461, tv_loss: 0.017423423007130623\n",
      "iteration 2418, dc_loss: 0.06558951735496521, tv_loss: 0.017423471435904503\n",
      "iteration 2419, dc_loss: 0.0655895322561264, tv_loss: 0.017423896118998528\n",
      "iteration 2420, dc_loss: 0.06558950990438461, tv_loss: 0.01742386259138584\n",
      "iteration 2421, dc_loss: 0.06558948010206223, tv_loss: 0.017423393204808235\n",
      "iteration 2422, dc_loss: 0.06558947265148163, tv_loss: 0.017423462122678757\n",
      "iteration 2423, dc_loss: 0.06558950245380402, tv_loss: 0.017424002289772034\n",
      "iteration 2424, dc_loss: 0.06558950990438461, tv_loss: 0.017423583194613457\n",
      "iteration 2425, dc_loss: 0.06558950245380402, tv_loss: 0.017423441633582115\n",
      "iteration 2426, dc_loss: 0.06558950245380402, tv_loss: 0.017423875629901886\n",
      "iteration 2427, dc_loss: 0.06558950990438461, tv_loss: 0.017423763871192932\n",
      "iteration 2428, dc_loss: 0.06558950990438461, tv_loss: 0.0174238458275795\n",
      "iteration 2429, dc_loss: 0.06558948755264282, tv_loss: 0.017423799261450768\n",
      "iteration 2430, dc_loss: 0.06558949500322342, tv_loss: 0.017423830926418304\n",
      "iteration 2431, dc_loss: 0.06558950990438461, tv_loss: 0.017423667013645172\n",
      "iteration 2432, dc_loss: 0.0655895322561264, tv_loss: 0.017424147576093674\n",
      "iteration 2433, dc_loss: 0.0655895322561264, tv_loss: 0.017424000427126884\n",
      "iteration 2434, dc_loss: 0.06558950990438461, tv_loss: 0.01742386817932129\n",
      "iteration 2435, dc_loss: 0.06558950245380402, tv_loss: 0.017423903569579124\n",
      "iteration 2436, dc_loss: 0.06558948010206223, tv_loss: 0.017424099147319794\n",
      "iteration 2437, dc_loss: 0.06558950245380402, tv_loss: 0.017424020916223526\n",
      "iteration 2438, dc_loss: 0.06558950245380402, tv_loss: 0.017424101009964943\n",
      "iteration 2439, dc_loss: 0.06558948755264282, tv_loss: 0.01742415688931942\n",
      "iteration 2440, dc_loss: 0.06558948010206223, tv_loss: 0.017424065619707108\n",
      "iteration 2441, dc_loss: 0.06558950245380402, tv_loss: 0.017423780634999275\n",
      "iteration 2442, dc_loss: 0.0655895248055458, tv_loss: 0.017423788085579872\n",
      "iteration 2443, dc_loss: 0.0655895471572876, tv_loss: 0.017423931509256363\n",
      "iteration 2444, dc_loss: 0.065589539706707, tv_loss: 0.017423802986741066\n",
      "iteration 2445, dc_loss: 0.06558950990438461, tv_loss: 0.017423521727323532\n",
      "iteration 2446, dc_loss: 0.06558948755264282, tv_loss: 0.01742342673242092\n",
      "iteration 2447, dc_loss: 0.06558948755264282, tv_loss: 0.017424052581191063\n",
      "iteration 2448, dc_loss: 0.06558948755264282, tv_loss: 0.017423709854483604\n",
      "iteration 2449, dc_loss: 0.06558948755264282, tv_loss: 0.017423732206225395\n",
      "iteration 2450, dc_loss: 0.06558948010206223, tv_loss: 0.01742364838719368\n",
      "iteration 2451, dc_loss: 0.06558951735496521, tv_loss: 0.017423806712031364\n",
      "iteration 2452, dc_loss: 0.0655895322561264, tv_loss: 0.017423713579773903\n",
      "iteration 2453, dc_loss: 0.0655895471572876, tv_loss: 0.017423808574676514\n",
      "iteration 2454, dc_loss: 0.0655895248055458, tv_loss: 0.01742374710738659\n",
      "iteration 2455, dc_loss: 0.06558950245380402, tv_loss: 0.017423590645194054\n",
      "iteration 2456, dc_loss: 0.06558949500322342, tv_loss: 0.017423663288354874\n",
      "iteration 2457, dc_loss: 0.06558948010206223, tv_loss: 0.017423702403903008\n",
      "iteration 2458, dc_loss: 0.06558949500322342, tv_loss: 0.017423836514353752\n",
      "iteration 2459, dc_loss: 0.065589539706707, tv_loss: 0.017423726618289948\n",
      "iteration 2460, dc_loss: 0.0655895471572876, tv_loss: 0.017424006015062332\n",
      "iteration 2461, dc_loss: 0.0655895248055458, tv_loss: 0.017423974350094795\n",
      "iteration 2462, dc_loss: 0.06558950245380402, tv_loss: 0.01742394082248211\n",
      "iteration 2463, dc_loss: 0.06558946520090103, tv_loss: 0.01742376945912838\n",
      "iteration 2464, dc_loss: 0.06558944284915924, tv_loss: 0.01742367632687092\n",
      "iteration 2465, dc_loss: 0.06558948755264282, tv_loss: 0.01742389053106308\n",
      "iteration 2466, dc_loss: 0.0655895471572876, tv_loss: 0.017423784360289574\n",
      "iteration 2467, dc_loss: 0.06558958441019058, tv_loss: 0.01742357760667801\n",
      "iteration 2468, dc_loss: 0.06558956205844879, tv_loss: 0.017423566430807114\n",
      "iteration 2469, dc_loss: 0.06558950245380402, tv_loss: 0.017423691228032112\n",
      "iteration 2470, dc_loss: 0.06558945775032043, tv_loss: 0.01742391847074032\n",
      "iteration 2471, dc_loss: 0.06558944284915924, tv_loss: 0.017423667013645172\n",
      "iteration 2472, dc_loss: 0.06558948755264282, tv_loss: 0.017423806712031364\n",
      "iteration 2473, dc_loss: 0.0655895546078682, tv_loss: 0.017423875629901886\n",
      "iteration 2474, dc_loss: 0.06558957695960999, tv_loss: 0.017423881217837334\n",
      "iteration 2475, dc_loss: 0.0655895471572876, tv_loss: 0.017423585057258606\n",
      "iteration 2476, dc_loss: 0.06558950245380402, tv_loss: 0.017423974350094795\n",
      "iteration 2477, dc_loss: 0.06558948010206223, tv_loss: 0.0174240842461586\n",
      "iteration 2478, dc_loss: 0.06558947265148163, tv_loss: 0.017424006015062332\n",
      "iteration 2479, dc_loss: 0.06558948010206223, tv_loss: 0.017423799261450768\n",
      "iteration 2480, dc_loss: 0.06558950990438461, tv_loss: 0.017423391342163086\n",
      "iteration 2481, dc_loss: 0.06558950990438461, tv_loss: 0.01742359809577465\n",
      "iteration 2482, dc_loss: 0.0655895248055458, tv_loss: 0.01742374897003174\n",
      "iteration 2483, dc_loss: 0.0655895471572876, tv_loss: 0.017423395067453384\n",
      "iteration 2484, dc_loss: 0.0655895322561264, tv_loss: 0.017423342913389206\n",
      "iteration 2485, dc_loss: 0.06558948755264282, tv_loss: 0.017423758283257484\n",
      "iteration 2486, dc_loss: 0.06558948010206223, tv_loss: 0.017424065619707108\n",
      "iteration 2487, dc_loss: 0.06558950245380402, tv_loss: 0.017423901706933975\n",
      "iteration 2488, dc_loss: 0.06558950245380402, tv_loss: 0.017423519864678383\n",
      "iteration 2489, dc_loss: 0.06558950990438461, tv_loss: 0.01742355339229107\n",
      "iteration 2490, dc_loss: 0.06558951735496521, tv_loss: 0.017423637211322784\n",
      "iteration 2491, dc_loss: 0.0655895322561264, tv_loss: 0.017423497512936592\n",
      "iteration 2492, dc_loss: 0.0655895248055458, tv_loss: 0.017423095181584358\n",
      "iteration 2493, dc_loss: 0.06558950990438461, tv_loss: 0.017423352226614952\n",
      "iteration 2494, dc_loss: 0.06558948010206223, tv_loss: 0.017423614859580994\n",
      "iteration 2495, dc_loss: 0.06558945775032043, tv_loss: 0.017423510551452637\n",
      "iteration 2496, dc_loss: 0.06558947265148163, tv_loss: 0.01742357574403286\n",
      "iteration 2497, dc_loss: 0.06558950990438461, tv_loss: 0.0174233540892601\n",
      "iteration 2498, dc_loss: 0.0655895322561264, tv_loss: 0.017423586919903755\n",
      "iteration 2499, dc_loss: 0.0655895322561264, tv_loss: 0.017423514276742935\n",
      "iteration 2500, dc_loss: 0.0655895248055458, tv_loss: 0.017423225566744804\n",
      "iteration 2501, dc_loss: 0.06558951735496521, tv_loss: 0.01742345653474331\n",
      "iteration 2502, dc_loss: 0.06558951735496521, tv_loss: 0.017423618584871292\n",
      "iteration 2503, dc_loss: 0.06558950245380402, tv_loss: 0.017423616722226143\n",
      "iteration 2504, dc_loss: 0.06558948010206223, tv_loss: 0.01742321252822876\n",
      "iteration 2505, dc_loss: 0.06558946520090103, tv_loss: 0.017423544079065323\n",
      "iteration 2506, dc_loss: 0.06558947265148163, tv_loss: 0.017423901706933975\n",
      "iteration 2507, dc_loss: 0.065589539706707, tv_loss: 0.017423683777451515\n",
      "iteration 2508, dc_loss: 0.06558956205844879, tv_loss: 0.017423277720808983\n",
      "iteration 2509, dc_loss: 0.0655895471572876, tv_loss: 0.017423318699002266\n",
      "iteration 2510, dc_loss: 0.06558951735496521, tv_loss: 0.017423557117581367\n",
      "iteration 2511, dc_loss: 0.06558948010206223, tv_loss: 0.01742357201874256\n",
      "iteration 2512, dc_loss: 0.06558947265148163, tv_loss: 0.017423393204808235\n",
      "iteration 2513, dc_loss: 0.06558948755264282, tv_loss: 0.017423486337065697\n",
      "iteration 2514, dc_loss: 0.0655895248055458, tv_loss: 0.01742377318441868\n",
      "iteration 2515, dc_loss: 0.0655895471572876, tv_loss: 0.017423642799258232\n",
      "iteration 2516, dc_loss: 0.0655895546078682, tv_loss: 0.01742350123822689\n",
      "iteration 2517, dc_loss: 0.06558951735496521, tv_loss: 0.017423516139388084\n",
      "iteration 2518, dc_loss: 0.06558948010206223, tv_loss: 0.01742354780435562\n",
      "iteration 2519, dc_loss: 0.06558946520090103, tv_loss: 0.01742374151945114\n",
      "iteration 2520, dc_loss: 0.06558946520090103, tv_loss: 0.01742350123822689\n",
      "iteration 2521, dc_loss: 0.06558949500322342, tv_loss: 0.0174236036837101\n",
      "iteration 2522, dc_loss: 0.065589539706707, tv_loss: 0.017423663288354874\n",
      "iteration 2523, dc_loss: 0.06558956205844879, tv_loss: 0.01742345094680786\n",
      "iteration 2524, dc_loss: 0.0655895471572876, tv_loss: 0.017423337325453758\n",
      "iteration 2525, dc_loss: 0.06558950990438461, tv_loss: 0.017423434183001518\n",
      "iteration 2526, dc_loss: 0.06558948010206223, tv_loss: 0.01742330566048622\n",
      "iteration 2527, dc_loss: 0.06558946520090103, tv_loss: 0.017423534765839577\n",
      "iteration 2528, dc_loss: 0.06558950245380402, tv_loss: 0.01742365024983883\n",
      "iteration 2529, dc_loss: 0.06558950245380402, tv_loss: 0.017423458397388458\n",
      "iteration 2530, dc_loss: 0.06558948755264282, tv_loss: 0.017423313111066818\n",
      "iteration 2531, dc_loss: 0.06558949500322342, tv_loss: 0.01742360182106495\n",
      "iteration 2532, dc_loss: 0.06558950990438461, tv_loss: 0.017423663288354874\n",
      "iteration 2533, dc_loss: 0.0655895322561264, tv_loss: 0.017423534765839577\n",
      "iteration 2534, dc_loss: 0.06558950990438461, tv_loss: 0.017423568293452263\n",
      "iteration 2535, dc_loss: 0.06558949500322342, tv_loss: 0.017423689365386963\n",
      "iteration 2536, dc_loss: 0.06558948755264282, tv_loss: 0.017423557117581367\n",
      "iteration 2537, dc_loss: 0.06558951735496521, tv_loss: 0.017423642799258232\n",
      "iteration 2538, dc_loss: 0.0655895248055458, tv_loss: 0.01742382161319256\n",
      "iteration 2539, dc_loss: 0.06558951735496521, tv_loss: 0.017423760145902634\n",
      "iteration 2540, dc_loss: 0.06558948755264282, tv_loss: 0.017423540353775024\n",
      "iteration 2541, dc_loss: 0.06558946520090103, tv_loss: 0.017423739656805992\n",
      "iteration 2542, dc_loss: 0.06558948010206223, tv_loss: 0.017423689365386963\n",
      "iteration 2543, dc_loss: 0.0655895248055458, tv_loss: 0.017423711717128754\n",
      "iteration 2544, dc_loss: 0.0655895471572876, tv_loss: 0.01742362417280674\n",
      "iteration 2545, dc_loss: 0.0655895471572876, tv_loss: 0.017423640936613083\n",
      "iteration 2546, dc_loss: 0.0655895322561264, tv_loss: 0.017423786222934723\n",
      "iteration 2547, dc_loss: 0.06558951735496521, tv_loss: 0.017423728480935097\n",
      "iteration 2548, dc_loss: 0.06558947265148163, tv_loss: 0.01742369681596756\n",
      "iteration 2549, dc_loss: 0.06558947265148163, tv_loss: 0.01742362044751644\n",
      "iteration 2550, dc_loss: 0.06558948755264282, tv_loss: 0.017423825338482857\n",
      "iteration 2551, dc_loss: 0.06558950990438461, tv_loss: 0.017423827201128006\n",
      "iteration 2552, dc_loss: 0.0655895248055458, tv_loss: 0.01742382161319256\n",
      "iteration 2553, dc_loss: 0.06558950990438461, tv_loss: 0.017423849552869797\n",
      "iteration 2554, dc_loss: 0.06558948755264282, tv_loss: 0.017423881217837334\n",
      "iteration 2555, dc_loss: 0.06558948755264282, tv_loss: 0.01742386259138584\n",
      "iteration 2556, dc_loss: 0.06558950990438461, tv_loss: 0.01742386259138584\n",
      "iteration 2557, dc_loss: 0.0655895322561264, tv_loss: 0.0174240842461586\n",
      "iteration 2558, dc_loss: 0.06558951735496521, tv_loss: 0.017423812299966812\n",
      "iteration 2559, dc_loss: 0.06558948010206223, tv_loss: 0.01742389425635338\n",
      "iteration 2560, dc_loss: 0.06558945775032043, tv_loss: 0.01742401160299778\n",
      "iteration 2561, dc_loss: 0.06558946520090103, tv_loss: 0.017423754557967186\n",
      "iteration 2562, dc_loss: 0.06558950245380402, tv_loss: 0.01742365024983883\n",
      "iteration 2563, dc_loss: 0.06558956205844879, tv_loss: 0.017423756420612335\n",
      "iteration 2564, dc_loss: 0.0655895546078682, tv_loss: 0.017423879355192184\n",
      "iteration 2565, dc_loss: 0.06558951735496521, tv_loss: 0.017423853278160095\n",
      "iteration 2566, dc_loss: 0.06558948755264282, tv_loss: 0.01742406003177166\n",
      "iteration 2567, dc_loss: 0.06558948010206223, tv_loss: 0.017423951998353004\n",
      "iteration 2568, dc_loss: 0.06558948010206223, tv_loss: 0.017423905432224274\n",
      "iteration 2569, dc_loss: 0.06558948755264282, tv_loss: 0.017423810437321663\n",
      "iteration 2570, dc_loss: 0.06558951735496521, tv_loss: 0.017423762008547783\n",
      "iteration 2571, dc_loss: 0.065589539706707, tv_loss: 0.017423706129193306\n",
      "iteration 2572, dc_loss: 0.0655895322561264, tv_loss: 0.017423663288354874\n",
      "iteration 2573, dc_loss: 0.0655895248055458, tv_loss: 0.017423594370484352\n",
      "iteration 2574, dc_loss: 0.06558948755264282, tv_loss: 0.0174237247556448\n",
      "iteration 2575, dc_loss: 0.06558945775032043, tv_loss: 0.017424019053578377\n",
      "iteration 2576, dc_loss: 0.06558945775032043, tv_loss: 0.017423832789063454\n",
      "iteration 2577, dc_loss: 0.06558948755264282, tv_loss: 0.01742362417280674\n",
      "iteration 2578, dc_loss: 0.06558951735496521, tv_loss: 0.01742367073893547\n",
      "iteration 2579, dc_loss: 0.0655895471572876, tv_loss: 0.017423566430807114\n",
      "iteration 2580, dc_loss: 0.0655895322561264, tv_loss: 0.017423488199710846\n",
      "iteration 2581, dc_loss: 0.06558950990438461, tv_loss: 0.017423970624804497\n",
      "iteration 2582, dc_loss: 0.06558950990438461, tv_loss: 0.017423763871192932\n",
      "iteration 2583, dc_loss: 0.06558948010206223, tv_loss: 0.017423288896679878\n",
      "iteration 2584, dc_loss: 0.06558948010206223, tv_loss: 0.017423640936613083\n",
      "iteration 2585, dc_loss: 0.06558948010206223, tv_loss: 0.017424020916223526\n",
      "iteration 2586, dc_loss: 0.06558950245380402, tv_loss: 0.017423706129193306\n",
      "iteration 2587, dc_loss: 0.0655895322561264, tv_loss: 0.017423391342163086\n",
      "iteration 2588, dc_loss: 0.0655895322561264, tv_loss: 0.017423542216420174\n",
      "iteration 2589, dc_loss: 0.06558949500322342, tv_loss: 0.0174237210303545\n",
      "iteration 2590, dc_loss: 0.06558947265148163, tv_loss: 0.017423583194613457\n",
      "iteration 2591, dc_loss: 0.06558948010206223, tv_loss: 0.017423449084162712\n",
      "iteration 2592, dc_loss: 0.06558950245380402, tv_loss: 0.017423702403903008\n",
      "iteration 2593, dc_loss: 0.06558950990438461, tv_loss: 0.017423629760742188\n",
      "iteration 2594, dc_loss: 0.0655895248055458, tv_loss: 0.017423514276742935\n",
      "iteration 2595, dc_loss: 0.0655895248055458, tv_loss: 0.017423564568161964\n",
      "iteration 2596, dc_loss: 0.06558950245380402, tv_loss: 0.01742374897003174\n",
      "iteration 2597, dc_loss: 0.06558949500322342, tv_loss: 0.017423760145902634\n",
      "iteration 2598, dc_loss: 0.06558950990438461, tv_loss: 0.017423640936613083\n",
      "iteration 2599, dc_loss: 0.06558950990438461, tv_loss: 0.01742367073893547\n",
      "iteration 2600, dc_loss: 0.06558950990438461, tv_loss: 0.01742374897003174\n",
      "iteration 2601, dc_loss: 0.06558950245380402, tv_loss: 0.01742379367351532\n",
      "iteration 2602, dc_loss: 0.06558950990438461, tv_loss: 0.017423421144485474\n",
      "iteration 2603, dc_loss: 0.0655895248055458, tv_loss: 0.017423467710614204\n",
      "iteration 2604, dc_loss: 0.06558951735496521, tv_loss: 0.017423758283257484\n",
      "iteration 2605, dc_loss: 0.06558950245380402, tv_loss: 0.017423609271645546\n",
      "iteration 2606, dc_loss: 0.06558950245380402, tv_loss: 0.01742328330874443\n",
      "iteration 2607, dc_loss: 0.06558948010206223, tv_loss: 0.017423590645194054\n",
      "iteration 2608, dc_loss: 0.06558948755264282, tv_loss: 0.017424022778868675\n",
      "iteration 2609, dc_loss: 0.06558948755264282, tv_loss: 0.017423763871192932\n",
      "iteration 2610, dc_loss: 0.06558948755264282, tv_loss: 0.017423542216420174\n",
      "iteration 2611, dc_loss: 0.06558950245380402, tv_loss: 0.01742352545261383\n",
      "iteration 2612, dc_loss: 0.06558951735496521, tv_loss: 0.01742366887629032\n",
      "iteration 2613, dc_loss: 0.0655895248055458, tv_loss: 0.017423579469323158\n",
      "iteration 2614, dc_loss: 0.06558950990438461, tv_loss: 0.01742374897003174\n",
      "iteration 2615, dc_loss: 0.06558950245380402, tv_loss: 0.017423637211322784\n",
      "iteration 2616, dc_loss: 0.06558948010206223, tv_loss: 0.017423855140805244\n",
      "iteration 2617, dc_loss: 0.06558948755264282, tv_loss: 0.017423851415514946\n",
      "iteration 2618, dc_loss: 0.06558950245380402, tv_loss: 0.017423389479517937\n",
      "iteration 2619, dc_loss: 0.06558950990438461, tv_loss: 0.017423678189516068\n",
      "iteration 2620, dc_loss: 0.06558949500322342, tv_loss: 0.017424046993255615\n",
      "iteration 2621, dc_loss: 0.06558950245380402, tv_loss: 0.017423801124095917\n",
      "iteration 2622, dc_loss: 0.0655895322561264, tv_loss: 0.017423449084162712\n",
      "iteration 2623, dc_loss: 0.0655895322561264, tv_loss: 0.017423488199710846\n",
      "iteration 2624, dc_loss: 0.0655895248055458, tv_loss: 0.017423849552869797\n",
      "iteration 2625, dc_loss: 0.06558948010206223, tv_loss: 0.017423806712031364\n",
      "iteration 2626, dc_loss: 0.06558948010206223, tv_loss: 0.017423689365386963\n",
      "iteration 2627, dc_loss: 0.06558951735496521, tv_loss: 0.01742365024983883\n",
      "iteration 2628, dc_loss: 0.0655895248055458, tv_loss: 0.01742359809577465\n",
      "iteration 2629, dc_loss: 0.06558950990438461, tv_loss: 0.017423713579773903\n",
      "iteration 2630, dc_loss: 0.06558948755264282, tv_loss: 0.01742376945912838\n",
      "iteration 2631, dc_loss: 0.06558946520090103, tv_loss: 0.01742362789809704\n",
      "iteration 2632, dc_loss: 0.06558947265148163, tv_loss: 0.017423706129193306\n",
      "iteration 2633, dc_loss: 0.06558951735496521, tv_loss: 0.017423739656805992\n",
      "iteration 2634, dc_loss: 0.0655895322561264, tv_loss: 0.017423659563064575\n",
      "iteration 2635, dc_loss: 0.0655895322561264, tv_loss: 0.01742378994822502\n",
      "iteration 2636, dc_loss: 0.06558951735496521, tv_loss: 0.017424114048480988\n",
      "iteration 2637, dc_loss: 0.06558948755264282, tv_loss: 0.017423855140805244\n",
      "iteration 2638, dc_loss: 0.06558946520090103, tv_loss: 0.017423605546355247\n",
      "iteration 2639, dc_loss: 0.06558945029973984, tv_loss: 0.017423586919903755\n",
      "iteration 2640, dc_loss: 0.06558946520090103, tv_loss: 0.01742379181087017\n",
      "iteration 2641, dc_loss: 0.06558950990438461, tv_loss: 0.01742388866841793\n",
      "iteration 2642, dc_loss: 0.06558956205844879, tv_loss: 0.017423732206225395\n",
      "iteration 2643, dc_loss: 0.06558956950902939, tv_loss: 0.01742352731525898\n",
      "iteration 2644, dc_loss: 0.0655895248055458, tv_loss: 0.01742381788790226\n",
      "iteration 2645, dc_loss: 0.06558948755264282, tv_loss: 0.017423879355192184\n",
      "iteration 2646, dc_loss: 0.06558944284915924, tv_loss: 0.01742369495332241\n",
      "iteration 2647, dc_loss: 0.06558945775032043, tv_loss: 0.01742379181087017\n",
      "iteration 2648, dc_loss: 0.06558948010206223, tv_loss: 0.017423832789063454\n",
      "iteration 2649, dc_loss: 0.06558950990438461, tv_loss: 0.01742381416261196\n",
      "iteration 2650, dc_loss: 0.0655895322561264, tv_loss: 0.017423558980226517\n",
      "iteration 2651, dc_loss: 0.0655895546078682, tv_loss: 0.017423832789063454\n",
      "iteration 2652, dc_loss: 0.0655895546078682, tv_loss: 0.017423713579773903\n",
      "iteration 2653, dc_loss: 0.0655895248055458, tv_loss: 0.0174234751611948\n",
      "iteration 2654, dc_loss: 0.06558950990438461, tv_loss: 0.017423760145902634\n",
      "iteration 2655, dc_loss: 0.06558947265148163, tv_loss: 0.01742376759648323\n",
      "iteration 2656, dc_loss: 0.06558946520090103, tv_loss: 0.01742369309067726\n",
      "iteration 2657, dc_loss: 0.06558950245380402, tv_loss: 0.01742362789809704\n",
      "iteration 2658, dc_loss: 0.065589539706707, tv_loss: 0.01742355339229107\n",
      "iteration 2659, dc_loss: 0.065589539706707, tv_loss: 0.01742376945912838\n",
      "iteration 2660, dc_loss: 0.06558950990438461, tv_loss: 0.017423726618289948\n",
      "iteration 2661, dc_loss: 0.06558950245380402, tv_loss: 0.01742382161319256\n",
      "iteration 2662, dc_loss: 0.06558950990438461, tv_loss: 0.017423760145902634\n",
      "iteration 2663, dc_loss: 0.06558950990438461, tv_loss: 0.01742367632687092\n",
      "iteration 2664, dc_loss: 0.06558950990438461, tv_loss: 0.01742388866841793\n",
      "iteration 2665, dc_loss: 0.06558950245380402, tv_loss: 0.017423836514353752\n",
      "iteration 2666, dc_loss: 0.06558948755264282, tv_loss: 0.01742367446422577\n",
      "iteration 2667, dc_loss: 0.06558950245380402, tv_loss: 0.01742355152964592\n",
      "iteration 2668, dc_loss: 0.0655895248055458, tv_loss: 0.017423825338482857\n",
      "iteration 2669, dc_loss: 0.0655895471572876, tv_loss: 0.017424114048480988\n",
      "iteration 2670, dc_loss: 0.0655895322561264, tv_loss: 0.01742393709719181\n",
      "iteration 2671, dc_loss: 0.06558950245380402, tv_loss: 0.01742369495332241\n",
      "iteration 2672, dc_loss: 0.06558948755264282, tv_loss: 0.01742406189441681\n",
      "iteration 2673, dc_loss: 0.06558948010206223, tv_loss: 0.017424246296286583\n",
      "iteration 2674, dc_loss: 0.06558948010206223, tv_loss: 0.017423812299966812\n",
      "iteration 2675, dc_loss: 0.06558951735496521, tv_loss: 0.017423836514353752\n",
      "iteration 2676, dc_loss: 0.0655895322561264, tv_loss: 0.01742394082248211\n",
      "iteration 2677, dc_loss: 0.0655895322561264, tv_loss: 0.01742389425635338\n",
      "iteration 2678, dc_loss: 0.06558951735496521, tv_loss: 0.017423667013645172\n",
      "iteration 2679, dc_loss: 0.06558948010206223, tv_loss: 0.01742386259138584\n",
      "iteration 2680, dc_loss: 0.06558946520090103, tv_loss: 0.017423948273062706\n",
      "iteration 2681, dc_loss: 0.06558946520090103, tv_loss: 0.017423778772354126\n",
      "iteration 2682, dc_loss: 0.06558950245380402, tv_loss: 0.01742369309067726\n",
      "iteration 2683, dc_loss: 0.0655895322561264, tv_loss: 0.017423875629901886\n",
      "iteration 2684, dc_loss: 0.0655895248055458, tv_loss: 0.01742386817932129\n",
      "iteration 2685, dc_loss: 0.06558950990438461, tv_loss: 0.01742350496351719\n",
      "iteration 2686, dc_loss: 0.06558948010206223, tv_loss: 0.017423639073967934\n",
      "iteration 2687, dc_loss: 0.06558948755264282, tv_loss: 0.017423752695322037\n",
      "iteration 2688, dc_loss: 0.06558948755264282, tv_loss: 0.01742386445403099\n",
      "iteration 2689, dc_loss: 0.06558951735496521, tv_loss: 0.017423659563064575\n",
      "iteration 2690, dc_loss: 0.0655895248055458, tv_loss: 0.017423687502741814\n",
      "iteration 2691, dc_loss: 0.06558950990438461, tv_loss: 0.01742377318441868\n",
      "iteration 2692, dc_loss: 0.06558950245380402, tv_loss: 0.017423368990421295\n",
      "iteration 2693, dc_loss: 0.06558950990438461, tv_loss: 0.01742369867861271\n",
      "iteration 2694, dc_loss: 0.06558948010206223, tv_loss: 0.0174239631742239\n",
      "iteration 2695, dc_loss: 0.06558944284915924, tv_loss: 0.01742379739880562\n",
      "iteration 2696, dc_loss: 0.06558946520090103, tv_loss: 0.017423581331968307\n",
      "iteration 2697, dc_loss: 0.06558951735496521, tv_loss: 0.017423750832676888\n",
      "iteration 2698, dc_loss: 0.0655895546078682, tv_loss: 0.01742374897003174\n",
      "iteration 2699, dc_loss: 0.06558956950902939, tv_loss: 0.01742343232035637\n",
      "iteration 2700, dc_loss: 0.0655895471572876, tv_loss: 0.01742355339229107\n",
      "iteration 2701, dc_loss: 0.06558950245380402, tv_loss: 0.017423799261450768\n",
      "iteration 2702, dc_loss: 0.06558946520090103, tv_loss: 0.017423715442419052\n",
      "iteration 2703, dc_loss: 0.06558945775032043, tv_loss: 0.01742399111390114\n",
      "iteration 2704, dc_loss: 0.06558945775032043, tv_loss: 0.017423808574676514\n",
      "iteration 2705, dc_loss: 0.06558947265148163, tv_loss: 0.017423460260033607\n",
      "iteration 2706, dc_loss: 0.06558950245380402, tv_loss: 0.01742345653474331\n",
      "iteration 2707, dc_loss: 0.0655895471572876, tv_loss: 0.01742367632687092\n",
      "iteration 2708, dc_loss: 0.0655895546078682, tv_loss: 0.01742362044751644\n",
      "iteration 2709, dc_loss: 0.06558951735496521, tv_loss: 0.017423629760742188\n",
      "iteration 2710, dc_loss: 0.06558948010206223, tv_loss: 0.017423583194613457\n",
      "iteration 2711, dc_loss: 0.06558948010206223, tv_loss: 0.017423642799258232\n",
      "iteration 2712, dc_loss: 0.06558950245380402, tv_loss: 0.01742357574403286\n",
      "iteration 2713, dc_loss: 0.0655895248055458, tv_loss: 0.017423540353775024\n",
      "iteration 2714, dc_loss: 0.0655895322561264, tv_loss: 0.0174233578145504\n",
      "iteration 2715, dc_loss: 0.06558951735496521, tv_loss: 0.01742335595190525\n",
      "iteration 2716, dc_loss: 0.06558948010206223, tv_loss: 0.017423395067453384\n",
      "iteration 2717, dc_loss: 0.06558950245380402, tv_loss: 0.01742367260158062\n",
      "iteration 2718, dc_loss: 0.06558951735496521, tv_loss: 0.017423836514353752\n",
      "iteration 2719, dc_loss: 0.06558950990438461, tv_loss: 0.017423590645194054\n",
      "iteration 2720, dc_loss: 0.06558950245380402, tv_loss: 0.0174237173050642\n",
      "iteration 2721, dc_loss: 0.06558950245380402, tv_loss: 0.017423566430807114\n",
      "iteration 2722, dc_loss: 0.06558950245380402, tv_loss: 0.017423393204808235\n",
      "iteration 2723, dc_loss: 0.06558951735496521, tv_loss: 0.01742352731525898\n",
      "iteration 2724, dc_loss: 0.06558951735496521, tv_loss: 0.017423730343580246\n",
      "iteration 2725, dc_loss: 0.0655895248055458, tv_loss: 0.017423756420612335\n",
      "iteration 2726, dc_loss: 0.065589539706707, tv_loss: 0.017423583194613457\n",
      "iteration 2727, dc_loss: 0.0655895322561264, tv_loss: 0.017423536628484726\n",
      "iteration 2728, dc_loss: 0.06558950245380402, tv_loss: 0.0174235999584198\n",
      "iteration 2729, dc_loss: 0.06558947265148163, tv_loss: 0.017423799261450768\n",
      "iteration 2730, dc_loss: 0.06558947265148163, tv_loss: 0.01742411032319069\n",
      "iteration 2731, dc_loss: 0.06558950990438461, tv_loss: 0.017423754557967186\n",
      "iteration 2732, dc_loss: 0.06558956205844879, tv_loss: 0.017423562705516815\n",
      "iteration 2733, dc_loss: 0.0655895471572876, tv_loss: 0.017423633486032486\n",
      "iteration 2734, dc_loss: 0.06558950990438461, tv_loss: 0.01742367632687092\n",
      "iteration 2735, dc_loss: 0.06558947265148163, tv_loss: 0.01742364652454853\n",
      "iteration 2736, dc_loss: 0.06558946520090103, tv_loss: 0.017423851415514946\n",
      "iteration 2737, dc_loss: 0.06558948755264282, tv_loss: 0.017423851415514946\n",
      "iteration 2738, dc_loss: 0.0655895248055458, tv_loss: 0.01742379367351532\n",
      "iteration 2739, dc_loss: 0.065589539706707, tv_loss: 0.017423681914806366\n",
      "iteration 2740, dc_loss: 0.0655895248055458, tv_loss: 0.01742401160299778\n",
      "iteration 2741, dc_loss: 0.06558950990438461, tv_loss: 0.01742389239370823\n",
      "iteration 2742, dc_loss: 0.06558948755264282, tv_loss: 0.01742374710738659\n",
      "iteration 2743, dc_loss: 0.06558948010206223, tv_loss: 0.0174240842461586\n",
      "iteration 2744, dc_loss: 0.06558948755264282, tv_loss: 0.017424043267965317\n",
      "iteration 2745, dc_loss: 0.06558950990438461, tv_loss: 0.017423806712031364\n",
      "iteration 2746, dc_loss: 0.0655895322561264, tv_loss: 0.017423642799258232\n",
      "iteration 2747, dc_loss: 0.0655895546078682, tv_loss: 0.017423825338482857\n",
      "iteration 2748, dc_loss: 0.0655895322561264, tv_loss: 0.017423884943127632\n",
      "iteration 2749, dc_loss: 0.06558950245380402, tv_loss: 0.0174238458275795\n",
      "iteration 2750, dc_loss: 0.06558946520090103, tv_loss: 0.0174236036837101\n",
      "iteration 2751, dc_loss: 0.06558943539857864, tv_loss: 0.017423704266548157\n",
      "iteration 2752, dc_loss: 0.06558948755264282, tv_loss: 0.01742374338209629\n",
      "iteration 2753, dc_loss: 0.06558956205844879, tv_loss: 0.017423929646611214\n",
      "iteration 2754, dc_loss: 0.06558958441019058, tv_loss: 0.01742362231016159\n",
      "iteration 2755, dc_loss: 0.06558957695960999, tv_loss: 0.01742367073893547\n",
      "iteration 2756, dc_loss: 0.0655895322561264, tv_loss: 0.01742388680577278\n",
      "iteration 2757, dc_loss: 0.06558948010206223, tv_loss: 0.01742366887629032\n",
      "iteration 2758, dc_loss: 0.06558942794799805, tv_loss: 0.017423495650291443\n",
      "iteration 2759, dc_loss: 0.06558943539857864, tv_loss: 0.01742354780435562\n",
      "iteration 2760, dc_loss: 0.06558948010206223, tv_loss: 0.01742387004196644\n",
      "iteration 2761, dc_loss: 0.0655895471572876, tv_loss: 0.017423849552869797\n",
      "iteration 2762, dc_loss: 0.06558957695960999, tv_loss: 0.01742367073893547\n",
      "iteration 2763, dc_loss: 0.0655895546078682, tv_loss: 0.017423756420612335\n",
      "iteration 2764, dc_loss: 0.06558950245380402, tv_loss: 0.01742381788790226\n",
      "iteration 2765, dc_loss: 0.06558946520090103, tv_loss: 0.017423640936613083\n",
      "iteration 2766, dc_loss: 0.06558946520090103, tv_loss: 0.017423948273062706\n",
      "iteration 2767, dc_loss: 0.06558948755264282, tv_loss: 0.017423735931515694\n",
      "iteration 2768, dc_loss: 0.06558951735496521, tv_loss: 0.01742369495332241\n",
      "iteration 2769, dc_loss: 0.0655895248055458, tv_loss: 0.017423832789063454\n",
      "iteration 2770, dc_loss: 0.06558950990438461, tv_loss: 0.01742362789809704\n",
      "iteration 2771, dc_loss: 0.06558951735496521, tv_loss: 0.01742367446422577\n",
      "iteration 2772, dc_loss: 0.06558950245380402, tv_loss: 0.01742366887629032\n",
      "iteration 2773, dc_loss: 0.06558950990438461, tv_loss: 0.017423484474420547\n",
      "iteration 2774, dc_loss: 0.06558950990438461, tv_loss: 0.01742343232035637\n",
      "iteration 2775, dc_loss: 0.06558950245380402, tv_loss: 0.017423685640096664\n",
      "iteration 2776, dc_loss: 0.06558947265148163, tv_loss: 0.017423707991838455\n",
      "iteration 2777, dc_loss: 0.06558948755264282, tv_loss: 0.01742345280945301\n",
      "iteration 2778, dc_loss: 0.06558950990438461, tv_loss: 0.017423562705516815\n",
      "iteration 2779, dc_loss: 0.0655895248055458, tv_loss: 0.017423411831259727\n",
      "iteration 2780, dc_loss: 0.0655895248055458, tv_loss: 0.01742362789809704\n",
      "iteration 2781, dc_loss: 0.06558950990438461, tv_loss: 0.017423439770936966\n",
      "iteration 2782, dc_loss: 0.06558950990438461, tv_loss: 0.017423512414097786\n",
      "iteration 2783, dc_loss: 0.06558951735496521, tv_loss: 0.017423713579773903\n",
      "iteration 2784, dc_loss: 0.06558951735496521, tv_loss: 0.01742379553616047\n",
      "iteration 2785, dc_loss: 0.06558950245380402, tv_loss: 0.017423642799258232\n",
      "iteration 2786, dc_loss: 0.06558948010206223, tv_loss: 0.017423612996935844\n",
      "iteration 2787, dc_loss: 0.06558950245380402, tv_loss: 0.017423491925001144\n",
      "iteration 2788, dc_loss: 0.06558950245380402, tv_loss: 0.017423514276742935\n",
      "iteration 2789, dc_loss: 0.06558950245380402, tv_loss: 0.01742360182106495\n",
      "iteration 2790, dc_loss: 0.06558948755264282, tv_loss: 0.017423706129193306\n",
      "iteration 2791, dc_loss: 0.06558948755264282, tv_loss: 0.017423631623387337\n",
      "iteration 2792, dc_loss: 0.06558951735496521, tv_loss: 0.01742367260158062\n",
      "iteration 2793, dc_loss: 0.06558950990438461, tv_loss: 0.01742338202893734\n",
      "iteration 2794, dc_loss: 0.06558950990438461, tv_loss: 0.017423249781131744\n",
      "iteration 2795, dc_loss: 0.06558951735496521, tv_loss: 0.017423493787646294\n",
      "iteration 2796, dc_loss: 0.0655895248055458, tv_loss: 0.01742364652454853\n",
      "iteration 2797, dc_loss: 0.0655895322561264, tv_loss: 0.01742349937558174\n",
      "iteration 2798, dc_loss: 0.06558950990438461, tv_loss: 0.017423314973711967\n",
      "iteration 2799, dc_loss: 0.06558946520090103, tv_loss: 0.017423322424292564\n",
      "iteration 2800, dc_loss: 0.06558943539857864, tv_loss: 0.017423465847969055\n",
      "iteration 2801, dc_loss: 0.06558947265148163, tv_loss: 0.017423516139388084\n",
      "iteration 2802, dc_loss: 0.0655895248055458, tv_loss: 0.0174232330173254\n",
      "iteration 2803, dc_loss: 0.06558957695960999, tv_loss: 0.017423054203391075\n",
      "iteration 2804, dc_loss: 0.06558956950902939, tv_loss: 0.01742340438067913\n",
      "iteration 2805, dc_loss: 0.06558951735496521, tv_loss: 0.017423588782548904\n",
      "iteration 2806, dc_loss: 0.06558947265148163, tv_loss: 0.01742355152964592\n",
      "iteration 2807, dc_loss: 0.06558945775032043, tv_loss: 0.0174234751611948\n",
      "iteration 2808, dc_loss: 0.06558947265148163, tv_loss: 0.01742352545261383\n",
      "iteration 2809, dc_loss: 0.06558950990438461, tv_loss: 0.01742345653474331\n",
      "iteration 2810, dc_loss: 0.0655895248055458, tv_loss: 0.017423538491129875\n",
      "iteration 2811, dc_loss: 0.0655895322561264, tv_loss: 0.017423493787646294\n",
      "iteration 2812, dc_loss: 0.0655895248055458, tv_loss: 0.017423510551452637\n",
      "iteration 2813, dc_loss: 0.06558951735496521, tv_loss: 0.01742370054125786\n",
      "iteration 2814, dc_loss: 0.06558948010206223, tv_loss: 0.017423640936613083\n",
      "iteration 2815, dc_loss: 0.06558948755264282, tv_loss: 0.017423653975129128\n",
      "iteration 2816, dc_loss: 0.06558950990438461, tv_loss: 0.017423534765839577\n",
      "iteration 2817, dc_loss: 0.06558951735496521, tv_loss: 0.017423255369067192\n",
      "iteration 2818, dc_loss: 0.06558950990438461, tv_loss: 0.017423314973711967\n",
      "iteration 2819, dc_loss: 0.06558950990438461, tv_loss: 0.017423449084162712\n",
      "iteration 2820, dc_loss: 0.06558948755264282, tv_loss: 0.01742354966700077\n",
      "iteration 2821, dc_loss: 0.06558948010206223, tv_loss: 0.01742357574403286\n",
      "iteration 2822, dc_loss: 0.06558950990438461, tv_loss: 0.017423518002033234\n",
      "iteration 2823, dc_loss: 0.06558951735496521, tv_loss: 0.017423266544938087\n",
      "iteration 2824, dc_loss: 0.06558950990438461, tv_loss: 0.017423288896679878\n",
      "iteration 2825, dc_loss: 0.06558950990438461, tv_loss: 0.01742350496351719\n",
      "iteration 2826, dc_loss: 0.06558950990438461, tv_loss: 0.0174232330173254\n",
      "iteration 2827, dc_loss: 0.0655895322561264, tv_loss: 0.017423374578356743\n",
      "iteration 2828, dc_loss: 0.06558950245380402, tv_loss: 0.017423570156097412\n",
      "iteration 2829, dc_loss: 0.06558948010206223, tv_loss: 0.017423536628484726\n",
      "iteration 2830, dc_loss: 0.06558946520090103, tv_loss: 0.017423124983906746\n",
      "iteration 2831, dc_loss: 0.06558948755264282, tv_loss: 0.01742321066558361\n",
      "iteration 2832, dc_loss: 0.0655895248055458, tv_loss: 0.017423415556550026\n",
      "iteration 2833, dc_loss: 0.0655895471572876, tv_loss: 0.017423391342163086\n",
      "iteration 2834, dc_loss: 0.065589539706707, tv_loss: 0.017423421144485474\n",
      "iteration 2835, dc_loss: 0.06558950990438461, tv_loss: 0.017423534765839577\n",
      "iteration 2836, dc_loss: 0.06558948755264282, tv_loss: 0.017423242330551147\n",
      "iteration 2837, dc_loss: 0.06558950990438461, tv_loss: 0.017423290759325027\n",
      "iteration 2838, dc_loss: 0.06558950990438461, tv_loss: 0.017423823475837708\n",
      "iteration 2839, dc_loss: 0.06558950245380402, tv_loss: 0.01742365211248398\n",
      "iteration 2840, dc_loss: 0.06558948755264282, tv_loss: 0.0174234751611948\n",
      "iteration 2841, dc_loss: 0.06558948755264282, tv_loss: 0.01742319017648697\n",
      "iteration 2842, dc_loss: 0.06558949500322342, tv_loss: 0.017423734068870544\n",
      "iteration 2843, dc_loss: 0.0655895322561264, tv_loss: 0.017423762008547783\n",
      "iteration 2844, dc_loss: 0.0655895322561264, tv_loss: 0.01742326281964779\n",
      "iteration 2845, dc_loss: 0.0655895248055458, tv_loss: 0.017423318699002266\n",
      "iteration 2846, dc_loss: 0.06558951735496521, tv_loss: 0.017423581331968307\n",
      "iteration 2847, dc_loss: 0.06558949500322342, tv_loss: 0.01742367632687092\n",
      "iteration 2848, dc_loss: 0.06558947265148163, tv_loss: 0.017423471435904503\n",
      "iteration 2849, dc_loss: 0.06558947265148163, tv_loss: 0.017423294484615326\n",
      "iteration 2850, dc_loss: 0.06558950245380402, tv_loss: 0.017423588782548904\n",
      "iteration 2851, dc_loss: 0.0655895322561264, tv_loss: 0.017423827201128006\n",
      "iteration 2852, dc_loss: 0.0655895322561264, tv_loss: 0.017423581331968307\n",
      "iteration 2853, dc_loss: 0.06558951735496521, tv_loss: 0.0174234788864851\n",
      "iteration 2854, dc_loss: 0.06558947265148163, tv_loss: 0.017423421144485474\n",
      "iteration 2855, dc_loss: 0.06558944284915924, tv_loss: 0.01742357574403286\n",
      "iteration 2856, dc_loss: 0.06558944284915924, tv_loss: 0.017423972487449646\n",
      "iteration 2857, dc_loss: 0.06558948755264282, tv_loss: 0.01742374897003174\n",
      "iteration 2858, dc_loss: 0.065589539706707, tv_loss: 0.01742345467209816\n",
      "iteration 2859, dc_loss: 0.0655895471572876, tv_loss: 0.017423465847969055\n",
      "iteration 2860, dc_loss: 0.06558951735496521, tv_loss: 0.01742374897003174\n",
      "iteration 2861, dc_loss: 0.06558950245380402, tv_loss: 0.01742381975054741\n",
      "iteration 2862, dc_loss: 0.06558948010206223, tv_loss: 0.017423782497644424\n",
      "iteration 2863, dc_loss: 0.06558946520090103, tv_loss: 0.017423806712031364\n",
      "iteration 2864, dc_loss: 0.06558948010206223, tv_loss: 0.017423659563064575\n",
      "iteration 2865, dc_loss: 0.06558950245380402, tv_loss: 0.017423903569579124\n",
      "iteration 2866, dc_loss: 0.06558951735496521, tv_loss: 0.017423734068870544\n",
      "iteration 2867, dc_loss: 0.0655895546078682, tv_loss: 0.017423639073967934\n",
      "iteration 2868, dc_loss: 0.0655895546078682, tv_loss: 0.0174238458275795\n",
      "iteration 2869, dc_loss: 0.0655895322561264, tv_loss: 0.017423827201128006\n",
      "iteration 2870, dc_loss: 0.06558947265148163, tv_loss: 0.01742369867861271\n",
      "iteration 2871, dc_loss: 0.06558943539857864, tv_loss: 0.01742367446422577\n",
      "iteration 2872, dc_loss: 0.06558945775032043, tv_loss: 0.017423534765839577\n",
      "iteration 2873, dc_loss: 0.06558949500322342, tv_loss: 0.01742357201874256\n",
      "iteration 2874, dc_loss: 0.0655895248055458, tv_loss: 0.01742365024983883\n",
      "iteration 2875, dc_loss: 0.0655895322561264, tv_loss: 0.017423536628484726\n",
      "iteration 2876, dc_loss: 0.0655895248055458, tv_loss: 0.01742350123822689\n",
      "iteration 2877, dc_loss: 0.06558950245380402, tv_loss: 0.01742355152964592\n",
      "iteration 2878, dc_loss: 0.06558948010206223, tv_loss: 0.01742371916770935\n",
      "iteration 2879, dc_loss: 0.06558948755264282, tv_loss: 0.017423484474420547\n",
      "iteration 2880, dc_loss: 0.06558950245380402, tv_loss: 0.01742347702383995\n",
      "iteration 2881, dc_loss: 0.06558950990438461, tv_loss: 0.017423629760742188\n",
      "iteration 2882, dc_loss: 0.0655895248055458, tv_loss: 0.017423387616872787\n",
      "iteration 2883, dc_loss: 0.06558950990438461, tv_loss: 0.017423506826162338\n",
      "iteration 2884, dc_loss: 0.06558948010206223, tv_loss: 0.01742377318441868\n",
      "iteration 2885, dc_loss: 0.06558948755264282, tv_loss: 0.01742367073893547\n",
      "iteration 2886, dc_loss: 0.06558950245380402, tv_loss: 0.017423506826162338\n",
      "iteration 2887, dc_loss: 0.06558950990438461, tv_loss: 0.01742350496351719\n",
      "iteration 2888, dc_loss: 0.06558950990438461, tv_loss: 0.017423544079065323\n",
      "iteration 2889, dc_loss: 0.06558950245380402, tv_loss: 0.017423667013645172\n",
      "iteration 2890, dc_loss: 0.06558947265148163, tv_loss: 0.017423534765839577\n",
      "iteration 2891, dc_loss: 0.06558948010206223, tv_loss: 0.0174235999584198\n",
      "iteration 2892, dc_loss: 0.06558950990438461, tv_loss: 0.017423612996935844\n",
      "iteration 2893, dc_loss: 0.0655895322561264, tv_loss: 0.01742349937558174\n",
      "iteration 2894, dc_loss: 0.0655895471572876, tv_loss: 0.017423532903194427\n",
      "iteration 2895, dc_loss: 0.06558951735496521, tv_loss: 0.017423521727323532\n",
      "iteration 2896, dc_loss: 0.06558948755264282, tv_loss: 0.017423540353775024\n",
      "iteration 2897, dc_loss: 0.06558950245380402, tv_loss: 0.0174234788864851\n",
      "iteration 2898, dc_loss: 0.06558950245380402, tv_loss: 0.01742348074913025\n",
      "iteration 2899, dc_loss: 0.06558951735496521, tv_loss: 0.0174237173050642\n",
      "iteration 2900, dc_loss: 0.06558950990438461, tv_loss: 0.017423629760742188\n",
      "iteration 2901, dc_loss: 0.06558950990438461, tv_loss: 0.01742369495332241\n",
      "iteration 2902, dc_loss: 0.06558950990438461, tv_loss: 0.017423536628484726\n",
      "iteration 2903, dc_loss: 0.0655895248055458, tv_loss: 0.017423661425709724\n",
      "iteration 2904, dc_loss: 0.06558950245380402, tv_loss: 0.017423713579773903\n",
      "iteration 2905, dc_loss: 0.06558948010206223, tv_loss: 0.017423639073967934\n",
      "iteration 2906, dc_loss: 0.06558948755264282, tv_loss: 0.017423521727323532\n",
      "iteration 2907, dc_loss: 0.06558951735496521, tv_loss: 0.017423568293452263\n",
      "iteration 2908, dc_loss: 0.06558951735496521, tv_loss: 0.017424017190933228\n",
      "iteration 2909, dc_loss: 0.06558951735496521, tv_loss: 0.0174238421022892\n",
      "iteration 2910, dc_loss: 0.06558950245380402, tv_loss: 0.017423754557967186\n",
      "iteration 2911, dc_loss: 0.06558948010206223, tv_loss: 0.017423756420612335\n",
      "iteration 2912, dc_loss: 0.06558948755264282, tv_loss: 0.017423763871192932\n",
      "iteration 2913, dc_loss: 0.06558950990438461, tv_loss: 0.01742388866841793\n",
      "iteration 2914, dc_loss: 0.06558950990438461, tv_loss: 0.017423881217837334\n",
      "iteration 2915, dc_loss: 0.06558950990438461, tv_loss: 0.01742367632687092\n",
      "iteration 2916, dc_loss: 0.06558950245380402, tv_loss: 0.017423667013645172\n",
      "iteration 2917, dc_loss: 0.06558950990438461, tv_loss: 0.017423776909708977\n",
      "iteration 2918, dc_loss: 0.06558950245380402, tv_loss: 0.017423776909708977\n",
      "iteration 2919, dc_loss: 0.06558948010206223, tv_loss: 0.01742393709719181\n",
      "iteration 2920, dc_loss: 0.06558948010206223, tv_loss: 0.017424020916223526\n",
      "iteration 2921, dc_loss: 0.06558950245380402, tv_loss: 0.01742389239370823\n",
      "iteration 2922, dc_loss: 0.0655895248055458, tv_loss: 0.017423493787646294\n",
      "iteration 2923, dc_loss: 0.0655895471572876, tv_loss: 0.0174237173050642\n",
      "iteration 2924, dc_loss: 0.0655895248055458, tv_loss: 0.01742355339229107\n",
      "iteration 2925, dc_loss: 0.06558948755264282, tv_loss: 0.01742345467209816\n",
      "iteration 2926, dc_loss: 0.06558946520090103, tv_loss: 0.017423540353775024\n",
      "iteration 2927, dc_loss: 0.06558949500322342, tv_loss: 0.017423441633582115\n",
      "iteration 2928, dc_loss: 0.06558950245380402, tv_loss: 0.01742333546280861\n",
      "iteration 2929, dc_loss: 0.06558950990438461, tv_loss: 0.017423328012228012\n",
      "iteration 2930, dc_loss: 0.06558951735496521, tv_loss: 0.017423339188098907\n",
      "iteration 2931, dc_loss: 0.0655895322561264, tv_loss: 0.01742328144609928\n",
      "iteration 2932, dc_loss: 0.0655895248055458, tv_loss: 0.017423441633582115\n",
      "iteration 2933, dc_loss: 0.06558950245380402, tv_loss: 0.01742328144609928\n",
      "iteration 2934, dc_loss: 0.06558948755264282, tv_loss: 0.017423512414097786\n",
      "iteration 2935, dc_loss: 0.06558948010206223, tv_loss: 0.017423545941710472\n",
      "iteration 2936, dc_loss: 0.06558948755264282, tv_loss: 0.017423709854483604\n",
      "iteration 2937, dc_loss: 0.06558950990438461, tv_loss: 0.017423728480935097\n",
      "iteration 2938, dc_loss: 0.06558950990438461, tv_loss: 0.017423732206225395\n",
      "iteration 2939, dc_loss: 0.06558951735496521, tv_loss: 0.017423823475837708\n",
      "iteration 2940, dc_loss: 0.06558950990438461, tv_loss: 0.017423858866095543\n",
      "iteration 2941, dc_loss: 0.06558951735496521, tv_loss: 0.017423732206225395\n",
      "iteration 2942, dc_loss: 0.0655895322561264, tv_loss: 0.01742376573383808\n",
      "iteration 2943, dc_loss: 0.06558950245380402, tv_loss: 0.017424117773771286\n",
      "iteration 2944, dc_loss: 0.06558949500322342, tv_loss: 0.017423860728740692\n",
      "iteration 2945, dc_loss: 0.06558948010206223, tv_loss: 0.0174236036837101\n",
      "iteration 2946, dc_loss: 0.06558948755264282, tv_loss: 0.017423784360289574\n",
      "iteration 2947, dc_loss: 0.06558950990438461, tv_loss: 0.017423901706933975\n",
      "iteration 2948, dc_loss: 0.06558951735496521, tv_loss: 0.017423849552869797\n",
      "iteration 2949, dc_loss: 0.06558950245380402, tv_loss: 0.017423802986741066\n",
      "iteration 2950, dc_loss: 0.06558947265148163, tv_loss: 0.01742365211248398\n",
      "iteration 2951, dc_loss: 0.06558947265148163, tv_loss: 0.017423884943127632\n",
      "iteration 2952, dc_loss: 0.06558950245380402, tv_loss: 0.0174238458275795\n",
      "iteration 2953, dc_loss: 0.0655895248055458, tv_loss: 0.017423879355192184\n",
      "iteration 2954, dc_loss: 0.0655895248055458, tv_loss: 0.017423486337065697\n",
      "iteration 2955, dc_loss: 0.0655895322561264, tv_loss: 0.01742374151945114\n",
      "iteration 2956, dc_loss: 0.06558951735496521, tv_loss: 0.017423802986741066\n",
      "iteration 2957, dc_loss: 0.06558950990438461, tv_loss: 0.017423702403903008\n",
      "iteration 2958, dc_loss: 0.06558948010206223, tv_loss: 0.01742370054125786\n",
      "iteration 2959, dc_loss: 0.06558947265148163, tv_loss: 0.017423849552869797\n",
      "iteration 2960, dc_loss: 0.06558948010206223, tv_loss: 0.017423445358872414\n",
      "iteration 2961, dc_loss: 0.06558947265148163, tv_loss: 0.01742352358996868\n",
      "iteration 2962, dc_loss: 0.0655895248055458, tv_loss: 0.017423931509256363\n",
      "iteration 2963, dc_loss: 0.06558956205844879, tv_loss: 0.017423667013645172\n",
      "iteration 2964, dc_loss: 0.065589539706707, tv_loss: 0.017423171550035477\n",
      "iteration 2965, dc_loss: 0.06558950990438461, tv_loss: 0.017423659563064575\n",
      "iteration 2966, dc_loss: 0.06558948010206223, tv_loss: 0.017424041405320168\n",
      "iteration 2967, dc_loss: 0.06558946520090103, tv_loss: 0.01742377132177353\n",
      "iteration 2968, dc_loss: 0.06558945775032043, tv_loss: 0.01742330752313137\n",
      "iteration 2969, dc_loss: 0.06558948755264282, tv_loss: 0.017423339188098907\n",
      "iteration 2970, dc_loss: 0.0655895248055458, tv_loss: 0.01742354780435562\n",
      "iteration 2971, dc_loss: 0.065589539706707, tv_loss: 0.01742335967719555\n",
      "iteration 2972, dc_loss: 0.0655895248055458, tv_loss: 0.017423441633582115\n",
      "iteration 2973, dc_loss: 0.06558950245380402, tv_loss: 0.017423570156097412\n",
      "iteration 2974, dc_loss: 0.06558947265148163, tv_loss: 0.017423443496227264\n",
      "iteration 2975, dc_loss: 0.06558946520090103, tv_loss: 0.017423460260033607\n",
      "iteration 2976, dc_loss: 0.06558948755264282, tv_loss: 0.01742331124842167\n",
      "iteration 2977, dc_loss: 0.0655895322561264, tv_loss: 0.01742335595190525\n",
      "iteration 2978, dc_loss: 0.0655895322561264, tv_loss: 0.017423400655388832\n",
      "iteration 2979, dc_loss: 0.0655895248055458, tv_loss: 0.017423296347260475\n",
      "iteration 2980, dc_loss: 0.06558948755264282, tv_loss: 0.017423300072550774\n",
      "iteration 2981, dc_loss: 0.06558949500322342, tv_loss: 0.01742340438067913\n",
      "iteration 2982, dc_loss: 0.06558950245380402, tv_loss: 0.017423497512936592\n",
      "iteration 2983, dc_loss: 0.06558951735496521, tv_loss: 0.017423398792743683\n",
      "iteration 2984, dc_loss: 0.06558951735496521, tv_loss: 0.017423143610358238\n",
      "iteration 2985, dc_loss: 0.06558948755264282, tv_loss: 0.017423288896679878\n",
      "iteration 2986, dc_loss: 0.06558948010206223, tv_loss: 0.01742352917790413\n",
      "iteration 2987, dc_loss: 0.06558950990438461, tv_loss: 0.01742328330874443\n",
      "iteration 2988, dc_loss: 0.06558951735496521, tv_loss: 0.017423352226614952\n",
      "iteration 2989, dc_loss: 0.06558949500322342, tv_loss: 0.01742345094680786\n",
      "iteration 2990, dc_loss: 0.06558948010206223, tv_loss: 0.017423583194613457\n",
      "iteration 2991, dc_loss: 0.06558948755264282, tv_loss: 0.0174237210303545\n",
      "iteration 2992, dc_loss: 0.06558948755264282, tv_loss: 0.0174238421022892\n",
      "iteration 2993, dc_loss: 0.06558951735496521, tv_loss: 0.01742376945912838\n",
      "iteration 2994, dc_loss: 0.0655895248055458, tv_loss: 0.01742362231016159\n",
      "iteration 2995, dc_loss: 0.0655895248055458, tv_loss: 0.01742362044751644\n",
      "iteration 2996, dc_loss: 0.06558950990438461, tv_loss: 0.017423689365386963\n",
      "iteration 2997, dc_loss: 0.06558950245380402, tv_loss: 0.01742357388138771\n",
      "iteration 2998, dc_loss: 0.06558948755264282, tv_loss: 0.01742345094680786\n",
      "iteration 2999, dc_loss: 0.06558948755264282, tv_loss: 0.017423590645194054\n",
      "iteration 3000, dc_loss: 0.06558950990438461, tv_loss: 0.017423778772354126\n",
      "iteration 3001, dc_loss: 0.0655895248055458, tv_loss: 0.017423925921320915\n",
      "iteration 3002, dc_loss: 0.0655895471572876, tv_loss: 0.01742340810596943\n",
      "iteration 3003, dc_loss: 0.0655895248055458, tv_loss: 0.017423251643776894\n",
      "iteration 3004, dc_loss: 0.06558948755264282, tv_loss: 0.017423352226614952\n",
      "iteration 3005, dc_loss: 0.06558948010206223, tv_loss: 0.01742379367351532\n",
      "iteration 3006, dc_loss: 0.06558948010206223, tv_loss: 0.017423423007130623\n",
      "iteration 3007, dc_loss: 0.06558950990438461, tv_loss: 0.01742313988506794\n",
      "iteration 3008, dc_loss: 0.0655895248055458, tv_loss: 0.0174232330173254\n",
      "iteration 3009, dc_loss: 0.06558950245380402, tv_loss: 0.01742350310087204\n",
      "iteration 3010, dc_loss: 0.06558946520090103, tv_loss: 0.01742369867861271\n",
      "iteration 3011, dc_loss: 0.06558948010206223, tv_loss: 0.017423570156097412\n",
      "iteration 3012, dc_loss: 0.06558950990438461, tv_loss: 0.017423275858163834\n",
      "iteration 3013, dc_loss: 0.0655895322561264, tv_loss: 0.017423216253519058\n",
      "iteration 3014, dc_loss: 0.0655895248055458, tv_loss: 0.017423521727323532\n",
      "iteration 3015, dc_loss: 0.06558948755264282, tv_loss: 0.01742369867861271\n",
      "iteration 3016, dc_loss: 0.06558945775032043, tv_loss: 0.017423292621970177\n",
      "iteration 3017, dc_loss: 0.06558946520090103, tv_loss: 0.0174232367426157\n",
      "iteration 3018, dc_loss: 0.06558948755264282, tv_loss: 0.01742350123822689\n",
      "iteration 3019, dc_loss: 0.0655895248055458, tv_loss: 0.01742386817932129\n",
      "iteration 3020, dc_loss: 0.0655895248055458, tv_loss: 0.017423531040549278\n",
      "iteration 3021, dc_loss: 0.06558951735496521, tv_loss: 0.017423300072550774\n",
      "iteration 3022, dc_loss: 0.06558950990438461, tv_loss: 0.017423400655388832\n",
      "iteration 3023, dc_loss: 0.06558950245380402, tv_loss: 0.017423471435904503\n",
      "iteration 3024, dc_loss: 0.06558948010206223, tv_loss: 0.017423802986741066\n",
      "iteration 3025, dc_loss: 0.06558948755264282, tv_loss: 0.017423581331968307\n",
      "iteration 3026, dc_loss: 0.06558950990438461, tv_loss: 0.017423242330551147\n",
      "iteration 3027, dc_loss: 0.0655895322561264, tv_loss: 0.017423570156097412\n",
      "iteration 3028, dc_loss: 0.0655895248055458, tv_loss: 0.01742386445403099\n",
      "iteration 3029, dc_loss: 0.06558950245380402, tv_loss: 0.01742381975054741\n",
      "iteration 3030, dc_loss: 0.06558947265148163, tv_loss: 0.01742362417280674\n",
      "iteration 3031, dc_loss: 0.06558945775032043, tv_loss: 0.017423320561647415\n",
      "iteration 3032, dc_loss: 0.06558950245380402, tv_loss: 0.017423881217837334\n",
      "iteration 3033, dc_loss: 0.06558951735496521, tv_loss: 0.017423730343580246\n",
      "iteration 3034, dc_loss: 0.06558950245380402, tv_loss: 0.017423689365386963\n",
      "iteration 3035, dc_loss: 0.06558951735496521, tv_loss: 0.017423836514353752\n",
      "iteration 3036, dc_loss: 0.06558951735496521, tv_loss: 0.017423877492547035\n",
      "iteration 3037, dc_loss: 0.06558950245380402, tv_loss: 0.017423879355192184\n",
      "iteration 3038, dc_loss: 0.06558948755264282, tv_loss: 0.01742369309067726\n",
      "iteration 3039, dc_loss: 0.06558947265148163, tv_loss: 0.017423933371901512\n",
      "iteration 3040, dc_loss: 0.06558947265148163, tv_loss: 0.01742401160299778\n",
      "iteration 3041, dc_loss: 0.06558948755264282, tv_loss: 0.017423929646611214\n",
      "iteration 3042, dc_loss: 0.06558951735496521, tv_loss: 0.017423879355192184\n",
      "iteration 3043, dc_loss: 0.0655895471572876, tv_loss: 0.017423972487449646\n",
      "iteration 3044, dc_loss: 0.06558951735496521, tv_loss: 0.017423979938030243\n",
      "iteration 3045, dc_loss: 0.06558947265148163, tv_loss: 0.01742374151945114\n",
      "iteration 3046, dc_loss: 0.06558945775032043, tv_loss: 0.01742386445403099\n",
      "iteration 3047, dc_loss: 0.06558948010206223, tv_loss: 0.0174237173050642\n",
      "iteration 3048, dc_loss: 0.06558950990438461, tv_loss: 0.017423633486032486\n",
      "iteration 3049, dc_loss: 0.06558950990438461, tv_loss: 0.017423884943127632\n",
      "iteration 3050, dc_loss: 0.06558950990438461, tv_loss: 0.017423752695322037\n",
      "iteration 3051, dc_loss: 0.06558950990438461, tv_loss: 0.01742347702383995\n",
      "iteration 3052, dc_loss: 0.06558950245380402, tv_loss: 0.01742354780435562\n",
      "iteration 3053, dc_loss: 0.06558951735496521, tv_loss: 0.01742364652454853\n",
      "iteration 3054, dc_loss: 0.06558950245380402, tv_loss: 0.01742362417280674\n",
      "iteration 3055, dc_loss: 0.06558948755264282, tv_loss: 0.01742340438067913\n",
      "iteration 3056, dc_loss: 0.06558948755264282, tv_loss: 0.017423506826162338\n",
      "iteration 3057, dc_loss: 0.06558948755264282, tv_loss: 0.01742357760667801\n",
      "iteration 3058, dc_loss: 0.06558948010206223, tv_loss: 0.017423568293452263\n",
      "iteration 3059, dc_loss: 0.06558949500322342, tv_loss: 0.017423540353775024\n",
      "iteration 3060, dc_loss: 0.06558950245380402, tv_loss: 0.017423560842871666\n",
      "iteration 3061, dc_loss: 0.06558950990438461, tv_loss: 0.017423637211322784\n",
      "iteration 3062, dc_loss: 0.06558951735496521, tv_loss: 0.01742354780435562\n",
      "iteration 3063, dc_loss: 0.06558950990438461, tv_loss: 0.017423685640096664\n",
      "iteration 3064, dc_loss: 0.06558948010206223, tv_loss: 0.017423594370484352\n",
      "iteration 3065, dc_loss: 0.06558947265148163, tv_loss: 0.01742362789809704\n",
      "iteration 3066, dc_loss: 0.06558946520090103, tv_loss: 0.017423713579773903\n",
      "iteration 3067, dc_loss: 0.06558950990438461, tv_loss: 0.017423702403903008\n",
      "iteration 3068, dc_loss: 0.0655895322561264, tv_loss: 0.017423434183001518\n",
      "iteration 3069, dc_loss: 0.0655895248055458, tv_loss: 0.017423434183001518\n",
      "iteration 3070, dc_loss: 0.06558950245380402, tv_loss: 0.017423560842871666\n",
      "iteration 3071, dc_loss: 0.06558948755264282, tv_loss: 0.017423970624804497\n",
      "iteration 3072, dc_loss: 0.06558948010206223, tv_loss: 0.017423901706933975\n",
      "iteration 3073, dc_loss: 0.06558946520090103, tv_loss: 0.01742378994822502\n",
      "iteration 3074, dc_loss: 0.06558948755264282, tv_loss: 0.01742384023964405\n",
      "iteration 3075, dc_loss: 0.06558951735496521, tv_loss: 0.017423884943127632\n",
      "iteration 3076, dc_loss: 0.0655895248055458, tv_loss: 0.017423739656805992\n",
      "iteration 3077, dc_loss: 0.0655895322561264, tv_loss: 0.017423763871192932\n",
      "iteration 3078, dc_loss: 0.06558950245380402, tv_loss: 0.017423484474420547\n",
      "iteration 3079, dc_loss: 0.06558947265148163, tv_loss: 0.01742364466190338\n",
      "iteration 3080, dc_loss: 0.06558944284915924, tv_loss: 0.017423896118998528\n",
      "iteration 3081, dc_loss: 0.06558946520090103, tv_loss: 0.01742352731525898\n",
      "iteration 3082, dc_loss: 0.06558950990438461, tv_loss: 0.01742352917790413\n",
      "iteration 3083, dc_loss: 0.06558956950902939, tv_loss: 0.01742378994822502\n",
      "iteration 3084, dc_loss: 0.06558956205844879, tv_loss: 0.017423445358872414\n",
      "iteration 3085, dc_loss: 0.06558951735496521, tv_loss: 0.017423447221517563\n",
      "iteration 3086, dc_loss: 0.06558948010206223, tv_loss: 0.01742391660809517\n",
      "iteration 3087, dc_loss: 0.06558944284915924, tv_loss: 0.01742370054125786\n",
      "iteration 3088, dc_loss: 0.06558946520090103, tv_loss: 0.017423782497644424\n",
      "iteration 3089, dc_loss: 0.06558948755264282, tv_loss: 0.017423957586288452\n",
      "iteration 3090, dc_loss: 0.0655895248055458, tv_loss: 0.01742396503686905\n",
      "iteration 3091, dc_loss: 0.065589539706707, tv_loss: 0.01742386259138584\n",
      "iteration 3092, dc_loss: 0.0655895322561264, tv_loss: 0.017423855140805244\n",
      "iteration 3093, dc_loss: 0.06558950245380402, tv_loss: 0.017424052581191063\n",
      "iteration 3094, dc_loss: 0.06558944284915924, tv_loss: 0.017423957586288452\n",
      "iteration 3095, dc_loss: 0.06558944284915924, tv_loss: 0.017423655837774277\n",
      "iteration 3096, dc_loss: 0.06558948010206223, tv_loss: 0.017423704266548157\n",
      "iteration 3097, dc_loss: 0.0655895248055458, tv_loss: 0.017423899844288826\n",
      "iteration 3098, dc_loss: 0.0655895546078682, tv_loss: 0.017423642799258232\n",
      "iteration 3099, dc_loss: 0.0655895471572876, tv_loss: 0.017423108220100403\n",
      "iteration 3100, dc_loss: 0.06558949500322342, tv_loss: 0.01742345467209816\n",
      "iteration 3101, dc_loss: 0.06558945029973984, tv_loss: 0.017423607409000397\n",
      "iteration 3102, dc_loss: 0.06558946520090103, tv_loss: 0.017423531040549278\n",
      "iteration 3103, dc_loss: 0.06558948755264282, tv_loss: 0.017423495650291443\n",
      "iteration 3104, dc_loss: 0.06558949500322342, tv_loss: 0.017423605546355247\n",
      "iteration 3105, dc_loss: 0.06558948755264282, tv_loss: 0.01742352545261383\n",
      "iteration 3106, dc_loss: 0.06558948755264282, tv_loss: 0.017423531040549278\n",
      "iteration 3107, dc_loss: 0.0655895248055458, tv_loss: 0.017423463985323906\n",
      "iteration 3108, dc_loss: 0.0655895248055458, tv_loss: 0.01742370054125786\n",
      "iteration 3109, dc_loss: 0.0655895248055458, tv_loss: 0.017423637211322784\n",
      "iteration 3110, dc_loss: 0.06558949500322342, tv_loss: 0.017423488199710846\n",
      "iteration 3111, dc_loss: 0.06558946520090103, tv_loss: 0.01742352545261383\n",
      "iteration 3112, dc_loss: 0.06558947265148163, tv_loss: 0.01742415316402912\n",
      "iteration 3113, dc_loss: 0.06558950245380402, tv_loss: 0.017424268648028374\n",
      "iteration 3114, dc_loss: 0.06558950245380402, tv_loss: 0.01742367446422577\n",
      "iteration 3115, dc_loss: 0.06558950245380402, tv_loss: 0.017423877492547035\n",
      "iteration 3116, dc_loss: 0.06558950990438461, tv_loss: 0.017424190416932106\n",
      "iteration 3117, dc_loss: 0.06558950990438461, tv_loss: 0.01742410473525524\n",
      "iteration 3118, dc_loss: 0.06558949500322342, tv_loss: 0.01742399111390114\n",
      "iteration 3119, dc_loss: 0.06558948755264282, tv_loss: 0.017423689365386963\n",
      "iteration 3120, dc_loss: 0.06558948755264282, tv_loss: 0.017423834651708603\n",
      "iteration 3121, dc_loss: 0.06558949500322342, tv_loss: 0.01742396131157875\n",
      "iteration 3122, dc_loss: 0.06558950245380402, tv_loss: 0.017424123361706734\n",
      "iteration 3123, dc_loss: 0.0655895248055458, tv_loss: 0.017423847690224648\n",
      "iteration 3124, dc_loss: 0.06558951735496521, tv_loss: 0.017423776909708977\n",
      "iteration 3125, dc_loss: 0.06558948755264282, tv_loss: 0.017423832789063454\n",
      "iteration 3126, dc_loss: 0.06558948755264282, tv_loss: 0.017423950135707855\n",
      "iteration 3127, dc_loss: 0.06558950245380402, tv_loss: 0.017423851415514946\n",
      "iteration 3128, dc_loss: 0.06558950245380402, tv_loss: 0.017423802986741066\n",
      "iteration 3129, dc_loss: 0.06558946520090103, tv_loss: 0.017423732206225395\n",
      "iteration 3130, dc_loss: 0.06558946520090103, tv_loss: 0.01742376573383808\n",
      "iteration 3131, dc_loss: 0.06558950245380402, tv_loss: 0.0174238421022892\n",
      "iteration 3132, dc_loss: 0.0655895322561264, tv_loss: 0.01742364652454853\n",
      "iteration 3133, dc_loss: 0.0655895546078682, tv_loss: 0.01742357574403286\n",
      "iteration 3134, dc_loss: 0.0655895471572876, tv_loss: 0.017423726618289948\n",
      "iteration 3135, dc_loss: 0.06558948755264282, tv_loss: 0.017424052581191063\n",
      "iteration 3136, dc_loss: 0.06558945029973984, tv_loss: 0.0174238458275795\n",
      "iteration 3137, dc_loss: 0.06558948010206223, tv_loss: 0.01742362231016159\n",
      "iteration 3138, dc_loss: 0.06558950245380402, tv_loss: 0.01742372289299965\n",
      "iteration 3139, dc_loss: 0.06558950990438461, tv_loss: 0.017423776909708977\n",
      "iteration 3140, dc_loss: 0.06558950245380402, tv_loss: 0.017423471435904503\n",
      "iteration 3141, dc_loss: 0.06558948010206223, tv_loss: 0.017423486337065697\n",
      "iteration 3142, dc_loss: 0.06558948010206223, tv_loss: 0.01742354780435562\n",
      "iteration 3143, dc_loss: 0.06558948010206223, tv_loss: 0.01742355339229107\n",
      "iteration 3144, dc_loss: 0.06558950245380402, tv_loss: 0.01742354966700077\n",
      "iteration 3145, dc_loss: 0.0655895322561264, tv_loss: 0.017423612996935844\n",
      "iteration 3146, dc_loss: 0.0655895471572876, tv_loss: 0.017423711717128754\n",
      "iteration 3147, dc_loss: 0.0655895322561264, tv_loss: 0.017423365265130997\n",
      "iteration 3148, dc_loss: 0.06558950245380402, tv_loss: 0.017423473298549652\n",
      "iteration 3149, dc_loss: 0.06558947265148163, tv_loss: 0.017423799261450768\n",
      "iteration 3150, dc_loss: 0.06558944284915924, tv_loss: 0.017423715442419052\n",
      "iteration 3151, dc_loss: 0.06558945775032043, tv_loss: 0.01742369495332241\n",
      "iteration 3152, dc_loss: 0.06558951735496521, tv_loss: 0.01742362417280674\n",
      "iteration 3153, dc_loss: 0.0655895322561264, tv_loss: 0.017423424869775772\n",
      "iteration 3154, dc_loss: 0.0655895322561264, tv_loss: 0.01742340810596943\n",
      "iteration 3155, dc_loss: 0.0655895322561264, tv_loss: 0.0174238458275795\n",
      "iteration 3156, dc_loss: 0.06558950245380402, tv_loss: 0.01742374151945114\n",
      "iteration 3157, dc_loss: 0.06558948755264282, tv_loss: 0.017423707991838455\n",
      "iteration 3158, dc_loss: 0.06558949500322342, tv_loss: 0.017423469573259354\n",
      "iteration 3159, dc_loss: 0.06558948010206223, tv_loss: 0.017423711717128754\n",
      "iteration 3160, dc_loss: 0.06558946520090103, tv_loss: 0.017423760145902634\n",
      "iteration 3161, dc_loss: 0.06558948010206223, tv_loss: 0.01742370054125786\n",
      "iteration 3162, dc_loss: 0.06558950990438461, tv_loss: 0.017423802986741066\n",
      "iteration 3163, dc_loss: 0.0655895471572876, tv_loss: 0.017423538491129875\n",
      "iteration 3164, dc_loss: 0.0655895248055458, tv_loss: 0.017423640936613083\n",
      "iteration 3165, dc_loss: 0.06558950245380402, tv_loss: 0.01742357388138771\n",
      "iteration 3166, dc_loss: 0.06558948010206223, tv_loss: 0.01742347702383995\n",
      "iteration 3167, dc_loss: 0.06558948755264282, tv_loss: 0.01742350123822689\n",
      "iteration 3168, dc_loss: 0.06558950245380402, tv_loss: 0.01742354780435562\n",
      "iteration 3169, dc_loss: 0.06558948755264282, tv_loss: 0.01742333360016346\n",
      "iteration 3170, dc_loss: 0.06558948010206223, tv_loss: 0.017423367127776146\n",
      "iteration 3171, dc_loss: 0.06558948010206223, tv_loss: 0.017423657700419426\n",
      "iteration 3172, dc_loss: 0.06558950245380402, tv_loss: 0.017423391342163086\n",
      "iteration 3173, dc_loss: 0.0655895322561264, tv_loss: 0.0174234751611948\n",
      "iteration 3174, dc_loss: 0.0655895248055458, tv_loss: 0.01742340438067913\n",
      "iteration 3175, dc_loss: 0.06558950245380402, tv_loss: 0.017423447221517563\n",
      "iteration 3176, dc_loss: 0.06558948010206223, tv_loss: 0.0174238421022892\n",
      "iteration 3177, dc_loss: 0.06558947265148163, tv_loss: 0.017423616722226143\n",
      "iteration 3178, dc_loss: 0.06558945775032043, tv_loss: 0.017423324286937714\n",
      "iteration 3179, dc_loss: 0.06558950245380402, tv_loss: 0.01742354780435562\n",
      "iteration 3180, dc_loss: 0.06558950990438461, tv_loss: 0.017423616722226143\n",
      "iteration 3181, dc_loss: 0.06558950990438461, tv_loss: 0.017423518002033234\n",
      "iteration 3182, dc_loss: 0.06558950990438461, tv_loss: 0.017423244193196297\n",
      "iteration 3183, dc_loss: 0.06558950245380402, tv_loss: 0.017423439770936966\n",
      "iteration 3184, dc_loss: 0.06558948010206223, tv_loss: 0.017423611134290695\n",
      "iteration 3185, dc_loss: 0.06558946520090103, tv_loss: 0.017423583194613457\n",
      "iteration 3186, dc_loss: 0.06558950245380402, tv_loss: 0.01742357388138771\n",
      "iteration 3187, dc_loss: 0.0655895471572876, tv_loss: 0.017423344776034355\n",
      "iteration 3188, dc_loss: 0.0655895471572876, tv_loss: 0.017423225566744804\n",
      "iteration 3189, dc_loss: 0.06558951735496521, tv_loss: 0.017423510551452637\n",
      "iteration 3190, dc_loss: 0.06558947265148163, tv_loss: 0.01742359809577465\n",
      "iteration 3191, dc_loss: 0.06558945775032043, tv_loss: 0.01742325909435749\n",
      "iteration 3192, dc_loss: 0.06558948010206223, tv_loss: 0.017423266544938087\n",
      "iteration 3193, dc_loss: 0.06558951735496521, tv_loss: 0.017423607409000397\n",
      "iteration 3194, dc_loss: 0.0655895471572876, tv_loss: 0.017423484474420547\n",
      "iteration 3195, dc_loss: 0.0655895322561264, tv_loss: 0.01742333360016346\n",
      "iteration 3196, dc_loss: 0.06558950990438461, tv_loss: 0.01742302067577839\n",
      "iteration 3197, dc_loss: 0.06558950245380402, tv_loss: 0.017423231154680252\n",
      "iteration 3198, dc_loss: 0.06558948010206223, tv_loss: 0.017423657700419426\n",
      "iteration 3199, dc_loss: 0.06558946520090103, tv_loss: 0.017423352226614952\n",
      "iteration 3200, dc_loss: 0.06558948010206223, tv_loss: 0.01742306724190712\n",
      "iteration 3201, dc_loss: 0.06558948755264282, tv_loss: 0.01742347702383995\n",
      "iteration 3202, dc_loss: 0.0655895248055458, tv_loss: 0.017423557117581367\n",
      "iteration 3203, dc_loss: 0.0655895546078682, tv_loss: 0.017423342913389206\n",
      "iteration 3204, dc_loss: 0.0655895248055458, tv_loss: 0.017423341050744057\n",
      "iteration 3205, dc_loss: 0.06558948755264282, tv_loss: 0.017423441633582115\n",
      "iteration 3206, dc_loss: 0.06558944284915924, tv_loss: 0.017423585057258606\n",
      "iteration 3207, dc_loss: 0.06558946520090103, tv_loss: 0.017423758283257484\n",
      "iteration 3208, dc_loss: 0.06558948010206223, tv_loss: 0.01742378994822502\n",
      "iteration 3209, dc_loss: 0.06558950245380402, tv_loss: 0.017423687502741814\n",
      "iteration 3210, dc_loss: 0.06558950990438461, tv_loss: 0.0174239668995142\n",
      "iteration 3211, dc_loss: 0.0655895248055458, tv_loss: 0.01742369495332241\n",
      "iteration 3212, dc_loss: 0.06558951735496521, tv_loss: 0.017423352226614952\n",
      "iteration 3213, dc_loss: 0.06558949500322342, tv_loss: 0.01742362789809704\n",
      "iteration 3214, dc_loss: 0.06558948010206223, tv_loss: 0.017423836514353752\n",
      "iteration 3215, dc_loss: 0.06558947265148163, tv_loss: 0.01742365024983883\n",
      "iteration 3216, dc_loss: 0.06558948755264282, tv_loss: 0.01742348074913025\n",
      "iteration 3217, dc_loss: 0.06558950990438461, tv_loss: 0.017423497512936592\n",
      "iteration 3218, dc_loss: 0.06558951735496521, tv_loss: 0.017423775047063828\n",
      "iteration 3219, dc_loss: 0.06558950990438461, tv_loss: 0.01742378994822502\n",
      "iteration 3220, dc_loss: 0.06558947265148163, tv_loss: 0.01742330938577652\n",
      "iteration 3221, dc_loss: 0.06558948010206223, tv_loss: 0.017423467710614204\n",
      "iteration 3222, dc_loss: 0.06558950245380402, tv_loss: 0.01742355339229107\n",
      "iteration 3223, dc_loss: 0.06558950990438461, tv_loss: 0.01742345280945301\n",
      "iteration 3224, dc_loss: 0.06558950990438461, tv_loss: 0.017423266544938087\n",
      "iteration 3225, dc_loss: 0.06558950245380402, tv_loss: 0.01742335595190525\n",
      "iteration 3226, dc_loss: 0.06558950245380402, tv_loss: 0.017423374578356743\n",
      "iteration 3227, dc_loss: 0.06558950245380402, tv_loss: 0.017423482611775398\n",
      "iteration 3228, dc_loss: 0.06558950245380402, tv_loss: 0.017423484474420547\n",
      "iteration 3229, dc_loss: 0.06558950990438461, tv_loss: 0.0174233540892601\n",
      "iteration 3230, dc_loss: 0.06558948010206223, tv_loss: 0.0174233578145504\n",
      "iteration 3231, dc_loss: 0.06558946520090103, tv_loss: 0.017423361539840698\n",
      "iteration 3232, dc_loss: 0.06558948010206223, tv_loss: 0.01742357760667801\n",
      "iteration 3233, dc_loss: 0.06558950245380402, tv_loss: 0.017423441633582115\n",
      "iteration 3234, dc_loss: 0.06558950990438461, tv_loss: 0.01742321066558361\n",
      "iteration 3235, dc_loss: 0.0655895322561264, tv_loss: 0.01742340251803398\n",
      "iteration 3236, dc_loss: 0.0655895322561264, tv_loss: 0.017423778772354126\n",
      "iteration 3237, dc_loss: 0.06558950990438461, tv_loss: 0.017423374578356743\n",
      "iteration 3238, dc_loss: 0.06558948010206223, tv_loss: 0.01742355339229107\n",
      "iteration 3239, dc_loss: 0.06558946520090103, tv_loss: 0.017423732206225395\n",
      "iteration 3240, dc_loss: 0.06558946520090103, tv_loss: 0.01742326281964779\n",
      "iteration 3241, dc_loss: 0.06558947265148163, tv_loss: 0.0174235999584198\n",
      "iteration 3242, dc_loss: 0.06558951735496521, tv_loss: 0.01742362417280674\n",
      "iteration 3243, dc_loss: 0.0655895471572876, tv_loss: 0.017423609271645546\n",
      "iteration 3244, dc_loss: 0.0655895322561264, tv_loss: 0.01742340438067913\n",
      "iteration 3245, dc_loss: 0.06558950245380402, tv_loss: 0.01742342859506607\n",
      "iteration 3246, dc_loss: 0.06558948755264282, tv_loss: 0.01742371916770935\n",
      "iteration 3247, dc_loss: 0.06558948755264282, tv_loss: 0.017423836514353752\n",
      "iteration 3248, dc_loss: 0.06558948010206223, tv_loss: 0.017423829063773155\n",
      "iteration 3249, dc_loss: 0.06558948755264282, tv_loss: 0.017423778772354126\n",
      "iteration 3250, dc_loss: 0.06558950245380402, tv_loss: 0.01742357574403286\n",
      "iteration 3251, dc_loss: 0.0655895248055458, tv_loss: 0.01742345094680786\n",
      "iteration 3252, dc_loss: 0.0655895322561264, tv_loss: 0.017423557117581367\n",
      "iteration 3253, dc_loss: 0.0655895248055458, tv_loss: 0.017423536628484726\n",
      "iteration 3254, dc_loss: 0.06558950990438461, tv_loss: 0.017423411831259727\n",
      "iteration 3255, dc_loss: 0.06558946520090103, tv_loss: 0.01742328517138958\n",
      "iteration 3256, dc_loss: 0.06558947265148163, tv_loss: 0.0174234788864851\n",
      "iteration 3257, dc_loss: 0.06558950245380402, tv_loss: 0.017423616722226143\n",
      "iteration 3258, dc_loss: 0.06558950990438461, tv_loss: 0.017423611134290695\n",
      "iteration 3259, dc_loss: 0.0655895248055458, tv_loss: 0.017423205077648163\n",
      "iteration 3260, dc_loss: 0.06558951735496521, tv_loss: 0.017423242330551147\n",
      "iteration 3261, dc_loss: 0.06558948755264282, tv_loss: 0.017423521727323532\n",
      "iteration 3262, dc_loss: 0.06558945775032043, tv_loss: 0.017423508688807487\n",
      "iteration 3263, dc_loss: 0.06558948755264282, tv_loss: 0.017423531040549278\n",
      "iteration 3264, dc_loss: 0.06558951735496521, tv_loss: 0.01742314174771309\n",
      "iteration 3265, dc_loss: 0.0655895322561264, tv_loss: 0.017423246055841446\n",
      "iteration 3266, dc_loss: 0.0655895322561264, tv_loss: 0.017423607409000397\n",
      "iteration 3267, dc_loss: 0.0655895248055458, tv_loss: 0.017423510551452637\n",
      "iteration 3268, dc_loss: 0.06558950245380402, tv_loss: 0.01742330752313137\n",
      "iteration 3269, dc_loss: 0.06558946520090103, tv_loss: 0.017423540353775024\n",
      "iteration 3270, dc_loss: 0.06558946520090103, tv_loss: 0.017423732206225395\n",
      "iteration 3271, dc_loss: 0.06558947265148163, tv_loss: 0.01742359809577465\n",
      "iteration 3272, dc_loss: 0.06558951735496521, tv_loss: 0.017423702403903008\n",
      "iteration 3273, dc_loss: 0.06558951735496521, tv_loss: 0.017423685640096664\n",
      "iteration 3274, dc_loss: 0.06558950990438461, tv_loss: 0.01742345653474331\n",
      "iteration 3275, dc_loss: 0.06558950245380402, tv_loss: 0.017423588782548904\n",
      "iteration 3276, dc_loss: 0.06558947265148163, tv_loss: 0.0174238383769989\n",
      "iteration 3277, dc_loss: 0.06558948010206223, tv_loss: 0.01742376945912838\n",
      "iteration 3278, dc_loss: 0.06558948755264282, tv_loss: 0.01742374897003174\n",
      "iteration 3279, dc_loss: 0.06558948755264282, tv_loss: 0.0174235999584198\n",
      "iteration 3280, dc_loss: 0.06558948010206223, tv_loss: 0.01742357201874256\n",
      "iteration 3281, dc_loss: 0.06558950245380402, tv_loss: 0.01742357388138771\n",
      "iteration 3282, dc_loss: 0.0655895248055458, tv_loss: 0.01742377318441868\n",
      "iteration 3283, dc_loss: 0.0655895248055458, tv_loss: 0.017423558980226517\n",
      "iteration 3284, dc_loss: 0.06558950990438461, tv_loss: 0.01742340251803398\n",
      "iteration 3285, dc_loss: 0.06558948010206223, tv_loss: 0.017423661425709724\n",
      "iteration 3286, dc_loss: 0.06558946520090103, tv_loss: 0.01742393523454666\n",
      "iteration 3287, dc_loss: 0.06558948010206223, tv_loss: 0.01742333173751831\n",
      "iteration 3288, dc_loss: 0.06558949500322342, tv_loss: 0.01742304116487503\n",
      "iteration 3289, dc_loss: 0.06558950245380402, tv_loss: 0.017423558980226517\n",
      "iteration 3290, dc_loss: 0.06558948755264282, tv_loss: 0.01742394082248211\n",
      "iteration 3291, dc_loss: 0.06558950990438461, tv_loss: 0.017423035576939583\n",
      "iteration 3292, dc_loss: 0.06558951735496521, tv_loss: 0.017422767356038094\n",
      "iteration 3293, dc_loss: 0.06558951735496521, tv_loss: 0.017423203215003014\n",
      "iteration 3294, dc_loss: 0.06558950990438461, tv_loss: 0.017423486337065697\n",
      "iteration 3295, dc_loss: 0.06558948755264282, tv_loss: 0.017423244193196297\n",
      "iteration 3296, dc_loss: 0.06558947265148163, tv_loss: 0.01742328517138958\n",
      "iteration 3297, dc_loss: 0.06558945775032043, tv_loss: 0.0174234751611948\n",
      "iteration 3298, dc_loss: 0.06558948755264282, tv_loss: 0.017423387616872787\n",
      "iteration 3299, dc_loss: 0.0655895248055458, tv_loss: 0.01742357388138771\n",
      "iteration 3300, dc_loss: 0.0655895248055458, tv_loss: 0.017423633486032486\n",
      "iteration 3301, dc_loss: 0.0655895322561264, tv_loss: 0.017423346638679504\n",
      "iteration 3302, dc_loss: 0.06558950990438461, tv_loss: 0.017423801124095917\n",
      "iteration 3303, dc_loss: 0.06558948010206223, tv_loss: 0.017423847690224648\n",
      "iteration 3304, dc_loss: 0.06558945029973984, tv_loss: 0.01742362417280674\n",
      "iteration 3305, dc_loss: 0.06558946520090103, tv_loss: 0.017423734068870544\n",
      "iteration 3306, dc_loss: 0.06558950990438461, tv_loss: 0.017423735931515694\n",
      "iteration 3307, dc_loss: 0.0655895546078682, tv_loss: 0.017423640936613083\n",
      "iteration 3308, dc_loss: 0.0655895248055458, tv_loss: 0.017423763871192932\n",
      "iteration 3309, dc_loss: 0.06558948010206223, tv_loss: 0.017423715442419052\n",
      "iteration 3310, dc_loss: 0.06558944284915924, tv_loss: 0.01742343232035637\n",
      "iteration 3311, dc_loss: 0.06558942049741745, tv_loss: 0.017423518002033234\n",
      "iteration 3312, dc_loss: 0.06558947265148163, tv_loss: 0.017423683777451515\n",
      "iteration 3313, dc_loss: 0.0655895322561264, tv_loss: 0.017423391342163086\n",
      "iteration 3314, dc_loss: 0.0655895546078682, tv_loss: 0.017423158511519432\n",
      "iteration 3315, dc_loss: 0.0655895471572876, tv_loss: 0.017423385754227638\n",
      "iteration 3316, dc_loss: 0.0655895248055458, tv_loss: 0.01742369681596756\n",
      "iteration 3317, dc_loss: 0.06558951735496521, tv_loss: 0.01742393523454666\n",
      "iteration 3318, dc_loss: 0.06558948010206223, tv_loss: 0.01742359809577465\n",
      "iteration 3319, dc_loss: 0.06558947265148163, tv_loss: 0.01742330566048622\n",
      "iteration 3320, dc_loss: 0.06558948010206223, tv_loss: 0.01742357574403286\n",
      "iteration 3321, dc_loss: 0.06558948010206223, tv_loss: 0.01742377318441868\n",
      "iteration 3322, dc_loss: 0.06558950245380402, tv_loss: 0.017423488199710846\n",
      "iteration 3323, dc_loss: 0.0655895322561264, tv_loss: 0.01742328517138958\n",
      "iteration 3324, dc_loss: 0.0655895322561264, tv_loss: 0.01742355152964592\n",
      "iteration 3325, dc_loss: 0.06558950990438461, tv_loss: 0.017423640936613083\n",
      "iteration 3326, dc_loss: 0.06558950990438461, tv_loss: 0.01742357760667801\n",
      "iteration 3327, dc_loss: 0.06558949500322342, tv_loss: 0.017423497512936592\n",
      "iteration 3328, dc_loss: 0.06558948010206223, tv_loss: 0.017423510551452637\n",
      "iteration 3329, dc_loss: 0.06558948010206223, tv_loss: 0.01742343045771122\n",
      "iteration 3330, dc_loss: 0.06558949500322342, tv_loss: 0.01742340251803398\n",
      "iteration 3331, dc_loss: 0.06558951735496521, tv_loss: 0.017423853278160095\n",
      "iteration 3332, dc_loss: 0.06558950990438461, tv_loss: 0.017423730343580246\n",
      "iteration 3333, dc_loss: 0.06558950990438461, tv_loss: 0.01742340810596943\n",
      "iteration 3334, dc_loss: 0.06558949500322342, tv_loss: 0.0174234788864851\n",
      "iteration 3335, dc_loss: 0.06558948755264282, tv_loss: 0.017423782497644424\n",
      "iteration 3336, dc_loss: 0.06558948755264282, tv_loss: 0.017423657700419426\n",
      "iteration 3337, dc_loss: 0.06558950245380402, tv_loss: 0.017423445358872414\n",
      "iteration 3338, dc_loss: 0.06558950990438461, tv_loss: 0.01742357760667801\n",
      "iteration 3339, dc_loss: 0.06558948010206223, tv_loss: 0.017423642799258232\n",
      "iteration 3340, dc_loss: 0.06558948010206223, tv_loss: 0.017423618584871292\n",
      "iteration 3341, dc_loss: 0.06558948755264282, tv_loss: 0.017423300072550774\n",
      "iteration 3342, dc_loss: 0.06558950990438461, tv_loss: 0.017423521727323532\n",
      "iteration 3343, dc_loss: 0.0655895248055458, tv_loss: 0.01742376573383808\n",
      "iteration 3344, dc_loss: 0.06558951735496521, tv_loss: 0.017423616722226143\n",
      "iteration 3345, dc_loss: 0.06558948755264282, tv_loss: 0.017423734068870544\n",
      "iteration 3346, dc_loss: 0.06558946520090103, tv_loss: 0.017423782497644424\n",
      "iteration 3347, dc_loss: 0.06558948755264282, tv_loss: 0.017423979938030243\n",
      "iteration 3348, dc_loss: 0.06558950990438461, tv_loss: 0.017423734068870544\n",
      "iteration 3349, dc_loss: 0.06558950990438461, tv_loss: 0.01742398552596569\n",
      "iteration 3350, dc_loss: 0.06558948010206223, tv_loss: 0.0174240805208683\n",
      "iteration 3351, dc_loss: 0.06558945775032043, tv_loss: 0.017424263060092926\n",
      "iteration 3352, dc_loss: 0.06558947265148163, tv_loss: 0.017423858866095543\n",
      "iteration 3353, dc_loss: 0.06558949500322342, tv_loss: 0.017423518002033234\n",
      "iteration 3354, dc_loss: 0.0655895248055458, tv_loss: 0.01742379181087017\n",
      "iteration 3355, dc_loss: 0.0655895471572876, tv_loss: 0.01742393895983696\n",
      "iteration 3356, dc_loss: 0.06558950990438461, tv_loss: 0.017423799261450768\n",
      "iteration 3357, dc_loss: 0.06558948010206223, tv_loss: 0.017423586919903755\n",
      "iteration 3358, dc_loss: 0.06558945029973984, tv_loss: 0.01742396131157875\n",
      "iteration 3359, dc_loss: 0.06558944284915924, tv_loss: 0.01742399111390114\n",
      "iteration 3360, dc_loss: 0.06558945775032043, tv_loss: 0.01742374897003174\n",
      "iteration 3361, dc_loss: 0.06558948755264282, tv_loss: 0.017423611134290695\n",
      "iteration 3362, dc_loss: 0.0655895546078682, tv_loss: 0.01742364652454853\n",
      "iteration 3363, dc_loss: 0.06558956950902939, tv_loss: 0.017423758283257484\n",
      "iteration 3364, dc_loss: 0.0655895322561264, tv_loss: 0.017423395067453384\n",
      "iteration 3365, dc_loss: 0.06558950245380402, tv_loss: 0.017423588782548904\n",
      "iteration 3366, dc_loss: 0.06558947265148163, tv_loss: 0.017423825338482857\n",
      "iteration 3367, dc_loss: 0.06558945775032043, tv_loss: 0.0174234751611948\n",
      "iteration 3368, dc_loss: 0.06558948010206223, tv_loss: 0.017423350363969803\n",
      "iteration 3369, dc_loss: 0.06558950245380402, tv_loss: 0.017423659563064575\n",
      "iteration 3370, dc_loss: 0.06558951735496521, tv_loss: 0.017423802986741066\n",
      "iteration 3371, dc_loss: 0.065589539706707, tv_loss: 0.01742332987487316\n",
      "iteration 3372, dc_loss: 0.0655895322561264, tv_loss: 0.017423441633582115\n",
      "iteration 3373, dc_loss: 0.0655895248055458, tv_loss: 0.017423808574676514\n",
      "iteration 3374, dc_loss: 0.06558948010206223, tv_loss: 0.01742369681596756\n",
      "iteration 3375, dc_loss: 0.06558945029973984, tv_loss: 0.017423629760742188\n",
      "iteration 3376, dc_loss: 0.06558945775032043, tv_loss: 0.017423998564481735\n",
      "iteration 3377, dc_loss: 0.06558948755264282, tv_loss: 0.017423514276742935\n",
      "iteration 3378, dc_loss: 0.06558950990438461, tv_loss: 0.01742335595190525\n",
      "iteration 3379, dc_loss: 0.0655895322561264, tv_loss: 0.017423516139388084\n",
      "iteration 3380, dc_loss: 0.06558950990438461, tv_loss: 0.017423518002033234\n",
      "iteration 3381, dc_loss: 0.06558946520090103, tv_loss: 0.01742374710738659\n",
      "iteration 3382, dc_loss: 0.06558948010206223, tv_loss: 0.01742389239370823\n",
      "iteration 3383, dc_loss: 0.06558951735496521, tv_loss: 0.0174237173050642\n",
      "iteration 3384, dc_loss: 0.06558951735496521, tv_loss: 0.017423270270228386\n",
      "iteration 3385, dc_loss: 0.06558950990438461, tv_loss: 0.01742316596210003\n",
      "iteration 3386, dc_loss: 0.06558950245380402, tv_loss: 0.01742352917790413\n",
      "iteration 3387, dc_loss: 0.06558950990438461, tv_loss: 0.017423495650291443\n",
      "iteration 3388, dc_loss: 0.06558946520090103, tv_loss: 0.01742323860526085\n",
      "iteration 3389, dc_loss: 0.06558947265148163, tv_loss: 0.017423303797841072\n",
      "iteration 3390, dc_loss: 0.06558948755264282, tv_loss: 0.017423609271645546\n",
      "iteration 3391, dc_loss: 0.06558950990438461, tv_loss: 0.017423883080482483\n",
      "iteration 3392, dc_loss: 0.0655895322561264, tv_loss: 0.017423324286937714\n",
      "iteration 3393, dc_loss: 0.0655895248055458, tv_loss: 0.01742343232035637\n",
      "iteration 3394, dc_loss: 0.06558950245380402, tv_loss: 0.017423762008547783\n",
      "iteration 3395, dc_loss: 0.06558948010206223, tv_loss: 0.017423614859580994\n",
      "iteration 3396, dc_loss: 0.06558948010206223, tv_loss: 0.017423270270228386\n",
      "iteration 3397, dc_loss: 0.06558950245380402, tv_loss: 0.017423667013645172\n",
      "iteration 3398, dc_loss: 0.06558950990438461, tv_loss: 0.01742374710738659\n",
      "iteration 3399, dc_loss: 0.06558948755264282, tv_loss: 0.01742355152964592\n",
      "iteration 3400, dc_loss: 0.06558947265148163, tv_loss: 0.01742352358996868\n",
      "iteration 3401, dc_loss: 0.06558947265148163, tv_loss: 0.017423611134290695\n",
      "iteration 3402, dc_loss: 0.06558948755264282, tv_loss: 0.017423706129193306\n",
      "iteration 3403, dc_loss: 0.0655895248055458, tv_loss: 0.01742365024983883\n",
      "iteration 3404, dc_loss: 0.06558951735496521, tv_loss: 0.01742357388138771\n",
      "iteration 3405, dc_loss: 0.06558948010206223, tv_loss: 0.01742352358996868\n",
      "iteration 3406, dc_loss: 0.06558946520090103, tv_loss: 0.01742369495332241\n",
      "iteration 3407, dc_loss: 0.06558947265148163, tv_loss: 0.01742391102015972\n",
      "iteration 3408, dc_loss: 0.06558948755264282, tv_loss: 0.017423829063773155\n",
      "iteration 3409, dc_loss: 0.06558949500322342, tv_loss: 0.017423901706933975\n",
      "iteration 3410, dc_loss: 0.06558950990438461, tv_loss: 0.01742364466190338\n",
      "iteration 3411, dc_loss: 0.0655895248055458, tv_loss: 0.01742367073893547\n",
      "iteration 3412, dc_loss: 0.06558950990438461, tv_loss: 0.017423851415514946\n",
      "iteration 3413, dc_loss: 0.06558950990438461, tv_loss: 0.01742388866841793\n",
      "iteration 3414, dc_loss: 0.06558950990438461, tv_loss: 0.017423704266548157\n",
      "iteration 3415, dc_loss: 0.06558948755264282, tv_loss: 0.017423361539840698\n",
      "iteration 3416, dc_loss: 0.06558948010206223, tv_loss: 0.017423763871192932\n",
      "iteration 3417, dc_loss: 0.06558948010206223, tv_loss: 0.01742389239370823\n",
      "iteration 3418, dc_loss: 0.06558949500322342, tv_loss: 0.017423968762159348\n",
      "iteration 3419, dc_loss: 0.06558950245380402, tv_loss: 0.01742374338209629\n",
      "iteration 3420, dc_loss: 0.06558949500322342, tv_loss: 0.01742374897003174\n",
      "iteration 3421, dc_loss: 0.06558950990438461, tv_loss: 0.017423780634999275\n",
      "iteration 3422, dc_loss: 0.06558951735496521, tv_loss: 0.017423678189516068\n",
      "iteration 3423, dc_loss: 0.06558948010206223, tv_loss: 0.017423706129193306\n",
      "iteration 3424, dc_loss: 0.06558947265148163, tv_loss: 0.017423786222934723\n",
      "iteration 3425, dc_loss: 0.06558948755264282, tv_loss: 0.017423825338482857\n",
      "iteration 3426, dc_loss: 0.06558948755264282, tv_loss: 0.017423685640096664\n",
      "iteration 3427, dc_loss: 0.0655895248055458, tv_loss: 0.01742342673242092\n",
      "iteration 3428, dc_loss: 0.06558950990438461, tv_loss: 0.017423395067453384\n",
      "iteration 3429, dc_loss: 0.06558948755264282, tv_loss: 0.017423491925001144\n",
      "iteration 3430, dc_loss: 0.06558944284915924, tv_loss: 0.01742362417280674\n",
      "iteration 3431, dc_loss: 0.06558946520090103, tv_loss: 0.017423713579773903\n",
      "iteration 3432, dc_loss: 0.06558951735496521, tv_loss: 0.017423518002033234\n",
      "iteration 3433, dc_loss: 0.0655895322561264, tv_loss: 0.01742345467209816\n",
      "iteration 3434, dc_loss: 0.0655895248055458, tv_loss: 0.01742357388138771\n",
      "iteration 3435, dc_loss: 0.06558950990438461, tv_loss: 0.017423667013645172\n",
      "iteration 3436, dc_loss: 0.06558948010206223, tv_loss: 0.017423806712031364\n",
      "iteration 3437, dc_loss: 0.06558945775032043, tv_loss: 0.01742350496351719\n",
      "iteration 3438, dc_loss: 0.06558948010206223, tv_loss: 0.017423583194613457\n",
      "iteration 3439, dc_loss: 0.06558950245380402, tv_loss: 0.017423491925001144\n",
      "iteration 3440, dc_loss: 0.0655895248055458, tv_loss: 0.01742391474545002\n",
      "iteration 3441, dc_loss: 0.06558950990438461, tv_loss: 0.017423443496227264\n",
      "iteration 3442, dc_loss: 0.06558950990438461, tv_loss: 0.01742316037416458\n",
      "iteration 3443, dc_loss: 0.06558951735496521, tv_loss: 0.01742348074913025\n",
      "iteration 3444, dc_loss: 0.06558948755264282, tv_loss: 0.017423659563064575\n",
      "iteration 3445, dc_loss: 0.06558948010206223, tv_loss: 0.017423594370484352\n",
      "iteration 3446, dc_loss: 0.06558948755264282, tv_loss: 0.017423609271645546\n",
      "iteration 3447, dc_loss: 0.06558948755264282, tv_loss: 0.017423301935195923\n",
      "iteration 3448, dc_loss: 0.06558948755264282, tv_loss: 0.017423303797841072\n",
      "iteration 3449, dc_loss: 0.06558949500322342, tv_loss: 0.017423521727323532\n",
      "iteration 3450, dc_loss: 0.06558950990438461, tv_loss: 0.01742362417280674\n",
      "iteration 3451, dc_loss: 0.06558950245380402, tv_loss: 0.017423396930098534\n",
      "iteration 3452, dc_loss: 0.06558950245380402, tv_loss: 0.017423510551452637\n",
      "iteration 3453, dc_loss: 0.06558949500322342, tv_loss: 0.017423518002033234\n",
      "iteration 3454, dc_loss: 0.06558948010206223, tv_loss: 0.0174238383769989\n",
      "iteration 3455, dc_loss: 0.06558948010206223, tv_loss: 0.01742378994822502\n",
      "iteration 3456, dc_loss: 0.06558950245380402, tv_loss: 0.017423445358872414\n",
      "iteration 3457, dc_loss: 0.0655895322561264, tv_loss: 0.0174232330173254\n",
      "iteration 3458, dc_loss: 0.0655895248055458, tv_loss: 0.01742349937558174\n",
      "iteration 3459, dc_loss: 0.06558951735496521, tv_loss: 0.017423436045646667\n",
      "iteration 3460, dc_loss: 0.06558949500322342, tv_loss: 0.017423542216420174\n",
      "iteration 3461, dc_loss: 0.06558948010206223, tv_loss: 0.017423350363969803\n",
      "iteration 3462, dc_loss: 0.06558948010206223, tv_loss: 0.017423437908291817\n",
      "iteration 3463, dc_loss: 0.06558948010206223, tv_loss: 0.017423629760742188\n",
      "iteration 3464, dc_loss: 0.06558948010206223, tv_loss: 0.017423633486032486\n",
      "iteration 3465, dc_loss: 0.06558945775032043, tv_loss: 0.01742367260158062\n",
      "iteration 3466, dc_loss: 0.06558948010206223, tv_loss: 0.01742367260158062\n",
      "iteration 3467, dc_loss: 0.0655895248055458, tv_loss: 0.017423272132873535\n",
      "iteration 3468, dc_loss: 0.0655895322561264, tv_loss: 0.01742333173751831\n",
      "iteration 3469, dc_loss: 0.06558951735496521, tv_loss: 0.017423778772354126\n",
      "iteration 3470, dc_loss: 0.06558948010206223, tv_loss: 0.017423730343580246\n",
      "iteration 3471, dc_loss: 0.06558945775032043, tv_loss: 0.017423415556550026\n",
      "iteration 3472, dc_loss: 0.06558944284915924, tv_loss: 0.01742345280945301\n",
      "iteration 3473, dc_loss: 0.06558948755264282, tv_loss: 0.017423352226614952\n",
      "iteration 3474, dc_loss: 0.0655895322561264, tv_loss: 0.017423339188098907\n",
      "iteration 3475, dc_loss: 0.0655895471572876, tv_loss: 0.017423471435904503\n",
      "iteration 3476, dc_loss: 0.06558950990438461, tv_loss: 0.017423590645194054\n",
      "iteration 3477, dc_loss: 0.06558948010206223, tv_loss: 0.01742328330874443\n",
      "iteration 3478, dc_loss: 0.06558947265148163, tv_loss: 0.017423609271645546\n",
      "iteration 3479, dc_loss: 0.06558944284915924, tv_loss: 0.01742372289299965\n",
      "iteration 3480, dc_loss: 0.06558945775032043, tv_loss: 0.017423540353775024\n",
      "iteration 3481, dc_loss: 0.06558950245380402, tv_loss: 0.017423342913389206\n",
      "iteration 3482, dc_loss: 0.065589539706707, tv_loss: 0.01742321252822876\n",
      "iteration 3483, dc_loss: 0.065589539706707, tv_loss: 0.01742362603545189\n",
      "iteration 3484, dc_loss: 0.06558950245380402, tv_loss: 0.017423540353775024\n",
      "iteration 3485, dc_loss: 0.06558948010206223, tv_loss: 0.017423417419195175\n",
      "iteration 3486, dc_loss: 0.06558945029973984, tv_loss: 0.01742376945912838\n",
      "iteration 3487, dc_loss: 0.06558949500322342, tv_loss: 0.017423562705516815\n",
      "iteration 3488, dc_loss: 0.0655895322561264, tv_loss: 0.017423441633582115\n",
      "iteration 3489, dc_loss: 0.0655895248055458, tv_loss: 0.01742374897003174\n",
      "iteration 3490, dc_loss: 0.06558950990438461, tv_loss: 0.01742391847074032\n",
      "iteration 3491, dc_loss: 0.06558950990438461, tv_loss: 0.017423531040549278\n",
      "iteration 3492, dc_loss: 0.06558950245380402, tv_loss: 0.017423462122678757\n",
      "iteration 3493, dc_loss: 0.06558948010206223, tv_loss: 0.01742374151945114\n",
      "iteration 3494, dc_loss: 0.06558948755264282, tv_loss: 0.017423544079065323\n",
      "iteration 3495, dc_loss: 0.06558948010206223, tv_loss: 0.01742352731525898\n",
      "iteration 3496, dc_loss: 0.06558948755264282, tv_loss: 0.01742348074913025\n",
      "iteration 3497, dc_loss: 0.0655895248055458, tv_loss: 0.017423445358872414\n",
      "iteration 3498, dc_loss: 0.065589539706707, tv_loss: 0.017423292621970177\n",
      "iteration 3499, dc_loss: 0.065589539706707, tv_loss: 0.01742335967719555\n",
      "iteration 3500, dc_loss: 0.06558948755264282, tv_loss: 0.017423409968614578\n",
      "iteration 3501, dc_loss: 0.06558946520090103, tv_loss: 0.017423339188098907\n",
      "iteration 3502, dc_loss: 0.06558947265148163, tv_loss: 0.017423156648874283\n",
      "iteration 3503, dc_loss: 0.06558948755264282, tv_loss: 0.0174228735268116\n",
      "iteration 3504, dc_loss: 0.06558950245380402, tv_loss: 0.017423216253519058\n",
      "iteration 3505, dc_loss: 0.06558950990438461, tv_loss: 0.01742338202893734\n",
      "iteration 3506, dc_loss: 0.0655895248055458, tv_loss: 0.01742284931242466\n",
      "iteration 3507, dc_loss: 0.0655895248055458, tv_loss: 0.01742313615977764\n",
      "iteration 3508, dc_loss: 0.06558950245380402, tv_loss: 0.01742333173751831\n",
      "iteration 3509, dc_loss: 0.06558948755264282, tv_loss: 0.017423296347260475\n",
      "iteration 3510, dc_loss: 0.06558947265148163, tv_loss: 0.0174232330173254\n",
      "iteration 3511, dc_loss: 0.06558948010206223, tv_loss: 0.017423266544938087\n",
      "iteration 3512, dc_loss: 0.06558950990438461, tv_loss: 0.01742330938577652\n",
      "iteration 3513, dc_loss: 0.0655895322561264, tv_loss: 0.017422998324036598\n",
      "iteration 3514, dc_loss: 0.06558950990438461, tv_loss: 0.017423240467905998\n",
      "iteration 3515, dc_loss: 0.06558951735496521, tv_loss: 0.017423266544938087\n",
      "iteration 3516, dc_loss: 0.06558950990438461, tv_loss: 0.017423322424292564\n",
      "iteration 3517, dc_loss: 0.06558950245380402, tv_loss: 0.017423458397388458\n",
      "iteration 3518, dc_loss: 0.06558950245380402, tv_loss: 0.017423169687390327\n",
      "iteration 3519, dc_loss: 0.06558947265148163, tv_loss: 0.017423246055841446\n",
      "iteration 3520, dc_loss: 0.06558945775032043, tv_loss: 0.017423473298549652\n",
      "iteration 3521, dc_loss: 0.06558946520090103, tv_loss: 0.017422987148165703\n",
      "iteration 3522, dc_loss: 0.06558950245380402, tv_loss: 0.017423031851649284\n",
      "iteration 3523, dc_loss: 0.0655895471572876, tv_loss: 0.017423482611775398\n",
      "iteration 3524, dc_loss: 0.0655895248055458, tv_loss: 0.017423361539840698\n",
      "iteration 3525, dc_loss: 0.06558950245380402, tv_loss: 0.017423205077648163\n",
      "iteration 3526, dc_loss: 0.06558948010206223, tv_loss: 0.01742308773100376\n",
      "iteration 3527, dc_loss: 0.06558948755264282, tv_loss: 0.017423100769519806\n",
      "iteration 3528, dc_loss: 0.06558946520090103, tv_loss: 0.01742350123822689\n",
      "iteration 3529, dc_loss: 0.06558945775032043, tv_loss: 0.0174237247556448\n",
      "iteration 3530, dc_loss: 0.06558948010206223, tv_loss: 0.01742330566048622\n",
      "iteration 3531, dc_loss: 0.06558950245380402, tv_loss: 0.017423242330551147\n",
      "iteration 3532, dc_loss: 0.0655895248055458, tv_loss: 0.01742350123822689\n",
      "iteration 3533, dc_loss: 0.0655895471572876, tv_loss: 0.017423326149582863\n",
      "iteration 3534, dc_loss: 0.0655895248055458, tv_loss: 0.017423192039132118\n",
      "iteration 3535, dc_loss: 0.06558947265148163, tv_loss: 0.01742333546280861\n",
      "iteration 3536, dc_loss: 0.06558946520090103, tv_loss: 0.017423400655388832\n",
      "iteration 3537, dc_loss: 0.06558947265148163, tv_loss: 0.017423493787646294\n",
      "iteration 3538, dc_loss: 0.06558947265148163, tv_loss: 0.01742332987487316\n",
      "iteration 3539, dc_loss: 0.06558949500322342, tv_loss: 0.017422955483198166\n",
      "iteration 3540, dc_loss: 0.06558950990438461, tv_loss: 0.017423223704099655\n",
      "iteration 3541, dc_loss: 0.06558950990438461, tv_loss: 0.01742343232035637\n",
      "iteration 3542, dc_loss: 0.06558949500322342, tv_loss: 0.01742328517138958\n",
      "iteration 3543, dc_loss: 0.06558950245380402, tv_loss: 0.017423061653971672\n",
      "iteration 3544, dc_loss: 0.06558949500322342, tv_loss: 0.017423270270228386\n",
      "iteration 3545, dc_loss: 0.06558948755264282, tv_loss: 0.0174234788864851\n",
      "iteration 3546, dc_loss: 0.06558948755264282, tv_loss: 0.01742297224700451\n",
      "iteration 3547, dc_loss: 0.0655895248055458, tv_loss: 0.017422931268811226\n",
      "iteration 3548, dc_loss: 0.06558951735496521, tv_loss: 0.017423301935195923\n",
      "iteration 3549, dc_loss: 0.06558947265148163, tv_loss: 0.017423240467905998\n",
      "iteration 3550, dc_loss: 0.06558946520090103, tv_loss: 0.017422998324036598\n",
      "iteration 3551, dc_loss: 0.06558948010206223, tv_loss: 0.0174231119453907\n",
      "iteration 3552, dc_loss: 0.06558948755264282, tv_loss: 0.017423268407583237\n",
      "iteration 3553, dc_loss: 0.06558950245380402, tv_loss: 0.01742365024983883\n",
      "iteration 3554, dc_loss: 0.06558950245380402, tv_loss: 0.017423512414097786\n",
      "iteration 3555, dc_loss: 0.06558949500322342, tv_loss: 0.0174231119453907\n",
      "iteration 3556, dc_loss: 0.06558946520090103, tv_loss: 0.017423290759325027\n",
      "iteration 3557, dc_loss: 0.06558948755264282, tv_loss: 0.01742328517138958\n",
      "iteration 3558, dc_loss: 0.06558951735496521, tv_loss: 0.0174232367426157\n",
      "iteration 3559, dc_loss: 0.06558951735496521, tv_loss: 0.017423106357455254\n",
      "iteration 3560, dc_loss: 0.06558950245380402, tv_loss: 0.017423100769519806\n",
      "iteration 3561, dc_loss: 0.06558948010206223, tv_loss: 0.01742338389158249\n",
      "iteration 3562, dc_loss: 0.06558949500322342, tv_loss: 0.01742316223680973\n",
      "iteration 3563, dc_loss: 0.06558950245380402, tv_loss: 0.017423082143068314\n",
      "iteration 3564, dc_loss: 0.06558950990438461, tv_loss: 0.017423326149582863\n",
      "iteration 3565, dc_loss: 0.06558950245380402, tv_loss: 0.01742367073893547\n",
      "iteration 3566, dc_loss: 0.06558948755264282, tv_loss: 0.01742354780435562\n",
      "iteration 3567, dc_loss: 0.06558948755264282, tv_loss: 0.01742343045771122\n",
      "iteration 3568, dc_loss: 0.06558948755264282, tv_loss: 0.017423497512936592\n",
      "iteration 3569, dc_loss: 0.06558948755264282, tv_loss: 0.017423301935195923\n",
      "iteration 3570, dc_loss: 0.06558948010206223, tv_loss: 0.01742340810596943\n",
      "iteration 3571, dc_loss: 0.06558950990438461, tv_loss: 0.017423242330551147\n",
      "iteration 3572, dc_loss: 0.0655895248055458, tv_loss: 0.017423132434487343\n",
      "iteration 3573, dc_loss: 0.06558950245380402, tv_loss: 0.017423471435904503\n",
      "iteration 3574, dc_loss: 0.06558947265148163, tv_loss: 0.017423763871192932\n",
      "iteration 3575, dc_loss: 0.06558946520090103, tv_loss: 0.017423612996935844\n",
      "iteration 3576, dc_loss: 0.06558946520090103, tv_loss: 0.017423689365386963\n",
      "iteration 3577, dc_loss: 0.06558946520090103, tv_loss: 0.017423449084162712\n",
      "iteration 3578, dc_loss: 0.06558950990438461, tv_loss: 0.017423518002033234\n",
      "iteration 3579, dc_loss: 0.0655895248055458, tv_loss: 0.017423829063773155\n",
      "iteration 3580, dc_loss: 0.06558950245380402, tv_loss: 0.017423637211322784\n",
      "iteration 3581, dc_loss: 0.06558948010206223, tv_loss: 0.017423760145902634\n",
      "iteration 3582, dc_loss: 0.06558948755264282, tv_loss: 0.017423784360289574\n",
      "iteration 3583, dc_loss: 0.06558950245380402, tv_loss: 0.017423756420612335\n",
      "iteration 3584, dc_loss: 0.06558947265148163, tv_loss: 0.017423802986741066\n",
      "iteration 3585, dc_loss: 0.06558948010206223, tv_loss: 0.017423611134290695\n",
      "iteration 3586, dc_loss: 0.06558950245380402, tv_loss: 0.017423493787646294\n",
      "iteration 3587, dc_loss: 0.0655895471572876, tv_loss: 0.017423754557967186\n",
      "iteration 3588, dc_loss: 0.06558951735496521, tv_loss: 0.01742384023964405\n",
      "iteration 3589, dc_loss: 0.06558948755264282, tv_loss: 0.017423467710614204\n",
      "iteration 3590, dc_loss: 0.06558947265148163, tv_loss: 0.017423711717128754\n",
      "iteration 3591, dc_loss: 0.06558944284915924, tv_loss: 0.01742422580718994\n",
      "iteration 3592, dc_loss: 0.06558947265148163, tv_loss: 0.017423929646611214\n",
      "iteration 3593, dc_loss: 0.06558950990438461, tv_loss: 0.017423633486032486\n",
      "iteration 3594, dc_loss: 0.0655895248055458, tv_loss: 0.01742371916770935\n",
      "iteration 3595, dc_loss: 0.0655895471572876, tv_loss: 0.017423713579773903\n",
      "iteration 3596, dc_loss: 0.0655895248055458, tv_loss: 0.01742367260158062\n",
      "iteration 3597, dc_loss: 0.06558948010206223, tv_loss: 0.01742355152964592\n",
      "iteration 3598, dc_loss: 0.06558943539857864, tv_loss: 0.017423437908291817\n",
      "iteration 3599, dc_loss: 0.06558944284915924, tv_loss: 0.017423409968614578\n",
      "iteration 3600, dc_loss: 0.06558948010206223, tv_loss: 0.017423361539840698\n",
      "iteration 3601, dc_loss: 0.06558950990438461, tv_loss: 0.017423424869775772\n",
      "iteration 3602, dc_loss: 0.065589539706707, tv_loss: 0.017423583194613457\n",
      "iteration 3603, dc_loss: 0.0655895546078682, tv_loss: 0.017423484474420547\n",
      "iteration 3604, dc_loss: 0.06558950245380402, tv_loss: 0.01742337830364704\n",
      "iteration 3605, dc_loss: 0.06558945775032043, tv_loss: 0.0174235999584198\n",
      "iteration 3606, dc_loss: 0.06558945775032043, tv_loss: 0.017423560842871666\n",
      "iteration 3607, dc_loss: 0.06558948010206223, tv_loss: 0.017423702403903008\n",
      "iteration 3608, dc_loss: 0.06558950245380402, tv_loss: 0.01742369309067726\n",
      "iteration 3609, dc_loss: 0.06558948010206223, tv_loss: 0.017423605546355247\n",
      "iteration 3610, dc_loss: 0.06558948755264282, tv_loss: 0.017423586919903755\n",
      "iteration 3611, dc_loss: 0.06558950245380402, tv_loss: 0.017423780634999275\n",
      "iteration 3612, dc_loss: 0.06558948755264282, tv_loss: 0.017423471435904503\n",
      "iteration 3613, dc_loss: 0.06558948755264282, tv_loss: 0.0174235999584198\n",
      "iteration 3614, dc_loss: 0.06558950990438461, tv_loss: 0.017423555254936218\n",
      "iteration 3615, dc_loss: 0.06558948010206223, tv_loss: 0.017423802986741066\n",
      "iteration 3616, dc_loss: 0.06558947265148163, tv_loss: 0.017423659563064575\n",
      "iteration 3617, dc_loss: 0.06558950245380402, tv_loss: 0.01742365024983883\n",
      "iteration 3618, dc_loss: 0.06558951735496521, tv_loss: 0.017423687502741814\n",
      "iteration 3619, dc_loss: 0.06558950245380402, tv_loss: 0.017423616722226143\n",
      "iteration 3620, dc_loss: 0.06558949500322342, tv_loss: 0.017423612996935844\n",
      "iteration 3621, dc_loss: 0.06558950245380402, tv_loss: 0.017423801124095917\n",
      "iteration 3622, dc_loss: 0.06558950245380402, tv_loss: 0.017423726618289948\n",
      "iteration 3623, dc_loss: 0.06558950245380402, tv_loss: 0.017423583194613457\n",
      "iteration 3624, dc_loss: 0.06558948755264282, tv_loss: 0.01742333546280861\n",
      "iteration 3625, dc_loss: 0.06558947265148163, tv_loss: 0.01742342673242092\n",
      "iteration 3626, dc_loss: 0.06558946520090103, tv_loss: 0.017423538491129875\n",
      "iteration 3627, dc_loss: 0.06558950245380402, tv_loss: 0.01742357388138771\n",
      "iteration 3628, dc_loss: 0.06558951735496521, tv_loss: 0.01742302067577839\n",
      "iteration 3629, dc_loss: 0.06558951735496521, tv_loss: 0.017423026263713837\n",
      "iteration 3630, dc_loss: 0.0655895248055458, tv_loss: 0.017423199489712715\n",
      "iteration 3631, dc_loss: 0.06558951735496521, tv_loss: 0.017423352226614952\n",
      "iteration 3632, dc_loss: 0.06558950990438461, tv_loss: 0.017423195764422417\n",
      "iteration 3633, dc_loss: 0.06558947265148163, tv_loss: 0.017422882840037346\n",
      "iteration 3634, dc_loss: 0.06558947265148163, tv_loss: 0.01742321066558361\n",
      "iteration 3635, dc_loss: 0.06558950990438461, tv_loss: 0.017423348501324654\n",
      "iteration 3636, dc_loss: 0.0655895248055458, tv_loss: 0.017423104494810104\n",
      "iteration 3637, dc_loss: 0.0655895322561264, tv_loss: 0.01742297038435936\n",
      "iteration 3638, dc_loss: 0.06558950990438461, tv_loss: 0.01742311753332615\n",
      "iteration 3639, dc_loss: 0.06558947265148163, tv_loss: 0.017423372715711594\n",
      "iteration 3640, dc_loss: 0.06558944284915924, tv_loss: 0.0174232367426157\n",
      "iteration 3641, dc_loss: 0.06558946520090103, tv_loss: 0.01742297224700451\n",
      "iteration 3642, dc_loss: 0.06558951735496521, tv_loss: 0.017423246055841446\n",
      "iteration 3643, dc_loss: 0.0655895471572876, tv_loss: 0.017423368990421295\n",
      "iteration 3644, dc_loss: 0.0655895322561264, tv_loss: 0.0174229945987463\n",
      "iteration 3645, dc_loss: 0.06558949500322342, tv_loss: 0.017423413693904877\n",
      "iteration 3646, dc_loss: 0.06558946520090103, tv_loss: 0.01742338389158249\n",
      "iteration 3647, dc_loss: 0.06558945775032043, tv_loss: 0.017423415556550026\n",
      "iteration 3648, dc_loss: 0.06558948755264282, tv_loss: 0.017423467710614204\n",
      "iteration 3649, dc_loss: 0.06558950245380402, tv_loss: 0.01742318458855152\n",
      "iteration 3650, dc_loss: 0.06558950990438461, tv_loss: 0.017423434183001518\n",
      "iteration 3651, dc_loss: 0.06558950990438461, tv_loss: 0.01742333173751831\n",
      "iteration 3652, dc_loss: 0.06558950245380402, tv_loss: 0.017423182725906372\n",
      "iteration 3653, dc_loss: 0.06558950245380402, tv_loss: 0.017423219978809357\n",
      "iteration 3654, dc_loss: 0.06558947265148163, tv_loss: 0.017423536628484726\n",
      "iteration 3655, dc_loss: 0.06558947265148163, tv_loss: 0.017423706129193306\n",
      "iteration 3656, dc_loss: 0.06558948755264282, tv_loss: 0.01742325723171234\n",
      "iteration 3657, dc_loss: 0.06558950990438461, tv_loss: 0.01742325909435749\n",
      "iteration 3658, dc_loss: 0.0655895248055458, tv_loss: 0.017423447221517563\n",
      "iteration 3659, dc_loss: 0.0655895322561264, tv_loss: 0.017423564568161964\n",
      "iteration 3660, dc_loss: 0.06558950245380402, tv_loss: 0.017423642799258232\n",
      "iteration 3661, dc_loss: 0.06558948010206223, tv_loss: 0.017423341050744057\n",
      "iteration 3662, dc_loss: 0.06558948755264282, tv_loss: 0.01742330752313137\n",
      "iteration 3663, dc_loss: 0.06558948010206223, tv_loss: 0.017423413693904877\n",
      "iteration 3664, dc_loss: 0.06558946520090103, tv_loss: 0.0174235999584198\n",
      "iteration 3665, dc_loss: 0.06558948010206223, tv_loss: 0.017423400655388832\n",
      "iteration 3666, dc_loss: 0.06558950990438461, tv_loss: 0.017423173412680626\n",
      "iteration 3667, dc_loss: 0.065589539706707, tv_loss: 0.017423318699002266\n",
      "iteration 3668, dc_loss: 0.06558951735496521, tv_loss: 0.017423361539840698\n",
      "iteration 3669, dc_loss: 0.06558948755264282, tv_loss: 0.01742349937558174\n",
      "iteration 3670, dc_loss: 0.06558946520090103, tv_loss: 0.017423367127776146\n",
      "iteration 3671, dc_loss: 0.06558945775032043, tv_loss: 0.017423328012228012\n",
      "iteration 3672, dc_loss: 0.06558948755264282, tv_loss: 0.017423318699002266\n",
      "iteration 3673, dc_loss: 0.06558950990438461, tv_loss: 0.017423368990421295\n",
      "iteration 3674, dc_loss: 0.06558950990438461, tv_loss: 0.017423147335648537\n",
      "iteration 3675, dc_loss: 0.06558948755264282, tv_loss: 0.017423659563064575\n",
      "iteration 3676, dc_loss: 0.06558948755264282, tv_loss: 0.01742354966700077\n",
      "iteration 3677, dc_loss: 0.0655895248055458, tv_loss: 0.017423370853066444\n",
      "iteration 3678, dc_loss: 0.0655895248055458, tv_loss: 0.017423361539840698\n",
      "iteration 3679, dc_loss: 0.06558950245380402, tv_loss: 0.017423344776034355\n",
      "iteration 3680, dc_loss: 0.06558947265148163, tv_loss: 0.01742376759648323\n",
      "iteration 3681, dc_loss: 0.06558944284915924, tv_loss: 0.017423851415514946\n",
      "iteration 3682, dc_loss: 0.06558944284915924, tv_loss: 0.017423678189516068\n",
      "iteration 3683, dc_loss: 0.06558950245380402, tv_loss: 0.017423754557967186\n",
      "iteration 3684, dc_loss: 0.0655895322561264, tv_loss: 0.017424024641513824\n",
      "iteration 3685, dc_loss: 0.0655895248055458, tv_loss: 0.017423858866095543\n",
      "iteration 3686, dc_loss: 0.06558950990438461, tv_loss: 0.017423633486032486\n",
      "iteration 3687, dc_loss: 0.06558950990438461, tv_loss: 0.017423875629901886\n",
      "iteration 3688, dc_loss: 0.06558948755264282, tv_loss: 0.017423948273062706\n",
      "iteration 3689, dc_loss: 0.06558944284915924, tv_loss: 0.017423739656805992\n",
      "iteration 3690, dc_loss: 0.06558945775032043, tv_loss: 0.017424123361706734\n",
      "iteration 3691, dc_loss: 0.06558950990438461, tv_loss: 0.01742389239370823\n",
      "iteration 3692, dc_loss: 0.0655895248055458, tv_loss: 0.01742369309067726\n",
      "iteration 3693, dc_loss: 0.0655895248055458, tv_loss: 0.01742364652454853\n",
      "iteration 3694, dc_loss: 0.06558950990438461, tv_loss: 0.017423754557967186\n",
      "iteration 3695, dc_loss: 0.06558948755264282, tv_loss: 0.01742382161319256\n",
      "iteration 3696, dc_loss: 0.06558947265148163, tv_loss: 0.017423970624804497\n",
      "iteration 3697, dc_loss: 0.06558950245380402, tv_loss: 0.01742391474545002\n",
      "iteration 3698, dc_loss: 0.06558951735496521, tv_loss: 0.017423368990421295\n",
      "iteration 3699, dc_loss: 0.06558950245380402, tv_loss: 0.0174237247556448\n",
      "iteration 3700, dc_loss: 0.06558948010206223, tv_loss: 0.017423972487449646\n",
      "iteration 3701, dc_loss: 0.06558947265148163, tv_loss: 0.017423825338482857\n",
      "iteration 3702, dc_loss: 0.06558950245380402, tv_loss: 0.017423607409000397\n",
      "iteration 3703, dc_loss: 0.06558950245380402, tv_loss: 0.017423246055841446\n",
      "iteration 3704, dc_loss: 0.06558951735496521, tv_loss: 0.01742335967719555\n",
      "iteration 3705, dc_loss: 0.06558950990438461, tv_loss: 0.017423713579773903\n",
      "iteration 3706, dc_loss: 0.06558950245380402, tv_loss: 0.017423612996935844\n",
      "iteration 3707, dc_loss: 0.06558951735496521, tv_loss: 0.01742309145629406\n",
      "iteration 3708, dc_loss: 0.06558950245380402, tv_loss: 0.017423469573259354\n",
      "iteration 3709, dc_loss: 0.06558948755264282, tv_loss: 0.01742338389158249\n",
      "iteration 3710, dc_loss: 0.06558945775032043, tv_loss: 0.01742323860526085\n",
      "iteration 3711, dc_loss: 0.06558945775032043, tv_loss: 0.017423272132873535\n",
      "iteration 3712, dc_loss: 0.06558948755264282, tv_loss: 0.017423292621970177\n",
      "iteration 3713, dc_loss: 0.0655895248055458, tv_loss: 0.017423341050744057\n",
      "iteration 3714, dc_loss: 0.0655895322561264, tv_loss: 0.017423229292035103\n",
      "iteration 3715, dc_loss: 0.0655895471572876, tv_loss: 0.017423279583454132\n",
      "iteration 3716, dc_loss: 0.06558951735496521, tv_loss: 0.017423612996935844\n",
      "iteration 3717, dc_loss: 0.06558947265148163, tv_loss: 0.01742345094680786\n",
      "iteration 3718, dc_loss: 0.06558946520090103, tv_loss: 0.017423132434487343\n",
      "iteration 3719, dc_loss: 0.06558948010206223, tv_loss: 0.01742345467209816\n",
      "iteration 3720, dc_loss: 0.06558948755264282, tv_loss: 0.017423691228032112\n",
      "iteration 3721, dc_loss: 0.06558950245380402, tv_loss: 0.017423657700419426\n",
      "iteration 3722, dc_loss: 0.0655895248055458, tv_loss: 0.017423266544938087\n",
      "iteration 3723, dc_loss: 0.0655895322561264, tv_loss: 0.017423173412680626\n",
      "iteration 3724, dc_loss: 0.06558950990438461, tv_loss: 0.017423521727323532\n",
      "iteration 3725, dc_loss: 0.06558948755264282, tv_loss: 0.017423467710614204\n",
      "iteration 3726, dc_loss: 0.06558948755264282, tv_loss: 0.017423352226614952\n",
      "iteration 3727, dc_loss: 0.06558948755264282, tv_loss: 0.017423409968614578\n",
      "iteration 3728, dc_loss: 0.06558947265148163, tv_loss: 0.017423462122678757\n",
      "iteration 3729, dc_loss: 0.06558946520090103, tv_loss: 0.017423294484615326\n",
      "iteration 3730, dc_loss: 0.06558947265148163, tv_loss: 0.017423242330551147\n",
      "iteration 3731, dc_loss: 0.06558948755264282, tv_loss: 0.01742333546280861\n",
      "iteration 3732, dc_loss: 0.06558950990438461, tv_loss: 0.017423264682292938\n",
      "iteration 3733, dc_loss: 0.0655895322561264, tv_loss: 0.017423348501324654\n",
      "iteration 3734, dc_loss: 0.06558951735496521, tv_loss: 0.017423586919903755\n",
      "iteration 3735, dc_loss: 0.06558948010206223, tv_loss: 0.01742328330874443\n",
      "iteration 3736, dc_loss: 0.06558946520090103, tv_loss: 0.017423182725906372\n",
      "iteration 3737, dc_loss: 0.06558948010206223, tv_loss: 0.017423201352357864\n",
      "iteration 3738, dc_loss: 0.06558948755264282, tv_loss: 0.017423229292035103\n",
      "iteration 3739, dc_loss: 0.06558951735496521, tv_loss: 0.01742347702383995\n",
      "iteration 3740, dc_loss: 0.0655895322561264, tv_loss: 0.017423242330551147\n",
      "iteration 3741, dc_loss: 0.06558950245380402, tv_loss: 0.017423147335648537\n",
      "iteration 3742, dc_loss: 0.06558948755264282, tv_loss: 0.017423449084162712\n",
      "iteration 3743, dc_loss: 0.06558948010206223, tv_loss: 0.017423588782548904\n",
      "iteration 3744, dc_loss: 0.06558948010206223, tv_loss: 0.017423434183001518\n",
      "iteration 3745, dc_loss: 0.06558944284915924, tv_loss: 0.017423273995518684\n",
      "iteration 3746, dc_loss: 0.06558946520090103, tv_loss: 0.017423368990421295\n",
      "iteration 3747, dc_loss: 0.0655895248055458, tv_loss: 0.0174234751611948\n",
      "iteration 3748, dc_loss: 0.0655895322561264, tv_loss: 0.017423506826162338\n",
      "iteration 3749, dc_loss: 0.065589539706707, tv_loss: 0.01742352917790413\n",
      "iteration 3750, dc_loss: 0.06558950990438461, tv_loss: 0.0174236036837101\n",
      "iteration 3751, dc_loss: 0.06558948010206223, tv_loss: 0.01742365211248398\n",
      "iteration 3752, dc_loss: 0.06558945775032043, tv_loss: 0.017423825338482857\n",
      "iteration 3753, dc_loss: 0.06558948010206223, tv_loss: 0.017423637211322784\n",
      "iteration 3754, dc_loss: 0.06558950245380402, tv_loss: 0.017423493787646294\n",
      "iteration 3755, dc_loss: 0.06558951735496521, tv_loss: 0.017423352226614952\n",
      "iteration 3756, dc_loss: 0.06558950990438461, tv_loss: 0.01742343045771122\n",
      "iteration 3757, dc_loss: 0.06558950245380402, tv_loss: 0.017423778772354126\n",
      "iteration 3758, dc_loss: 0.06558948010206223, tv_loss: 0.017423661425709724\n",
      "iteration 3759, dc_loss: 0.06558943539857864, tv_loss: 0.017423244193196297\n",
      "iteration 3760, dc_loss: 0.06558944284915924, tv_loss: 0.017423253506422043\n",
      "iteration 3761, dc_loss: 0.06558950990438461, tv_loss: 0.017423558980226517\n",
      "iteration 3762, dc_loss: 0.0655895471572876, tv_loss: 0.017423531040549278\n",
      "iteration 3763, dc_loss: 0.06558956205844879, tv_loss: 0.01742304302752018\n",
      "iteration 3764, dc_loss: 0.06558951735496521, tv_loss: 0.01742311380803585\n",
      "iteration 3765, dc_loss: 0.06558944284915924, tv_loss: 0.017423385754227638\n",
      "iteration 3766, dc_loss: 0.06558940559625626, tv_loss: 0.017423519864678383\n",
      "iteration 3767, dc_loss: 0.06558943539857864, tv_loss: 0.017423368990421295\n",
      "iteration 3768, dc_loss: 0.06558949500322342, tv_loss: 0.01742299646139145\n",
      "iteration 3769, dc_loss: 0.065589539706707, tv_loss: 0.017423611134290695\n",
      "iteration 3770, dc_loss: 0.0655895471572876, tv_loss: 0.017423521727323532\n",
      "iteration 3771, dc_loss: 0.065589539706707, tv_loss: 0.017423398792743683\n",
      "iteration 3772, dc_loss: 0.06558950990438461, tv_loss: 0.01742335967719555\n",
      "iteration 3773, dc_loss: 0.06558948755264282, tv_loss: 0.017423367127776146\n",
      "iteration 3774, dc_loss: 0.06558947265148163, tv_loss: 0.01742345094680786\n",
      "iteration 3775, dc_loss: 0.06558948010206223, tv_loss: 0.01742367260158062\n",
      "iteration 3776, dc_loss: 0.06558950245380402, tv_loss: 0.017423830926418304\n",
      "iteration 3777, dc_loss: 0.06558950990438461, tv_loss: 0.01742371916770935\n",
      "iteration 3778, dc_loss: 0.06558950990438461, tv_loss: 0.017423538491129875\n",
      "iteration 3779, dc_loss: 0.06558950990438461, tv_loss: 0.017423445358872414\n",
      "iteration 3780, dc_loss: 0.06558948010206223, tv_loss: 0.017423374578356743\n",
      "iteration 3781, dc_loss: 0.06558948010206223, tv_loss: 0.017423441633582115\n",
      "iteration 3782, dc_loss: 0.06558948755264282, tv_loss: 0.01742340624332428\n",
      "iteration 3783, dc_loss: 0.06558948755264282, tv_loss: 0.017423253506422043\n",
      "iteration 3784, dc_loss: 0.06558948755264282, tv_loss: 0.017423124983906746\n",
      "iteration 3785, dc_loss: 0.06558949500322342, tv_loss: 0.017423275858163834\n",
      "iteration 3786, dc_loss: 0.06558950990438461, tv_loss: 0.017423393204808235\n",
      "iteration 3787, dc_loss: 0.0655895322561264, tv_loss: 0.017423244193196297\n",
      "iteration 3788, dc_loss: 0.06558951735496521, tv_loss: 0.017423344776034355\n",
      "iteration 3789, dc_loss: 0.06558948010206223, tv_loss: 0.017423473298549652\n",
      "iteration 3790, dc_loss: 0.06558943539857864, tv_loss: 0.01742350310087204\n",
      "iteration 3791, dc_loss: 0.06558944284915924, tv_loss: 0.017423246055841446\n",
      "iteration 3792, dc_loss: 0.06558948755264282, tv_loss: 0.017423363402485847\n",
      "iteration 3793, dc_loss: 0.06558950990438461, tv_loss: 0.017423799261450768\n",
      "iteration 3794, dc_loss: 0.06558950990438461, tv_loss: 0.017423350363969803\n",
      "iteration 3795, dc_loss: 0.06558951735496521, tv_loss: 0.017423339188098907\n",
      "iteration 3796, dc_loss: 0.06558950990438461, tv_loss: 0.017423713579773903\n",
      "iteration 3797, dc_loss: 0.06558950990438461, tv_loss: 0.01742337830364704\n",
      "iteration 3798, dc_loss: 0.06558949500322342, tv_loss: 0.017423052340745926\n",
      "iteration 3799, dc_loss: 0.06558947265148163, tv_loss: 0.017423003911972046\n",
      "iteration 3800, dc_loss: 0.06558946520090103, tv_loss: 0.017423512414097786\n",
      "iteration 3801, dc_loss: 0.06558945029973984, tv_loss: 0.01742364466190338\n",
      "iteration 3802, dc_loss: 0.06558947265148163, tv_loss: 0.017423031851649284\n",
      "iteration 3803, dc_loss: 0.0655895322561264, tv_loss: 0.01742294616997242\n",
      "iteration 3804, dc_loss: 0.0655895471572876, tv_loss: 0.01742306724190712\n",
      "iteration 3805, dc_loss: 0.0655895322561264, tv_loss: 0.01742328517138958\n",
      "iteration 3806, dc_loss: 0.06558950245380402, tv_loss: 0.017423085868358612\n",
      "iteration 3807, dc_loss: 0.06558947265148163, tv_loss: 0.01742321066558361\n",
      "iteration 3808, dc_loss: 0.06558942794799805, tv_loss: 0.017423482611775398\n",
      "iteration 3809, dc_loss: 0.06558945775032043, tv_loss: 0.017423493787646294\n",
      "iteration 3810, dc_loss: 0.06558950245380402, tv_loss: 0.017423482611775398\n",
      "iteration 3811, dc_loss: 0.0655895471572876, tv_loss: 0.01742320880293846\n",
      "iteration 3812, dc_loss: 0.0655895471572876, tv_loss: 0.017423050478100777\n",
      "iteration 3813, dc_loss: 0.06558950245380402, tv_loss: 0.01742345094680786\n",
      "iteration 3814, dc_loss: 0.06558946520090103, tv_loss: 0.017423458397388458\n",
      "iteration 3815, dc_loss: 0.06558942794799805, tv_loss: 0.017423195764422417\n",
      "iteration 3816, dc_loss: 0.06558945775032043, tv_loss: 0.017423288896679878\n",
      "iteration 3817, dc_loss: 0.06558950990438461, tv_loss: 0.017423078417778015\n",
      "iteration 3818, dc_loss: 0.0655895471572876, tv_loss: 0.017423169687390327\n",
      "iteration 3819, dc_loss: 0.0655895471572876, tv_loss: 0.01742345094680786\n",
      "iteration 3820, dc_loss: 0.06558950245380402, tv_loss: 0.017423050478100777\n",
      "iteration 3821, dc_loss: 0.06558947265148163, tv_loss: 0.017423154786229134\n",
      "iteration 3822, dc_loss: 0.06558945775032043, tv_loss: 0.01742357388138771\n",
      "iteration 3823, dc_loss: 0.06558948755264282, tv_loss: 0.01742337830364704\n",
      "iteration 3824, dc_loss: 0.06558950245380402, tv_loss: 0.017423057928681374\n",
      "iteration 3825, dc_loss: 0.06558948755264282, tv_loss: 0.017423398792743683\n",
      "iteration 3826, dc_loss: 0.06558948010206223, tv_loss: 0.017423288896679878\n",
      "iteration 3827, dc_loss: 0.06558951735496521, tv_loss: 0.017423342913389206\n",
      "iteration 3828, dc_loss: 0.06558950990438461, tv_loss: 0.01742318645119667\n",
      "iteration 3829, dc_loss: 0.06558948010206223, tv_loss: 0.017423521727323532\n",
      "iteration 3830, dc_loss: 0.06558948755264282, tv_loss: 0.01742379181087017\n",
      "iteration 3831, dc_loss: 0.06558948755264282, tv_loss: 0.017423545941710472\n",
      "iteration 3832, dc_loss: 0.06558950990438461, tv_loss: 0.017423247918486595\n",
      "iteration 3833, dc_loss: 0.06558950245380402, tv_loss: 0.017423518002033234\n",
      "iteration 3834, dc_loss: 0.06558948755264282, tv_loss: 0.017423581331968307\n",
      "iteration 3835, dc_loss: 0.06558949500322342, tv_loss: 0.0174232367426157\n",
      "iteration 3836, dc_loss: 0.06558947265148163, tv_loss: 0.017423316836357117\n",
      "iteration 3837, dc_loss: 0.06558948755264282, tv_loss: 0.01742350123822689\n",
      "iteration 3838, dc_loss: 0.06558950990438461, tv_loss: 0.01742316037416458\n",
      "iteration 3839, dc_loss: 0.06558951735496521, tv_loss: 0.017423046752810478\n",
      "iteration 3840, dc_loss: 0.06558950245380402, tv_loss: 0.0174232330173254\n",
      "iteration 3841, dc_loss: 0.06558949500322342, tv_loss: 0.017423376441001892\n",
      "iteration 3842, dc_loss: 0.06558950245380402, tv_loss: 0.01742338202893734\n",
      "iteration 3843, dc_loss: 0.06558948010206223, tv_loss: 0.01742331124842167\n",
      "iteration 3844, dc_loss: 0.06558948010206223, tv_loss: 0.0174233578145504\n",
      "iteration 3845, dc_loss: 0.06558948755264282, tv_loss: 0.017423275858163834\n",
      "iteration 3846, dc_loss: 0.06558950990438461, tv_loss: 0.017423057928681374\n",
      "iteration 3847, dc_loss: 0.0655895248055458, tv_loss: 0.017423050478100777\n",
      "iteration 3848, dc_loss: 0.0655895322561264, tv_loss: 0.017423033714294434\n",
      "iteration 3849, dc_loss: 0.06558948755264282, tv_loss: 0.017423128709197044\n",
      "iteration 3850, dc_loss: 0.06558945029973984, tv_loss: 0.017423350363969803\n",
      "iteration 3851, dc_loss: 0.06558948755264282, tv_loss: 0.01742343232035637\n",
      "iteration 3852, dc_loss: 0.0655895248055458, tv_loss: 0.01742311753332615\n",
      "iteration 3853, dc_loss: 0.0655895322561264, tv_loss: 0.0174232330173254\n",
      "iteration 3854, dc_loss: 0.06558951735496521, tv_loss: 0.017423445358872414\n",
      "iteration 3855, dc_loss: 0.06558948010206223, tv_loss: 0.017423532903194427\n",
      "iteration 3856, dc_loss: 0.06558946520090103, tv_loss: 0.017423421144485474\n",
      "iteration 3857, dc_loss: 0.06558948010206223, tv_loss: 0.01742330752313137\n",
      "iteration 3858, dc_loss: 0.06558950990438461, tv_loss: 0.017423590645194054\n",
      "iteration 3859, dc_loss: 0.0655895322561264, tv_loss: 0.017423780634999275\n",
      "iteration 3860, dc_loss: 0.06558951735496521, tv_loss: 0.0174234788864851\n",
      "iteration 3861, dc_loss: 0.06558948010206223, tv_loss: 0.017423570156097412\n",
      "iteration 3862, dc_loss: 0.06558946520090103, tv_loss: 0.017423782497644424\n",
      "iteration 3863, dc_loss: 0.06558948010206223, tv_loss: 0.01742374338209629\n",
      "iteration 3864, dc_loss: 0.06558948010206223, tv_loss: 0.01742376573383808\n",
      "iteration 3865, dc_loss: 0.06558948755264282, tv_loss: 0.017423685640096664\n",
      "iteration 3866, dc_loss: 0.06558951735496521, tv_loss: 0.017423680052161217\n",
      "iteration 3867, dc_loss: 0.065589539706707, tv_loss: 0.017423776909708977\n",
      "iteration 3868, dc_loss: 0.06558950990438461, tv_loss: 0.01742367632687092\n",
      "iteration 3869, dc_loss: 0.06558948755264282, tv_loss: 0.017423605546355247\n",
      "iteration 3870, dc_loss: 0.06558948010206223, tv_loss: 0.017423637211322784\n",
      "iteration 3871, dc_loss: 0.06558946520090103, tv_loss: 0.01742372289299965\n",
      "iteration 3872, dc_loss: 0.06558948010206223, tv_loss: 0.017423423007130623\n",
      "iteration 3873, dc_loss: 0.06558950245380402, tv_loss: 0.01742328517138958\n",
      "iteration 3874, dc_loss: 0.06558950245380402, tv_loss: 0.0174234751611948\n",
      "iteration 3875, dc_loss: 0.0655895248055458, tv_loss: 0.017423607409000397\n",
      "iteration 3876, dc_loss: 0.06558950245380402, tv_loss: 0.01742340251803398\n",
      "iteration 3877, dc_loss: 0.06558950245380402, tv_loss: 0.017423326149582863\n",
      "iteration 3878, dc_loss: 0.06558948010206223, tv_loss: 0.017423659563064575\n",
      "iteration 3879, dc_loss: 0.06558946520090103, tv_loss: 0.01742369309067726\n",
      "iteration 3880, dc_loss: 0.06558947265148163, tv_loss: 0.017423253506422043\n",
      "iteration 3881, dc_loss: 0.06558948755264282, tv_loss: 0.017423367127776146\n",
      "iteration 3882, dc_loss: 0.0655895248055458, tv_loss: 0.01742367446422577\n",
      "iteration 3883, dc_loss: 0.0655895471572876, tv_loss: 0.017423516139388084\n",
      "iteration 3884, dc_loss: 0.06558950990438461, tv_loss: 0.01742325909435749\n",
      "iteration 3885, dc_loss: 0.06558944284915924, tv_loss: 0.017423849552869797\n",
      "iteration 3886, dc_loss: 0.06558943539857864, tv_loss: 0.01742367632687092\n",
      "iteration 3887, dc_loss: 0.06558947265148163, tv_loss: 0.017423611134290695\n",
      "iteration 3888, dc_loss: 0.06558950990438461, tv_loss: 0.017423734068870544\n",
      "iteration 3889, dc_loss: 0.06558951735496521, tv_loss: 0.017423607409000397\n",
      "iteration 3890, dc_loss: 0.06558950245380402, tv_loss: 0.017423449084162712\n",
      "iteration 3891, dc_loss: 0.06558950990438461, tv_loss: 0.017423313111066818\n",
      "iteration 3892, dc_loss: 0.06558948755264282, tv_loss: 0.017423493787646294\n",
      "iteration 3893, dc_loss: 0.06558948755264282, tv_loss: 0.017423657700419426\n",
      "iteration 3894, dc_loss: 0.06558948010206223, tv_loss: 0.0174237173050642\n",
      "iteration 3895, dc_loss: 0.06558947265148163, tv_loss: 0.017423491925001144\n",
      "iteration 3896, dc_loss: 0.06558948010206223, tv_loss: 0.017423367127776146\n",
      "iteration 3897, dc_loss: 0.06558948010206223, tv_loss: 0.017423657700419426\n",
      "iteration 3898, dc_loss: 0.06558948755264282, tv_loss: 0.01742381975054741\n",
      "iteration 3899, dc_loss: 0.06558950990438461, tv_loss: 0.017423633486032486\n",
      "iteration 3900, dc_loss: 0.06558951735496521, tv_loss: 0.017423352226614952\n",
      "iteration 3901, dc_loss: 0.06558950990438461, tv_loss: 0.017423449084162712\n",
      "iteration 3902, dc_loss: 0.06558950990438461, tv_loss: 0.01742374151945114\n",
      "iteration 3903, dc_loss: 0.06558947265148163, tv_loss: 0.017423421144485474\n",
      "iteration 3904, dc_loss: 0.06558945775032043, tv_loss: 0.017423415556550026\n",
      "iteration 3905, dc_loss: 0.06558947265148163, tv_loss: 0.017423467710614204\n",
      "iteration 3906, dc_loss: 0.06558948755264282, tv_loss: 0.017423611134290695\n",
      "iteration 3907, dc_loss: 0.0655895322561264, tv_loss: 0.017423488199710846\n",
      "iteration 3908, dc_loss: 0.0655895546078682, tv_loss: 0.0174232330173254\n",
      "iteration 3909, dc_loss: 0.0655895248055458, tv_loss: 0.01742338202893734\n",
      "iteration 3910, dc_loss: 0.06558948010206223, tv_loss: 0.017423516139388084\n",
      "iteration 3911, dc_loss: 0.06558947265148163, tv_loss: 0.0174233578145504\n",
      "iteration 3912, dc_loss: 0.06558948755264282, tv_loss: 0.017423033714294434\n",
      "iteration 3913, dc_loss: 0.06558948010206223, tv_loss: 0.017423246055841446\n",
      "iteration 3914, dc_loss: 0.06558948755264282, tv_loss: 0.017423275858163834\n",
      "iteration 3915, dc_loss: 0.06558950245380402, tv_loss: 0.017423011362552643\n",
      "iteration 3916, dc_loss: 0.06558949500322342, tv_loss: 0.017423102632164955\n",
      "iteration 3917, dc_loss: 0.06558950990438461, tv_loss: 0.017423199489712715\n",
      "iteration 3918, dc_loss: 0.0655895248055458, tv_loss: 0.017423320561647415\n",
      "iteration 3919, dc_loss: 0.06558950990438461, tv_loss: 0.01742306351661682\n",
      "iteration 3920, dc_loss: 0.06558947265148163, tv_loss: 0.017423046752810478\n",
      "iteration 3921, dc_loss: 0.06558945029973984, tv_loss: 0.01742321252822876\n",
      "iteration 3922, dc_loss: 0.06558948010206223, tv_loss: 0.017423272132873535\n",
      "iteration 3923, dc_loss: 0.06558951735496521, tv_loss: 0.017422933131456375\n",
      "iteration 3924, dc_loss: 0.06558950990438461, tv_loss: 0.017422975972294807\n",
      "iteration 3925, dc_loss: 0.06558950990438461, tv_loss: 0.017423199489712715\n",
      "iteration 3926, dc_loss: 0.06558950245380402, tv_loss: 0.017423084005713463\n",
      "iteration 3927, dc_loss: 0.06558946520090103, tv_loss: 0.01742309331893921\n",
      "iteration 3928, dc_loss: 0.06558946520090103, tv_loss: 0.01742318645119667\n",
      "iteration 3929, dc_loss: 0.06558948755264282, tv_loss: 0.017423314973711967\n",
      "iteration 3930, dc_loss: 0.06558950245380402, tv_loss: 0.017423240467905998\n",
      "iteration 3931, dc_loss: 0.0655895248055458, tv_loss: 0.01742333173751831\n",
      "iteration 3932, dc_loss: 0.0655895322561264, tv_loss: 0.017423151060938835\n",
      "iteration 3933, dc_loss: 0.06558950990438461, tv_loss: 0.017422983422875404\n",
      "iteration 3934, dc_loss: 0.06558947265148163, tv_loss: 0.017423540353775024\n",
      "iteration 3935, dc_loss: 0.06558947265148163, tv_loss: 0.017423570156097412\n",
      "iteration 3936, dc_loss: 0.06558948010206223, tv_loss: 0.01742325909435749\n",
      "iteration 3937, dc_loss: 0.06558950245380402, tv_loss: 0.017423033714294434\n",
      "iteration 3938, dc_loss: 0.06558950990438461, tv_loss: 0.017423512414097786\n",
      "iteration 3939, dc_loss: 0.06558950245380402, tv_loss: 0.01742362044751644\n",
      "iteration 3940, dc_loss: 0.06558946520090103, tv_loss: 0.017423273995518684\n",
      "iteration 3941, dc_loss: 0.06558944284915924, tv_loss: 0.017423061653971672\n",
      "iteration 3942, dc_loss: 0.06558948010206223, tv_loss: 0.01742343045771122\n",
      "iteration 3943, dc_loss: 0.0655895322561264, tv_loss: 0.01742342859506607\n",
      "iteration 3944, dc_loss: 0.0655895471572876, tv_loss: 0.017423195764422417\n",
      "iteration 3945, dc_loss: 0.06558950990438461, tv_loss: 0.017423115670681\n",
      "iteration 3946, dc_loss: 0.06558948755264282, tv_loss: 0.017423611134290695\n",
      "iteration 3947, dc_loss: 0.06558950990438461, tv_loss: 0.017424000427126884\n",
      "iteration 3948, dc_loss: 0.06558948755264282, tv_loss: 0.01742362044751644\n",
      "iteration 3949, dc_loss: 0.06558948010206223, tv_loss: 0.017423618584871292\n",
      "iteration 3950, dc_loss: 0.06558948010206223, tv_loss: 0.017423642799258232\n",
      "iteration 3951, dc_loss: 0.06558947265148163, tv_loss: 0.01742376573383808\n",
      "iteration 3952, dc_loss: 0.06558948010206223, tv_loss: 0.017423881217837334\n",
      "iteration 3953, dc_loss: 0.06558950990438461, tv_loss: 0.01742355152964592\n",
      "iteration 3954, dc_loss: 0.06558950990438461, tv_loss: 0.01742352358996868\n",
      "iteration 3955, dc_loss: 0.06558950990438461, tv_loss: 0.01742359809577465\n",
      "iteration 3956, dc_loss: 0.06558950245380402, tv_loss: 0.01742359809577465\n",
      "iteration 3957, dc_loss: 0.06558948755264282, tv_loss: 0.017423322424292564\n",
      "iteration 3958, dc_loss: 0.06558947265148163, tv_loss: 0.017423437908291817\n",
      "iteration 3959, dc_loss: 0.06558947265148163, tv_loss: 0.01742377132177353\n",
      "iteration 3960, dc_loss: 0.06558948755264282, tv_loss: 0.017423445358872414\n",
      "iteration 3961, dc_loss: 0.06558950990438461, tv_loss: 0.017423203215003014\n",
      "iteration 3962, dc_loss: 0.06558950990438461, tv_loss: 0.017423385754227638\n",
      "iteration 3963, dc_loss: 0.0655895248055458, tv_loss: 0.01742330938577652\n",
      "iteration 3964, dc_loss: 0.06558950245380402, tv_loss: 0.017423434183001518\n",
      "iteration 3965, dc_loss: 0.06558948755264282, tv_loss: 0.017423508688807487\n",
      "iteration 3966, dc_loss: 0.06558949500322342, tv_loss: 0.017423508688807487\n",
      "iteration 3967, dc_loss: 0.06558950990438461, tv_loss: 0.017423409968614578\n",
      "iteration 3968, dc_loss: 0.06558949500322342, tv_loss: 0.017423072829842567\n",
      "iteration 3969, dc_loss: 0.06558945029973984, tv_loss: 0.017423247918486595\n",
      "iteration 3970, dc_loss: 0.06558945775032043, tv_loss: 0.01742350496351719\n",
      "iteration 3971, dc_loss: 0.06558950990438461, tv_loss: 0.017423462122678757\n",
      "iteration 3972, dc_loss: 0.0655895471572876, tv_loss: 0.01742313802242279\n",
      "iteration 3973, dc_loss: 0.0655895546078682, tv_loss: 0.017423033714294434\n",
      "iteration 3974, dc_loss: 0.06558951735496521, tv_loss: 0.017423251643776894\n",
      "iteration 3975, dc_loss: 0.06558945775032043, tv_loss: 0.01742325723171234\n",
      "iteration 3976, dc_loss: 0.06558942049741745, tv_loss: 0.01742304302752018\n",
      "iteration 3977, dc_loss: 0.06558946520090103, tv_loss: 0.017423104494810104\n",
      "iteration 3978, dc_loss: 0.06558950245380402, tv_loss: 0.017422925680875778\n",
      "iteration 3979, dc_loss: 0.065589539706707, tv_loss: 0.017422892153263092\n",
      "iteration 3980, dc_loss: 0.0655895322561264, tv_loss: 0.017422815784811974\n",
      "iteration 3981, dc_loss: 0.06558950990438461, tv_loss: 0.017422866076231003\n",
      "iteration 3982, dc_loss: 0.06558949500322342, tv_loss: 0.017423003911972046\n",
      "iteration 3983, dc_loss: 0.06558948010206223, tv_loss: 0.017423026263713837\n",
      "iteration 3984, dc_loss: 0.06558948010206223, tv_loss: 0.01742289401590824\n",
      "iteration 3985, dc_loss: 0.06558950245380402, tv_loss: 0.017422854900360107\n",
      "iteration 3986, dc_loss: 0.06558950990438461, tv_loss: 0.017423080280423164\n",
      "iteration 3987, dc_loss: 0.0655895248055458, tv_loss: 0.017423449084162712\n",
      "iteration 3988, dc_loss: 0.06558950990438461, tv_loss: 0.017423173412680626\n",
      "iteration 3989, dc_loss: 0.06558948010206223, tv_loss: 0.017423156648874283\n",
      "iteration 3990, dc_loss: 0.06558945775032043, tv_loss: 0.017423244193196297\n",
      "iteration 3991, dc_loss: 0.06558946520090103, tv_loss: 0.017423270270228386\n",
      "iteration 3992, dc_loss: 0.06558950245380402, tv_loss: 0.017423182725906372\n",
      "iteration 3993, dc_loss: 0.06558950990438461, tv_loss: 0.01742332987487316\n",
      "iteration 3994, dc_loss: 0.06558951735496521, tv_loss: 0.017423292621970177\n",
      "iteration 3995, dc_loss: 0.0655895248055458, tv_loss: 0.017423314973711967\n",
      "iteration 3996, dc_loss: 0.06558950245380402, tv_loss: 0.01742321252822876\n",
      "iteration 3997, dc_loss: 0.06558948010206223, tv_loss: 0.017423339188098907\n",
      "iteration 3998, dc_loss: 0.06558948010206223, tv_loss: 0.01742355152964592\n",
      "iteration 3999, dc_loss: 0.06558948010206223, tv_loss: 0.017423568293452263\n",
      "iteration 4000, dc_loss: 0.06558948755264282, tv_loss: 0.017423415556550026\n",
      "PSNR Value mt1: 49.78164855801286\n",
      "SSIM Value mt1: 0.9552404327320312\n",
      "tensor([[[6.0409e-04, 7.4484e-05]]]) tensor([[[1.2870, 1.2171]]])\n",
      "torch.Size([1, 1, 2]) torch.Size([1, 1, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| params.keys(): dict_keys(['grid'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "iteration 1, dc_loss: 2.9683542251586914, tv_loss: 0.0\n",
      "iteration 2, dc_loss: 2.9084701538085938, tv_loss: 0.0002550158533267677\n",
      "iteration 3, dc_loss: 2.8496828079223633, tv_loss: 0.0005004259292036295\n",
      "iteration 4, dc_loss: 2.7919692993164062, tv_loss: 0.000743578711990267\n",
      "iteration 5, dc_loss: 2.7353129386901855, tv_loss: 0.0009861089056357741\n",
      "iteration 6, dc_loss: 2.679697036743164, tv_loss: 0.001228003529831767\n",
      "iteration 7, dc_loss: 2.625107526779175, tv_loss: 0.0014684407506138086\n",
      "iteration 8, dc_loss: 2.571531295776367, tv_loss: 0.0017087629530578852\n",
      "iteration 9, dc_loss: 2.518953800201416, tv_loss: 0.0019449278479442\n",
      "iteration 10, dc_loss: 2.4673619270324707, tv_loss: 0.0021796380169689655\n",
      "iteration 11, dc_loss: 2.4167418479919434, tv_loss: 0.0024107550270855427\n",
      "iteration 12, dc_loss: 2.367079734802246, tv_loss: 0.002638190519064665\n",
      "iteration 13, dc_loss: 2.318361520767212, tv_loss: 0.0028632809408009052\n",
      "iteration 14, dc_loss: 2.2705740928649902, tv_loss: 0.0030845371074974537\n",
      "iteration 15, dc_loss: 2.2237038612365723, tv_loss: 0.003302824916318059\n",
      "iteration 16, dc_loss: 2.177736759185791, tv_loss: 0.003517122007906437\n",
      "iteration 17, dc_loss: 2.132659912109375, tv_loss: 0.003728085896000266\n",
      "iteration 18, dc_loss: 2.0884597301483154, tv_loss: 0.0039369347505271435\n",
      "iteration 19, dc_loss: 2.045123338699341, tv_loss: 0.004142390564084053\n",
      "iteration 20, dc_loss: 2.0026373863220215, tv_loss: 0.004344697576016188\n",
      "iteration 21, dc_loss: 1.9609885215759277, tv_loss: 0.004543742630630732\n",
      "iteration 22, dc_loss: 1.9201627969741821, tv_loss: 0.004738968797028065\n",
      "iteration 23, dc_loss: 1.8801475763320923, tv_loss: 0.004931892734020948\n",
      "iteration 24, dc_loss: 1.8409289121627808, tv_loss: 0.005122939124703407\n",
      "iteration 25, dc_loss: 1.802493929862976, tv_loss: 0.005310963839292526\n",
      "iteration 26, dc_loss: 1.7648292779922485, tv_loss: 0.005494273267686367\n",
      "iteration 27, dc_loss: 1.727921724319458, tv_loss: 0.0056763747707009315\n",
      "iteration 28, dc_loss: 1.6917580366134644, tv_loss: 0.005855370778590441\n",
      "iteration 29, dc_loss: 1.6563254594802856, tv_loss: 0.006031463388353586\n",
      "iteration 30, dc_loss: 1.6216106414794922, tv_loss: 0.006204111035913229\n",
      "iteration 31, dc_loss: 1.5876013040542603, tv_loss: 0.006375157739967108\n",
      "iteration 32, dc_loss: 1.5542840957641602, tv_loss: 0.006543669383972883\n",
      "iteration 33, dc_loss: 1.5216467380523682, tv_loss: 0.00670956913381815\n",
      "iteration 34, dc_loss: 1.4896764755249023, tv_loss: 0.006872211117297411\n",
      "iteration 35, dc_loss: 1.458361268043518, tv_loss: 0.007032518740743399\n",
      "iteration 36, dc_loss: 1.427688717842102, tv_loss: 0.00719147315248847\n",
      "iteration 37, dc_loss: 1.3976469039916992, tv_loss: 0.007348138373345137\n",
      "iteration 38, dc_loss: 1.3682236671447754, tv_loss: 0.0075019728392362595\n",
      "iteration 39, dc_loss: 1.3394076824188232, tv_loss: 0.0076532610692083836\n",
      "iteration 40, dc_loss: 1.3111869096755981, tv_loss: 0.007802636828273535\n",
      "iteration 41, dc_loss: 1.2835506200790405, tv_loss: 0.007950318977236748\n",
      "iteration 42, dc_loss: 1.256487250328064, tv_loss: 0.008095880039036274\n",
      "iteration 43, dc_loss: 1.2299858331680298, tv_loss: 0.008239584043622017\n",
      "iteration 44, dc_loss: 1.2040352821350098, tv_loss: 0.008382013998925686\n",
      "iteration 45, dc_loss: 1.178625226020813, tv_loss: 0.00852237455546856\n",
      "iteration 46, dc_loss: 1.1537448167800903, tv_loss: 0.0086606964468956\n",
      "iteration 47, dc_loss: 1.12938392162323, tv_loss: 0.00879665743559599\n",
      "iteration 48, dc_loss: 1.1055320501327515, tv_loss: 0.008931951597332954\n",
      "iteration 49, dc_loss: 1.0821791887283325, tv_loss: 0.009065582416951656\n",
      "iteration 50, dc_loss: 1.05931556224823, tv_loss: 0.009197265841066837\n",
      "iteration 51, dc_loss: 1.0369311571121216, tv_loss: 0.009326810017228127\n",
      "iteration 52, dc_loss: 1.0150166749954224, tv_loss: 0.009454053826630116\n",
      "iteration 53, dc_loss: 0.9935625195503235, tv_loss: 0.009581368416547775\n",
      "iteration 54, dc_loss: 0.9725592732429504, tv_loss: 0.009708094410598278\n",
      "iteration 55, dc_loss: 0.9519981741905212, tv_loss: 0.009832924231886864\n",
      "iteration 56, dc_loss: 0.93187016248703, tv_loss: 0.009954981505870819\n",
      "iteration 57, dc_loss: 0.9121665358543396, tv_loss: 0.010076139122247696\n",
      "iteration 58, dc_loss: 0.8928784132003784, tv_loss: 0.010196181014180183\n",
      "iteration 59, dc_loss: 0.8739978075027466, tv_loss: 0.010314787738025188\n",
      "iteration 60, dc_loss: 0.8555161356925964, tv_loss: 0.01043205801397562\n",
      "iteration 61, dc_loss: 0.8374252319335938, tv_loss: 0.010548261925578117\n",
      "iteration 62, dc_loss: 0.8197171688079834, tv_loss: 0.010662471875548363\n",
      "iteration 63, dc_loss: 0.8023841381072998, tv_loss: 0.010774882510304451\n",
      "iteration 64, dc_loss: 0.7854182124137878, tv_loss: 0.01088754367083311\n",
      "iteration 65, dc_loss: 0.7688121795654297, tv_loss: 0.010998483747243881\n",
      "iteration 66, dc_loss: 0.7525582909584045, tv_loss: 0.011106397025287151\n",
      "iteration 67, dc_loss: 0.7366495132446289, tv_loss: 0.011214768514037132\n",
      "iteration 68, dc_loss: 0.7210785746574402, tv_loss: 0.011322344653308392\n",
      "iteration 69, dc_loss: 0.7058383822441101, tv_loss: 0.011428105644881725\n",
      "iteration 70, dc_loss: 0.6909223198890686, tv_loss: 0.011531013995409012\n",
      "iteration 71, dc_loss: 0.6763233542442322, tv_loss: 0.011635108850896358\n",
      "iteration 72, dc_loss: 0.6620350480079651, tv_loss: 0.011737866327166557\n",
      "iteration 73, dc_loss: 0.6480509042739868, tv_loss: 0.011838588863611221\n",
      "iteration 74, dc_loss: 0.6343644857406616, tv_loss: 0.011937598697841167\n",
      "iteration 75, dc_loss: 0.6209697127342224, tv_loss: 0.012036442756652832\n",
      "iteration 76, dc_loss: 0.6078605055809021, tv_loss: 0.01213501300662756\n",
      "iteration 77, dc_loss: 0.5950307846069336, tv_loss: 0.012231018394231796\n",
      "iteration 78, dc_loss: 0.5824748277664185, tv_loss: 0.012325597926974297\n",
      "iteration 79, dc_loss: 0.5701867341995239, tv_loss: 0.012419641017913818\n",
      "iteration 80, dc_loss: 0.5581610798835754, tv_loss: 0.012513259425759315\n",
      "iteration 81, dc_loss: 0.5463922023773193, tv_loss: 0.012605790980160236\n",
      "iteration 82, dc_loss: 0.5348747968673706, tv_loss: 0.012696688994765282\n",
      "iteration 83, dc_loss: 0.523603618144989, tv_loss: 0.012785589322447777\n",
      "iteration 84, dc_loss: 0.5125734210014343, tv_loss: 0.012874209322035313\n",
      "iteration 85, dc_loss: 0.5017792582511902, tv_loss: 0.01296327542513609\n",
      "iteration 86, dc_loss: 0.4912160038948059, tv_loss: 0.013049786910414696\n",
      "iteration 87, dc_loss: 0.48087888956069946, tv_loss: 0.01313532330095768\n",
      "iteration 88, dc_loss: 0.4707632064819336, tv_loss: 0.013219749554991722\n",
      "iteration 89, dc_loss: 0.46086421608924866, tv_loss: 0.013302535749971867\n",
      "iteration 90, dc_loss: 0.45117729902267456, tv_loss: 0.013385877013206482\n",
      "iteration 91, dc_loss: 0.4416981041431427, tv_loss: 0.01346819568425417\n",
      "iteration 92, dc_loss: 0.4324222207069397, tv_loss: 0.013548433780670166\n",
      "iteration 93, dc_loss: 0.42334526777267456, tv_loss: 0.013627439737319946\n",
      "iteration 94, dc_loss: 0.4144631624221802, tv_loss: 0.013705506920814514\n",
      "iteration 95, dc_loss: 0.4057716727256775, tv_loss: 0.013783451169729233\n",
      "iteration 96, dc_loss: 0.39726683497428894, tv_loss: 0.013860859908163548\n",
      "iteration 97, dc_loss: 0.3889448046684265, tv_loss: 0.013936299830675125\n",
      "iteration 98, dc_loss: 0.3808015286922455, tv_loss: 0.014010230079293251\n",
      "iteration 99, dc_loss: 0.3728333115577698, tv_loss: 0.014084581285715103\n",
      "iteration 100, dc_loss: 0.3650364875793457, tv_loss: 0.014157404191792011\n",
      "iteration 101, dc_loss: 0.3574073612689972, tv_loss: 0.014229396358132362\n",
      "iteration 102, dc_loss: 0.3499423861503601, tv_loss: 0.014300122857093811\n",
      "iteration 103, dc_loss: 0.3426380753517151, tv_loss: 0.014369945041835308\n",
      "iteration 104, dc_loss: 0.3354910910129547, tv_loss: 0.014439534395933151\n",
      "iteration 105, dc_loss: 0.32849806547164917, tv_loss: 0.0145078981295228\n",
      "iteration 106, dc_loss: 0.3216556906700134, tv_loss: 0.014574222266674042\n",
      "iteration 107, dc_loss: 0.314960777759552, tv_loss: 0.014640813693404198\n",
      "iteration 108, dc_loss: 0.30841031670570374, tv_loss: 0.014706655405461788\n",
      "iteration 109, dc_loss: 0.302001029253006, tv_loss: 0.014771778136491776\n",
      "iteration 110, dc_loss: 0.2957301139831543, tv_loss: 0.014834539964795113\n",
      "iteration 111, dc_loss: 0.28959453105926514, tv_loss: 0.014897826127707958\n",
      "iteration 112, dc_loss: 0.2835913896560669, tv_loss: 0.014960472472012043\n",
      "iteration 113, dc_loss: 0.2777179479598999, tv_loss: 0.015021413564682007\n",
      "iteration 114, dc_loss: 0.2719714343547821, tv_loss: 0.015080573037266731\n",
      "iteration 115, dc_loss: 0.26634910702705383, tv_loss: 0.015140837989747524\n",
      "iteration 116, dc_loss: 0.26084834337234497, tv_loss: 0.015200459398329258\n",
      "iteration 117, dc_loss: 0.2554665207862854, tv_loss: 0.015257578343153\n",
      "iteration 118, dc_loss: 0.25020116567611694, tv_loss: 0.015313425101339817\n",
      "iteration 119, dc_loss: 0.24504971504211426, tv_loss: 0.015371697023510933\n",
      "iteration 120, dc_loss: 0.2400098592042923, tv_loss: 0.015427871607244015\n",
      "iteration 121, dc_loss: 0.23507915437221527, tv_loss: 0.015481682494282722\n",
      "iteration 122, dc_loss: 0.23025529086589813, tv_loss: 0.015534873120486736\n",
      "iteration 123, dc_loss: 0.22553600370883942, tv_loss: 0.015589069575071335\n",
      "iteration 124, dc_loss: 0.22091905772686005, tv_loss: 0.015642357990145683\n",
      "iteration 125, dc_loss: 0.21640226244926453, tv_loss: 0.015693727880716324\n",
      "iteration 126, dc_loss: 0.21198344230651855, tv_loss: 0.01574418880045414\n",
      "iteration 127, dc_loss: 0.207660511136055, tv_loss: 0.015794415026903152\n",
      "iteration 128, dc_loss: 0.20343144237995148, tv_loss: 0.015845131129026413\n",
      "iteration 129, dc_loss: 0.199294313788414, tv_loss: 0.015894513577222824\n",
      "iteration 130, dc_loss: 0.19524706900119781, tv_loss: 0.01594291441142559\n",
      "iteration 131, dc_loss: 0.1912878155708313, tv_loss: 0.015989940613508224\n",
      "iteration 132, dc_loss: 0.1874147206544876, tv_loss: 0.016036106273531914\n",
      "iteration 133, dc_loss: 0.18362589180469513, tv_loss: 0.016083551570773125\n",
      "iteration 134, dc_loss: 0.17991957068443298, tv_loss: 0.016129272058606148\n",
      "iteration 135, dc_loss: 0.1762939691543579, tv_loss: 0.016173288226127625\n",
      "iteration 136, dc_loss: 0.17274735867977142, tv_loss: 0.01621733233332634\n",
      "iteration 137, dc_loss: 0.16927804052829742, tv_loss: 0.01626146212220192\n",
      "iteration 138, dc_loss: 0.1658843606710434, tv_loss: 0.01630394347012043\n",
      "iteration 139, dc_loss: 0.1625647395849228, tv_loss: 0.01634625717997551\n",
      "iteration 140, dc_loss: 0.15931755304336548, tv_loss: 0.016388030722737312\n",
      "iteration 141, dc_loss: 0.1561412811279297, tv_loss: 0.016429442912340164\n",
      "iteration 142, dc_loss: 0.15303437411785126, tv_loss: 0.016469744965434074\n",
      "iteration 143, dc_loss: 0.14999538660049438, tv_loss: 0.01650908775627613\n",
      "iteration 144, dc_loss: 0.14702287316322327, tv_loss: 0.01654835231602192\n",
      "iteration 145, dc_loss: 0.1441153734922409, tv_loss: 0.01658766344189644\n",
      "iteration 146, dc_loss: 0.1412714719772339, tv_loss: 0.016625728458166122\n",
      "iteration 147, dc_loss: 0.13848985731601715, tv_loss: 0.016662823036313057\n",
      "iteration 148, dc_loss: 0.13576917350292206, tv_loss: 0.016700690612196922\n",
      "iteration 149, dc_loss: 0.13310815393924713, tv_loss: 0.01673687808215618\n",
      "iteration 150, dc_loss: 0.1305054873228073, tv_loss: 0.016772840172052383\n",
      "iteration 151, dc_loss: 0.1279599368572235, tv_loss: 0.01680837571620941\n",
      "iteration 152, dc_loss: 0.12547025084495544, tv_loss: 0.01684325374662876\n",
      "iteration 153, dc_loss: 0.1230352446436882, tv_loss: 0.01687769778072834\n",
      "iteration 154, dc_loss: 0.12065376341342926, tv_loss: 0.016911501064896584\n",
      "iteration 155, dc_loss: 0.11832461506128311, tv_loss: 0.016944970935583115\n",
      "iteration 156, dc_loss: 0.11604670435190201, tv_loss: 0.016978029161691666\n",
      "iteration 157, dc_loss: 0.1138189435005188, tv_loss: 0.017010310664772987\n",
      "iteration 158, dc_loss: 0.11164022237062454, tv_loss: 0.017042862251400948\n",
      "iteration 159, dc_loss: 0.10950953513383865, tv_loss: 0.017074380069971085\n",
      "iteration 160, dc_loss: 0.10742581635713577, tv_loss: 0.017105037346482277\n",
      "iteration 161, dc_loss: 0.1053880825638771, tv_loss: 0.017135726287961006\n",
      "iteration 162, dc_loss: 0.10339531302452087, tv_loss: 0.017165567725896835\n",
      "iteration 163, dc_loss: 0.10144653171300888, tv_loss: 0.017195116728544235\n",
      "iteration 164, dc_loss: 0.09954079985618591, tv_loss: 0.017224391922354698\n",
      "iteration 165, dc_loss: 0.09767719358205795, tv_loss: 0.017252862453460693\n",
      "iteration 166, dc_loss: 0.09585478156805038, tv_loss: 0.0172813069075346\n",
      "iteration 167, dc_loss: 0.09407272189855576, tv_loss: 0.017308825626969337\n",
      "iteration 168, dc_loss: 0.09233014285564423, tv_loss: 0.017336392775177956\n",
      "iteration 169, dc_loss: 0.09062615782022476, tv_loss: 0.017363131046295166\n",
      "iteration 170, dc_loss: 0.0889599546790123, tv_loss: 0.017389623448252678\n",
      "iteration 171, dc_loss: 0.08733070641756058, tv_loss: 0.017415745183825493\n",
      "iteration 172, dc_loss: 0.08573761582374573, tv_loss: 0.01744120381772518\n",
      "iteration 173, dc_loss: 0.08417991548776627, tv_loss: 0.01746639609336853\n",
      "iteration 174, dc_loss: 0.08265683799982071, tv_loss: 0.017490893602371216\n",
      "iteration 175, dc_loss: 0.08116764575242996, tv_loss: 0.01751520298421383\n",
      "iteration 176, dc_loss: 0.07971158623695374, tv_loss: 0.017539311200380325\n",
      "iteration 177, dc_loss: 0.07828795909881592, tv_loss: 0.0175627414137125\n",
      "iteration 178, dc_loss: 0.0768960490822792, tv_loss: 0.01758565939962864\n",
      "iteration 179, dc_loss: 0.07553516328334808, tv_loss: 0.01760854572057724\n",
      "iteration 180, dc_loss: 0.07420460879802704, tv_loss: 0.017631428316235542\n",
      "iteration 181, dc_loss: 0.07290377467870712, tv_loss: 0.017653506249189377\n",
      "iteration 182, dc_loss: 0.0716320052742958, tv_loss: 0.017674772068858147\n",
      "iteration 183, dc_loss: 0.07038865983486176, tv_loss: 0.017695967108011246\n",
      "iteration 184, dc_loss: 0.06917311996221542, tv_loss: 0.01771710067987442\n",
      "iteration 185, dc_loss: 0.06798477470874786, tv_loss: 0.01773803122341633\n",
      "iteration 186, dc_loss: 0.06682302802801132, tv_loss: 0.017758479341864586\n",
      "iteration 187, dc_loss: 0.06568735092878342, tv_loss: 0.017778493463993073\n",
      "iteration 188, dc_loss: 0.0645771324634552, tv_loss: 0.01779831200838089\n",
      "iteration 189, dc_loss: 0.06349179893732071, tv_loss: 0.017817389219999313\n",
      "iteration 190, dc_loss: 0.06243083253502846, tv_loss: 0.017836393788456917\n",
      "iteration 191, dc_loss: 0.06139371171593666, tv_loss: 0.017855683341622353\n",
      "iteration 192, dc_loss: 0.06037989631295204, tv_loss: 0.017874538898468018\n",
      "iteration 193, dc_loss: 0.059388913214206696, tv_loss: 0.017892129719257355\n",
      "iteration 194, dc_loss: 0.05842023342847824, tv_loss: 0.017909742891788483\n",
      "iteration 195, dc_loss: 0.057473402470350266, tv_loss: 0.017928097397089005\n",
      "iteration 196, dc_loss: 0.056547921150922775, tv_loss: 0.017945514991879463\n",
      "iteration 197, dc_loss: 0.05564328655600548, tv_loss: 0.017962152138352394\n",
      "iteration 198, dc_loss: 0.05475906655192375, tv_loss: 0.017978517338633537\n",
      "iteration 199, dc_loss: 0.05389484390616417, tv_loss: 0.017995091155171394\n",
      "iteration 200, dc_loss: 0.05305017530918121, tv_loss: 0.01801116019487381\n",
      "iteration 201, dc_loss: 0.05222460627555847, tv_loss: 0.01802665926516056\n",
      "iteration 202, dc_loss: 0.051417719572782516, tv_loss: 0.018041880801320076\n",
      "iteration 203, dc_loss: 0.050629109144210815, tv_loss: 0.0180569626390934\n",
      "iteration 204, dc_loss: 0.04985836148262024, tv_loss: 0.018072618171572685\n",
      "iteration 205, dc_loss: 0.049105070531368256, tv_loss: 0.018087496981024742\n",
      "iteration 206, dc_loss: 0.048368871212005615, tv_loss: 0.018101265653967857\n",
      "iteration 207, dc_loss: 0.04764941334724426, tv_loss: 0.01811491698026657\n",
      "iteration 208, dc_loss: 0.04694630578160286, tv_loss: 0.018129486590623856\n",
      "iteration 209, dc_loss: 0.04625919461250305, tv_loss: 0.018143171444535255\n",
      "iteration 210, dc_loss: 0.045587703585624695, tv_loss: 0.018155531957745552\n",
      "iteration 211, dc_loss: 0.044931504875421524, tv_loss: 0.01816871389746666\n",
      "iteration 212, dc_loss: 0.04429025948047638, tv_loss: 0.018182069063186646\n",
      "iteration 213, dc_loss: 0.04366365447640419, tv_loss: 0.018194478005170822\n",
      "iteration 214, dc_loss: 0.0430513359606266, tv_loss: 0.018206262961030006\n",
      "iteration 215, dc_loss: 0.042452991008758545, tv_loss: 0.0182189978659153\n",
      "iteration 216, dc_loss: 0.04186829924583435, tv_loss: 0.018231486901640892\n",
      "iteration 217, dc_loss: 0.041296977549791336, tv_loss: 0.01824280619621277\n",
      "iteration 218, dc_loss: 0.04073876515030861, tv_loss: 0.01825365237891674\n",
      "iteration 219, dc_loss: 0.040193330496549606, tv_loss: 0.018265072256326675\n",
      "iteration 220, dc_loss: 0.03966040536761284, tv_loss: 0.018276352435350418\n",
      "iteration 221, dc_loss: 0.03913970664143562, tv_loss: 0.01828732155263424\n",
      "iteration 222, dc_loss: 0.03863093629479408, tv_loss: 0.018297463655471802\n",
      "iteration 223, dc_loss: 0.038133807480335236, tv_loss: 0.018307702615857124\n",
      "iteration 224, dc_loss: 0.03764808550477028, tv_loss: 0.01831846311688423\n",
      "iteration 225, dc_loss: 0.037173543125391006, tv_loss: 0.018328694626688957\n",
      "iteration 226, dc_loss: 0.03670993447303772, tv_loss: 0.01833808608353138\n",
      "iteration 227, dc_loss: 0.036256980150938034, tv_loss: 0.018347695469856262\n",
      "iteration 228, dc_loss: 0.03581445291638374, tv_loss: 0.018357068300247192\n",
      "iteration 229, dc_loss: 0.035382144153118134, tv_loss: 0.01836671307682991\n",
      "iteration 230, dc_loss: 0.0349598154425621, tv_loss: 0.01837560534477234\n",
      "iteration 231, dc_loss: 0.03454724699258804, tv_loss: 0.018384259194135666\n",
      "iteration 232, dc_loss: 0.03414420783519745, tv_loss: 0.018392976373434067\n",
      "iteration 233, dc_loss: 0.03375048190355301, tv_loss: 0.018401890993118286\n",
      "iteration 234, dc_loss: 0.033365827053785324, tv_loss: 0.018410474061965942\n",
      "iteration 235, dc_loss: 0.03299006074666977, tv_loss: 0.018418719992041588\n",
      "iteration 236, dc_loss: 0.032622989267110825, tv_loss: 0.0184266846626997\n",
      "iteration 237, dc_loss: 0.03226444125175476, tv_loss: 0.01843414455652237\n",
      "iteration 238, dc_loss: 0.03191421553492546, tv_loss: 0.018442068248987198\n",
      "iteration 239, dc_loss: 0.0315721333026886, tv_loss: 0.018449677154421806\n",
      "iteration 240, dc_loss: 0.031237972900271416, tv_loss: 0.018456829711794853\n",
      "iteration 241, dc_loss: 0.03091157041490078, tv_loss: 0.018463818356394768\n",
      "iteration 242, dc_loss: 0.030592724680900574, tv_loss: 0.018471019342541695\n",
      "iteration 243, dc_loss: 0.030281279236078262, tv_loss: 0.018478062003850937\n",
      "iteration 244, dc_loss: 0.02997705526649952, tv_loss: 0.018484588712453842\n",
      "iteration 245, dc_loss: 0.029679911211133003, tv_loss: 0.01849132589995861\n",
      "iteration 246, dc_loss: 0.029389677569270134, tv_loss: 0.018498072400689125\n",
      "iteration 247, dc_loss: 0.02910623885691166, tv_loss: 0.01850404404103756\n",
      "iteration 248, dc_loss: 0.028829384595155716, tv_loss: 0.018510034307837486\n",
      "iteration 249, dc_loss: 0.028558971360325813, tv_loss: 0.01851617731153965\n",
      "iteration 250, dc_loss: 0.028294848278164864, tv_loss: 0.01852227933704853\n",
      "iteration 251, dc_loss: 0.028036905452609062, tv_loss: 0.01852787658572197\n",
      "iteration 252, dc_loss: 0.027784988284111023, tv_loss: 0.01853320188820362\n",
      "iteration 253, dc_loss: 0.027538947761058807, tv_loss: 0.01853867806494236\n",
      "iteration 254, dc_loss: 0.027298646047711372, tv_loss: 0.018544012680649757\n",
      "iteration 255, dc_loss: 0.02706395648419857, tv_loss: 0.018549229949712753\n",
      "iteration 256, dc_loss: 0.0268347579985857, tv_loss: 0.018553810194134712\n",
      "iteration 257, dc_loss: 0.02661092020571232, tv_loss: 0.01855875924229622\n",
      "iteration 258, dc_loss: 0.026392294093966484, tv_loss: 0.0185637678951025\n",
      "iteration 259, dc_loss: 0.026178786531090736, tv_loss: 0.018568577244877815\n",
      "iteration 260, dc_loss: 0.025970276445150375, tv_loss: 0.01857275888323784\n",
      "iteration 261, dc_loss: 0.025766689330339432, tv_loss: 0.0185772143304348\n",
      "iteration 262, dc_loss: 0.02556788921356201, tv_loss: 0.018581770360469818\n",
      "iteration 263, dc_loss: 0.025373732671141624, tv_loss: 0.01858571171760559\n",
      "iteration 264, dc_loss: 0.02518412470817566, tv_loss: 0.018589861690998077\n",
      "iteration 265, dc_loss: 0.024998942390084267, tv_loss: 0.01859385147690773\n",
      "iteration 266, dc_loss: 0.02481808327138424, tv_loss: 0.018597830086946487\n",
      "iteration 267, dc_loss: 0.024641448631882668, tv_loss: 0.018601519986987114\n",
      "iteration 268, dc_loss: 0.02446894347667694, tv_loss: 0.018604587763547897\n",
      "iteration 269, dc_loss: 0.024300510063767433, tv_loss: 0.01860835589468479\n",
      "iteration 270, dc_loss: 0.024136072024703026, tv_loss: 0.018612045794725418\n",
      "iteration 271, dc_loss: 0.02397550642490387, tv_loss: 0.01861533150076866\n",
      "iteration 272, dc_loss: 0.02381870709359646, tv_loss: 0.018618298694491386\n",
      "iteration 273, dc_loss: 0.023665595799684525, tv_loss: 0.018621638417243958\n",
      "iteration 274, dc_loss: 0.0235160980373621, tv_loss: 0.018624557182192802\n",
      "iteration 275, dc_loss: 0.02337011694908142, tv_loss: 0.018627509474754333\n",
      "iteration 276, dc_loss: 0.023227551952004433, tv_loss: 0.018630243837833405\n",
      "iteration 277, dc_loss: 0.023088328540325165, tv_loss: 0.018633201718330383\n",
      "iteration 278, dc_loss: 0.022952375933527946, tv_loss: 0.01863580383360386\n",
      "iteration 279, dc_loss: 0.022819606587290764, tv_loss: 0.018638499081134796\n",
      "iteration 280, dc_loss: 0.022690000012516975, tv_loss: 0.018640875816345215\n",
      "iteration 281, dc_loss: 0.022563457489013672, tv_loss: 0.018643077462911606\n",
      "iteration 282, dc_loss: 0.022439919412136078, tv_loss: 0.018645303323864937\n",
      "iteration 283, dc_loss: 0.022319290786981583, tv_loss: 0.018647504970431328\n",
      "iteration 284, dc_loss: 0.022201476618647575, tv_loss: 0.018649594858288765\n",
      "iteration 285, dc_loss: 0.022086402401328087, tv_loss: 0.0186519343405962\n",
      "iteration 286, dc_loss: 0.021974017843604088, tv_loss: 0.018653906881809235\n",
      "iteration 287, dc_loss: 0.021864281967282295, tv_loss: 0.018655547872185707\n",
      "iteration 288, dc_loss: 0.021757138893008232, tv_loss: 0.018657581880688667\n",
      "iteration 289, dc_loss: 0.02165253646671772, tv_loss: 0.018659433349967003\n",
      "iteration 290, dc_loss: 0.021550413221120834, tv_loss: 0.01866128481924534\n",
      "iteration 291, dc_loss: 0.021450694650411606, tv_loss: 0.01866268366575241\n",
      "iteration 292, dc_loss: 0.021353337913751602, tv_loss: 0.01866408996284008\n",
      "iteration 293, dc_loss: 0.0212582778185606, tv_loss: 0.018665749579668045\n",
      "iteration 294, dc_loss: 0.021165460348129272, tv_loss: 0.018667232245206833\n",
      "iteration 295, dc_loss: 0.021074816584587097, tv_loss: 0.018668223172426224\n",
      "iteration 296, dc_loss: 0.020986303687095642, tv_loss: 0.01866932399570942\n",
      "iteration 297, dc_loss: 0.020899873226881027, tv_loss: 0.01867097057402134\n",
      "iteration 298, dc_loss: 0.020815476775169373, tv_loss: 0.018671905621886253\n",
      "iteration 299, dc_loss: 0.0207330584526062, tv_loss: 0.018672920763492584\n",
      "iteration 300, dc_loss: 0.020652558654546738, tv_loss: 0.018673861399292946\n",
      "iteration 301, dc_loss: 0.020573943853378296, tv_loss: 0.018674787133932114\n",
      "iteration 302, dc_loss: 0.020497197285294533, tv_loss: 0.018675675615668297\n",
      "iteration 303, dc_loss: 0.020422279834747314, tv_loss: 0.01867653988301754\n",
      "iteration 304, dc_loss: 0.020349128171801567, tv_loss: 0.018677089363336563\n",
      "iteration 305, dc_loss: 0.02027769573032856, tv_loss: 0.018677759915590286\n",
      "iteration 306, dc_loss: 0.020207932218909264, tv_loss: 0.018678613007068634\n",
      "iteration 307, dc_loss: 0.0201397892087698, tv_loss: 0.018679091706871986\n",
      "iteration 308, dc_loss: 0.020073246210813522, tv_loss: 0.018679436296224594\n",
      "iteration 309, dc_loss: 0.020008254796266556, tv_loss: 0.01868000440299511\n",
      "iteration 310, dc_loss: 0.019944800063967705, tv_loss: 0.01868058554828167\n",
      "iteration 311, dc_loss: 0.019882837310433388, tv_loss: 0.01868060603737831\n",
      "iteration 312, dc_loss: 0.019822319969534874, tv_loss: 0.01868101768195629\n",
      "iteration 313, dc_loss: 0.019763227552175522, tv_loss: 0.018681146204471588\n",
      "iteration 314, dc_loss: 0.019705548882484436, tv_loss: 0.018681539222598076\n",
      "iteration 315, dc_loss: 0.019649218767881393, tv_loss: 0.018681732937693596\n",
      "iteration 316, dc_loss: 0.019594212993979454, tv_loss: 0.018681678920984268\n",
      "iteration 317, dc_loss: 0.019540483132004738, tv_loss: 0.01868147775530815\n",
      "iteration 318, dc_loss: 0.019488004967570305, tv_loss: 0.018681593239307404\n",
      "iteration 319, dc_loss: 0.019436748698353767, tv_loss: 0.018681783229112625\n",
      "iteration 320, dc_loss: 0.01938667893409729, tv_loss: 0.018681500107049942\n",
      "iteration 321, dc_loss: 0.019337745383381844, tv_loss: 0.018681488931179047\n",
      "iteration 322, dc_loss: 0.019289959222078323, tv_loss: 0.018681427463889122\n",
      "iteration 323, dc_loss: 0.019243301823735237, tv_loss: 0.018681291490793228\n",
      "iteration 324, dc_loss: 0.019197745248675346, tv_loss: 0.01868092641234398\n",
      "iteration 325, dc_loss: 0.019153255969285965, tv_loss: 0.018680419772863388\n",
      "iteration 326, dc_loss: 0.01910981722176075, tv_loss: 0.01868005469441414\n",
      "iteration 327, dc_loss: 0.01906738430261612, tv_loss: 0.01867988146841526\n",
      "iteration 328, dc_loss: 0.019025955349206924, tv_loss: 0.018679408356547356\n",
      "iteration 329, dc_loss: 0.01898547075688839, tv_loss: 0.01867871917784214\n",
      "iteration 330, dc_loss: 0.018945936113595963, tv_loss: 0.01867818832397461\n",
      "iteration 331, dc_loss: 0.018907319754362106, tv_loss: 0.018677663058042526\n",
      "iteration 332, dc_loss: 0.018869591876864433, tv_loss: 0.01867707073688507\n",
      "iteration 333, dc_loss: 0.018832707777619362, tv_loss: 0.018676459789276123\n",
      "iteration 334, dc_loss: 0.018796667456626892, tv_loss: 0.018675511702895164\n",
      "iteration 335, dc_loss: 0.01876145415008068, tv_loss: 0.01867493987083435\n",
      "iteration 336, dc_loss: 0.018727073445916176, tv_loss: 0.0186740905046463\n",
      "iteration 337, dc_loss: 0.018693501129746437, tv_loss: 0.018673142418265343\n",
      "iteration 338, dc_loss: 0.018660685047507286, tv_loss: 0.01867220737040043\n",
      "iteration 339, dc_loss: 0.01862863264977932, tv_loss: 0.01867138221859932\n",
      "iteration 340, dc_loss: 0.018597327172756195, tv_loss: 0.018670327961444855\n",
      "iteration 341, dc_loss: 0.018566740676760674, tv_loss: 0.018669065088033676\n",
      "iteration 342, dc_loss: 0.018536850810050964, tv_loss: 0.018668297678232193\n",
      "iteration 343, dc_loss: 0.018507642671465874, tv_loss: 0.01866729184985161\n",
      "iteration 344, dc_loss: 0.018479082733392715, tv_loss: 0.018666179850697517\n",
      "iteration 345, dc_loss: 0.018451178446412086, tv_loss: 0.018664982169866562\n",
      "iteration 346, dc_loss: 0.0184238962829113, tv_loss: 0.018663892522454262\n",
      "iteration 347, dc_loss: 0.01839725486934185, tv_loss: 0.01866268925368786\n",
      "iteration 348, dc_loss: 0.01837121695280075, tv_loss: 0.0186615027487278\n",
      "iteration 349, dc_loss: 0.018345758318901062, tv_loss: 0.01866050623357296\n",
      "iteration 350, dc_loss: 0.018320875242352486, tv_loss: 0.018659399822354317\n",
      "iteration 351, dc_loss: 0.018296560272574425, tv_loss: 0.01865811087191105\n",
      "iteration 352, dc_loss: 0.01827278546988964, tv_loss: 0.01865677908062935\n",
      "iteration 353, dc_loss: 0.018249521031975746, tv_loss: 0.018655579537153244\n",
      "iteration 354, dc_loss: 0.018226781859993935, tv_loss: 0.018654251471161842\n",
      "iteration 355, dc_loss: 0.018204577267169952, tv_loss: 0.018652819097042084\n",
      "iteration 356, dc_loss: 0.018182910978794098, tv_loss: 0.018651487305760384\n",
      "iteration 357, dc_loss: 0.018161743879318237, tv_loss: 0.01865001581609249\n",
      "iteration 358, dc_loss: 0.018141040578484535, tv_loss: 0.018648488447070122\n",
      "iteration 359, dc_loss: 0.018120799213647842, tv_loss: 0.01864677108824253\n",
      "iteration 360, dc_loss: 0.01810098998248577, tv_loss: 0.018645264208316803\n",
      "iteration 361, dc_loss: 0.018081584945321083, tv_loss: 0.018644049763679504\n",
      "iteration 362, dc_loss: 0.018062615767121315, tv_loss: 0.018642526119947433\n",
      "iteration 363, dc_loss: 0.018044061958789825, tv_loss: 0.018640756607055664\n",
      "iteration 364, dc_loss: 0.018025878816843033, tv_loss: 0.018639549612998962\n",
      "iteration 365, dc_loss: 0.018008112907409668, tv_loss: 0.018638035282492638\n",
      "iteration 366, dc_loss: 0.01799076981842518, tv_loss: 0.018636304885149002\n",
      "iteration 367, dc_loss: 0.017973804846405983, tv_loss: 0.01863488182425499\n",
      "iteration 368, dc_loss: 0.01795722357928753, tv_loss: 0.018633108586072922\n",
      "iteration 369, dc_loss: 0.01794101484119892, tv_loss: 0.01863141916692257\n",
      "iteration 370, dc_loss: 0.017925141379237175, tv_loss: 0.018629826605319977\n",
      "iteration 371, dc_loss: 0.01790960691869259, tv_loss: 0.018627984449267387\n",
      "iteration 372, dc_loss: 0.017894381657242775, tv_loss: 0.018626438453793526\n",
      "iteration 373, dc_loss: 0.017879489809274673, tv_loss: 0.01862499862909317\n",
      "iteration 374, dc_loss: 0.017864933237433434, tv_loss: 0.018623199313879013\n",
      "iteration 375, dc_loss: 0.017850708216428757, tv_loss: 0.018621115013957024\n",
      "iteration 376, dc_loss: 0.017836805433034897, tv_loss: 0.018619555979967117\n",
      "iteration 377, dc_loss: 0.017823193222284317, tv_loss: 0.018617894500494003\n",
      "iteration 378, dc_loss: 0.017809879034757614, tv_loss: 0.01861584186553955\n",
      "iteration 379, dc_loss: 0.017796829342842102, tv_loss: 0.01861405000090599\n",
      "iteration 380, dc_loss: 0.01778407394886017, tv_loss: 0.01861228235065937\n",
      "iteration 381, dc_loss: 0.01777159608900547, tv_loss: 0.018610414117574692\n",
      "iteration 382, dc_loss: 0.01775936968624592, tv_loss: 0.01860847696661949\n",
      "iteration 383, dc_loss: 0.01774737797677517, tv_loss: 0.018606485798954964\n",
      "iteration 384, dc_loss: 0.017735609784722328, tv_loss: 0.01860463246703148\n",
      "iteration 385, dc_loss: 0.017724089324474335, tv_loss: 0.018602702766656876\n",
      "iteration 386, dc_loss: 0.017712809145450592, tv_loss: 0.018600821495056152\n",
      "iteration 387, dc_loss: 0.017701763659715652, tv_loss: 0.018598895519971848\n",
      "iteration 388, dc_loss: 0.017690956592559814, tv_loss: 0.018596846610307693\n",
      "iteration 389, dc_loss: 0.01768038235604763, tv_loss: 0.018594931811094284\n",
      "iteration 390, dc_loss: 0.01767003908753395, tv_loss: 0.018592923879623413\n",
      "iteration 391, dc_loss: 0.01765989512205124, tv_loss: 0.0185907743871212\n",
      "iteration 392, dc_loss: 0.017649967223405838, tv_loss: 0.018588926643133163\n",
      "iteration 393, dc_loss: 0.0176402498036623, tv_loss: 0.018587056547403336\n",
      "iteration 394, dc_loss: 0.01763075217604637, tv_loss: 0.01858469843864441\n",
      "iteration 395, dc_loss: 0.017621435225009918, tv_loss: 0.018582601100206375\n",
      "iteration 396, dc_loss: 0.017612289637327194, tv_loss: 0.018580691888928413\n",
      "iteration 397, dc_loss: 0.017603283748030663, tv_loss: 0.01857868582010269\n",
      "iteration 398, dc_loss: 0.017594486474990845, tv_loss: 0.01857657916843891\n",
      "iteration 399, dc_loss: 0.01758587546646595, tv_loss: 0.018574481830000877\n",
      "iteration 400, dc_loss: 0.01757742464542389, tv_loss: 0.018572330474853516\n",
      "iteration 401, dc_loss: 0.01756913587450981, tv_loss: 0.01857035420835018\n",
      "iteration 402, dc_loss: 0.01756102219223976, tv_loss: 0.01856830157339573\n",
      "iteration 403, dc_loss: 0.017553089186549187, tv_loss: 0.01856612227857113\n",
      "iteration 404, dc_loss: 0.0175453033298254, tv_loss: 0.01856406405568123\n",
      "iteration 405, dc_loss: 0.017537642270326614, tv_loss: 0.018561899662017822\n",
      "iteration 406, dc_loss: 0.017530124634504318, tv_loss: 0.018559813499450684\n",
      "iteration 407, dc_loss: 0.017522767186164856, tv_loss: 0.0185577180236578\n",
      "iteration 408, dc_loss: 0.017515558749437332, tv_loss: 0.01855548843741417\n",
      "iteration 409, dc_loss: 0.017508480697870255, tv_loss: 0.018553128466010094\n",
      "iteration 410, dc_loss: 0.017501527443528175, tv_loss: 0.018551230430603027\n",
      "iteration 411, dc_loss: 0.01749471202492714, tv_loss: 0.01854912005364895\n",
      "iteration 412, dc_loss: 0.01748804748058319, tv_loss: 0.018546903505921364\n",
      "iteration 413, dc_loss: 0.017481504008173943, tv_loss: 0.018544670194387436\n",
      "iteration 414, dc_loss: 0.017475074157118797, tv_loss: 0.018542557954788208\n",
      "iteration 415, dc_loss: 0.01746874488890171, tv_loss: 0.018540477380156517\n",
      "iteration 416, dc_loss: 0.01746252179145813, tv_loss: 0.018538320437073708\n",
      "iteration 417, dc_loss: 0.0174564179033041, tv_loss: 0.018536128103733063\n",
      "iteration 418, dc_loss: 0.01745041087269783, tv_loss: 0.018534056842327118\n",
      "iteration 419, dc_loss: 0.017444534227252007, tv_loss: 0.018531808629631996\n",
      "iteration 420, dc_loss: 0.017438773065805435, tv_loss: 0.018529724329710007\n",
      "iteration 421, dc_loss: 0.01743311993777752, tv_loss: 0.018527567386627197\n",
      "iteration 422, dc_loss: 0.01742756925523281, tv_loss: 0.018525423482060432\n",
      "iteration 423, dc_loss: 0.017422117292881012, tv_loss: 0.01852301135659218\n",
      "iteration 424, dc_loss: 0.017416782677173615, tv_loss: 0.018520772457122803\n",
      "iteration 425, dc_loss: 0.017411569133400917, tv_loss: 0.018518546596169472\n",
      "iteration 426, dc_loss: 0.01740643009543419, tv_loss: 0.018516262993216515\n",
      "iteration 427, dc_loss: 0.01740134507417679, tv_loss: 0.01851389929652214\n",
      "iteration 428, dc_loss: 0.017396308481693268, tv_loss: 0.018511902540922165\n",
      "iteration 429, dc_loss: 0.01739134080708027, tv_loss: 0.018509626388549805\n",
      "iteration 430, dc_loss: 0.017386477440595627, tv_loss: 0.018507473170757294\n",
      "iteration 431, dc_loss: 0.01738172583281994, tv_loss: 0.01850532554090023\n",
      "iteration 432, dc_loss: 0.017377110198140144, tv_loss: 0.01850307360291481\n",
      "iteration 433, dc_loss: 0.017372580245137215, tv_loss: 0.01850063167512417\n",
      "iteration 434, dc_loss: 0.017368100583553314, tv_loss: 0.01849829964339733\n",
      "iteration 435, dc_loss: 0.01736365258693695, tv_loss: 0.018495960161089897\n",
      "iteration 436, dc_loss: 0.017359266057610512, tv_loss: 0.0184938944876194\n",
      "iteration 437, dc_loss: 0.017354976385831833, tv_loss: 0.018491700291633606\n",
      "iteration 438, dc_loss: 0.017350735142827034, tv_loss: 0.0184892900288105\n",
      "iteration 439, dc_loss: 0.017346592620015144, tv_loss: 0.01848697103559971\n",
      "iteration 440, dc_loss: 0.01734253019094467, tv_loss: 0.018484676256775856\n",
      "iteration 441, dc_loss: 0.01733853667974472, tv_loss: 0.0184823889285326\n",
      "iteration 442, dc_loss: 0.01733461208641529, tv_loss: 0.018480174243450165\n",
      "iteration 443, dc_loss: 0.017330745235085487, tv_loss: 0.018477847799658775\n",
      "iteration 444, dc_loss: 0.01732693985104561, tv_loss: 0.018475491553544998\n",
      "iteration 445, dc_loss: 0.01732315681874752, tv_loss: 0.01847309246659279\n",
      "iteration 446, dc_loss: 0.017319414764642715, tv_loss: 0.018470972776412964\n",
      "iteration 447, dc_loss: 0.01731574535369873, tv_loss: 0.018468761816620827\n",
      "iteration 448, dc_loss: 0.017312142997980118, tv_loss: 0.018466439098119736\n",
      "iteration 449, dc_loss: 0.017308605834841728, tv_loss: 0.01846437342464924\n",
      "iteration 450, dc_loss: 0.017305098474025726, tv_loss: 0.01846197433769703\n",
      "iteration 451, dc_loss: 0.017301669344305992, tv_loss: 0.0184597447514534\n",
      "iteration 452, dc_loss: 0.01729828491806984, tv_loss: 0.018457436934113503\n",
      "iteration 453, dc_loss: 0.01729494519531727, tv_loss: 0.01845521293580532\n",
      "iteration 454, dc_loss: 0.01729164645075798, tv_loss: 0.01845303550362587\n",
      "iteration 455, dc_loss: 0.017288407310843468, tv_loss: 0.01845076121389866\n",
      "iteration 456, dc_loss: 0.017285214737057686, tv_loss: 0.01844845525920391\n",
      "iteration 457, dc_loss: 0.017282066866755486, tv_loss: 0.018446218222379684\n",
      "iteration 458, dc_loss: 0.01727895997464657, tv_loss: 0.018443992361426353\n",
      "iteration 459, dc_loss: 0.01727590709924698, tv_loss: 0.01844167709350586\n",
      "iteration 460, dc_loss: 0.017272889614105225, tv_loss: 0.01843930408358574\n",
      "iteration 461, dc_loss: 0.017269937321543694, tv_loss: 0.018437044695019722\n",
      "iteration 462, dc_loss: 0.017267055809497833, tv_loss: 0.018434815108776093\n",
      "iteration 463, dc_loss: 0.017264215275645256, tv_loss: 0.01843259483575821\n",
      "iteration 464, dc_loss: 0.01726134866476059, tv_loss: 0.01843034289777279\n",
      "iteration 465, dc_loss: 0.01725851558148861, tv_loss: 0.01842779852449894\n",
      "iteration 466, dc_loss: 0.017255723476409912, tv_loss: 0.0184257160872221\n",
      "iteration 467, dc_loss: 0.017252959311008453, tv_loss: 0.018423717468976974\n",
      "iteration 468, dc_loss: 0.01725025475025177, tv_loss: 0.01842132955789566\n",
      "iteration 469, dc_loss: 0.017247598618268967, tv_loss: 0.018419058993458748\n",
      "iteration 470, dc_loss: 0.01724497228860855, tv_loss: 0.018416782841086388\n",
      "iteration 471, dc_loss: 0.017242349684238434, tv_loss: 0.018414627760648727\n",
      "iteration 472, dc_loss: 0.017239751294255257, tv_loss: 0.018412496894598007\n",
      "iteration 473, dc_loss: 0.017237195745110512, tv_loss: 0.018410172313451767\n",
      "iteration 474, dc_loss: 0.01723470166325569, tv_loss: 0.01840795762836933\n",
      "iteration 475, dc_loss: 0.017232248559594154, tv_loss: 0.018405772745609283\n",
      "iteration 476, dc_loss: 0.01722983457148075, tv_loss: 0.01840348169207573\n",
      "iteration 477, dc_loss: 0.017227446660399437, tv_loss: 0.01840103045105934\n",
      "iteration 478, dc_loss: 0.017225077375769615, tv_loss: 0.018398817628622055\n",
      "iteration 479, dc_loss: 0.017222747206687927, tv_loss: 0.01839670166373253\n",
      "iteration 480, dc_loss: 0.017220426350831985, tv_loss: 0.018394332379102707\n",
      "iteration 481, dc_loss: 0.01721814274787903, tv_loss: 0.018392201513051987\n",
      "iteration 482, dc_loss: 0.01721588708460331, tv_loss: 0.018390070647001266\n",
      "iteration 483, dc_loss: 0.017213666811585426, tv_loss: 0.01838773488998413\n",
      "iteration 484, dc_loss: 0.01721148006618023, tv_loss: 0.01838543638586998\n",
      "iteration 485, dc_loss: 0.017209310084581375, tv_loss: 0.018383238464593887\n",
      "iteration 486, dc_loss: 0.01720716804265976, tv_loss: 0.018380900844931602\n",
      "iteration 487, dc_loss: 0.017205065116286278, tv_loss: 0.01837865635752678\n",
      "iteration 488, dc_loss: 0.017202982679009438, tv_loss: 0.018376611173152924\n",
      "iteration 489, dc_loss: 0.01720092073082924, tv_loss: 0.01837422139942646\n",
      "iteration 490, dc_loss: 0.017198875546455383, tv_loss: 0.018371976912021637\n",
      "iteration 491, dc_loss: 0.017196841537952423, tv_loss: 0.018369795754551888\n",
      "iteration 492, dc_loss: 0.017194796353578568, tv_loss: 0.018367605283856392\n",
      "iteration 493, dc_loss: 0.01719275303184986, tv_loss: 0.018365412950515747\n",
      "iteration 494, dc_loss: 0.017190754413604736, tv_loss: 0.01836344040930271\n",
      "iteration 495, dc_loss: 0.017188794910907745, tv_loss: 0.01836119405925274\n",
      "iteration 496, dc_loss: 0.017186881974339485, tv_loss: 0.018358903005719185\n",
      "iteration 497, dc_loss: 0.017185019329190254, tv_loss: 0.018356679007411003\n",
      "iteration 498, dc_loss: 0.017183175310492516, tv_loss: 0.01835438422858715\n",
      "iteration 499, dc_loss: 0.017181333154439926, tv_loss: 0.01835225149989128\n",
      "iteration 500, dc_loss: 0.017179502174258232, tv_loss: 0.018349995836615562\n",
      "iteration 501, dc_loss: 0.017177673056721687, tv_loss: 0.01834775134921074\n",
      "iteration 502, dc_loss: 0.017175860702991486, tv_loss: 0.0183455478399992\n",
      "iteration 503, dc_loss: 0.017174076288938522, tv_loss: 0.018343491479754448\n",
      "iteration 504, dc_loss: 0.017172297462821007, tv_loss: 0.018341273069381714\n",
      "iteration 505, dc_loss: 0.017170537263154984, tv_loss: 0.01833910308778286\n",
      "iteration 506, dc_loss: 0.01716878078877926, tv_loss: 0.018336957320570946\n",
      "iteration 507, dc_loss: 0.017167041078209877, tv_loss: 0.018334832042455673\n",
      "iteration 508, dc_loss: 0.017165355384349823, tv_loss: 0.01833268627524376\n",
      "iteration 509, dc_loss: 0.017163708806037903, tv_loss: 0.018330520018935204\n",
      "iteration 510, dc_loss: 0.017162064090371132, tv_loss: 0.018328171223402023\n",
      "iteration 511, dc_loss: 0.017160415649414062, tv_loss: 0.01832602359354496\n",
      "iteration 512, dc_loss: 0.01715877279639244, tv_loss: 0.01832379773259163\n",
      "iteration 513, dc_loss: 0.017157146707177162, tv_loss: 0.018321819603443146\n",
      "iteration 514, dc_loss: 0.017155537381768227, tv_loss: 0.018319660797715187\n",
      "iteration 515, dc_loss: 0.017153944820165634, tv_loss: 0.018317468464374542\n",
      "iteration 516, dc_loss: 0.01715235225856304, tv_loss: 0.01831541396677494\n",
      "iteration 517, dc_loss: 0.01715078391134739, tv_loss: 0.01831328123807907\n",
      "iteration 518, dc_loss: 0.01714925654232502, tv_loss: 0.01831125095486641\n",
      "iteration 519, dc_loss: 0.017147740349173546, tv_loss: 0.01830906979739666\n",
      "iteration 520, dc_loss: 0.017146214842796326, tv_loss: 0.018306804820895195\n",
      "iteration 521, dc_loss: 0.017144709825515747, tv_loss: 0.018304815515875816\n",
      "iteration 522, dc_loss: 0.01714322157204151, tv_loss: 0.018302544951438904\n",
      "iteration 523, dc_loss: 0.01714175194501877, tv_loss: 0.018300365656614304\n",
      "iteration 524, dc_loss: 0.01714032143354416, tv_loss: 0.018298260867595673\n",
      "iteration 525, dc_loss: 0.01713891699910164, tv_loss: 0.01829616352915764\n",
      "iteration 526, dc_loss: 0.017137523740530014, tv_loss: 0.018293946981430054\n",
      "iteration 527, dc_loss: 0.017136115580797195, tv_loss: 0.01829187013208866\n",
      "iteration 528, dc_loss: 0.01713470183312893, tv_loss: 0.018289759755134583\n",
      "iteration 529, dc_loss: 0.01713329553604126, tv_loss: 0.01828761398792267\n",
      "iteration 530, dc_loss: 0.017131896689534187, tv_loss: 0.018285511061549187\n",
      "iteration 531, dc_loss: 0.017130503430962563, tv_loss: 0.018283503130078316\n",
      "iteration 532, dc_loss: 0.01712910644710064, tv_loss: 0.018281394615769386\n",
      "iteration 533, dc_loss: 0.017127761617302895, tv_loss: 0.018279211595654488\n",
      "iteration 534, dc_loss: 0.017126455903053284, tv_loss: 0.01827717013657093\n",
      "iteration 535, dc_loss: 0.017125124111771584, tv_loss: 0.0182751826941967\n",
      "iteration 536, dc_loss: 0.017123809084296227, tv_loss: 0.018273107707500458\n",
      "iteration 537, dc_loss: 0.017122510820627213, tv_loss: 0.018270868808031082\n",
      "iteration 538, dc_loss: 0.017121199518442154, tv_loss: 0.018268942832946777\n",
      "iteration 539, dc_loss: 0.017119908705353737, tv_loss: 0.018266906961798668\n",
      "iteration 540, dc_loss: 0.01711861416697502, tv_loss: 0.018264837563037872\n",
      "iteration 541, dc_loss: 0.017117328941822052, tv_loss: 0.018262775614857674\n",
      "iteration 542, dc_loss: 0.017116045579314232, tv_loss: 0.018260708078742027\n",
      "iteration 543, dc_loss: 0.017114803194999695, tv_loss: 0.018258867785334587\n",
      "iteration 544, dc_loss: 0.017113564535975456, tv_loss: 0.018256787210702896\n",
      "iteration 545, dc_loss: 0.017112325876951218, tv_loss: 0.018254518508911133\n",
      "iteration 546, dc_loss: 0.01711110770702362, tv_loss: 0.018252616748213768\n",
      "iteration 547, dc_loss: 0.017109932377934456, tv_loss: 0.018250614404678345\n",
      "iteration 548, dc_loss: 0.017108771950006485, tv_loss: 0.018248332664370537\n",
      "iteration 549, dc_loss: 0.017107613384723663, tv_loss: 0.018246283754706383\n",
      "iteration 550, dc_loss: 0.01710641011595726, tv_loss: 0.018244333565235138\n",
      "iteration 551, dc_loss: 0.017105182632803917, tv_loss: 0.01824251376092434\n",
      "iteration 552, dc_loss: 0.017103932797908783, tv_loss: 0.018240327015519142\n",
      "iteration 553, dc_loss: 0.017102733254432678, tv_loss: 0.018238414078950882\n",
      "iteration 554, dc_loss: 0.017101595178246498, tv_loss: 0.0182364322245121\n",
      "iteration 555, dc_loss: 0.017100494354963303, tv_loss: 0.018234336748719215\n",
      "iteration 556, dc_loss: 0.017099402844905853, tv_loss: 0.018232354894280434\n",
      "iteration 557, dc_loss: 0.01709829643368721, tv_loss: 0.018230194225907326\n",
      "iteration 558, dc_loss: 0.017097191885113716, tv_loss: 0.018228190019726753\n",
      "iteration 559, dc_loss: 0.017096074298024178, tv_loss: 0.018226435407996178\n",
      "iteration 560, dc_loss: 0.017094960436224937, tv_loss: 0.018224449828267097\n",
      "iteration 561, dc_loss: 0.01709383726119995, tv_loss: 0.018222207203507423\n",
      "iteration 562, dc_loss: 0.017092710360884666, tv_loss: 0.01822040230035782\n",
      "iteration 563, dc_loss: 0.017091600224375725, tv_loss: 0.018218470737338066\n",
      "iteration 564, dc_loss: 0.017090504989027977, tv_loss: 0.018216505646705627\n",
      "iteration 565, dc_loss: 0.01708943210542202, tv_loss: 0.018214581534266472\n",
      "iteration 566, dc_loss: 0.0170883908867836, tv_loss: 0.018212659284472466\n",
      "iteration 567, dc_loss: 0.017087368294596672, tv_loss: 0.01821056753396988\n",
      "iteration 568, dc_loss: 0.01708635501563549, tv_loss: 0.018208829686045647\n",
      "iteration 569, dc_loss: 0.017085323110222816, tv_loss: 0.018206855282187462\n",
      "iteration 570, dc_loss: 0.0170842744410038, tv_loss: 0.018204867839813232\n",
      "iteration 571, dc_loss: 0.01708325184881687, tv_loss: 0.01820296049118042\n",
      "iteration 572, dc_loss: 0.01708226092159748, tv_loss: 0.0182011891156435\n",
      "iteration 573, dc_loss: 0.017081258818507195, tv_loss: 0.01819908246397972\n",
      "iteration 574, dc_loss: 0.017080238088965416, tv_loss: 0.018197057768702507\n",
      "iteration 575, dc_loss: 0.01707920990884304, tv_loss: 0.018195275217294693\n",
      "iteration 576, dc_loss: 0.017078204080462456, tv_loss: 0.018193338066339493\n",
      "iteration 577, dc_loss: 0.017077190801501274, tv_loss: 0.018191417679190636\n",
      "iteration 578, dc_loss: 0.01707618497312069, tv_loss: 0.018189523369073868\n",
      "iteration 579, dc_loss: 0.01707519218325615, tv_loss: 0.018187573179602623\n",
      "iteration 580, dc_loss: 0.01707419566810131, tv_loss: 0.01818591170012951\n",
      "iteration 581, dc_loss: 0.017073247581720352, tv_loss: 0.018183842301368713\n",
      "iteration 582, dc_loss: 0.017072319984436035, tv_loss: 0.018181925639510155\n",
      "iteration 583, dc_loss: 0.01707141473889351, tv_loss: 0.018180208280682564\n",
      "iteration 584, dc_loss: 0.017070481553673744, tv_loss: 0.018178116530179977\n",
      "iteration 585, dc_loss: 0.017069542780518532, tv_loss: 0.018176062032580376\n",
      "iteration 586, dc_loss: 0.01706862449645996, tv_loss: 0.0181743111461401\n",
      "iteration 587, dc_loss: 0.017067696899175644, tv_loss: 0.018172506242990494\n",
      "iteration 588, dc_loss: 0.01706673577427864, tv_loss: 0.018170645460486412\n",
      "iteration 589, dc_loss: 0.017065733671188354, tv_loss: 0.018168780952692032\n",
      "iteration 590, dc_loss: 0.01706472411751747, tv_loss: 0.01816701889038086\n",
      "iteration 591, dc_loss: 0.017063766717910767, tv_loss: 0.018165353685617447\n",
      "iteration 592, dc_loss: 0.017062893137335777, tv_loss: 0.018163496628403664\n",
      "iteration 593, dc_loss: 0.01706203818321228, tv_loss: 0.018161455169320107\n",
      "iteration 594, dc_loss: 0.017061147838830948, tv_loss: 0.018159540370106697\n",
      "iteration 595, dc_loss: 0.01706025004386902, tv_loss: 0.01815774478018284\n",
      "iteration 596, dc_loss: 0.017059382051229477, tv_loss: 0.01815585419535637\n",
      "iteration 597, dc_loss: 0.01705848053097725, tv_loss: 0.018154125660657883\n",
      "iteration 598, dc_loss: 0.017057573422789574, tv_loss: 0.018152164295315742\n",
      "iteration 599, dc_loss: 0.017056673765182495, tv_loss: 0.018150363117456436\n",
      "iteration 600, dc_loss: 0.0170558113604784, tv_loss: 0.018148791044950485\n",
      "iteration 601, dc_loss: 0.017054984346032143, tv_loss: 0.018146870657801628\n",
      "iteration 602, dc_loss: 0.017054138705134392, tv_loss: 0.01814490370452404\n",
      "iteration 603, dc_loss: 0.0170532688498497, tv_loss: 0.018143363296985626\n",
      "iteration 604, dc_loss: 0.01705242693424225, tv_loss: 0.018141478300094604\n",
      "iteration 605, dc_loss: 0.017051633447408676, tv_loss: 0.018139464780688286\n",
      "iteration 606, dc_loss: 0.017050806432962418, tv_loss: 0.018137730658054352\n",
      "iteration 607, dc_loss: 0.017049990594387054, tv_loss: 0.018135974183678627\n",
      "iteration 608, dc_loss: 0.017049171030521393, tv_loss: 0.018134063109755516\n",
      "iteration 609, dc_loss: 0.017048325389623642, tv_loss: 0.018132459372282028\n",
      "iteration 610, dc_loss: 0.0170474573969841, tv_loss: 0.018130730837583542\n",
      "iteration 611, dc_loss: 0.01704655960202217, tv_loss: 0.018129032105207443\n",
      "iteration 612, dc_loss: 0.017045680433511734, tv_loss: 0.018127398565411568\n",
      "iteration 613, dc_loss: 0.01704486645758152, tv_loss: 0.018125800415873528\n",
      "iteration 614, dc_loss: 0.0170440711081028, tv_loss: 0.018123798072338104\n",
      "iteration 615, dc_loss: 0.017043285071849823, tv_loss: 0.01812201552093029\n",
      "iteration 616, dc_loss: 0.017042504623532295, tv_loss: 0.018120398744940758\n",
      "iteration 617, dc_loss: 0.017041726037859917, tv_loss: 0.01811852864921093\n",
      "iteration 618, dc_loss: 0.017040934413671494, tv_loss: 0.018116796389222145\n",
      "iteration 619, dc_loss: 0.01704014278948307, tv_loss: 0.018115146085619926\n",
      "iteration 620, dc_loss: 0.01703934371471405, tv_loss: 0.01811346784234047\n",
      "iteration 621, dc_loss: 0.017038557678461075, tv_loss: 0.018111560493707657\n",
      "iteration 622, dc_loss: 0.01703776977956295, tv_loss: 0.018109923228621483\n",
      "iteration 623, dc_loss: 0.017036987468600273, tv_loss: 0.01810828596353531\n",
      "iteration 624, dc_loss: 0.017036197707057, tv_loss: 0.018106579780578613\n",
      "iteration 625, dc_loss: 0.01703541912138462, tv_loss: 0.01810477487742901\n",
      "iteration 626, dc_loss: 0.01703464612364769, tv_loss: 0.018103208392858505\n",
      "iteration 627, dc_loss: 0.017033899202942848, tv_loss: 0.018101630732417107\n",
      "iteration 628, dc_loss: 0.01703316904604435, tv_loss: 0.018099769949913025\n",
      "iteration 629, dc_loss: 0.01703246869146824, tv_loss: 0.018097996711730957\n",
      "iteration 630, dc_loss: 0.017031755298376083, tv_loss: 0.01809631660580635\n",
      "iteration 631, dc_loss: 0.01703101210296154, tv_loss: 0.018094632774591446\n",
      "iteration 632, dc_loss: 0.017030270770192146, tv_loss: 0.01809295453131199\n",
      "iteration 633, dc_loss: 0.017029516398906708, tv_loss: 0.018091384321451187\n",
      "iteration 634, dc_loss: 0.017028773203492165, tv_loss: 0.018089640885591507\n",
      "iteration 635, dc_loss: 0.01702800579369068, tv_loss: 0.018088078126311302\n",
      "iteration 636, dc_loss: 0.017027266323566437, tv_loss: 0.01808643341064453\n",
      "iteration 637, dc_loss: 0.017026562243700027, tv_loss: 0.018084583804011345\n",
      "iteration 638, dc_loss: 0.017025835812091827, tv_loss: 0.018082916736602783\n",
      "iteration 639, dc_loss: 0.017025083303451538, tv_loss: 0.018081536516547203\n",
      "iteration 640, dc_loss: 0.017024360597133636, tv_loss: 0.018080132082104683\n",
      "iteration 641, dc_loss: 0.01702364906668663, tv_loss: 0.018078308552503586\n",
      "iteration 642, dc_loss: 0.01702294684946537, tv_loss: 0.01807648129761219\n",
      "iteration 643, dc_loss: 0.01702224276959896, tv_loss: 0.01807483099400997\n",
      "iteration 644, dc_loss: 0.017021572217345238, tv_loss: 0.018073247745633125\n",
      "iteration 645, dc_loss: 0.017020901665091515, tv_loss: 0.0180716160684824\n",
      "iteration 646, dc_loss: 0.01702023111283779, tv_loss: 0.018070006743073463\n",
      "iteration 647, dc_loss: 0.017019549384713173, tv_loss: 0.018068306148052216\n",
      "iteration 648, dc_loss: 0.0170188769698143, tv_loss: 0.018066667020320892\n",
      "iteration 649, dc_loss: 0.017018243670463562, tv_loss: 0.01806507259607315\n",
      "iteration 650, dc_loss: 0.01701759174466133, tv_loss: 0.01806335709989071\n",
      "iteration 651, dc_loss: 0.01701691560447216, tv_loss: 0.01806161180138588\n",
      "iteration 652, dc_loss: 0.017016198486089706, tv_loss: 0.01806013472378254\n",
      "iteration 653, dc_loss: 0.01701546646654606, tv_loss: 0.0180586576461792\n",
      "iteration 654, dc_loss: 0.017014751210808754, tv_loss: 0.018057003617286682\n",
      "iteration 655, dc_loss: 0.01701405830681324, tv_loss: 0.01805546134710312\n",
      "iteration 656, dc_loss: 0.017013413831591606, tv_loss: 0.018053928390145302\n",
      "iteration 657, dc_loss: 0.01701274886727333, tv_loss: 0.018052266910672188\n",
      "iteration 658, dc_loss: 0.01701209880411625, tv_loss: 0.018050776794552803\n",
      "iteration 659, dc_loss: 0.017011426389217377, tv_loss: 0.01804911345243454\n",
      "iteration 660, dc_loss: 0.017010768875479698, tv_loss: 0.018047574907541275\n",
      "iteration 661, dc_loss: 0.01701011136174202, tv_loss: 0.018045971170067787\n",
      "iteration 662, dc_loss: 0.017009472474455833, tv_loss: 0.018044358119368553\n",
      "iteration 663, dc_loss: 0.01700887270271778, tv_loss: 0.01804286241531372\n",
      "iteration 664, dc_loss: 0.01700824685394764, tv_loss: 0.018041260540485382\n",
      "iteration 665, dc_loss: 0.017007634043693542, tv_loss: 0.018039634451270103\n",
      "iteration 666, dc_loss: 0.01700701005756855, tv_loss: 0.018038025125861168\n",
      "iteration 667, dc_loss: 0.017006395384669304, tv_loss: 0.01803656294941902\n",
      "iteration 668, dc_loss: 0.017005756497383118, tv_loss: 0.018035026267170906\n",
      "iteration 669, dc_loss: 0.017005108296871185, tv_loss: 0.018033359199762344\n",
      "iteration 670, dc_loss: 0.017004461959004402, tv_loss: 0.018031803891062737\n",
      "iteration 671, dc_loss: 0.017003841698169708, tv_loss: 0.018030311912298203\n",
      "iteration 672, dc_loss: 0.017003195360302925, tv_loss: 0.018028883263468742\n",
      "iteration 673, dc_loss: 0.017002567648887634, tv_loss: 0.018027476966381073\n",
      "iteration 674, dc_loss: 0.017001936212182045, tv_loss: 0.018025821074843407\n",
      "iteration 675, dc_loss: 0.01700129173696041, tv_loss: 0.018024243414402008\n",
      "iteration 676, dc_loss: 0.01700061745941639, tv_loss: 0.018022898584604263\n",
      "iteration 677, dc_loss: 0.01699996367096901, tv_loss: 0.018021531403064728\n",
      "iteration 678, dc_loss: 0.016999343410134315, tv_loss: 0.018020033836364746\n",
      "iteration 679, dc_loss: 0.016998745501041412, tv_loss: 0.018018322065472603\n",
      "iteration 680, dc_loss: 0.0169981662184, tv_loss: 0.018016934394836426\n",
      "iteration 681, dc_loss: 0.016997607424855232, tv_loss: 0.018015533685684204\n",
      "iteration 682, dc_loss: 0.01699703186750412, tv_loss: 0.018013859167695045\n",
      "iteration 683, dc_loss: 0.016996487975120544, tv_loss: 0.01801222935318947\n",
      "iteration 684, dc_loss: 0.016995936632156372, tv_loss: 0.01801082119345665\n",
      "iteration 685, dc_loss: 0.016995379701256752, tv_loss: 0.018009355291724205\n",
      "iteration 686, dc_loss: 0.016994794830679893, tv_loss: 0.01800772175192833\n",
      "iteration 687, dc_loss: 0.016994213685393333, tv_loss: 0.018006308004260063\n",
      "iteration 688, dc_loss: 0.016993634402751923, tv_loss: 0.018004827201366425\n",
      "iteration 689, dc_loss: 0.01699303276836872, tv_loss: 0.0180034339427948\n",
      "iteration 690, dc_loss: 0.016992416232824326, tv_loss: 0.018002087250351906\n",
      "iteration 691, dc_loss: 0.01699180155992508, tv_loss: 0.01800069771707058\n",
      "iteration 692, dc_loss: 0.016991209238767624, tv_loss: 0.017999183386564255\n",
      "iteration 693, dc_loss: 0.01699061319231987, tv_loss: 0.01799764670431614\n",
      "iteration 694, dc_loss: 0.01699000597000122, tv_loss: 0.017996318638324738\n",
      "iteration 695, dc_loss: 0.01698942296206951, tv_loss: 0.017994897440075874\n",
      "iteration 696, dc_loss: 0.016988834366202354, tv_loss: 0.017993437126278877\n",
      "iteration 697, dc_loss: 0.016988253220915794, tv_loss: 0.017992088571190834\n",
      "iteration 698, dc_loss: 0.016987692564725876, tv_loss: 0.01799061708152294\n",
      "iteration 699, dc_loss: 0.01698712445795536, tv_loss: 0.01798909343779087\n",
      "iteration 700, dc_loss: 0.016986560076475143, tv_loss: 0.017987653613090515\n",
      "iteration 701, dc_loss: 0.016985999420285225, tv_loss: 0.017986247316002846\n",
      "iteration 702, dc_loss: 0.0169854573905468, tv_loss: 0.01798498071730137\n",
      "iteration 703, dc_loss: 0.016984928399324417, tv_loss: 0.017983360216021538\n",
      "iteration 704, dc_loss: 0.016984397545456886, tv_loss: 0.017981842160224915\n",
      "iteration 705, dc_loss: 0.01698385179042816, tv_loss: 0.017980486154556274\n",
      "iteration 706, dc_loss: 0.016983281821012497, tv_loss: 0.017979145050048828\n",
      "iteration 707, dc_loss: 0.01698272116482258, tv_loss: 0.017977697774767876\n",
      "iteration 708, dc_loss: 0.0169821884483099, tv_loss: 0.017976460978388786\n",
      "iteration 709, dc_loss: 0.016981659457087517, tv_loss: 0.01797509379684925\n",
      "iteration 710, dc_loss: 0.016981126740574837, tv_loss: 0.01797359436750412\n",
      "iteration 711, dc_loss: 0.016980605199933052, tv_loss: 0.017972081899642944\n",
      "iteration 712, dc_loss: 0.016980072483420372, tv_loss: 0.0179706159979105\n",
      "iteration 713, dc_loss: 0.01697954535484314, tv_loss: 0.01796920783817768\n",
      "iteration 714, dc_loss: 0.01697903499007225, tv_loss: 0.01796799898147583\n",
      "iteration 715, dc_loss: 0.01697852835059166, tv_loss: 0.017966562882065773\n",
      "iteration 716, dc_loss: 0.01697799749672413, tv_loss: 0.017964938655495644\n",
      "iteration 717, dc_loss: 0.0169774629175663, tv_loss: 0.017963631078600883\n",
      "iteration 718, dc_loss: 0.016976909711956978, tv_loss: 0.01796242967247963\n",
      "iteration 719, dc_loss: 0.016976328566670418, tv_loss: 0.0179610513150692\n",
      "iteration 720, dc_loss: 0.01697573810815811, tv_loss: 0.017959674820303917\n",
      "iteration 721, dc_loss: 0.016975168138742447, tv_loss: 0.01795836165547371\n",
      "iteration 722, dc_loss: 0.01697464846074581, tv_loss: 0.017957022413611412\n",
      "iteration 723, dc_loss: 0.01697414182126522, tv_loss: 0.017955489456653595\n",
      "iteration 724, dc_loss: 0.016973668709397316, tv_loss: 0.01795404590666294\n",
      "iteration 725, dc_loss: 0.01697319932281971, tv_loss: 0.017952721565961838\n",
      "iteration 726, dc_loss: 0.016972733661532402, tv_loss: 0.017951535061001778\n",
      "iteration 727, dc_loss: 0.01697222888469696, tv_loss: 0.01795000396668911\n",
      "iteration 728, dc_loss: 0.01697169803082943, tv_loss: 0.017948579043149948\n",
      "iteration 729, dc_loss: 0.016971159726381302, tv_loss: 0.01794748567044735\n",
      "iteration 730, dc_loss: 0.01697065494954586, tv_loss: 0.017946166917681694\n",
      "iteration 731, dc_loss: 0.016970131546258926, tv_loss: 0.01794472336769104\n",
      "iteration 732, dc_loss: 0.01696961186826229, tv_loss: 0.017943525686860085\n",
      "iteration 733, dc_loss: 0.01696910336613655, tv_loss: 0.017942002043128014\n",
      "iteration 734, dc_loss: 0.016968628391623497, tv_loss: 0.017940664663910866\n",
      "iteration 735, dc_loss: 0.016968172043561935, tv_loss: 0.01793939433991909\n",
      "iteration 736, dc_loss: 0.016967739909887314, tv_loss: 0.017937714233994484\n",
      "iteration 737, dc_loss: 0.016967270523309708, tv_loss: 0.017936425283551216\n",
      "iteration 738, dc_loss: 0.01696675270795822, tv_loss: 0.017935356125235558\n",
      "iteration 739, dc_loss: 0.016966231167316437, tv_loss: 0.017934098839759827\n",
      "iteration 740, dc_loss: 0.016965724527835846, tv_loss: 0.017932839691638947\n",
      "iteration 741, dc_loss: 0.016965217888355255, tv_loss: 0.017931459471583366\n",
      "iteration 742, dc_loss: 0.016964726150035858, tv_loss: 0.017930064350366592\n",
      "iteration 743, dc_loss: 0.016964294016361237, tv_loss: 0.017928840592503548\n",
      "iteration 744, dc_loss: 0.016963835805654526, tv_loss: 0.017927536740899086\n",
      "iteration 745, dc_loss: 0.01696336455643177, tv_loss: 0.017926214262843132\n",
      "iteration 746, dc_loss: 0.01696288026869297, tv_loss: 0.017924807965755463\n",
      "iteration 747, dc_loss: 0.01696234941482544, tv_loss: 0.017923541367053986\n",
      "iteration 748, dc_loss: 0.0169618371874094, tv_loss: 0.017922379076480865\n",
      "iteration 749, dc_loss: 0.016961373388767242, tv_loss: 0.017921073362231255\n",
      "iteration 750, dc_loss: 0.016960935667157173, tv_loss: 0.01791977696120739\n",
      "iteration 751, dc_loss: 0.01696045882999897, tv_loss: 0.017918411642313004\n",
      "iteration 752, dc_loss: 0.016959957778453827, tv_loss: 0.017917297780513763\n",
      "iteration 753, dc_loss: 0.016959453001618385, tv_loss: 0.017916034907102585\n",
      "iteration 754, dc_loss: 0.0169589351862669, tv_loss: 0.0179147869348526\n",
      "iteration 755, dc_loss: 0.016958437860012054, tv_loss: 0.017913637682795525\n",
      "iteration 756, dc_loss: 0.016957981511950493, tv_loss: 0.017912238836288452\n",
      "iteration 757, dc_loss: 0.016957540065050125, tv_loss: 0.017910977825522423\n",
      "iteration 758, dc_loss: 0.0169571153819561, tv_loss: 0.017909573391079903\n",
      "iteration 759, dc_loss: 0.016956700012087822, tv_loss: 0.017908381298184395\n",
      "iteration 760, dc_loss: 0.016956284642219543, tv_loss: 0.01790710538625717\n",
      "iteration 761, dc_loss: 0.01695585995912552, tv_loss: 0.01790575496852398\n",
      "iteration 762, dc_loss: 0.016955412924289703, tv_loss: 0.017904484644532204\n",
      "iteration 763, dc_loss: 0.016954954713582993, tv_loss: 0.017903286963701248\n",
      "iteration 764, dc_loss: 0.016954481601715088, tv_loss: 0.01790199987590313\n",
      "iteration 765, dc_loss: 0.01695399358868599, tv_loss: 0.017900841310620308\n",
      "iteration 766, dc_loss: 0.01695350743830204, tv_loss: 0.0178997665643692\n",
      "iteration 767, dc_loss: 0.016953011974692345, tv_loss: 0.01789853349328041\n",
      "iteration 768, dc_loss: 0.016952529549598694, tv_loss: 0.01789727807044983\n",
      "iteration 769, dc_loss: 0.01695207692682743, tv_loss: 0.01789615862071514\n",
      "iteration 770, dc_loss: 0.01695164479315281, tv_loss: 0.017894864082336426\n",
      "iteration 771, dc_loss: 0.016951195895671844, tv_loss: 0.017893612384796143\n",
      "iteration 772, dc_loss: 0.016950763761997223, tv_loss: 0.017892474308609962\n",
      "iteration 773, dc_loss: 0.016950348392128944, tv_loss: 0.017891116440296173\n",
      "iteration 774, dc_loss: 0.016949927434325218, tv_loss: 0.017889779061079025\n",
      "iteration 775, dc_loss: 0.016949493438005447, tv_loss: 0.017888586968183517\n",
      "iteration 776, dc_loss: 0.016949042677879333, tv_loss: 0.01788754016160965\n",
      "iteration 777, dc_loss: 0.016948554664850235, tv_loss: 0.017886387184262276\n",
      "iteration 778, dc_loss: 0.016948066651821136, tv_loss: 0.017885079607367516\n",
      "iteration 779, dc_loss: 0.016947638243436813, tv_loss: 0.017883874475955963\n",
      "iteration 780, dc_loss: 0.016947226598858833, tv_loss: 0.017882823944091797\n",
      "iteration 781, dc_loss: 0.016946841031312943, tv_loss: 0.01788145862519741\n",
      "iteration 782, dc_loss: 0.016946492716670036, tv_loss: 0.017880072817206383\n",
      "iteration 783, dc_loss: 0.01694612391293049, tv_loss: 0.01787886582314968\n",
      "iteration 784, dc_loss: 0.016945717856287956, tv_loss: 0.017877692356705666\n",
      "iteration 785, dc_loss: 0.016945291310548782, tv_loss: 0.01787652261555195\n",
      "iteration 786, dc_loss: 0.01694483868777752, tv_loss: 0.01787540316581726\n",
      "iteration 787, dc_loss: 0.016944382339715958, tv_loss: 0.017873983830213547\n",
      "iteration 788, dc_loss: 0.016943907365202904, tv_loss: 0.017873022705316544\n",
      "iteration 789, dc_loss: 0.01694343611598015, tv_loss: 0.017871873453259468\n",
      "iteration 790, dc_loss: 0.016942979767918587, tv_loss: 0.01787070371210575\n",
      "iteration 791, dc_loss: 0.01694256067276001, tv_loss: 0.0178694948554039\n",
      "iteration 792, dc_loss: 0.01694214902818203, tv_loss: 0.01786835305392742\n",
      "iteration 793, dc_loss: 0.016941772773861885, tv_loss: 0.01786717399954796\n",
      "iteration 794, dc_loss: 0.016941359266638756, tv_loss: 0.017865929752588272\n",
      "iteration 795, dc_loss: 0.01694093644618988, tv_loss: 0.017864873632788658\n",
      "iteration 796, dc_loss: 0.016940515488386154, tv_loss: 0.017863640561699867\n",
      "iteration 797, dc_loss: 0.016940109431743622, tv_loss: 0.017862459644675255\n",
      "iteration 798, dc_loss: 0.01693970151245594, tv_loss: 0.017861351370811462\n",
      "iteration 799, dc_loss: 0.016939295455813408, tv_loss: 0.017860079184174538\n",
      "iteration 800, dc_loss: 0.01693887822329998, tv_loss: 0.017858879640698433\n",
      "iteration 801, dc_loss: 0.016938457265496254, tv_loss: 0.017857903614640236\n",
      "iteration 802, dc_loss: 0.016938049346208572, tv_loss: 0.017856765538454056\n",
      "iteration 803, dc_loss: 0.01693764515221119, tv_loss: 0.017855554819107056\n",
      "iteration 804, dc_loss: 0.016937246546149254, tv_loss: 0.01785447634756565\n",
      "iteration 805, dc_loss: 0.016936859115958214, tv_loss: 0.01785314455628395\n",
      "iteration 806, dc_loss: 0.01693645864725113, tv_loss: 0.017852121964097023\n",
      "iteration 807, dc_loss: 0.016936056315898895, tv_loss: 0.017851028591394424\n",
      "iteration 808, dc_loss: 0.01693565584719181, tv_loss: 0.01784980483353138\n",
      "iteration 809, dc_loss: 0.016935277730226517, tv_loss: 0.017848718911409378\n",
      "iteration 810, dc_loss: 0.016934888437390327, tv_loss: 0.017847660928964615\n",
      "iteration 811, dc_loss: 0.016934502869844437, tv_loss: 0.01784655451774597\n",
      "iteration 812, dc_loss: 0.016934087499976158, tv_loss: 0.017845438793301582\n",
      "iteration 813, dc_loss: 0.016933690756559372, tv_loss: 0.017844170331954956\n",
      "iteration 814, dc_loss: 0.016933299601078033, tv_loss: 0.017843058332800865\n",
      "iteration 815, dc_loss: 0.01693291775882244, tv_loss: 0.017842011526226997\n",
      "iteration 816, dc_loss: 0.016932571306824684, tv_loss: 0.017840750515460968\n",
      "iteration 817, dc_loss: 0.01693219505250454, tv_loss: 0.017839603126049042\n",
      "iteration 818, dc_loss: 0.01693178340792656, tv_loss: 0.017838571220636368\n",
      "iteration 819, dc_loss: 0.016931330785155296, tv_loss: 0.017837490886449814\n",
      "iteration 820, dc_loss: 0.01693090796470642, tv_loss: 0.01783628575503826\n",
      "iteration 821, dc_loss: 0.016930483281612396, tv_loss: 0.017835335806012154\n",
      "iteration 822, dc_loss: 0.01693008281290531, tv_loss: 0.017834318801760674\n",
      "iteration 823, dc_loss: 0.01692969910800457, tv_loss: 0.017833132296800613\n",
      "iteration 824, dc_loss: 0.016929322853684425, tv_loss: 0.017831889912486076\n",
      "iteration 825, dc_loss: 0.016928954049944878, tv_loss: 0.017830828204751015\n",
      "iteration 826, dc_loss: 0.016928577795624733, tv_loss: 0.017829880118370056\n",
      "iteration 827, dc_loss: 0.016928186640143394, tv_loss: 0.01782887615263462\n",
      "iteration 828, dc_loss: 0.016927797347307205, tv_loss: 0.01782762072980404\n",
      "iteration 829, dc_loss: 0.01692742481827736, tv_loss: 0.017826467752456665\n",
      "iteration 830, dc_loss: 0.016927039250731468, tv_loss: 0.017825504764914513\n",
      "iteration 831, dc_loss: 0.016926657408475876, tv_loss: 0.017824457958340645\n",
      "iteration 832, dc_loss: 0.016926299780607224, tv_loss: 0.017823239788413048\n",
      "iteration 833, dc_loss: 0.0169259961694479, tv_loss: 0.017821993678808212\n",
      "iteration 834, dc_loss: 0.01692568138241768, tv_loss: 0.01782088167965412\n",
      "iteration 835, dc_loss: 0.016925351694226265, tv_loss: 0.017819928005337715\n",
      "iteration 836, dc_loss: 0.01692500151693821, tv_loss: 0.017818909138441086\n",
      "iteration 837, dc_loss: 0.016924617812037468, tv_loss: 0.01781771518290043\n",
      "iteration 838, dc_loss: 0.016924215480685234, tv_loss: 0.017816560342907906\n",
      "iteration 839, dc_loss: 0.016923805698752403, tv_loss: 0.017815707251429558\n",
      "iteration 840, dc_loss: 0.016923369839787483, tv_loss: 0.017814775928854942\n",
      "iteration 841, dc_loss: 0.016922950744628906, tv_loss: 0.01781371235847473\n",
      "iteration 842, dc_loss: 0.016922589391469955, tv_loss: 0.01781255379319191\n",
      "iteration 843, dc_loss: 0.016922252252697945, tv_loss: 0.01781160570681095\n",
      "iteration 844, dc_loss: 0.016921915113925934, tv_loss: 0.01781061664223671\n",
      "iteration 845, dc_loss: 0.01692158915102482, tv_loss: 0.01780940778553486\n",
      "iteration 846, dc_loss: 0.0169212706387043, tv_loss: 0.0178082138299942\n",
      "iteration 847, dc_loss: 0.016920946538448334, tv_loss: 0.017807135358452797\n",
      "iteration 848, dc_loss: 0.016920579597353935, tv_loss: 0.017806142568588257\n",
      "iteration 849, dc_loss: 0.016920195892453194, tv_loss: 0.01780517026782036\n",
      "iteration 850, dc_loss: 0.016919825226068497, tv_loss: 0.01780407316982746\n",
      "iteration 851, dc_loss: 0.01691947877407074, tv_loss: 0.017803054302930832\n",
      "iteration 852, dc_loss: 0.01691914163529873, tv_loss: 0.01780194230377674\n",
      "iteration 853, dc_loss: 0.01691877469420433, tv_loss: 0.017800835892558098\n",
      "iteration 854, dc_loss: 0.016918383538722992, tv_loss: 0.017799891531467438\n",
      "iteration 855, dc_loss: 0.016918016597628593, tv_loss: 0.017798984423279762\n",
      "iteration 856, dc_loss: 0.0169176384806633, tv_loss: 0.017797935754060745\n",
      "iteration 857, dc_loss: 0.01691722683608532, tv_loss: 0.017796840518712997\n",
      "iteration 858, dc_loss: 0.01691686548292637, tv_loss: 0.017795974388718605\n",
      "iteration 859, dc_loss: 0.0169165451079607, tv_loss: 0.01779492385685444\n",
      "iteration 860, dc_loss: 0.01691622845828533, tv_loss: 0.01779371313750744\n",
      "iteration 861, dc_loss: 0.016915922984480858, tv_loss: 0.017792731523513794\n",
      "iteration 862, dc_loss: 0.016915613785386086, tv_loss: 0.017791759222745895\n",
      "iteration 863, dc_loss: 0.016915280371904373, tv_loss: 0.017790621146559715\n",
      "iteration 864, dc_loss: 0.016914965584874153, tv_loss: 0.017789442092180252\n",
      "iteration 865, dc_loss: 0.016914622858166695, tv_loss: 0.017788497731089592\n",
      "iteration 866, dc_loss: 0.016914252191781998, tv_loss: 0.017787596210837364\n",
      "iteration 867, dc_loss: 0.016913870349526405, tv_loss: 0.017786668613553047\n",
      "iteration 868, dc_loss: 0.016913512721657753, tv_loss: 0.017785603180527687\n",
      "iteration 869, dc_loss: 0.016913168132305145, tv_loss: 0.017784351482987404\n",
      "iteration 870, dc_loss: 0.01691284589469433, tv_loss: 0.017783496528863907\n",
      "iteration 871, dc_loss: 0.0169125497341156, tv_loss: 0.01778247579932213\n",
      "iteration 872, dc_loss: 0.016912249848246574, tv_loss: 0.01778140477836132\n",
      "iteration 873, dc_loss: 0.016911910846829414, tv_loss: 0.017780473455786705\n",
      "iteration 874, dc_loss: 0.016911562532186508, tv_loss: 0.017779367044568062\n",
      "iteration 875, dc_loss: 0.016911234706640244, tv_loss: 0.017778344452381134\n",
      "iteration 876, dc_loss: 0.016910919919610023, tv_loss: 0.017777223140001297\n",
      "iteration 877, dc_loss: 0.016910618171095848, tv_loss: 0.0177761260420084\n",
      "iteration 878, dc_loss: 0.016910312697291374, tv_loss: 0.017775390297174454\n",
      "iteration 879, dc_loss: 0.016910003498196602, tv_loss: 0.017774252220988274\n",
      "iteration 880, dc_loss: 0.01690967008471489, tv_loss: 0.017773104831576347\n",
      "iteration 881, dc_loss: 0.016909347847104073, tv_loss: 0.01777205988764763\n",
      "iteration 882, dc_loss: 0.016909025609493256, tv_loss: 0.017771264538168907\n",
      "iteration 883, dc_loss: 0.01690870150923729, tv_loss: 0.017770448699593544\n",
      "iteration 884, dc_loss: 0.01690838672220707, tv_loss: 0.017769115045666695\n",
      "iteration 885, dc_loss: 0.016908057034015656, tv_loss: 0.017768047749996185\n",
      "iteration 886, dc_loss: 0.016907788813114166, tv_loss: 0.017767224460840225\n",
      "iteration 887, dc_loss: 0.016907498240470886, tv_loss: 0.017766205593943596\n",
      "iteration 888, dc_loss: 0.016907161101698875, tv_loss: 0.017765069380402565\n",
      "iteration 889, dc_loss: 0.01690678671002388, tv_loss: 0.01776408776640892\n",
      "iteration 890, dc_loss: 0.01690640114247799, tv_loss: 0.017763258889317513\n",
      "iteration 891, dc_loss: 0.01690603233873844, tv_loss: 0.017762452363967896\n",
      "iteration 892, dc_loss: 0.016905691474676132, tv_loss: 0.017761314287781715\n",
      "iteration 893, dc_loss: 0.016905371099710464, tv_loss: 0.017760509625077248\n",
      "iteration 894, dc_loss: 0.016905078664422035, tv_loss: 0.017759433016180992\n",
      "iteration 895, dc_loss: 0.01690480299293995, tv_loss: 0.017758307978510857\n",
      "iteration 896, dc_loss: 0.016904519870877266, tv_loss: 0.0177573524415493\n",
      "iteration 897, dc_loss: 0.016904199495911598, tv_loss: 0.017756329849362373\n",
      "iteration 898, dc_loss: 0.01690390706062317, tv_loss: 0.017755404114723206\n",
      "iteration 899, dc_loss: 0.016903594136238098, tv_loss: 0.01775452494621277\n",
      "iteration 900, dc_loss: 0.016903262585401535, tv_loss: 0.017753463238477707\n",
      "iteration 901, dc_loss: 0.01690291427075863, tv_loss: 0.017752505838871002\n",
      "iteration 902, dc_loss: 0.016902580857276917, tv_loss: 0.01775168813765049\n",
      "iteration 903, dc_loss: 0.016902245581150055, tv_loss: 0.017750756815075874\n",
      "iteration 904, dc_loss: 0.016901930794119835, tv_loss: 0.017749641090631485\n",
      "iteration 905, dc_loss: 0.016901638358831406, tv_loss: 0.017748774960637093\n",
      "iteration 906, dc_loss: 0.016901372000575066, tv_loss: 0.01774776726961136\n",
      "iteration 907, dc_loss: 0.016901090741157532, tv_loss: 0.017746802419424057\n",
      "iteration 908, dc_loss: 0.0169008020311594, tv_loss: 0.01774575375020504\n",
      "iteration 909, dc_loss: 0.01690051145851612, tv_loss: 0.01774471253156662\n",
      "iteration 910, dc_loss: 0.016900189220905304, tv_loss: 0.017743850126862526\n",
      "iteration 911, dc_loss: 0.016899872571229935, tv_loss: 0.017742862924933434\n",
      "iteration 912, dc_loss: 0.016899552196264267, tv_loss: 0.01774199679493904\n",
      "iteration 913, dc_loss: 0.0168992318212986, tv_loss: 0.017741065472364426\n",
      "iteration 914, dc_loss: 0.016898909583687782, tv_loss: 0.017740167677402496\n",
      "iteration 915, dc_loss: 0.01689860410988331, tv_loss: 0.01773916929960251\n",
      "iteration 916, dc_loss: 0.016898322850465775, tv_loss: 0.01773831434547901\n",
      "iteration 917, dc_loss: 0.01689804717898369, tv_loss: 0.01773732714354992\n",
      "iteration 918, dc_loss: 0.016897793859243393, tv_loss: 0.017736442387104034\n",
      "iteration 919, dc_loss: 0.016897525638341904, tv_loss: 0.01773538626730442\n",
      "iteration 920, dc_loss: 0.01689724251627922, tv_loss: 0.017734386026859283\n",
      "iteration 921, dc_loss: 0.016896896064281464, tv_loss: 0.017733486369252205\n",
      "iteration 922, dc_loss: 0.016896536573767662, tv_loss: 0.017732610926032066\n",
      "iteration 923, dc_loss: 0.016896191984415054, tv_loss: 0.01773184910416603\n",
      "iteration 924, dc_loss: 0.01689586602151394, tv_loss: 0.01773078925907612\n",
      "iteration 925, dc_loss: 0.016895554959774017, tv_loss: 0.017729980871081352\n",
      "iteration 926, dc_loss: 0.016895238310098648, tv_loss: 0.017729174345731735\n",
      "iteration 927, dc_loss: 0.01689491979777813, tv_loss: 0.01772809773683548\n",
      "iteration 928, dc_loss: 0.016894645988941193, tv_loss: 0.01772710308432579\n",
      "iteration 929, dc_loss: 0.0168943852186203, tv_loss: 0.017726285383105278\n",
      "iteration 930, dc_loss: 0.016894124448299408, tv_loss: 0.01772545464336872\n",
      "iteration 931, dc_loss: 0.016893837600946426, tv_loss: 0.017724400386214256\n",
      "iteration 932, dc_loss: 0.01689353957772255, tv_loss: 0.01772359386086464\n",
      "iteration 933, dc_loss: 0.016893241554498672, tv_loss: 0.01772269606590271\n",
      "iteration 934, dc_loss: 0.01689290627837181, tv_loss: 0.01772172562777996\n",
      "iteration 935, dc_loss: 0.0168925691395998, tv_loss: 0.017720799893140793\n",
      "iteration 936, dc_loss: 0.01689225435256958, tv_loss: 0.017719965428113937\n",
      "iteration 937, dc_loss: 0.016891978681087494, tv_loss: 0.017719168215990067\n",
      "iteration 938, dc_loss: 0.016891710460186005, tv_loss: 0.01771809719502926\n",
      "iteration 939, dc_loss: 0.016891485080122948, tv_loss: 0.017717117443680763\n",
      "iteration 940, dc_loss: 0.016891242936253548, tv_loss: 0.01771620288491249\n",
      "iteration 941, dc_loss: 0.01689097471535206, tv_loss: 0.017715351656079292\n",
      "iteration 942, dc_loss: 0.016890686005353928, tv_loss: 0.017714491114020348\n",
      "iteration 943, dc_loss: 0.0168903898447752, tv_loss: 0.017713580280542374\n",
      "iteration 944, dc_loss: 0.016890104860067368, tv_loss: 0.017712680622935295\n",
      "iteration 945, dc_loss: 0.01688983477652073, tv_loss: 0.017711762338876724\n",
      "iteration 946, dc_loss: 0.016889557242393494, tv_loss: 0.0177109707146883\n",
      "iteration 947, dc_loss: 0.01688925176858902, tv_loss: 0.017710017040371895\n",
      "iteration 948, dc_loss: 0.016888927668333054, tv_loss: 0.017709169536828995\n",
      "iteration 949, dc_loss: 0.016888659447431564, tv_loss: 0.01770816557109356\n",
      "iteration 950, dc_loss: 0.016888387501239777, tv_loss: 0.017707301303744316\n",
      "iteration 951, dc_loss: 0.016888132318854332, tv_loss: 0.017706451937556267\n",
      "iteration 952, dc_loss: 0.016887830570340157, tv_loss: 0.017705513164401054\n",
      "iteration 953, dc_loss: 0.01688753254711628, tv_loss: 0.017704635858535767\n",
      "iteration 954, dc_loss: 0.016887241974473, tv_loss: 0.017703836783766747\n",
      "iteration 955, dc_loss: 0.016886945813894272, tv_loss: 0.017703106626868248\n",
      "iteration 956, dc_loss: 0.01688666082918644, tv_loss: 0.017702186480164528\n",
      "iteration 957, dc_loss: 0.016886383295059204, tv_loss: 0.017701182514429092\n",
      "iteration 958, dc_loss: 0.016886116936802864, tv_loss: 0.01770021952688694\n",
      "iteration 959, dc_loss: 0.01688588783144951, tv_loss: 0.017699401825666428\n",
      "iteration 960, dc_loss: 0.016885634511709213, tv_loss: 0.0176986251026392\n",
      "iteration 961, dc_loss: 0.01688535138964653, tv_loss: 0.017697785049676895\n",
      "iteration 962, dc_loss: 0.016885051503777504, tv_loss: 0.017696835100650787\n",
      "iteration 963, dc_loss: 0.016884762793779373, tv_loss: 0.017695890739560127\n",
      "iteration 964, dc_loss: 0.016884470358490944, tv_loss: 0.01769520528614521\n",
      "iteration 965, dc_loss: 0.016884246841073036, tv_loss: 0.01769426465034485\n",
      "iteration 966, dc_loss: 0.01688402332365513, tv_loss: 0.01769336499273777\n",
      "iteration 967, dc_loss: 0.016883770003914833, tv_loss: 0.017692232504487038\n",
      "iteration 968, dc_loss: 0.01688350737094879, tv_loss: 0.017691370099782944\n",
      "iteration 969, dc_loss: 0.01688326522707939, tv_loss: 0.01769065298140049\n",
      "iteration 970, dc_loss: 0.016883019357919693, tv_loss: 0.017689742147922516\n",
      "iteration 971, dc_loss: 0.01688271202147007, tv_loss: 0.017688877880573273\n",
      "iteration 972, dc_loss: 0.016882402822375298, tv_loss: 0.017688006162643433\n",
      "iteration 973, dc_loss: 0.01688213087618351, tv_loss: 0.017687100917100906\n",
      "iteration 974, dc_loss: 0.016881855204701424, tv_loss: 0.017686430364847183\n",
      "iteration 975, dc_loss: 0.016881585121154785, tv_loss: 0.017685530707240105\n",
      "iteration 976, dc_loss: 0.016881300136446953, tv_loss: 0.01768461987376213\n",
      "iteration 977, dc_loss: 0.016881009563803673, tv_loss: 0.01768394187092781\n",
      "iteration 978, dc_loss: 0.016880737617611885, tv_loss: 0.017683211714029312\n",
      "iteration 979, dc_loss: 0.016880488023161888, tv_loss: 0.01768227480351925\n",
      "iteration 980, dc_loss: 0.01688024401664734, tv_loss: 0.01768127642571926\n",
      "iteration 981, dc_loss: 0.01687999628484249, tv_loss: 0.017680376768112183\n",
      "iteration 982, dc_loss: 0.016879769042134285, tv_loss: 0.017679603770375252\n",
      "iteration 983, dc_loss: 0.01687953621149063, tv_loss: 0.01767873391509056\n",
      "iteration 984, dc_loss: 0.016879288479685783, tv_loss: 0.017677852883934975\n",
      "iteration 985, dc_loss: 0.016878986731171608, tv_loss: 0.017677025869488716\n",
      "iteration 986, dc_loss: 0.016878658905625343, tv_loss: 0.017676347866654396\n",
      "iteration 987, dc_loss: 0.01687837392091751, tv_loss: 0.01767544634640217\n",
      "iteration 988, dc_loss: 0.016878126189112663, tv_loss: 0.01767451874911785\n",
      "iteration 989, dc_loss: 0.016877884045243263, tv_loss: 0.017673807218670845\n",
      "iteration 990, dc_loss: 0.016877666115760803, tv_loss: 0.0176729504019022\n",
      "iteration 991, dc_loss: 0.01687743328511715, tv_loss: 0.017671989277005196\n",
      "iteration 992, dc_loss: 0.0168771930038929, tv_loss: 0.017671091482043266\n",
      "iteration 993, dc_loss: 0.016876960173249245, tv_loss: 0.01767020858824253\n",
      "iteration 994, dc_loss: 0.01687672547996044, tv_loss: 0.017669549211859703\n",
      "iteration 995, dc_loss: 0.016876500099897385, tv_loss: 0.017668651416897774\n",
      "iteration 996, dc_loss: 0.01687624119222164, tv_loss: 0.017667794600129128\n",
      "iteration 997, dc_loss: 0.016875989735126495, tv_loss: 0.017667140811681747\n",
      "iteration 998, dc_loss: 0.016875755041837692, tv_loss: 0.017666280269622803\n",
      "iteration 999, dc_loss: 0.016875481233000755, tv_loss: 0.017665455117821693\n",
      "iteration 1000, dc_loss: 0.016875237226486206, tv_loss: 0.017664575949311256\n",
      "iteration 1001, dc_loss: 0.016874995082616806, tv_loss: 0.01766379177570343\n",
      "iteration 1002, dc_loss: 0.016874736174941063, tv_loss: 0.017662979662418365\n",
      "iteration 1003, dc_loss: 0.016874462366104126, tv_loss: 0.017662234604358673\n",
      "iteration 1004, dc_loss: 0.016874214634299278, tv_loss: 0.017661316320300102\n",
      "iteration 1005, dc_loss: 0.01687397062778473, tv_loss: 0.017660610377788544\n",
      "iteration 1006, dc_loss: 0.01687372475862503, tv_loss: 0.017659837380051613\n",
      "iteration 1007, dc_loss: 0.01687343418598175, tv_loss: 0.01765894517302513\n",
      "iteration 1008, dc_loss: 0.01687314361333847, tv_loss: 0.0176582932472229\n",
      "iteration 1009, dc_loss: 0.016872860491275787, tv_loss: 0.017657531425356865\n",
      "iteration 1010, dc_loss: 0.016872558742761612, tv_loss: 0.017656557261943817\n",
      "iteration 1011, dc_loss: 0.01687229983508587, tv_loss: 0.017655912786722183\n",
      "iteration 1012, dc_loss: 0.016872093081474304, tv_loss: 0.01765514351427555\n",
      "iteration 1013, dc_loss: 0.016871917992830276, tv_loss: 0.01765424758195877\n",
      "iteration 1014, dc_loss: 0.016871748492121696, tv_loss: 0.017653219401836395\n",
      "iteration 1015, dc_loss: 0.01687154360115528, tv_loss: 0.017652200534939766\n",
      "iteration 1016, dc_loss: 0.01687130145728588, tv_loss: 0.017651638016104698\n",
      "iteration 1017, dc_loss: 0.01687103882431984, tv_loss: 0.017650838941335678\n",
      "iteration 1018, dc_loss: 0.016870765015482903, tv_loss: 0.017650041729211807\n",
      "iteration 1019, dc_loss: 0.016870489344000816, tv_loss: 0.017649376764893532\n",
      "iteration 1020, dc_loss: 0.01687021180987358, tv_loss: 0.017648397013545036\n",
      "iteration 1021, dc_loss: 0.016869958490133286, tv_loss: 0.01764761097729206\n",
      "iteration 1022, dc_loss: 0.01686970144510269, tv_loss: 0.01764693669974804\n",
      "iteration 1023, dc_loss: 0.016869453713297844, tv_loss: 0.017646174877882004\n",
      "iteration 1024, dc_loss: 0.01686921715736389, tv_loss: 0.017645305022597313\n",
      "iteration 1025, dc_loss: 0.016868986189365387, tv_loss: 0.017644520848989487\n",
      "iteration 1026, dc_loss: 0.01686876267194748, tv_loss: 0.017643693834543228\n",
      "iteration 1027, dc_loss: 0.01686856895685196, tv_loss: 0.017642855644226074\n",
      "iteration 1028, dc_loss: 0.01686839945614338, tv_loss: 0.01764197088778019\n",
      "iteration 1029, dc_loss: 0.016868194565176964, tv_loss: 0.017641151323914528\n",
      "iteration 1030, dc_loss: 0.016867956146597862, tv_loss: 0.01764022372663021\n",
      "iteration 1031, dc_loss: 0.016867689788341522, tv_loss: 0.017639683559536934\n",
      "iteration 1032, dc_loss: 0.01686742715537548, tv_loss: 0.01763896830379963\n",
      "iteration 1033, dc_loss: 0.01686718687415123, tv_loss: 0.01763794757425785\n",
      "iteration 1034, dc_loss: 0.016866939142346382, tv_loss: 0.017637263983488083\n",
      "iteration 1035, dc_loss: 0.01686670258641243, tv_loss: 0.017636703327298164\n",
      "iteration 1036, dc_loss: 0.016866467893123627, tv_loss: 0.017635850235819817\n",
      "iteration 1037, dc_loss: 0.016866229474544525, tv_loss: 0.01763499155640602\n",
      "iteration 1038, dc_loss: 0.01686599850654602, tv_loss: 0.017634190618991852\n",
      "iteration 1039, dc_loss: 0.016865750774741173, tv_loss: 0.01763341948390007\n",
      "iteration 1040, dc_loss: 0.01686549000442028, tv_loss: 0.017632700502872467\n",
      "iteration 1041, dc_loss: 0.01686524972319603, tv_loss: 0.01763194240629673\n",
      "iteration 1042, dc_loss: 0.016865026205778122, tv_loss: 0.017631089314818382\n",
      "iteration 1043, dc_loss: 0.016864852979779243, tv_loss: 0.017630280926823616\n",
      "iteration 1044, dc_loss: 0.016864648088812828, tv_loss: 0.01762942597270012\n",
      "iteration 1045, dc_loss: 0.016864420846104622, tv_loss: 0.01762864738702774\n",
      "iteration 1046, dc_loss: 0.016864178702235222, tv_loss: 0.017627984285354614\n",
      "iteration 1047, dc_loss: 0.01686394028365612, tv_loss: 0.017627190798521042\n",
      "iteration 1048, dc_loss: 0.016863711178302765, tv_loss: 0.017626481130719185\n",
      "iteration 1049, dc_loss: 0.016863485798239708, tv_loss: 0.017625628039240837\n",
      "iteration 1050, dc_loss: 0.016863301396369934, tv_loss: 0.0176248736679554\n",
      "iteration 1051, dc_loss: 0.01686309464275837, tv_loss: 0.017624175176024437\n",
      "iteration 1052, dc_loss: 0.01686287857592106, tv_loss: 0.017623325809836388\n",
      "iteration 1053, dc_loss: 0.016862645745277405, tv_loss: 0.017622698098421097\n",
      "iteration 1054, dc_loss: 0.016862405464053154, tv_loss: 0.01762191765010357\n",
      "iteration 1055, dc_loss: 0.01686214841902256, tv_loss: 0.017621051520109177\n",
      "iteration 1056, dc_loss: 0.016861869022250175, tv_loss: 0.017620429396629333\n",
      "iteration 1057, dc_loss: 0.016861628741025925, tv_loss: 0.017619719728827477\n",
      "iteration 1058, dc_loss: 0.016861427575349808, tv_loss: 0.017618954181671143\n",
      "iteration 1059, dc_loss: 0.016861233860254288, tv_loss: 0.01761801354587078\n",
      "iteration 1060, dc_loss: 0.016861047595739365, tv_loss: 0.01761726662516594\n",
      "iteration 1061, dc_loss: 0.016860850155353546, tv_loss: 0.017616504803299904\n",
      "iteration 1062, dc_loss: 0.016860630363225937, tv_loss: 0.017615821212530136\n",
      "iteration 1063, dc_loss: 0.016860393807291985, tv_loss: 0.017615126445889473\n",
      "iteration 1064, dc_loss: 0.01686013489961624, tv_loss: 0.017614388838410378\n",
      "iteration 1065, dc_loss: 0.016859862953424454, tv_loss: 0.01761365681886673\n",
      "iteration 1066, dc_loss: 0.01685960777103901, tv_loss: 0.017612891271710396\n",
      "iteration 1067, dc_loss: 0.016859382390975952, tv_loss: 0.017612097784876823\n",
      "iteration 1068, dc_loss: 0.016859184950590134, tv_loss: 0.017611462622880936\n",
      "iteration 1069, dc_loss: 0.016858994960784912, tv_loss: 0.01761065237224102\n",
      "iteration 1070, dc_loss: 0.01685875654220581, tv_loss: 0.01760982535779476\n",
      "iteration 1071, dc_loss: 0.016858501359820366, tv_loss: 0.017609236761927605\n",
      "iteration 1072, dc_loss: 0.016858287155628204, tv_loss: 0.017608387395739555\n",
      "iteration 1073, dc_loss: 0.016858093440532684, tv_loss: 0.017607614398002625\n",
      "iteration 1074, dc_loss: 0.016857899725437164, tv_loss: 0.01760689727962017\n",
      "iteration 1075, dc_loss: 0.01685771346092224, tv_loss: 0.017606187611818314\n",
      "iteration 1076, dc_loss: 0.01685752160847187, tv_loss: 0.017605358734726906\n",
      "iteration 1077, dc_loss: 0.016857320442795753, tv_loss: 0.017604725435376167\n",
      "iteration 1078, dc_loss: 0.016857139766216278, tv_loss: 0.017603863030672073\n",
      "iteration 1079, dc_loss: 0.016856934875249863, tv_loss: 0.017603186890482903\n",
      "iteration 1080, dc_loss: 0.016856715083122253, tv_loss: 0.017602423205971718\n",
      "iteration 1081, dc_loss: 0.016856467351317406, tv_loss: 0.01760178804397583\n",
      "iteration 1082, dc_loss: 0.016856227070093155, tv_loss: 0.017601288855075836\n",
      "iteration 1083, dc_loss: 0.016855968162417412, tv_loss: 0.017600566148757935\n",
      "iteration 1084, dc_loss: 0.01685572974383831, tv_loss: 0.017599772661924362\n",
      "iteration 1085, dc_loss: 0.01685556210577488, tv_loss: 0.017599035054445267\n",
      "iteration 1086, dc_loss: 0.016855407506227493, tv_loss: 0.017598407343029976\n",
      "iteration 1087, dc_loss: 0.01685524731874466, tv_loss: 0.017597690224647522\n",
      "iteration 1088, dc_loss: 0.016855044290423393, tv_loss: 0.017596788704395294\n",
      "iteration 1089, dc_loss: 0.01685480959713459, tv_loss: 0.017596006393432617\n",
      "iteration 1090, dc_loss: 0.0168545451015234, tv_loss: 0.017595401033759117\n",
      "iteration 1091, dc_loss: 0.016854338347911835, tv_loss: 0.01759466901421547\n",
      "iteration 1092, dc_loss: 0.016854120418429375, tv_loss: 0.017593927681446075\n",
      "iteration 1093, dc_loss: 0.01685391180217266, tv_loss: 0.017593037337064743\n",
      "iteration 1094, dc_loss: 0.01685371808707714, tv_loss: 0.01759239472448826\n",
      "iteration 1095, dc_loss: 0.016853542998433113, tv_loss: 0.017591970041394234\n",
      "iteration 1096, dc_loss: 0.01685333251953125, tv_loss: 0.01759112998843193\n",
      "iteration 1097, dc_loss: 0.01685311459004879, tv_loss: 0.017590411007404327\n",
      "iteration 1098, dc_loss: 0.01685289666056633, tv_loss: 0.01758977212011814\n",
      "iteration 1099, dc_loss: 0.016852667555212975, tv_loss: 0.01758893020451069\n",
      "iteration 1100, dc_loss: 0.016852404922246933, tv_loss: 0.017588170245289803\n",
      "iteration 1101, dc_loss: 0.01685216836631298, tv_loss: 0.017587728798389435\n",
      "iteration 1102, dc_loss: 0.01685194857418537, tv_loss: 0.017586933448910713\n",
      "iteration 1103, dc_loss: 0.016851743683218956, tv_loss: 0.017586201429367065\n",
      "iteration 1104, dc_loss: 0.01685154438018799, tv_loss: 0.01758534461259842\n",
      "iteration 1105, dc_loss: 0.01685137301683426, tv_loss: 0.017584748566150665\n",
      "iteration 1106, dc_loss: 0.016851210966706276, tv_loss: 0.01758405566215515\n",
      "iteration 1107, dc_loss: 0.016851043328642845, tv_loss: 0.017583264037966728\n",
      "iteration 1108, dc_loss: 0.01685088872909546, tv_loss: 0.017582403495907784\n",
      "iteration 1109, dc_loss: 0.016850709915161133, tv_loss: 0.017581699416041374\n",
      "iteration 1110, dc_loss: 0.016850503161549568, tv_loss: 0.017581095919013023\n",
      "iteration 1111, dc_loss: 0.01685025729238987, tv_loss: 0.017580460757017136\n",
      "iteration 1112, dc_loss: 0.016850048676133156, tv_loss: 0.017579590901732445\n",
      "iteration 1113, dc_loss: 0.016849862411618233, tv_loss: 0.017579004168510437\n",
      "iteration 1114, dc_loss: 0.016849664971232414, tv_loss: 0.017578328028321266\n",
      "iteration 1115, dc_loss: 0.016849448904395103, tv_loss: 0.017577653750777245\n",
      "iteration 1116, dc_loss: 0.016849245876073837, tv_loss: 0.01757679507136345\n",
      "iteration 1117, dc_loss: 0.016849059611558914, tv_loss: 0.017576303333044052\n",
      "iteration 1118, dc_loss: 0.016848864033818245, tv_loss: 0.017575429752469063\n",
      "iteration 1119, dc_loss: 0.01684867963194847, tv_loss: 0.017574740573763847\n",
      "iteration 1120, dc_loss: 0.016848472878336906, tv_loss: 0.017574096098542213\n",
      "iteration 1121, dc_loss: 0.016848256811499596, tv_loss: 0.017573274672031403\n",
      "iteration 1122, dc_loss: 0.016848022118210793, tv_loss: 0.017572639510035515\n",
      "iteration 1123, dc_loss: 0.01684780977666378, tv_loss: 0.017572253942489624\n",
      "iteration 1124, dc_loss: 0.016847651451826096, tv_loss: 0.01757137104868889\n",
      "iteration 1125, dc_loss: 0.01684749871492386, tv_loss: 0.017570657655596733\n",
      "iteration 1126, dc_loss: 0.01684732548892498, tv_loss: 0.01756998896598816\n",
      "iteration 1127, dc_loss: 0.01684713363647461, tv_loss: 0.017569255083799362\n",
      "iteration 1128, dc_loss: 0.016846925020217896, tv_loss: 0.0175686776638031\n",
      "iteration 1129, dc_loss: 0.016846727579832077, tv_loss: 0.01756783202290535\n",
      "iteration 1130, dc_loss: 0.0168465469032526, tv_loss: 0.01756717637181282\n",
      "iteration 1131, dc_loss: 0.01684635505080223, tv_loss: 0.01756647229194641\n",
      "iteration 1132, dc_loss: 0.01684611290693283, tv_loss: 0.017565840855240822\n",
      "iteration 1133, dc_loss: 0.016845909878611565, tv_loss: 0.017565295100212097\n",
      "iteration 1134, dc_loss: 0.01684568263590336, tv_loss: 0.017564477398991585\n",
      "iteration 1135, dc_loss: 0.016845501959323883, tv_loss: 0.017563819885253906\n",
      "iteration 1136, dc_loss: 0.01684533804655075, tv_loss: 0.017563192173838615\n",
      "iteration 1137, dc_loss: 0.01684514991939068, tv_loss: 0.017562566325068474\n",
      "iteration 1138, dc_loss: 0.016844946891069412, tv_loss: 0.0175617802888155\n",
      "iteration 1139, dc_loss: 0.016844747588038445, tv_loss: 0.017561165615916252\n",
      "iteration 1140, dc_loss: 0.016844550147652626, tv_loss: 0.01756061241030693\n",
      "iteration 1141, dc_loss: 0.016844360157847404, tv_loss: 0.017559923231601715\n",
      "iteration 1142, dc_loss: 0.016844162717461586, tv_loss: 0.017559153959155083\n",
      "iteration 1143, dc_loss: 0.01684398017823696, tv_loss: 0.01755850948393345\n",
      "iteration 1144, dc_loss: 0.016843784600496292, tv_loss: 0.017557960003614426\n",
      "iteration 1145, dc_loss: 0.01684359461069107, tv_loss: 0.01755724474787712\n",
      "iteration 1146, dc_loss: 0.016843389719724655, tv_loss: 0.01755656488239765\n",
      "iteration 1147, dc_loss: 0.01684318296611309, tv_loss: 0.01755584217607975\n",
      "iteration 1148, dc_loss: 0.01684298738837242, tv_loss: 0.017555270344018936\n",
      "iteration 1149, dc_loss: 0.016842801123857498, tv_loss: 0.017554717138409615\n",
      "iteration 1150, dc_loss: 0.016842631623148918, tv_loss: 0.017553910613059998\n",
      "iteration 1151, dc_loss: 0.016842471435666084, tv_loss: 0.01755312643945217\n",
      "iteration 1152, dc_loss: 0.016842303797602654, tv_loss: 0.017552485689520836\n",
      "iteration 1153, dc_loss: 0.016842136159539223, tv_loss: 0.017551884055137634\n",
      "iteration 1154, dc_loss: 0.016841936856508255, tv_loss: 0.017551211640238762\n",
      "iteration 1155, dc_loss: 0.016841722652316093, tv_loss: 0.017550362274050713\n",
      "iteration 1156, dc_loss: 0.01684153638780117, tv_loss: 0.0175497867166996\n",
      "iteration 1157, dc_loss: 0.016841357573866844, tv_loss: 0.017549140378832817\n",
      "iteration 1158, dc_loss: 0.01684115082025528, tv_loss: 0.01754853129386902\n",
      "iteration 1159, dc_loss: 0.016840970143675804, tv_loss: 0.017547935247421265\n",
      "iteration 1160, dc_loss: 0.01684078946709633, tv_loss: 0.017547177150845528\n",
      "iteration 1161, dc_loss: 0.016840603202581406, tv_loss: 0.017546609044075012\n",
      "iteration 1162, dc_loss: 0.01684042438864708, tv_loss: 0.017545996233820915\n",
      "iteration 1163, dc_loss: 0.016840243712067604, tv_loss: 0.017545349895954132\n",
      "iteration 1164, dc_loss: 0.016840066760778427, tv_loss: 0.017544647678732872\n",
      "iteration 1165, dc_loss: 0.016839895397424698, tv_loss: 0.017543865367770195\n",
      "iteration 1166, dc_loss: 0.016839709132909775, tv_loss: 0.017543131485581398\n",
      "iteration 1167, dc_loss: 0.016839517280459404, tv_loss: 0.017542626708745956\n",
      "iteration 1168, dc_loss: 0.016839321702718735, tv_loss: 0.017541982233524323\n",
      "iteration 1169, dc_loss: 0.016839096322655678, tv_loss: 0.017541401088237762\n",
      "iteration 1170, dc_loss: 0.01683887094259262, tv_loss: 0.01754080131649971\n",
      "iteration 1171, dc_loss: 0.016838664188981056, tv_loss: 0.017540210857987404\n",
      "iteration 1172, dc_loss: 0.016838490962982178, tv_loss: 0.017539510503411293\n",
      "iteration 1173, dc_loss: 0.016838345676660538, tv_loss: 0.017538856714963913\n",
      "iteration 1174, dc_loss: 0.016838200390338898, tv_loss: 0.01753810979425907\n",
      "iteration 1175, dc_loss: 0.016838060691952705, tv_loss: 0.017537405714392662\n",
      "iteration 1176, dc_loss: 0.016837898641824722, tv_loss: 0.017536664381623268\n",
      "iteration 1177, dc_loss: 0.016837703064084053, tv_loss: 0.017536096274852753\n",
      "iteration 1178, dc_loss: 0.01683752052485943, tv_loss: 0.017535559833049774\n",
      "iteration 1179, dc_loss: 0.01683737151324749, tv_loss: 0.01753503456711769\n",
      "iteration 1180, dc_loss: 0.016837216913700104, tv_loss: 0.017534269019961357\n",
      "iteration 1181, dc_loss: 0.016837045550346375, tv_loss: 0.01753346249461174\n",
      "iteration 1182, dc_loss: 0.01683685928583145, tv_loss: 0.01753310114145279\n",
      "iteration 1183, dc_loss: 0.016836628317832947, tv_loss: 0.017532547935843468\n",
      "iteration 1184, dc_loss: 0.016836414113640785, tv_loss: 0.017531823366880417\n",
      "iteration 1185, dc_loss: 0.01683621108531952, tv_loss: 0.01753131113946438\n",
      "iteration 1186, dc_loss: 0.016836002469062805, tv_loss: 0.017530465498566628\n",
      "iteration 1187, dc_loss: 0.01683584600687027, tv_loss: 0.017530018463730812\n",
      "iteration 1188, dc_loss: 0.01683572307229042, tv_loss: 0.017529388889670372\n",
      "iteration 1189, dc_loss: 0.016835583373904228, tv_loss: 0.017528682947158813\n",
      "iteration 1190, dc_loss: 0.016835443675518036, tv_loss: 0.01752796396613121\n",
      "iteration 1191, dc_loss: 0.016835274174809456, tv_loss: 0.017527295276522636\n",
      "iteration 1192, dc_loss: 0.016835076734423637, tv_loss: 0.01752653159201145\n",
      "iteration 1193, dc_loss: 0.01683485321700573, tv_loss: 0.01752610318362713\n",
      "iteration 1194, dc_loss: 0.016834653913974762, tv_loss: 0.017525577917695045\n",
      "iteration 1195, dc_loss: 0.016834482550621033, tv_loss: 0.017525114119052887\n",
      "iteration 1196, dc_loss: 0.016834281384944916, tv_loss: 0.01752445101737976\n",
      "iteration 1197, dc_loss: 0.0168340802192688, tv_loss: 0.017523784190416336\n",
      "iteration 1198, dc_loss: 0.016833892092108727, tv_loss: 0.01752331852912903\n",
      "iteration 1199, dc_loss: 0.016833705827593803, tv_loss: 0.017522750422358513\n",
      "iteration 1200, dc_loss: 0.016833551228046417, tv_loss: 0.017521986737847328\n",
      "iteration 1201, dc_loss: 0.01683342456817627, tv_loss: 0.017521370202302933\n",
      "iteration 1202, dc_loss: 0.01683327555656433, tv_loss: 0.017520736902952194\n",
      "iteration 1203, dc_loss: 0.016833094879984856, tv_loss: 0.017520030960440636\n",
      "iteration 1204, dc_loss: 0.01683293655514717, tv_loss: 0.017519468441605568\n",
      "iteration 1205, dc_loss: 0.01683274284005165, tv_loss: 0.017519110813736916\n",
      "iteration 1206, dc_loss: 0.016832543537020683, tv_loss: 0.017518430948257446\n",
      "iteration 1207, dc_loss: 0.016832366585731506, tv_loss: 0.017517752945423126\n",
      "iteration 1208, dc_loss: 0.01683221571147442, tv_loss: 0.01751716248691082\n",
      "iteration 1209, dc_loss: 0.01683204062283039, tv_loss: 0.017516756430268288\n",
      "iteration 1210, dc_loss: 0.0168318934738636, tv_loss: 0.01751612313091755\n",
      "iteration 1211, dc_loss: 0.016831744462251663, tv_loss: 0.017515501007437706\n",
      "iteration 1212, dc_loss: 0.01683158613741398, tv_loss: 0.01751481555402279\n",
      "iteration 1213, dc_loss: 0.016831418499350548, tv_loss: 0.01751427911221981\n",
      "iteration 1214, dc_loss: 0.01683123968541622, tv_loss: 0.01751372404396534\n",
      "iteration 1215, dc_loss: 0.01683102920651436, tv_loss: 0.017513137310743332\n",
      "iteration 1216, dc_loss: 0.016830846667289734, tv_loss: 0.017512401565909386\n",
      "iteration 1217, dc_loss: 0.0168306864798069, tv_loss: 0.017511731013655663\n",
      "iteration 1218, dc_loss: 0.016830511391162872, tv_loss: 0.01751105487346649\n",
      "iteration 1219, dc_loss: 0.01683034375309944, tv_loss: 0.01751059480011463\n",
      "iteration 1220, dc_loss: 0.01683015190064907, tv_loss: 0.01751002110540867\n",
      "iteration 1221, dc_loss: 0.016829952597618103, tv_loss: 0.01750951074063778\n",
      "iteration 1222, dc_loss: 0.01682976633310318, tv_loss: 0.017508937045931816\n",
      "iteration 1223, dc_loss: 0.016829602420330048, tv_loss: 0.01750832609832287\n",
      "iteration 1224, dc_loss: 0.016829458996653557, tv_loss: 0.017507730051875114\n",
      "iteration 1225, dc_loss: 0.01682928577065468, tv_loss: 0.0175071582198143\n",
      "iteration 1226, dc_loss: 0.016829146072268486, tv_loss: 0.01750647835433483\n",
      "iteration 1227, dc_loss: 0.016829006373882294, tv_loss: 0.017505913972854614\n",
      "iteration 1228, dc_loss: 0.01682884991168976, tv_loss: 0.01750532165169716\n",
      "iteration 1229, dc_loss: 0.01682870462536812, tv_loss: 0.017504792660474777\n",
      "iteration 1230, dc_loss: 0.016828546300530434, tv_loss: 0.017504191026091576\n",
      "iteration 1231, dc_loss: 0.0168283861130476, tv_loss: 0.01750352419912815\n",
      "iteration 1232, dc_loss: 0.01682819426059723, tv_loss: 0.017503125593066216\n",
      "iteration 1233, dc_loss: 0.016828004270792007, tv_loss: 0.017502494156360626\n",
      "iteration 1234, dc_loss: 0.016827797517180443, tv_loss: 0.017501816153526306\n",
      "iteration 1235, dc_loss: 0.016827592626214027, tv_loss: 0.01750130206346512\n",
      "iteration 1236, dc_loss: 0.016827404499053955, tv_loss: 0.0175007451325655\n",
      "iteration 1237, dc_loss: 0.01682724803686142, tv_loss: 0.01750028505921364\n",
      "iteration 1238, dc_loss: 0.01682712323963642, tv_loss: 0.01749967597424984\n",
      "iteration 1239, dc_loss: 0.016826989129185677, tv_loss: 0.01749901846051216\n",
      "iteration 1240, dc_loss: 0.016826828941702843, tv_loss: 0.017498470842838287\n",
      "iteration 1241, dc_loss: 0.016826648265123367, tv_loss: 0.017497852444648743\n",
      "iteration 1242, dc_loss: 0.01682647317647934, tv_loss: 0.017497172579169273\n",
      "iteration 1243, dc_loss: 0.016826322302222252, tv_loss: 0.01749666966497898\n",
      "iteration 1244, dc_loss: 0.016826191917061806, tv_loss: 0.01749609038233757\n",
      "iteration 1245, dc_loss: 0.016826052218675613, tv_loss: 0.017495516687631607\n",
      "iteration 1246, dc_loss: 0.016825903207063675, tv_loss: 0.01749468222260475\n",
      "iteration 1247, dc_loss: 0.016825737431645393, tv_loss: 0.017494266852736473\n",
      "iteration 1248, dc_loss: 0.01682557910680771, tv_loss: 0.017493855208158493\n",
      "iteration 1249, dc_loss: 0.016825413331389427, tv_loss: 0.01749316230416298\n",
      "iteration 1250, dc_loss: 0.016825219616293907, tv_loss: 0.017492517828941345\n",
      "iteration 1251, dc_loss: 0.016825053840875626, tv_loss: 0.017492080107331276\n",
      "iteration 1252, dc_loss: 0.01682489551603794, tv_loss: 0.017491541802883148\n",
      "iteration 1253, dc_loss: 0.016824740916490555, tv_loss: 0.0174909345805645\n",
      "iteration 1254, dc_loss: 0.016824571415781975, tv_loss: 0.017490176483988762\n",
      "iteration 1255, dc_loss: 0.01682441122829914, tv_loss: 0.017489587888121605\n",
      "iteration 1256, dc_loss: 0.016824262216687202, tv_loss: 0.01748918928205967\n",
      "iteration 1257, dc_loss: 0.016824092715978622, tv_loss: 0.017488503828644753\n",
      "iteration 1258, dc_loss: 0.016823915764689445, tv_loss: 0.017488082870841026\n",
      "iteration 1259, dc_loss: 0.016823742538690567, tv_loss: 0.01748751476407051\n",
      "iteration 1260, dc_loss: 0.01682356745004654, tv_loss: 0.0174868144094944\n",
      "iteration 1261, dc_loss: 0.016823435202240944, tv_loss: 0.01748630963265896\n",
      "iteration 1262, dc_loss: 0.016823289915919304, tv_loss: 0.0174858421087265\n",
      "iteration 1263, dc_loss: 0.016823124140501022, tv_loss: 0.017485294491052628\n",
      "iteration 1264, dc_loss: 0.016822967678308487, tv_loss: 0.01748468726873398\n",
      "iteration 1265, dc_loss: 0.016822848469018936, tv_loss: 0.017484107986092567\n",
      "iteration 1266, dc_loss: 0.016822725534439087, tv_loss: 0.017483578994870186\n",
      "iteration 1267, dc_loss: 0.016822602599859238, tv_loss: 0.017482906579971313\n",
      "iteration 1268, dc_loss: 0.016822433099150658, tv_loss: 0.017482353374361992\n",
      "iteration 1269, dc_loss: 0.01682226173579693, tv_loss: 0.01748194731771946\n",
      "iteration 1270, dc_loss: 0.01682208478450775, tv_loss: 0.017481377348303795\n",
      "iteration 1271, dc_loss: 0.01682192087173462, tv_loss: 0.017480814829468727\n",
      "iteration 1272, dc_loss: 0.016821768134832382, tv_loss: 0.01748017407953739\n",
      "iteration 1273, dc_loss: 0.01682158187031746, tv_loss: 0.01747974008321762\n",
      "iteration 1274, dc_loss: 0.016821397468447685, tv_loss: 0.01747916452586651\n",
      "iteration 1275, dc_loss: 0.01682124473154545, tv_loss: 0.017478682100772858\n",
      "iteration 1276, dc_loss: 0.01682109758257866, tv_loss: 0.017478181049227715\n",
      "iteration 1277, dc_loss: 0.0168209969997406, tv_loss: 0.017477499321103096\n",
      "iteration 1278, dc_loss: 0.01682090386748314, tv_loss: 0.017476851120591164\n",
      "iteration 1279, dc_loss: 0.016820767894387245, tv_loss: 0.017476320266723633\n",
      "iteration 1280, dc_loss: 0.01682058908045292, tv_loss: 0.017475778236985207\n",
      "iteration 1281, dc_loss: 0.016820380464196205, tv_loss: 0.017475448548793793\n",
      "iteration 1282, dc_loss: 0.016820192337036133, tv_loss: 0.017474837601184845\n",
      "iteration 1283, dc_loss: 0.016820024698972702, tv_loss: 0.017474288120865822\n",
      "iteration 1284, dc_loss: 0.016819868236780167, tv_loss: 0.01747368462383747\n",
      "iteration 1285, dc_loss: 0.016819743439555168, tv_loss: 0.01747322641313076\n",
      "iteration 1286, dc_loss: 0.016819609329104424, tv_loss: 0.017472831532359123\n",
      "iteration 1287, dc_loss: 0.016819491982460022, tv_loss: 0.01747223548591137\n",
      "iteration 1288, dc_loss: 0.01681937277317047, tv_loss: 0.017471538856625557\n",
      "iteration 1289, dc_loss: 0.016819236800074577, tv_loss: 0.017470933496952057\n",
      "iteration 1290, dc_loss: 0.01681913062930107, tv_loss: 0.017470261082053185\n",
      "iteration 1291, dc_loss: 0.01681898720562458, tv_loss: 0.017469823360443115\n",
      "iteration 1292, dc_loss: 0.016818812116980553, tv_loss: 0.017469311133027077\n",
      "iteration 1293, dc_loss: 0.01681862212717533, tv_loss: 0.017468882724642754\n",
      "iteration 1294, dc_loss: 0.016818415373563766, tv_loss: 0.017468323931097984\n",
      "iteration 1295, dc_loss: 0.016818203032016754, tv_loss: 0.017467696219682693\n",
      "iteration 1296, dc_loss: 0.01681799814105034, tv_loss: 0.017467407509684563\n",
      "iteration 1297, dc_loss: 0.016817836090922356, tv_loss: 0.017466967925429344\n",
      "iteration 1298, dc_loss: 0.01681772619485855, tv_loss: 0.01746632345020771\n",
      "iteration 1299, dc_loss: 0.016817646101117134, tv_loss: 0.017465660348534584\n",
      "iteration 1300, dc_loss: 0.01681755855679512, tv_loss: 0.01746518723666668\n",
      "iteration 1301, dc_loss: 0.016817471012473106, tv_loss: 0.017464695498347282\n",
      "iteration 1302, dc_loss: 0.0168173685669899, tv_loss: 0.0174640491604805\n",
      "iteration 1303, dc_loss: 0.01681724190711975, tv_loss: 0.017463350668549538\n",
      "iteration 1304, dc_loss: 0.01681707613170147, tv_loss: 0.017462899908423424\n",
      "iteration 1305, dc_loss: 0.016816899180412292, tv_loss: 0.017462456598877907\n",
      "iteration 1306, dc_loss: 0.016816718503832817, tv_loss: 0.017461899667978287\n",
      "iteration 1307, dc_loss: 0.016816560178995132, tv_loss: 0.0174615029245615\n",
      "iteration 1308, dc_loss: 0.0168163925409317, tv_loss: 0.017460897564888\n",
      "iteration 1309, dc_loss: 0.016816232353448868, tv_loss: 0.017460359260439873\n",
      "iteration 1310, dc_loss: 0.016816066578030586, tv_loss: 0.017459936439990997\n",
      "iteration 1311, dc_loss: 0.01681588962674141, tv_loss: 0.017459392547607422\n",
      "iteration 1312, dc_loss: 0.016815723851323128, tv_loss: 0.017458777874708176\n",
      "iteration 1313, dc_loss: 0.016815606504678726, tv_loss: 0.01745833456516266\n",
      "iteration 1314, dc_loss: 0.016815485432744026, tv_loss: 0.01745779998600483\n",
      "iteration 1315, dc_loss: 0.01681537739932537, tv_loss: 0.01745733618736267\n",
      "iteration 1316, dc_loss: 0.016815202310681343, tv_loss: 0.017456846311688423\n",
      "iteration 1317, dc_loss: 0.01681504398584366, tv_loss: 0.017456263303756714\n",
      "iteration 1318, dc_loss: 0.01681492105126381, tv_loss: 0.017455803230404854\n",
      "iteration 1319, dc_loss: 0.016814790666103363, tv_loss: 0.017455289140343666\n",
      "iteration 1320, dc_loss: 0.016814660280942917, tv_loss: 0.01745464652776718\n",
      "iteration 1321, dc_loss: 0.016814518719911575, tv_loss: 0.017454111948609352\n",
      "iteration 1322, dc_loss: 0.01681439019739628, tv_loss: 0.01745368167757988\n",
      "iteration 1323, dc_loss: 0.01681428775191307, tv_loss: 0.017453277483582497\n",
      "iteration 1324, dc_loss: 0.01681414805352688, tv_loss: 0.017452599480748177\n",
      "iteration 1325, dc_loss: 0.01681399531662464, tv_loss: 0.017452029511332512\n",
      "iteration 1326, dc_loss: 0.016813863068819046, tv_loss: 0.017451558262109756\n",
      "iteration 1327, dc_loss: 0.016813741996884346, tv_loss: 0.017451170831918716\n",
      "iteration 1328, dc_loss: 0.0168136078864336, tv_loss: 0.017450744286179543\n",
      "iteration 1329, dc_loss: 0.01681346818804741, tv_loss: 0.017450153827667236\n",
      "iteration 1330, dc_loss: 0.01681332290172577, tv_loss: 0.01744955964386463\n",
      "iteration 1331, dc_loss: 0.016813166439533234, tv_loss: 0.017449265345931053\n",
      "iteration 1332, dc_loss: 0.016812996938824654, tv_loss: 0.01744884066283703\n",
      "iteration 1333, dc_loss: 0.016812872141599655, tv_loss: 0.01744811050593853\n",
      "iteration 1334, dc_loss: 0.0168127603828907, tv_loss: 0.017447521910071373\n",
      "iteration 1335, dc_loss: 0.016812650486826897, tv_loss: 0.01744712144136429\n",
      "iteration 1336, dc_loss: 0.016812507063150406, tv_loss: 0.017446717247366905\n",
      "iteration 1337, dc_loss: 0.016812339425086975, tv_loss: 0.017446117475628853\n",
      "iteration 1338, dc_loss: 0.01681220717728138, tv_loss: 0.017445577308535576\n",
      "iteration 1339, dc_loss: 0.016812069341540337, tv_loss: 0.017445171251893044\n",
      "iteration 1340, dc_loss: 0.01681191474199295, tv_loss: 0.017444677650928497\n",
      "iteration 1341, dc_loss: 0.016811762005090714, tv_loss: 0.01744421012699604\n",
      "iteration 1342, dc_loss: 0.01681159809231758, tv_loss: 0.01744375191628933\n",
      "iteration 1343, dc_loss: 0.016811437904834747, tv_loss: 0.017443303018808365\n",
      "iteration 1344, dc_loss: 0.016811296343803406, tv_loss: 0.017442770302295685\n",
      "iteration 1345, dc_loss: 0.016811171546578407, tv_loss: 0.0174423661082983\n",
      "iteration 1346, dc_loss: 0.016811072826385498, tv_loss: 0.01744193583726883\n",
      "iteration 1347, dc_loss: 0.016810977831482887, tv_loss: 0.017441190779209137\n",
      "iteration 1348, dc_loss: 0.016810854896903038, tv_loss: 0.01744072325527668\n",
      "iteration 1349, dc_loss: 0.01681072637438774, tv_loss: 0.017440391704440117\n",
      "iteration 1350, dc_loss: 0.016810545697808266, tv_loss: 0.01743984967470169\n",
      "iteration 1351, dc_loss: 0.016810351982712746, tv_loss: 0.017439329996705055\n",
      "iteration 1352, dc_loss: 0.01681019738316536, tv_loss: 0.017439009621739388\n",
      "iteration 1353, dc_loss: 0.016810083761811256, tv_loss: 0.017438452690839767\n",
      "iteration 1354, dc_loss: 0.0168099794536829, tv_loss: 0.017437878996133804\n",
      "iteration 1355, dc_loss: 0.01680983230471611, tv_loss: 0.01743733510375023\n",
      "iteration 1356, dc_loss: 0.016809681430459023, tv_loss: 0.017437035217881203\n",
      "iteration 1357, dc_loss: 0.01680954173207283, tv_loss: 0.017436426132917404\n",
      "iteration 1358, dc_loss: 0.01680941879749298, tv_loss: 0.017435945570468903\n",
      "iteration 1359, dc_loss: 0.016809282824397087, tv_loss: 0.017435600981116295\n",
      "iteration 1360, dc_loss: 0.016809163615107536, tv_loss: 0.017434945330023766\n",
      "iteration 1361, dc_loss: 0.01680905558168888, tv_loss: 0.017434431239962578\n",
      "iteration 1362, dc_loss: 0.016808919608592987, tv_loss: 0.017434177920222282\n",
      "iteration 1363, dc_loss: 0.016808776184916496, tv_loss: 0.01743350364267826\n",
      "iteration 1364, dc_loss: 0.016808627173304558, tv_loss: 0.017433077096939087\n",
      "iteration 1365, dc_loss: 0.016808494925498962, tv_loss: 0.01743270643055439\n",
      "iteration 1366, dc_loss: 0.016808360815048218, tv_loss: 0.01743217371404171\n",
      "iteration 1367, dc_loss: 0.01680821366608143, tv_loss: 0.017431583255529404\n",
      "iteration 1368, dc_loss: 0.016808079555630684, tv_loss: 0.017431208863854408\n",
      "iteration 1369, dc_loss: 0.016807930544018745, tv_loss: 0.017430810257792473\n",
      "iteration 1370, dc_loss: 0.016807788982987404, tv_loss: 0.017430100589990616\n",
      "iteration 1371, dc_loss: 0.016807690262794495, tv_loss: 0.017429670318961143\n",
      "iteration 1372, dc_loss: 0.01680758222937584, tv_loss: 0.01742936298251152\n",
      "iteration 1373, dc_loss: 0.016807472333312035, tv_loss: 0.017428776249289513\n",
      "iteration 1374, dc_loss: 0.016807356849312782, tv_loss: 0.017428291961550713\n",
      "iteration 1375, dc_loss: 0.016807204112410545, tv_loss: 0.017427898943424225\n",
      "iteration 1376, dc_loss: 0.016807038336992264, tv_loss: 0.01742730289697647\n",
      "iteration 1377, dc_loss: 0.01680688001215458, tv_loss: 0.01742692105472088\n",
      "iteration 1378, dc_loss: 0.016806738451123238, tv_loss: 0.0174263846129179\n",
      "iteration 1379, dc_loss: 0.01680663414299488, tv_loss: 0.0174260251224041\n",
      "iteration 1380, dc_loss: 0.01680656708776951, tv_loss: 0.017425356432795525\n",
      "iteration 1381, dc_loss: 0.016806498169898987, tv_loss: 0.017424702644348145\n",
      "iteration 1382, dc_loss: 0.016806399449706078, tv_loss: 0.017424242570996284\n",
      "iteration 1383, dc_loss: 0.016806289553642273, tv_loss: 0.017423927783966064\n",
      "iteration 1384, dc_loss: 0.016806161031126976, tv_loss: 0.017423374578356743\n",
      "iteration 1385, dc_loss: 0.01680600829422474, tv_loss: 0.0174228698015213\n",
      "iteration 1386, dc_loss: 0.01680581457912922, tv_loss: 0.017422394827008247\n",
      "iteration 1387, dc_loss: 0.01680561527609825, tv_loss: 0.017422234639525414\n",
      "iteration 1388, dc_loss: 0.016805436462163925, tv_loss: 0.017421862110495567\n",
      "iteration 1389, dc_loss: 0.01680532842874527, tv_loss: 0.017421269789338112\n",
      "iteration 1390, dc_loss: 0.016805242747068405, tv_loss: 0.01742076687514782\n",
      "iteration 1391, dc_loss: 0.016805127263069153, tv_loss: 0.01742028072476387\n",
      "iteration 1392, dc_loss: 0.01680501364171505, tv_loss: 0.01741979643702507\n",
      "iteration 1393, dc_loss: 0.016804901883006096, tv_loss: 0.01741933636367321\n",
      "iteration 1394, dc_loss: 0.01680477149784565, tv_loss: 0.017418868839740753\n",
      "iteration 1395, dc_loss: 0.01680462807416916, tv_loss: 0.01741838827729225\n",
      "iteration 1396, dc_loss: 0.01680447719991207, tv_loss: 0.017418093979358673\n",
      "iteration 1397, dc_loss: 0.016804363578557968, tv_loss: 0.017417702823877335\n",
      "iteration 1398, dc_loss: 0.01680426299571991, tv_loss: 0.01741725206375122\n",
      "iteration 1399, dc_loss: 0.016804169863462448, tv_loss: 0.01741652563214302\n",
      "iteration 1400, dc_loss: 0.01680404506623745, tv_loss: 0.017416000366210938\n",
      "iteration 1401, dc_loss: 0.0168039221316576, tv_loss: 0.017415842041373253\n",
      "iteration 1402, dc_loss: 0.01680380292236805, tv_loss: 0.01741546206176281\n",
      "iteration 1403, dc_loss: 0.016803691163659096, tv_loss: 0.017414864152669907\n",
      "iteration 1404, dc_loss: 0.016803501173853874, tv_loss: 0.017414432018995285\n",
      "iteration 1405, dc_loss: 0.016803322359919548, tv_loss: 0.017414093017578125\n",
      "iteration 1406, dc_loss: 0.016803164035081863, tv_loss: 0.017413703724741936\n",
      "iteration 1407, dc_loss: 0.016803041100502014, tv_loss: 0.017413241788744926\n",
      "iteration 1408, dc_loss: 0.016802940517663956, tv_loss: 0.017412720248103142\n",
      "iteration 1409, dc_loss: 0.016802847385406494, tv_loss: 0.01741219125688076\n",
      "iteration 1410, dc_loss: 0.016802743077278137, tv_loss: 0.01741170696914196\n",
      "iteration 1411, dc_loss: 0.016802635043859482, tv_loss: 0.01741129532456398\n",
      "iteration 1412, dc_loss: 0.01680251583456993, tv_loss: 0.017410848289728165\n",
      "iteration 1413, dc_loss: 0.016802379861474037, tv_loss: 0.017410244792699814\n",
      "iteration 1414, dc_loss: 0.016802247613668442, tv_loss: 0.017409970983862877\n",
      "iteration 1415, dc_loss: 0.016802119091153145, tv_loss: 0.01740948110818863\n",
      "iteration 1416, dc_loss: 0.01680203713476658, tv_loss: 0.017408888787031174\n",
      "iteration 1417, dc_loss: 0.01680193468928337, tv_loss: 0.017408384010195732\n",
      "iteration 1418, dc_loss: 0.016801806166768074, tv_loss: 0.017408166080713272\n",
      "iteration 1419, dc_loss: 0.016801638528704643, tv_loss: 0.017407633364200592\n",
      "iteration 1420, dc_loss: 0.016801467165350914, tv_loss: 0.017407232895493507\n",
      "iteration 1421, dc_loss: 0.016801320016384125, tv_loss: 0.01740669645369053\n",
      "iteration 1422, dc_loss: 0.016801219433546066, tv_loss: 0.017406223341822624\n",
      "iteration 1423, dc_loss: 0.016801131889224052, tv_loss: 0.017405811697244644\n",
      "iteration 1424, dc_loss: 0.0168010201305151, tv_loss: 0.017405293881893158\n",
      "iteration 1425, dc_loss: 0.01680092327296734, tv_loss: 0.01740473508834839\n",
      "iteration 1426, dc_loss: 0.01680082641541958, tv_loss: 0.0174043457955122\n",
      "iteration 1427, dc_loss: 0.01680070348083973, tv_loss: 0.01740390434861183\n",
      "iteration 1428, dc_loss: 0.0168005283921957, tv_loss: 0.01740362122654915\n",
      "iteration 1429, dc_loss: 0.016800381243228912, tv_loss: 0.017403163015842438\n",
      "iteration 1430, dc_loss: 0.01680024340748787, tv_loss: 0.0174027681350708\n",
      "iteration 1431, dc_loss: 0.01680011674761772, tv_loss: 0.017402229830622673\n",
      "iteration 1432, dc_loss: 0.01679997891187668, tv_loss: 0.017402030527591705\n",
      "iteration 1433, dc_loss: 0.016799870878458023, tv_loss: 0.017401475459337234\n",
      "iteration 1434, dc_loss: 0.016799751669168472, tv_loss: 0.017400825396180153\n",
      "iteration 1435, dc_loss: 0.01679960824549198, tv_loss: 0.017400695011019707\n",
      "iteration 1436, dc_loss: 0.016799455508589745, tv_loss: 0.01740025170147419\n",
      "iteration 1437, dc_loss: 0.0167993176728487, tv_loss: 0.017399659380316734\n",
      "iteration 1438, dc_loss: 0.016799231991171837, tv_loss: 0.017399176955223083\n",
      "iteration 1439, dc_loss: 0.016799164935946465, tv_loss: 0.017398769035935402\n",
      "iteration 1440, dc_loss: 0.0167990829795599, tv_loss: 0.01739833876490593\n",
      "iteration 1441, dc_loss: 0.016798995435237885, tv_loss: 0.01739790476858616\n",
      "iteration 1442, dc_loss: 0.01679888926446438, tv_loss: 0.01739744283258915\n",
      "iteration 1443, dc_loss: 0.01679876632988453, tv_loss: 0.017396893352270126\n",
      "iteration 1444, dc_loss: 0.016798630356788635, tv_loss: 0.01739669218659401\n",
      "iteration 1445, dc_loss: 0.01679849438369274, tv_loss: 0.017396237701177597\n",
      "iteration 1446, dc_loss: 0.01679837517440319, tv_loss: 0.017395684495568275\n",
      "iteration 1447, dc_loss: 0.016798242926597595, tv_loss: 0.01739538088440895\n",
      "iteration 1448, dc_loss: 0.016798103228211403, tv_loss: 0.017395153641700745\n",
      "iteration 1449, dc_loss: 0.016797957941889763, tv_loss: 0.017394721508026123\n",
      "iteration 1450, dc_loss: 0.016797790303826332, tv_loss: 0.01739429123699665\n",
      "iteration 1451, dc_loss: 0.016797643154859543, tv_loss: 0.017393898218870163\n",
      "iteration 1452, dc_loss: 0.016797533258795738, tv_loss: 0.017393527552485466\n",
      "iteration 1453, dc_loss: 0.016797423362731934, tv_loss: 0.017393002286553383\n",
      "iteration 1454, dc_loss: 0.016797330230474472, tv_loss: 0.017392532899975777\n",
      "iteration 1455, dc_loss: 0.01679728738963604, tv_loss: 0.017392048612236977\n",
      "iteration 1456, dc_loss: 0.01679723709821701, tv_loss: 0.017391586676239967\n",
      "iteration 1457, dc_loss: 0.016797153279185295, tv_loss: 0.01739112287759781\n",
      "iteration 1458, dc_loss: 0.01679704524576664, tv_loss: 0.01739059016108513\n",
      "iteration 1459, dc_loss: 0.016796905547380447, tv_loss: 0.017390219494700432\n",
      "iteration 1460, dc_loss: 0.016796760261058807, tv_loss: 0.017390012741088867\n",
      "iteration 1461, dc_loss: 0.016796628013253212, tv_loss: 0.017389334738254547\n",
      "iteration 1462, dc_loss: 0.016796497628092766, tv_loss: 0.017388848587870598\n",
      "iteration 1463, dc_loss: 0.01679636538028717, tv_loss: 0.017388708889484406\n",
      "iteration 1464, dc_loss: 0.016796227544546127, tv_loss: 0.017388319596648216\n",
      "iteration 1465, dc_loss: 0.016796080395579338, tv_loss: 0.017387881875038147\n",
      "iteration 1466, dc_loss: 0.01679595559835434, tv_loss: 0.017387619242072105\n",
      "iteration 1467, dc_loss: 0.016795845702290535, tv_loss: 0.017387252300977707\n",
      "iteration 1468, dc_loss: 0.01679573394358158, tv_loss: 0.017386656254529953\n",
      "iteration 1469, dc_loss: 0.016795609146356583, tv_loss: 0.01738632097840309\n",
      "iteration 1470, dc_loss: 0.016795475035905838, tv_loss: 0.01738588884472847\n",
      "iteration 1471, dc_loss: 0.01679535210132599, tv_loss: 0.01738545671105385\n",
      "iteration 1472, dc_loss: 0.016795221716165543, tv_loss: 0.01738504134118557\n",
      "iteration 1473, dc_loss: 0.016795089468359947, tv_loss: 0.017384640872478485\n",
      "iteration 1474, dc_loss: 0.016794972121715546, tv_loss: 0.017384406179189682\n",
      "iteration 1475, dc_loss: 0.016794854775071144, tv_loss: 0.017383698374032974\n",
      "iteration 1476, dc_loss: 0.01679476909339428, tv_loss: 0.017383377999067307\n",
      "iteration 1477, dc_loss: 0.01679469645023346, tv_loss: 0.017382893711328506\n",
      "iteration 1478, dc_loss: 0.016794629395008087, tv_loss: 0.017382347956299782\n",
      "iteration 1479, dc_loss: 0.016794534400105476, tv_loss: 0.017381878569722176\n",
      "iteration 1480, dc_loss: 0.016794441267848015, tv_loss: 0.01738155260682106\n",
      "iteration 1481, dc_loss: 0.016794320195913315, tv_loss: 0.017381103709340096\n",
      "iteration 1482, dc_loss: 0.01679416000843048, tv_loss: 0.01738075353205204\n",
      "iteration 1483, dc_loss: 0.0167939942330122, tv_loss: 0.017380516976118088\n",
      "iteration 1484, dc_loss: 0.016793837770819664, tv_loss: 0.017380161210894585\n",
      "iteration 1485, dc_loss: 0.01679372601211071, tv_loss: 0.017379742115736008\n",
      "iteration 1486, dc_loss: 0.0167936272919178, tv_loss: 0.01737937517464161\n",
      "iteration 1487, dc_loss: 0.01679350435733795, tv_loss: 0.017378877848386765\n",
      "iteration 1488, dc_loss: 0.016793394461274147, tv_loss: 0.017378438264131546\n",
      "iteration 1489, dc_loss: 0.01679331436753273, tv_loss: 0.017378056421875954\n",
      "iteration 1490, dc_loss: 0.01679324358701706, tv_loss: 0.017377661541104317\n",
      "iteration 1491, dc_loss: 0.016793150454759598, tv_loss: 0.017377125099301338\n",
      "iteration 1492, dc_loss: 0.016793007031083107, tv_loss: 0.017376886680722237\n",
      "iteration 1493, dc_loss: 0.01679285615682602, tv_loss: 0.017376529052853584\n",
      "iteration 1494, dc_loss: 0.016792722046375275, tv_loss: 0.01737622171640396\n",
      "iteration 1495, dc_loss: 0.016792604699730873, tv_loss: 0.01737569272518158\n",
      "iteration 1496, dc_loss: 0.016792496666312218, tv_loss: 0.017375463619828224\n",
      "iteration 1497, dc_loss: 0.016792364418506622, tv_loss: 0.017374958842992783\n",
      "iteration 1498, dc_loss: 0.016792302951216698, tv_loss: 0.017374375835061073\n",
      "iteration 1499, dc_loss: 0.016792256385087967, tv_loss: 0.01737397350370884\n",
      "iteration 1500, dc_loss: 0.016792181879281998, tv_loss: 0.017373476177453995\n",
      "iteration 1501, dc_loss: 0.016792064532637596, tv_loss: 0.017373131588101387\n",
      "iteration 1502, dc_loss: 0.01679195649921894, tv_loss: 0.017372775822877884\n",
      "iteration 1503, dc_loss: 0.016791842877864838, tv_loss: 0.01737251877784729\n",
      "iteration 1504, dc_loss: 0.01679171435534954, tv_loss: 0.017372123897075653\n",
      "iteration 1505, dc_loss: 0.01679159328341484, tv_loss: 0.017371611669659615\n",
      "iteration 1506, dc_loss: 0.01679142750799656, tv_loss: 0.01737142913043499\n",
      "iteration 1507, dc_loss: 0.01679125986993313, tv_loss: 0.017371000722050667\n",
      "iteration 1508, dc_loss: 0.016791105270385742, tv_loss: 0.017370469868183136\n",
      "iteration 1509, dc_loss: 0.01679099351167679, tv_loss: 0.017370101064443588\n",
      "iteration 1510, dc_loss: 0.01679091900587082, tv_loss: 0.017369745299220085\n",
      "iteration 1511, dc_loss: 0.01679087057709694, tv_loss: 0.017369359731674194\n",
      "iteration 1512, dc_loss: 0.016790827736258507, tv_loss: 0.017368806526064873\n",
      "iteration 1513, dc_loss: 0.016790762543678284, tv_loss: 0.017368389293551445\n",
      "iteration 1514, dc_loss: 0.016790656372904778, tv_loss: 0.01736786961555481\n",
      "iteration 1515, dc_loss: 0.01679050549864769, tv_loss: 0.01736760325729847\n",
      "iteration 1516, dc_loss: 0.01679033786058426, tv_loss: 0.01736721768975258\n",
      "iteration 1517, dc_loss: 0.016790194436907768, tv_loss: 0.017366889864206314\n",
      "iteration 1518, dc_loss: 0.016790084540843964, tv_loss: 0.017366617918014526\n",
      "iteration 1519, dc_loss: 0.016790024936199188, tv_loss: 0.017366060987114906\n",
      "iteration 1520, dc_loss: 0.016789967194199562, tv_loss: 0.01736564002931118\n",
      "iteration 1521, dc_loss: 0.01678984984755516, tv_loss: 0.01736517809331417\n",
      "iteration 1522, dc_loss: 0.016789769753813744, tv_loss: 0.01736477017402649\n",
      "iteration 1523, dc_loss: 0.01678970456123352, tv_loss: 0.017364395782351494\n",
      "iteration 1524, dc_loss: 0.01678956300020218, tv_loss: 0.017364079132676125\n",
      "iteration 1525, dc_loss: 0.01678941771388054, tv_loss: 0.017363760620355606\n",
      "iteration 1526, dc_loss: 0.01678929664194584, tv_loss: 0.01736338622868061\n",
      "iteration 1527, dc_loss: 0.01678922213613987, tv_loss: 0.01736288331449032\n",
      "iteration 1528, dc_loss: 0.016789112240076065, tv_loss: 0.01736251823604107\n",
      "iteration 1529, dc_loss: 0.016788985580205917, tv_loss: 0.017362108454108238\n",
      "iteration 1530, dc_loss: 0.016788842156529427, tv_loss: 0.01736188307404518\n",
      "iteration 1531, dc_loss: 0.01678870990872383, tv_loss: 0.01736147329211235\n",
      "iteration 1532, dc_loss: 0.016788626089692116, tv_loss: 0.017361031845211983\n",
      "iteration 1533, dc_loss: 0.016788534820079803, tv_loss: 0.01736067794263363\n",
      "iteration 1534, dc_loss: 0.016788484528660774, tv_loss: 0.01736009493470192\n",
      "iteration 1535, dc_loss: 0.016788391396403313, tv_loss: 0.017359746620059013\n",
      "iteration 1536, dc_loss: 0.01678832434117794, tv_loss: 0.01735949143767357\n",
      "iteration 1537, dc_loss: 0.016788236796855927, tv_loss: 0.017359009012579918\n",
      "iteration 1538, dc_loss: 0.016788111999630928, tv_loss: 0.01735861971974373\n",
      "iteration 1539, dc_loss: 0.01678795926272869, tv_loss: 0.017358269542455673\n",
      "iteration 1540, dc_loss: 0.01678779162466526, tv_loss: 0.017357967793941498\n",
      "iteration 1541, dc_loss: 0.016787642613053322, tv_loss: 0.01735755056142807\n",
      "iteration 1542, dc_loss: 0.016787543892860413, tv_loss: 0.017357319593429565\n",
      "iteration 1543, dc_loss: 0.016787467524409294, tv_loss: 0.01735709235072136\n",
      "iteration 1544, dc_loss: 0.01678740233182907, tv_loss: 0.017356542870402336\n",
      "iteration 1545, dc_loss: 0.016787303611636162, tv_loss: 0.017355995252728462\n",
      "iteration 1546, dc_loss: 0.016787221655249596, tv_loss: 0.01735563389956951\n",
      "iteration 1547, dc_loss: 0.016787126660346985, tv_loss: 0.01735547184944153\n",
      "iteration 1548, dc_loss: 0.016787035390734673, tv_loss: 0.017355045303702354\n",
      "iteration 1549, dc_loss: 0.016786929219961166, tv_loss: 0.017354516312479973\n",
      "iteration 1550, dc_loss: 0.016786837950348854, tv_loss: 0.01735405996441841\n",
      "iteration 1551, dc_loss: 0.016786731779575348, tv_loss: 0.017353802919387817\n",
      "iteration 1552, dc_loss: 0.016786623746156693, tv_loss: 0.017353525385260582\n",
      "iteration 1553, dc_loss: 0.01678648218512535, tv_loss: 0.01735309325158596\n",
      "iteration 1554, dc_loss: 0.01678634062409401, tv_loss: 0.01735258474946022\n",
      "iteration 1555, dc_loss: 0.01678621955215931, tv_loss: 0.01735231652855873\n",
      "iteration 1556, dc_loss: 0.0167861208319664, tv_loss: 0.017352065071463585\n",
      "iteration 1557, dc_loss: 0.01678602024912834, tv_loss: 0.01735142432153225\n",
      "iteration 1558, dc_loss: 0.016785921528935432, tv_loss: 0.017351076006889343\n",
      "iteration 1559, dc_loss: 0.016785848885774612, tv_loss: 0.017350956797599792\n",
      "iteration 1560, dc_loss: 0.016785772517323494, tv_loss: 0.017350386828184128\n",
      "iteration 1561, dc_loss: 0.016785668209195137, tv_loss: 0.017350012436509132\n",
      "iteration 1562, dc_loss: 0.016785534098744392, tv_loss: 0.017349669709801674\n",
      "iteration 1563, dc_loss: 0.016785405576229095, tv_loss: 0.017349224537611008\n",
      "iteration 1564, dc_loss: 0.01678531989455223, tv_loss: 0.017348870635032654\n",
      "iteration 1565, dc_loss: 0.016785206273198128, tv_loss: 0.01734847016632557\n",
      "iteration 1566, dc_loss: 0.016785141080617905, tv_loss: 0.01734796166419983\n",
      "iteration 1567, dc_loss: 0.01678507775068283, tv_loss: 0.017347561195492744\n",
      "iteration 1568, dc_loss: 0.016784988343715668, tv_loss: 0.017347142100334167\n",
      "iteration 1569, dc_loss: 0.016784870997071266, tv_loss: 0.01734691858291626\n",
      "iteration 1570, dc_loss: 0.01678476668894291, tv_loss: 0.01734657771885395\n",
      "iteration 1571, dc_loss: 0.016784679144620895, tv_loss: 0.017346080392599106\n",
      "iteration 1572, dc_loss: 0.016784587875008583, tv_loss: 0.017345700412988663\n",
      "iteration 1573, dc_loss: 0.016784491017460823, tv_loss: 0.01734538935124874\n",
      "iteration 1574, dc_loss: 0.01678437739610672, tv_loss: 0.017345013096928596\n",
      "iteration 1575, dc_loss: 0.016784265637397766, tv_loss: 0.017344530671834946\n",
      "iteration 1576, dc_loss: 0.01678413711488247, tv_loss: 0.017344355583190918\n",
      "iteration 1577, dc_loss: 0.01678401231765747, tv_loss: 0.01734406128525734\n",
      "iteration 1578, dc_loss: 0.016783928498625755, tv_loss: 0.01734359934926033\n",
      "iteration 1579, dc_loss: 0.016783835366368294, tv_loss: 0.017343174666166306\n",
      "iteration 1580, dc_loss: 0.01678374782204628, tv_loss: 0.01734280027449131\n",
      "iteration 1581, dc_loss: 0.01678362302482128, tv_loss: 0.01734256185591221\n",
      "iteration 1582, dc_loss: 0.016783462837338448, tv_loss: 0.017342325299978256\n",
      "iteration 1583, dc_loss: 0.016783341765403748, tv_loss: 0.017342057079076767\n",
      "iteration 1584, dc_loss: 0.016783257946372032, tv_loss: 0.017341595143079758\n",
      "iteration 1585, dc_loss: 0.016783181577920914, tv_loss: 0.017341243103146553\n",
      "iteration 1586, dc_loss: 0.0167830940335989, tv_loss: 0.01734091527760029\n",
      "iteration 1587, dc_loss: 0.01678302139043808, tv_loss: 0.01734045147895813\n",
      "iteration 1588, dc_loss: 0.01678295247256756, tv_loss: 0.01734011434018612\n",
      "iteration 1589, dc_loss: 0.0167828518897295, tv_loss: 0.017339693382382393\n",
      "iteration 1590, dc_loss: 0.01678275689482689, tv_loss: 0.017339300364255905\n",
      "iteration 1591, dc_loss: 0.01678265817463398, tv_loss: 0.017338819801807404\n",
      "iteration 1592, dc_loss: 0.016782555729150772, tv_loss: 0.01733863353729248\n",
      "iteration 1593, dc_loss: 0.016782406717538834, tv_loss: 0.0173383429646492\n",
      "iteration 1594, dc_loss: 0.016782274469733238, tv_loss: 0.017337961122393608\n",
      "iteration 1595, dc_loss: 0.01678217016160488, tv_loss: 0.017337612807750702\n",
      "iteration 1596, dc_loss: 0.016782082617282867, tv_loss: 0.01733718067407608\n",
      "iteration 1597, dc_loss: 0.016782034188508987, tv_loss: 0.01733686961233616\n",
      "iteration 1598, dc_loss: 0.016781968995928764, tv_loss: 0.017336471006274223\n",
      "iteration 1599, dc_loss: 0.016781901940703392, tv_loss: 0.01733599416911602\n",
      "iteration 1600, dc_loss: 0.01678183674812317, tv_loss: 0.01733560487627983\n",
      "iteration 1601, dc_loss: 0.016781756654381752, tv_loss: 0.017335204407572746\n",
      "iteration 1602, dc_loss: 0.01678159274160862, tv_loss: 0.017334958538413048\n",
      "iteration 1603, dc_loss: 0.01678142510354519, tv_loss: 0.01733466424047947\n",
      "iteration 1604, dc_loss: 0.016781315207481384, tv_loss: 0.017334286123514175\n",
      "iteration 1605, dc_loss: 0.016781242564320564, tv_loss: 0.01733400858938694\n",
      "iteration 1606, dc_loss: 0.016781184822320938, tv_loss: 0.017333606258034706\n",
      "iteration 1607, dc_loss: 0.01678108051419258, tv_loss: 0.017333179712295532\n",
      "iteration 1608, dc_loss: 0.01678093709051609, tv_loss: 0.017332905903458595\n",
      "iteration 1609, dc_loss: 0.016780799254775047, tv_loss: 0.01733250729739666\n",
      "iteration 1610, dc_loss: 0.01678067073225975, tv_loss: 0.01733219251036644\n",
      "iteration 1611, dc_loss: 0.016780590638518333, tv_loss: 0.017331918701529503\n",
      "iteration 1612, dc_loss: 0.0167805477976799, tv_loss: 0.017331600189208984\n",
      "iteration 1613, dc_loss: 0.01678045280277729, tv_loss: 0.017331261187791824\n",
      "iteration 1614, dc_loss: 0.016780365258455276, tv_loss: 0.017330896109342575\n",
      "iteration 1615, dc_loss: 0.016780300065875053, tv_loss: 0.017330527305603027\n",
      "iteration 1616, dc_loss: 0.01678023487329483, tv_loss: 0.01733003742992878\n",
      "iteration 1617, dc_loss: 0.016780132427811623, tv_loss: 0.017329752445220947\n",
      "iteration 1618, dc_loss: 0.01677999086678028, tv_loss: 0.017329538241028786\n",
      "iteration 1619, dc_loss: 0.016779843717813492, tv_loss: 0.017329243943095207\n",
      "iteration 1620, dc_loss: 0.01677972450852394, tv_loss: 0.017328893765807152\n",
      "iteration 1621, dc_loss: 0.016779592260718346, tv_loss: 0.017328526824712753\n",
      "iteration 1622, dc_loss: 0.016779521480202675, tv_loss: 0.01732826419174671\n",
      "iteration 1623, dc_loss: 0.016779465600848198, tv_loss: 0.017327886074781418\n",
      "iteration 1624, dc_loss: 0.016779426485300064, tv_loss: 0.017327547073364258\n",
      "iteration 1625, dc_loss: 0.0167793370783329, tv_loss: 0.017327116802334785\n",
      "iteration 1626, dc_loss: 0.01677924022078514, tv_loss: 0.017326723784208298\n",
      "iteration 1627, dc_loss: 0.01677914708852768, tv_loss: 0.017326464876532555\n",
      "iteration 1628, dc_loss: 0.016779037192463875, tv_loss: 0.017326241359114647\n",
      "iteration 1629, dc_loss: 0.016778863966464996, tv_loss: 0.01732608489692211\n",
      "iteration 1630, dc_loss: 0.016778696328401566, tv_loss: 0.017325663939118385\n",
      "iteration 1631, dc_loss: 0.016778578981757164, tv_loss: 0.01732531562447548\n",
      "iteration 1632, dc_loss: 0.016778508201241493, tv_loss: 0.01732495427131653\n",
      "iteration 1633, dc_loss: 0.01677849143743515, tv_loss: 0.017324579879641533\n",
      "iteration 1634, dc_loss: 0.016778474673628807, tv_loss: 0.01732414960861206\n",
      "iteration 1635, dc_loss: 0.016778424382209778, tv_loss: 0.01732364110648632\n",
      "iteration 1636, dc_loss: 0.016778359189629555, tv_loss: 0.017323141917586327\n",
      "iteration 1637, dc_loss: 0.016778286546468735, tv_loss: 0.017322944477200508\n",
      "iteration 1638, dc_loss: 0.016778185963630676, tv_loss: 0.017322750762104988\n",
      "iteration 1639, dc_loss: 0.01677805371582508, tv_loss: 0.01732226274907589\n",
      "iteration 1640, dc_loss: 0.016777925193309784, tv_loss: 0.017321886494755745\n",
      "iteration 1641, dc_loss: 0.016777832061052322, tv_loss: 0.01732163317501545\n",
      "iteration 1642, dc_loss: 0.016777729615569115, tv_loss: 0.01732136495411396\n",
      "iteration 1643, dc_loss: 0.016777608543634415, tv_loss: 0.017320998013019562\n",
      "iteration 1644, dc_loss: 0.01677747815847397, tv_loss: 0.017320720478892326\n",
      "iteration 1645, dc_loss: 0.016777336597442627, tv_loss: 0.017320292070508003\n",
      "iteration 1646, dc_loss: 0.01677723228931427, tv_loss: 0.017320098355412483\n",
      "iteration 1647, dc_loss: 0.016777150332927704, tv_loss: 0.017319755628705025\n",
      "iteration 1648, dc_loss: 0.01677706465125084, tv_loss: 0.017319312319159508\n",
      "iteration 1649, dc_loss: 0.016776952892541885, tv_loss: 0.0173189640045166\n",
      "iteration 1650, dc_loss: 0.016776815056800842, tv_loss: 0.0173187255859375\n",
      "iteration 1651, dc_loss: 0.016776712611317635, tv_loss: 0.017318395897746086\n",
      "iteration 1652, dc_loss: 0.016776639968156815, tv_loss: 0.017317939549684525\n",
      "iteration 1653, dc_loss: 0.01677658036351204, tv_loss: 0.01731771044433117\n",
      "iteration 1654, dc_loss: 0.016776515170931816, tv_loss: 0.017317121848464012\n",
      "iteration 1655, dc_loss: 0.016776468604803085, tv_loss: 0.01731666550040245\n",
      "iteration 1656, dc_loss: 0.016776422038674355, tv_loss: 0.017316430807113647\n",
      "iteration 1657, dc_loss: 0.01677633263170719, tv_loss: 0.017316292971372604\n",
      "iteration 1658, dc_loss: 0.016776202246546745, tv_loss: 0.017315980046987534\n",
      "iteration 1659, dc_loss: 0.016776058822870255, tv_loss: 0.01731564663350582\n",
      "iteration 1660, dc_loss: 0.016775954514741898, tv_loss: 0.017315326258540154\n",
      "iteration 1661, dc_loss: 0.016775889322161674, tv_loss: 0.01731497049331665\n",
      "iteration 1662, dc_loss: 0.016775816679000854, tv_loss: 0.017314668744802475\n",
      "iteration 1663, dc_loss: 0.016775740310549736, tv_loss: 0.017314231023192406\n",
      "iteration 1664, dc_loss: 0.016775663942098618, tv_loss: 0.017313964664936066\n",
      "iteration 1665, dc_loss: 0.016775574535131454, tv_loss: 0.01731371134519577\n",
      "iteration 1666, dc_loss: 0.01677543856203556, tv_loss: 0.017313437536358833\n",
      "iteration 1667, dc_loss: 0.01677532307803631, tv_loss: 0.017313150689005852\n",
      "iteration 1668, dc_loss: 0.016775231808423996, tv_loss: 0.017312828451395035\n",
      "iteration 1669, dc_loss: 0.016775136813521385, tv_loss: 0.017312321811914444\n",
      "iteration 1670, dc_loss: 0.01677505113184452, tv_loss: 0.017311982810497284\n",
      "iteration 1671, dc_loss: 0.016774959862232208, tv_loss: 0.01731184870004654\n",
      "iteration 1672, dc_loss: 0.016774838790297508, tv_loss: 0.017311478033661842\n",
      "iteration 1673, dc_loss: 0.016774753108620644, tv_loss: 0.0173110980540514\n",
      "iteration 1674, dc_loss: 0.01677464321255684, tv_loss: 0.017310798168182373\n",
      "iteration 1675, dc_loss: 0.016774531453847885, tv_loss: 0.017310502007603645\n",
      "iteration 1676, dc_loss: 0.016774440184235573, tv_loss: 0.017310261726379395\n",
      "iteration 1677, dc_loss: 0.016774337738752365, tv_loss: 0.0173098873347044\n",
      "iteration 1678, dc_loss: 0.016774246469140053, tv_loss: 0.017309507355093956\n",
      "iteration 1679, dc_loss: 0.016774166375398636, tv_loss: 0.017309142276644707\n",
      "iteration 1680, dc_loss: 0.016774093732237816, tv_loss: 0.01730884052813053\n",
      "iteration 1681, dc_loss: 0.016774039715528488, tv_loss: 0.017308471724390984\n",
      "iteration 1682, dc_loss: 0.016773980110883713, tv_loss: 0.017307983711361885\n",
      "iteration 1683, dc_loss: 0.016773896291851997, tv_loss: 0.017307808622717857\n",
      "iteration 1684, dc_loss: 0.01677379198372364, tv_loss: 0.017307603731751442\n",
      "iteration 1685, dc_loss: 0.016773691400885582, tv_loss: 0.017307136207818985\n",
      "iteration 1686, dc_loss: 0.01677360013127327, tv_loss: 0.017306862398982048\n",
      "iteration 1687, dc_loss: 0.016773486509919167, tv_loss: 0.017306627705693245\n",
      "iteration 1688, dc_loss: 0.01677335612475872, tv_loss: 0.017306357622146606\n",
      "iteration 1689, dc_loss: 0.016773240640759468, tv_loss: 0.01730605959892273\n",
      "iteration 1690, dc_loss: 0.01677316427230835, tv_loss: 0.017305733636021614\n",
      "iteration 1691, dc_loss: 0.01677311211824417, tv_loss: 0.017305346205830574\n",
      "iteration 1692, dc_loss: 0.016773024573922157, tv_loss: 0.01730501651763916\n",
      "iteration 1693, dc_loss: 0.01677294448018074, tv_loss: 0.017304683104157448\n",
      "iteration 1694, dc_loss: 0.01677286997437477, tv_loss: 0.017304377630352974\n",
      "iteration 1695, dc_loss: 0.01677282527089119, tv_loss: 0.0173040758818388\n",
      "iteration 1696, dc_loss: 0.016772743314504623, tv_loss: 0.0173037089407444\n",
      "iteration 1697, dc_loss: 0.016772648319602013, tv_loss: 0.017303453758358955\n",
      "iteration 1698, dc_loss: 0.016772562637925148, tv_loss: 0.01730307936668396\n",
      "iteration 1699, dc_loss: 0.016772441565990448, tv_loss: 0.017302755266427994\n",
      "iteration 1700, dc_loss: 0.016772307455539703, tv_loss: 0.01730259135365486\n",
      "iteration 1701, dc_loss: 0.016772225499153137, tv_loss: 0.017302365973591805\n",
      "iteration 1702, dc_loss: 0.016772182658314705, tv_loss: 0.01730187237262726\n",
      "iteration 1703, dc_loss: 0.016772156581282616, tv_loss: 0.017301319167017937\n",
      "iteration 1704, dc_loss: 0.01677212491631508, tv_loss: 0.01730106584727764\n",
      "iteration 1705, dc_loss: 0.0167720727622509, tv_loss: 0.017300758510828018\n",
      "iteration 1706, dc_loss: 0.016771968454122543, tv_loss: 0.01730036735534668\n",
      "iteration 1707, dc_loss: 0.016771825030446053, tv_loss: 0.01730027236044407\n",
      "iteration 1708, dc_loss: 0.016771666705608368, tv_loss: 0.017299804836511612\n",
      "iteration 1709, dc_loss: 0.016771536320447922, tv_loss: 0.017299573868513107\n",
      "iteration 1710, dc_loss: 0.01677144132554531, tv_loss: 0.017299292609095573\n",
      "iteration 1711, dc_loss: 0.016771377995610237, tv_loss: 0.017298996448516846\n",
      "iteration 1712, dc_loss: 0.01677127741277218, tv_loss: 0.017298642545938492\n",
      "iteration 1713, dc_loss: 0.01677117310464382, tv_loss: 0.017298374325037003\n",
      "iteration 1714, dc_loss: 0.01677107997238636, tv_loss: 0.017298076301813126\n",
      "iteration 1715, dc_loss: 0.0167709831148386, tv_loss: 0.017297828570008278\n",
      "iteration 1716, dc_loss: 0.01677088439464569, tv_loss: 0.017297623679041862\n",
      "iteration 1717, dc_loss: 0.016770819202065468, tv_loss: 0.017297156155109406\n",
      "iteration 1718, dc_loss: 0.016770752146840096, tv_loss: 0.017296813428401947\n",
      "iteration 1719, dc_loss: 0.016770649701356888, tv_loss: 0.01729666069149971\n",
      "iteration 1720, dc_loss: 0.016770586371421814, tv_loss: 0.017296357080340385\n",
      "iteration 1721, dc_loss: 0.01677052676677704, tv_loss: 0.017295842990279198\n",
      "iteration 1722, dc_loss: 0.016770483925938606, tv_loss: 0.01729554682970047\n",
      "iteration 1723, dc_loss: 0.016770437359809875, tv_loss: 0.017295412719249725\n",
      "iteration 1724, dc_loss: 0.016770396381616592, tv_loss: 0.017294863238930702\n",
      "iteration 1725, dc_loss: 0.016770312562584877, tv_loss: 0.01729442924261093\n",
      "iteration 1726, dc_loss: 0.016770226880908012, tv_loss: 0.01729409769177437\n",
      "iteration 1727, dc_loss: 0.016770116984844208, tv_loss: 0.01729389652609825\n",
      "iteration 1728, dc_loss: 0.016769954934716225, tv_loss: 0.01729384809732437\n",
      "iteration 1729, dc_loss: 0.016769807785749435, tv_loss: 0.01729336753487587\n",
      "iteration 1730, dc_loss: 0.016769694164395332, tv_loss: 0.017293112352490425\n",
      "iteration 1731, dc_loss: 0.01676957495510578, tv_loss: 0.017292870208621025\n",
      "iteration 1732, dc_loss: 0.016769446432590485, tv_loss: 0.017292624339461327\n",
      "iteration 1733, dc_loss: 0.01676936447620392, tv_loss: 0.017292283475399017\n",
      "iteration 1734, dc_loss: 0.01676931604743004, tv_loss: 0.017291896045207977\n",
      "iteration 1735, dc_loss: 0.01676926575601101, tv_loss: 0.017291683703660965\n",
      "iteration 1736, dc_loss: 0.016769196838140488, tv_loss: 0.017291447147727013\n",
      "iteration 1737, dc_loss: 0.016769103705883026, tv_loss: 0.017291104421019554\n",
      "iteration 1738, dc_loss: 0.016769012436270714, tv_loss: 0.017290910705924034\n",
      "iteration 1739, dc_loss: 0.016768941655755043, tv_loss: 0.017290497198700905\n",
      "iteration 1740, dc_loss: 0.016768895089626312, tv_loss: 0.017290079966187477\n",
      "iteration 1741, dc_loss: 0.01676885038614273, tv_loss: 0.017289770767092705\n",
      "iteration 1742, dc_loss: 0.016768774017691612, tv_loss: 0.017289558425545692\n",
      "iteration 1743, dc_loss: 0.016768671572208405, tv_loss: 0.017289239913225174\n",
      "iteration 1744, dc_loss: 0.016768546774983406, tv_loss: 0.017288966104388237\n",
      "iteration 1745, dc_loss: 0.016768384724855423, tv_loss: 0.01728871837258339\n",
      "iteration 1746, dc_loss: 0.016768259927630424, tv_loss: 0.01728834956884384\n",
      "iteration 1747, dc_loss: 0.016768166795372963, tv_loss: 0.017288099974393845\n",
      "iteration 1748, dc_loss: 0.016768133267760277, tv_loss: 0.017287803813815117\n",
      "iteration 1749, dc_loss: 0.016768110916018486, tv_loss: 0.01728757657110691\n",
      "iteration 1750, dc_loss: 0.01676810346543789, tv_loss: 0.01728713884949684\n",
      "iteration 1751, dc_loss: 0.016768062487244606, tv_loss: 0.01728677749633789\n",
      "iteration 1752, dc_loss: 0.016767993569374084, tv_loss: 0.01728619448840618\n",
      "iteration 1753, dc_loss: 0.016767891123890877, tv_loss: 0.01728609763085842\n",
      "iteration 1754, dc_loss: 0.01676776260137558, tv_loss: 0.017285868525505066\n",
      "iteration 1755, dc_loss: 0.016767650842666626, tv_loss: 0.017285384237766266\n",
      "iteration 1756, dc_loss: 0.01676756702363491, tv_loss: 0.017285175621509552\n",
      "iteration 1757, dc_loss: 0.016767462715506554, tv_loss: 0.01728500798344612\n",
      "iteration 1758, dc_loss: 0.01676737703382969, tv_loss: 0.01728474535048008\n",
      "iteration 1759, dc_loss: 0.01676730439066887, tv_loss: 0.017284376546740532\n",
      "iteration 1760, dc_loss: 0.01676718331873417, tv_loss: 0.01728404127061367\n",
      "iteration 1761, dc_loss: 0.016767097637057304, tv_loss: 0.017283907160162926\n",
      "iteration 1762, dc_loss: 0.01676701381802559, tv_loss: 0.017283789813518524\n",
      "iteration 1763, dc_loss: 0.01676691323518753, tv_loss: 0.017283335328102112\n",
      "iteration 1764, dc_loss: 0.01676683872938156, tv_loss: 0.0172830019146204\n",
      "iteration 1765, dc_loss: 0.016766764223575592, tv_loss: 0.017282826825976372\n",
      "iteration 1766, dc_loss: 0.016766661778092384, tv_loss: 0.017282556742429733\n",
      "iteration 1767, dc_loss: 0.016766589134931564, tv_loss: 0.017282232642173767\n",
      "iteration 1768, dc_loss: 0.01676652394235134, tv_loss: 0.0172819085419178\n",
      "iteration 1769, dc_loss: 0.016766468062996864, tv_loss: 0.017281459644436836\n",
      "iteration 1770, dc_loss: 0.01676640845835209, tv_loss: 0.01728108897805214\n",
      "iteration 1771, dc_loss: 0.016766350716352463, tv_loss: 0.01728079840540886\n",
      "iteration 1772, dc_loss: 0.016766270622611046, tv_loss: 0.01728055626153946\n",
      "iteration 1773, dc_loss: 0.016766201704740524, tv_loss: 0.01728019490838051\n",
      "iteration 1774, dc_loss: 0.01676611602306366, tv_loss: 0.017279857769608498\n",
      "iteration 1775, dc_loss: 0.016766035929322243, tv_loss: 0.017279744148254395\n",
      "iteration 1776, dc_loss: 0.016765965148806572, tv_loss: 0.01727942004799843\n",
      "iteration 1777, dc_loss: 0.016765905544161797, tv_loss: 0.017279017716646194\n",
      "iteration 1778, dc_loss: 0.016765832901000977, tv_loss: 0.01727866567671299\n",
      "iteration 1779, dc_loss: 0.016765760257840157, tv_loss: 0.017278403043746948\n",
      "iteration 1780, dc_loss: 0.01676570624113083, tv_loss: 0.017278185114264488\n",
      "iteration 1781, dc_loss: 0.016765601933002472, tv_loss: 0.017277810722589493\n",
      "iteration 1782, dc_loss: 0.016765499487519264, tv_loss: 0.0172775499522686\n",
      "iteration 1783, dc_loss: 0.01676538586616516, tv_loss: 0.017277346923947334\n",
      "iteration 1784, dc_loss: 0.016765283420681953, tv_loss: 0.017277028411626816\n",
      "iteration 1785, dc_loss: 0.016765199601650238, tv_loss: 0.017276767641305923\n",
      "iteration 1786, dc_loss: 0.016765128821134567, tv_loss: 0.017276406288146973\n",
      "iteration 1787, dc_loss: 0.01676507107913494, tv_loss: 0.017276229336857796\n",
      "iteration 1788, dc_loss: 0.016765011474490166, tv_loss: 0.017275959253311157\n",
      "iteration 1789, dc_loss: 0.016764968633651733, tv_loss: 0.017275413498282433\n",
      "iteration 1790, dc_loss: 0.016764897853136063, tv_loss: 0.01727522537112236\n",
      "iteration 1791, dc_loss: 0.016764787957072258, tv_loss: 0.017275165766477585\n",
      "iteration 1792, dc_loss: 0.016764657571911812, tv_loss: 0.017274867743253708\n",
      "iteration 1793, dc_loss: 0.016764536499977112, tv_loss: 0.01727466844022274\n",
      "iteration 1794, dc_loss: 0.016764439642429352, tv_loss: 0.017274336889386177\n",
      "iteration 1795, dc_loss: 0.016764380037784576, tv_loss: 0.017274171113967896\n",
      "iteration 1796, dc_loss: 0.016764311119914055, tv_loss: 0.017273753881454468\n",
      "iteration 1797, dc_loss: 0.016764232888817787, tv_loss: 0.01727348007261753\n",
      "iteration 1798, dc_loss: 0.016764167696237564, tv_loss: 0.017273059114813805\n",
      "iteration 1799, dc_loss: 0.016764143481850624, tv_loss: 0.017272619530558586\n",
      "iteration 1800, dc_loss: 0.01676410250365734, tv_loss: 0.017272403463721275\n",
      "iteration 1801, dc_loss: 0.01676403358578682, tv_loss: 0.01727212592959404\n",
      "iteration 1802, dc_loss: 0.01676393300294876, tv_loss: 0.01727178320288658\n",
      "iteration 1803, dc_loss: 0.016763847321271896, tv_loss: 0.017271526157855988\n",
      "iteration 1804, dc_loss: 0.01676376350224018, tv_loss: 0.017271343618631363\n",
      "iteration 1805, dc_loss: 0.01676367223262787, tv_loss: 0.01727106235921383\n",
      "iteration 1806, dc_loss: 0.016763566061854362, tv_loss: 0.017270654439926147\n",
      "iteration 1807, dc_loss: 0.016763458028435707, tv_loss: 0.01727038435637951\n",
      "iteration 1808, dc_loss: 0.01676337607204914, tv_loss: 0.017270218580961227\n",
      "iteration 1809, dc_loss: 0.016763299703598022, tv_loss: 0.01726999320089817\n",
      "iteration 1810, dc_loss: 0.016763264313340187, tv_loss: 0.017269346863031387\n",
      "iteration 1811, dc_loss: 0.016763215884566307, tv_loss: 0.017269061878323555\n",
      "iteration 1812, dc_loss: 0.016763130202889442, tv_loss: 0.017268942669034004\n",
      "iteration 1813, dc_loss: 0.01676304079592228, tv_loss: 0.017268629744648933\n",
      "iteration 1814, dc_loss: 0.01676294580101967, tv_loss: 0.017268270254135132\n",
      "iteration 1815, dc_loss: 0.01676286570727825, tv_loss: 0.01726795919239521\n",
      "iteration 1816, dc_loss: 0.016762802377343178, tv_loss: 0.017267689108848572\n",
      "iteration 1817, dc_loss: 0.016762731596827507, tv_loss: 0.017267458140850067\n",
      "iteration 1818, dc_loss: 0.01676265522837639, tv_loss: 0.017267316579818726\n",
      "iteration 1819, dc_loss: 0.016762517392635345, tv_loss: 0.01726710982620716\n",
      "iteration 1820, dc_loss: 0.01676240935921669, tv_loss: 0.01726660132408142\n",
      "iteration 1821, dc_loss: 0.016762377694249153, tv_loss: 0.017266353592276573\n",
      "iteration 1822, dc_loss: 0.016762342303991318, tv_loss: 0.017266126349568367\n",
      "iteration 1823, dc_loss: 0.016762319952249527, tv_loss: 0.017265954986214638\n",
      "iteration 1824, dc_loss: 0.016762273386120796, tv_loss: 0.017265459522604942\n",
      "iteration 1825, dc_loss: 0.016762204468250275, tv_loss: 0.017265141010284424\n",
      "iteration 1826, dc_loss: 0.01676211506128311, tv_loss: 0.01726491190493107\n",
      "iteration 1827, dc_loss: 0.016762007027864456, tv_loss: 0.01726483181118965\n",
      "iteration 1828, dc_loss: 0.0167619027197361, tv_loss: 0.017264455556869507\n",
      "iteration 1829, dc_loss: 0.016761785373091698, tv_loss: 0.01726410910487175\n",
      "iteration 1830, dc_loss: 0.01676166243851185, tv_loss: 0.017263995483517647\n",
      "iteration 1831, dc_loss: 0.016761576756834984, tv_loss: 0.017263835296034813\n",
      "iteration 1832, dc_loss: 0.01676151715219021, tv_loss: 0.01726341061294079\n",
      "iteration 1833, dc_loss: 0.01676151528954506, tv_loss: 0.017263079062104225\n",
      "iteration 1834, dc_loss: 0.016761522740125656, tv_loss: 0.017262708395719528\n",
      "iteration 1835, dc_loss: 0.01676146127283573, tv_loss: 0.017262397333979607\n",
      "iteration 1836, dc_loss: 0.01676139421761036, tv_loss: 0.017262183129787445\n",
      "iteration 1837, dc_loss: 0.01676131784915924, tv_loss: 0.017261916771531105\n",
      "iteration 1838, dc_loss: 0.016761230304837227, tv_loss: 0.017261598259210587\n",
      "iteration 1839, dc_loss: 0.01676109991967678, tv_loss: 0.017261389642953873\n",
      "iteration 1840, dc_loss: 0.01676097698509693, tv_loss: 0.01726113073527813\n",
      "iteration 1841, dc_loss: 0.016760872676968575, tv_loss: 0.01726105809211731\n",
      "iteration 1842, dc_loss: 0.01676081120967865, tv_loss: 0.017260754480957985\n",
      "iteration 1843, dc_loss: 0.016760755330324173, tv_loss: 0.01726040057837963\n",
      "iteration 1844, dc_loss: 0.01676069013774395, tv_loss: 0.017260009422898293\n",
      "iteration 1845, dc_loss: 0.01676061935722828, tv_loss: 0.017259852960705757\n",
      "iteration 1846, dc_loss: 0.016760533675551414, tv_loss: 0.01725950464606285\n",
      "iteration 1847, dc_loss: 0.016760429367423058, tv_loss: 0.017259199172258377\n",
      "iteration 1848, dc_loss: 0.01676034741103649, tv_loss: 0.01725912280380726\n",
      "iteration 1849, dc_loss: 0.016760297119617462, tv_loss: 0.01725873537361622\n",
      "iteration 1850, dc_loss: 0.016760269179940224, tv_loss: 0.017258450388908386\n",
      "iteration 1851, dc_loss: 0.016760211437940598, tv_loss: 0.017258139327168465\n",
      "iteration 1852, dc_loss: 0.01676011085510254, tv_loss: 0.017257828265428543\n",
      "iteration 1853, dc_loss: 0.016760002821683884, tv_loss: 0.0172576904296875\n",
      "iteration 1854, dc_loss: 0.016759907826781273, tv_loss: 0.017257383093237877\n",
      "iteration 1855, dc_loss: 0.01675984635949135, tv_loss: 0.01725711114704609\n",
      "iteration 1856, dc_loss: 0.016759786754846573, tv_loss: 0.017256909981369972\n",
      "iteration 1857, dc_loss: 0.0167597196996212, tv_loss: 0.017256751656532288\n",
      "iteration 1858, dc_loss: 0.016759654507040977, tv_loss: 0.017256394028663635\n",
      "iteration 1859, dc_loss: 0.016759628430008888, tv_loss: 0.017256075516343117\n",
      "iteration 1860, dc_loss: 0.016759565100073814, tv_loss: 0.01725582778453827\n",
      "iteration 1861, dc_loss: 0.016759494319558144, tv_loss: 0.017255570739507675\n",
      "iteration 1862, dc_loss: 0.016759417951107025, tv_loss: 0.017255084589123726\n",
      "iteration 1863, dc_loss: 0.016759326681494713, tv_loss: 0.01725492998957634\n",
      "iteration 1864, dc_loss: 0.016759222373366356, tv_loss: 0.01725493185222149\n",
      "iteration 1865, dc_loss: 0.016759101301431656, tv_loss: 0.017254555597901344\n",
      "iteration 1866, dc_loss: 0.016758983954787254, tv_loss: 0.017254222184419632\n",
      "iteration 1867, dc_loss: 0.016758941113948822, tv_loss: 0.01725415140390396\n",
      "iteration 1868, dc_loss: 0.016758915036916733, tv_loss: 0.017253829166293144\n",
      "iteration 1869, dc_loss: 0.01675887033343315, tv_loss: 0.01725347340106964\n",
      "iteration 1870, dc_loss: 0.01675877533853054, tv_loss: 0.017253179103136063\n",
      "iteration 1871, dc_loss: 0.016758708283305168, tv_loss: 0.017252853140234947\n",
      "iteration 1872, dc_loss: 0.0167586337774992, tv_loss: 0.017252644523978233\n",
      "iteration 1873, dc_loss: 0.016758518293499947, tv_loss: 0.01725248247385025\n",
      "iteration 1874, dc_loss: 0.01675838977098465, tv_loss: 0.017252184450626373\n",
      "iteration 1875, dc_loss: 0.016758279874920845, tv_loss: 0.017251906916499138\n",
      "iteration 1876, dc_loss: 0.016758209094405174, tv_loss: 0.01725175231695175\n",
      "iteration 1877, dc_loss: 0.0167581457644701, tv_loss: 0.017251458019018173\n",
      "iteration 1878, dc_loss: 0.01675812155008316, tv_loss: 0.017251072451472282\n",
      "iteration 1879, dc_loss: 0.01675812155008316, tv_loss: 0.01725071854889393\n",
      "iteration 1880, dc_loss: 0.01675807498395443, tv_loss: 0.017250487580895424\n",
      "iteration 1881, dc_loss: 0.01675800234079361, tv_loss: 0.01725030317902565\n",
      "iteration 1882, dc_loss: 0.01675792597234249, tv_loss: 0.017249969765543938\n",
      "iteration 1883, dc_loss: 0.01675783470273018, tv_loss: 0.01724967360496521\n",
      "iteration 1884, dc_loss: 0.01675775833427906, tv_loss: 0.017249533906579018\n",
      "iteration 1885, dc_loss: 0.016757702454924583, tv_loss: 0.017249085009098053\n",
      "iteration 1886, dc_loss: 0.01675763912498951, tv_loss: 0.017248813062906265\n",
      "iteration 1887, dc_loss: 0.016757581382989883, tv_loss: 0.0172487273812294\n",
      "iteration 1888, dc_loss: 0.01675751619040966, tv_loss: 0.017248237505555153\n",
      "iteration 1889, dc_loss: 0.016757452860474586, tv_loss: 0.017247790470719337\n",
      "iteration 1890, dc_loss: 0.016757385805249214, tv_loss: 0.01724780537188053\n",
      "iteration 1891, dc_loss: 0.01675732061266899, tv_loss: 0.017247656360268593\n",
      "iteration 1892, dc_loss: 0.016757190227508545, tv_loss: 0.01724720560014248\n",
      "iteration 1893, dc_loss: 0.016757067292928696, tv_loss: 0.017247028648853302\n",
      "iteration 1894, dc_loss: 0.01675698161125183, tv_loss: 0.017246929928660393\n",
      "iteration 1895, dc_loss: 0.016756920143961906, tv_loss: 0.0172465480864048\n",
      "iteration 1896, dc_loss: 0.01675684005022049, tv_loss: 0.017246272414922714\n",
      "iteration 1897, dc_loss: 0.016756756231188774, tv_loss: 0.017246190458536148\n",
      "iteration 1898, dc_loss: 0.016756709665060043, tv_loss: 0.01724587008357048\n",
      "iteration 1899, dc_loss: 0.01675667241215706, tv_loss: 0.017245426774024963\n",
      "iteration 1900, dc_loss: 0.016756685450673103, tv_loss: 0.017245223745703697\n",
      "iteration 1901, dc_loss: 0.016756683588027954, tv_loss: 0.017244862392544746\n",
      "iteration 1902, dc_loss: 0.016756638884544373, tv_loss: 0.01724446564912796\n",
      "iteration 1903, dc_loss: 0.016756579279899597, tv_loss: 0.0172442514449358\n",
      "iteration 1904, dc_loss: 0.016756482422351837, tv_loss: 0.0172441229224205\n",
      "iteration 1905, dc_loss: 0.016756337136030197, tv_loss: 0.017243817448616028\n",
      "iteration 1906, dc_loss: 0.016756173223257065, tv_loss: 0.01724371872842312\n",
      "iteration 1907, dc_loss: 0.016756046563386917, tv_loss: 0.017243647947907448\n",
      "iteration 1908, dc_loss: 0.01675596833229065, tv_loss: 0.017243381589651108\n",
      "iteration 1909, dc_loss: 0.01675591804087162, tv_loss: 0.017242925241589546\n",
      "iteration 1910, dc_loss: 0.016755875200033188, tv_loss: 0.017242783680558205\n",
      "iteration 1911, dc_loss: 0.016755839809775352, tv_loss: 0.01724252849817276\n",
      "iteration 1912, dc_loss: 0.016755830496549606, tv_loss: 0.01724223420023918\n",
      "iteration 1913, dc_loss: 0.016755767166614532, tv_loss: 0.01724190264940262\n",
      "iteration 1914, dc_loss: 0.0167557206004858, tv_loss: 0.01724165305495262\n",
      "iteration 1915, dc_loss: 0.01675565168261528, tv_loss: 0.01724148541688919\n",
      "iteration 1916, dc_loss: 0.016755592077970505, tv_loss: 0.017241090536117554\n",
      "iteration 1917, dc_loss: 0.016755562275648117, tv_loss: 0.01724080741405487\n",
      "iteration 1918, dc_loss: 0.01675553433597088, tv_loss: 0.017240459099411964\n",
      "iteration 1919, dc_loss: 0.016755487769842148, tv_loss: 0.017240187153220177\n",
      "iteration 1920, dc_loss: 0.016755403950810432, tv_loss: 0.017239952459931374\n",
      "iteration 1921, dc_loss: 0.016755297780036926, tv_loss: 0.017239676788449287\n",
      "iteration 1922, dc_loss: 0.016755228862166405, tv_loss: 0.017239447683095932\n",
      "iteration 1923, dc_loss: 0.016755148768424988, tv_loss: 0.017239367589354515\n",
      "iteration 1924, dc_loss: 0.01675504259765148, tv_loss: 0.017239069566130638\n",
      "iteration 1925, dc_loss: 0.016754938289523125, tv_loss: 0.0172386784106493\n",
      "iteration 1926, dc_loss: 0.01675485260784626, tv_loss: 0.01723860204219818\n",
      "iteration 1927, dc_loss: 0.01675480045378208, tv_loss: 0.017238300293684006\n",
      "iteration 1928, dc_loss: 0.016754740849137306, tv_loss: 0.01723811961710453\n",
      "iteration 1929, dc_loss: 0.01675465889275074, tv_loss: 0.01723783276975155\n",
      "iteration 1930, dc_loss: 0.016754554584622383, tv_loss: 0.01723763719201088\n",
      "iteration 1931, dc_loss: 0.01675446145236492, tv_loss: 0.017237454652786255\n",
      "iteration 1932, dc_loss: 0.016754386946558952, tv_loss: 0.017237180843949318\n",
      "iteration 1933, dc_loss: 0.016754355281591415, tv_loss: 0.01723683625459671\n",
      "iteration 1934, dc_loss: 0.016754351556301117, tv_loss: 0.017236623913049698\n",
      "iteration 1935, dc_loss: 0.01675434410572052, tv_loss: 0.017236337065696716\n",
      "iteration 1936, dc_loss: 0.016754288226366043, tv_loss: 0.017236005514860153\n",
      "iteration 1937, dc_loss: 0.016754236072301865, tv_loss: 0.0172357689589262\n",
      "iteration 1938, dc_loss: 0.016754161566495895, tv_loss: 0.017235485836863518\n",
      "iteration 1939, dc_loss: 0.016754088923335075, tv_loss: 0.01723509281873703\n",
      "iteration 1940, dc_loss: 0.016754014417529106, tv_loss: 0.017234990373253822\n",
      "iteration 1941, dc_loss: 0.016753943637013435, tv_loss: 0.01723477803170681\n",
      "iteration 1942, dc_loss: 0.016753850504755974, tv_loss: 0.01723441854119301\n",
      "iteration 1943, dc_loss: 0.016753798350691795, tv_loss: 0.017234206199645996\n",
      "iteration 1944, dc_loss: 0.01675374247133732, tv_loss: 0.017233965918421745\n",
      "iteration 1945, dc_loss: 0.016753632575273514, tv_loss: 0.017233656719326973\n",
      "iteration 1946, dc_loss: 0.01675349473953247, tv_loss: 0.017233658581972122\n",
      "iteration 1947, dc_loss: 0.016753362491726875, tv_loss: 0.017233626917004585\n",
      "iteration 1948, dc_loss: 0.016753271222114563, tv_loss: 0.017233319580554962\n",
      "iteration 1949, dc_loss: 0.01675322838127613, tv_loss: 0.017232805490493774\n",
      "iteration 1950, dc_loss: 0.016753187403082848, tv_loss: 0.017232829704880714\n",
      "iteration 1951, dc_loss: 0.016753150150179863, tv_loss: 0.01723247766494751\n",
      "iteration 1952, dc_loss: 0.01675310544669628, tv_loss: 0.017232203856110573\n",
      "iteration 1953, dc_loss: 0.0167530570179224, tv_loss: 0.0172318983823061\n",
      "iteration 1954, dc_loss: 0.016752999275922775, tv_loss: 0.017231635749340057\n",
      "iteration 1955, dc_loss: 0.016752922907471657, tv_loss: 0.017231417819857597\n",
      "iteration 1956, dc_loss: 0.016752848401665688, tv_loss: 0.01723119057714939\n",
      "iteration 1957, dc_loss: 0.016752779483795166, tv_loss: 0.017230983823537827\n",
      "iteration 1958, dc_loss: 0.016752708703279495, tv_loss: 0.017230739817023277\n",
      "iteration 1959, dc_loss: 0.01675262674689293, tv_loss: 0.01723058521747589\n",
      "iteration 1960, dc_loss: 0.0167525764554739, tv_loss: 0.017230240628123283\n",
      "iteration 1961, dc_loss: 0.016752544790506363, tv_loss: 0.017229778692126274\n",
      "iteration 1962, dc_loss: 0.016752513125538826, tv_loss: 0.017229633405804634\n",
      "iteration 1963, dc_loss: 0.016752464696764946, tv_loss: 0.01722942478954792\n",
      "iteration 1964, dc_loss: 0.01675240322947502, tv_loss: 0.01722905784845352\n",
      "iteration 1965, dc_loss: 0.01675230823457241, tv_loss: 0.017228849232196808\n",
      "iteration 1966, dc_loss: 0.0167522095143795, tv_loss: 0.017228830605745316\n",
      "iteration 1967, dc_loss: 0.01675211265683174, tv_loss: 0.017228715121746063\n",
      "iteration 1968, dc_loss: 0.016752030700445175, tv_loss: 0.017228225246071815\n",
      "iteration 1969, dc_loss: 0.01675194315612316, tv_loss: 0.017227912321686745\n",
      "iteration 1970, dc_loss: 0.016751859337091446, tv_loss: 0.01722785085439682\n",
      "iteration 1971, dc_loss: 0.016751794144511223, tv_loss: 0.017227722331881523\n",
      "iteration 1972, dc_loss: 0.016751764342188835, tv_loss: 0.01722739264369011\n",
      "iteration 1973, dc_loss: 0.0167517252266407, tv_loss: 0.017227234318852425\n",
      "iteration 1974, dc_loss: 0.016751689836382866, tv_loss: 0.01722700335085392\n",
      "iteration 1975, dc_loss: 0.01675163395702839, tv_loss: 0.017226697877049446\n",
      "iteration 1976, dc_loss: 0.016751568764448166, tv_loss: 0.01722659543156624\n",
      "iteration 1977, dc_loss: 0.01675144024193287, tv_loss: 0.017226312309503555\n",
      "iteration 1978, dc_loss: 0.01675131730735302, tv_loss: 0.01722612790763378\n",
      "iteration 1979, dc_loss: 0.016751253977417946, tv_loss: 0.017225945368409157\n",
      "iteration 1980, dc_loss: 0.016751212999224663, tv_loss: 0.017225729301571846\n",
      "iteration 1981, dc_loss: 0.016751205548644066, tv_loss: 0.01722547970712185\n",
      "iteration 1982, dc_loss: 0.016751201823353767, tv_loss: 0.01722504012286663\n",
      "iteration 1983, dc_loss: 0.016751177608966827, tv_loss: 0.017224643379449844\n",
      "iteration 1984, dc_loss: 0.016751116141676903, tv_loss: 0.017224784940481186\n",
      "iteration 1985, dc_loss: 0.016751015558838844, tv_loss: 0.017224421724677086\n",
      "iteration 1986, dc_loss: 0.016750896349549294, tv_loss: 0.017223995178937912\n",
      "iteration 1987, dc_loss: 0.01675078645348549, tv_loss: 0.017223868519067764\n",
      "iteration 1988, dc_loss: 0.01675071381032467, tv_loss: 0.01722385361790657\n",
      "iteration 1989, dc_loss: 0.016750633716583252, tv_loss: 0.017223594710230827\n",
      "iteration 1990, dc_loss: 0.016750585287809372, tv_loss: 0.017223123461008072\n",
      "iteration 1991, dc_loss: 0.016750549897551537, tv_loss: 0.01722305826842785\n",
      "iteration 1992, dc_loss: 0.016750536859035492, tv_loss: 0.017222736030817032\n",
      "iteration 1993, dc_loss: 0.0167505145072937, tv_loss: 0.017222322523593903\n",
      "iteration 1994, dc_loss: 0.016750484704971313, tv_loss: 0.017221951857209206\n",
      "iteration 1995, dc_loss: 0.016750425100326538, tv_loss: 0.017221851274371147\n",
      "iteration 1996, dc_loss: 0.01675037108361721, tv_loss: 0.017221592366695404\n",
      "iteration 1997, dc_loss: 0.016750279814004898, tv_loss: 0.017221437767148018\n",
      "iteration 1998, dc_loss: 0.016750166192650795, tv_loss: 0.017221374437212944\n",
      "iteration 1999, dc_loss: 0.01675005629658699, tv_loss: 0.017221074551343918\n",
      "iteration 2000, dc_loss: 0.01674998179078102, tv_loss: 0.017220789566636086\n",
      "iteration 2001, dc_loss: 0.016749922186136246, tv_loss: 0.017220674082636833\n",
      "iteration 2002, dc_loss: 0.016749843955039978, tv_loss: 0.017220424488186836\n",
      "iteration 2003, dc_loss: 0.016749760136008263, tv_loss: 0.017220089212059975\n",
      "iteration 2004, dc_loss: 0.016749678179621696, tv_loss: 0.017219925299286842\n",
      "iteration 2005, dc_loss: 0.016749631613492966, tv_loss: 0.01721968688070774\n",
      "iteration 2006, dc_loss: 0.01674959994852543, tv_loss: 0.017219293862581253\n",
      "iteration 2007, dc_loss: 0.016749583184719086, tv_loss: 0.017219163477420807\n",
      "iteration 2008, dc_loss: 0.016749583184719086, tv_loss: 0.01721894182264805\n",
      "iteration 2009, dc_loss: 0.016749534755945206, tv_loss: 0.017218604683876038\n",
      "iteration 2010, dc_loss: 0.016749469563364983, tv_loss: 0.017218342050909996\n",
      "iteration 2011, dc_loss: 0.016749391332268715, tv_loss: 0.017218148335814476\n",
      "iteration 2012, dc_loss: 0.016749301925301552, tv_loss: 0.01721797324717045\n",
      "iteration 2013, dc_loss: 0.016749240458011627, tv_loss: 0.017217790707945824\n",
      "iteration 2014, dc_loss: 0.01674916036427021, tv_loss: 0.01721758209168911\n",
      "iteration 2015, dc_loss: 0.01674908772110939, tv_loss: 0.01721734181046486\n",
      "iteration 2016, dc_loss: 0.01674901321530342, tv_loss: 0.017217183485627174\n",
      "iteration 2017, dc_loss: 0.016748955473303795, tv_loss: 0.01721694879233837\n",
      "iteration 2018, dc_loss: 0.01674892008304596, tv_loss: 0.017216769978404045\n",
      "iteration 2019, dc_loss: 0.01674889400601387, tv_loss: 0.017216499894857407\n",
      "iteration 2020, dc_loss: 0.0167488232254982, tv_loss: 0.01721620187163353\n",
      "iteration 2021, dc_loss: 0.016748756170272827, tv_loss: 0.017216024920344353\n",
      "iteration 2022, dc_loss: 0.016748683527112007, tv_loss: 0.017215659841895103\n",
      "iteration 2023, dc_loss: 0.016748620197176933, tv_loss: 0.017215510830283165\n",
      "iteration 2024, dc_loss: 0.016748521476984024, tv_loss: 0.01721547171473503\n",
      "iteration 2025, dc_loss: 0.01674840785562992, tv_loss: 0.017214974388480186\n",
      "iteration 2026, dc_loss: 0.016748376190662384, tv_loss: 0.01721479743719101\n",
      "iteration 2027, dc_loss: 0.016748331487178802, tv_loss: 0.017214812338352203\n",
      "iteration 2028, dc_loss: 0.016748275607824326, tv_loss: 0.01721448078751564\n",
      "iteration 2029, dc_loss: 0.016748247668147087, tv_loss: 0.017214056104421616\n",
      "iteration 2030, dc_loss: 0.01674819178879261, tv_loss: 0.01721380464732647\n",
      "iteration 2031, dc_loss: 0.01674814149737358, tv_loss: 0.01721360720694065\n",
      "iteration 2032, dc_loss: 0.016748102381825447, tv_loss: 0.017213312909007072\n",
      "iteration 2033, dc_loss: 0.016748037189245224, tv_loss: 0.01721314713358879\n",
      "iteration 2034, dc_loss: 0.01674792915582657, tv_loss: 0.017213016748428345\n",
      "iteration 2035, dc_loss: 0.016747815534472466, tv_loss: 0.017212826758623123\n",
      "iteration 2036, dc_loss: 0.01674772799015045, tv_loss: 0.017212655395269394\n",
      "iteration 2037, dc_loss: 0.016747673973441124, tv_loss: 0.017212538048624992\n",
      "iteration 2038, dc_loss: 0.016747625544667244, tv_loss: 0.017212150618433952\n",
      "iteration 2039, dc_loss: 0.01674758829176426, tv_loss: 0.01721205562353134\n",
      "iteration 2040, dc_loss: 0.01674756221473217, tv_loss: 0.017211800441145897\n",
      "iteration 2041, dc_loss: 0.016747519373893738, tv_loss: 0.01721152663230896\n",
      "iteration 2042, dc_loss: 0.016747426241636276, tv_loss: 0.0172113124281168\n",
      "iteration 2043, dc_loss: 0.01674734614789486, tv_loss: 0.01721101999282837\n",
      "iteration 2044, dc_loss: 0.016747284680604935, tv_loss: 0.01721074990928173\n",
      "iteration 2045, dc_loss: 0.016747241839766502, tv_loss: 0.017210666090250015\n",
      "iteration 2046, dc_loss: 0.016747163608670235, tv_loss: 0.017210422083735466\n",
      "iteration 2047, dc_loss: 0.01674705557525158, tv_loss: 0.01721023954451084\n",
      "iteration 2048, dc_loss: 0.016746968030929565, tv_loss: 0.017209963873028755\n",
      "iteration 2049, dc_loss: 0.01674693450331688, tv_loss: 0.017209606245160103\n",
      "iteration 2050, dc_loss: 0.016746947541832924, tv_loss: 0.017209546640515327\n",
      "iteration 2051, dc_loss: 0.01674693077802658, tv_loss: 0.017209423705935478\n",
      "iteration 2052, dc_loss: 0.01674690470099449, tv_loss: 0.017209023237228394\n",
      "iteration 2053, dc_loss: 0.016746873036026955, tv_loss: 0.01720857247710228\n",
      "iteration 2054, dc_loss: 0.016746822744607925, tv_loss: 0.017208373174071312\n",
      "iteration 2055, dc_loss: 0.016746746376156807, tv_loss: 0.017208268865942955\n",
      "iteration 2056, dc_loss: 0.01674664579331875, tv_loss: 0.01720813475549221\n",
      "iteration 2057, dc_loss: 0.016746509820222855, tv_loss: 0.01720798760652542\n",
      "iteration 2058, dc_loss: 0.016746383160352707, tv_loss: 0.017207777127623558\n",
      "iteration 2059, dc_loss: 0.016746291890740395, tv_loss: 0.01720755361020565\n",
      "iteration 2060, dc_loss: 0.016746221110224724, tv_loss: 0.017207453027367592\n",
      "iteration 2061, dc_loss: 0.01674617826938629, tv_loss: 0.017207276076078415\n",
      "iteration 2062, dc_loss: 0.016746168956160545, tv_loss: 0.017206966876983643\n",
      "iteration 2063, dc_loss: 0.01674618571996689, tv_loss: 0.017206666991114616\n",
      "iteration 2064, dc_loss: 0.016746172681450844, tv_loss: 0.01720641925930977\n",
      "iteration 2065, dc_loss: 0.016746103763580322, tv_loss: 0.017206184566020966\n",
      "iteration 2066, dc_loss: 0.016746029257774353, tv_loss: 0.017205936834216118\n",
      "iteration 2067, dc_loss: 0.01674596033990383, tv_loss: 0.01720586232841015\n",
      "iteration 2068, dc_loss: 0.016745856031775475, tv_loss: 0.01720566675066948\n",
      "iteration 2069, dc_loss: 0.016745761036872864, tv_loss: 0.017205609008669853\n",
      "iteration 2070, dc_loss: 0.016745682805776596, tv_loss: 0.017205335199832916\n",
      "iteration 2071, dc_loss: 0.016745619475841522, tv_loss: 0.01720501109957695\n",
      "iteration 2072, dc_loss: 0.016745587810873985, tv_loss: 0.017204999923706055\n",
      "iteration 2073, dc_loss: 0.016745563596487045, tv_loss: 0.01720469817519188\n",
      "iteration 2074, dc_loss: 0.016745558008551598, tv_loss: 0.01720433123409748\n",
      "iteration 2075, dc_loss: 0.016745535656809807, tv_loss: 0.01720418967306614\n",
      "iteration 2076, dc_loss: 0.016745466738939285, tv_loss: 0.017204059287905693\n",
      "iteration 2077, dc_loss: 0.016745375469326973, tv_loss: 0.017203819006681442\n",
      "iteration 2078, dc_loss: 0.016745297238230705, tv_loss: 0.017203442752361298\n",
      "iteration 2079, dc_loss: 0.01674523763358593, tv_loss: 0.017203304916620255\n",
      "iteration 2080, dc_loss: 0.016745172441005707, tv_loss: 0.017203086987137794\n",
      "iteration 2081, dc_loss: 0.016745105385780334, tv_loss: 0.01720300316810608\n",
      "iteration 2082, dc_loss: 0.016745060682296753, tv_loss: 0.017202824354171753\n",
      "iteration 2083, dc_loss: 0.016744988039135933, tv_loss: 0.01720256358385086\n",
      "iteration 2084, dc_loss: 0.016744880005717278, tv_loss: 0.017202377319335938\n",
      "iteration 2085, dc_loss: 0.016744745895266533, tv_loss: 0.017202232033014297\n",
      "iteration 2086, dc_loss: 0.01674468442797661, tv_loss: 0.017202114686369896\n",
      "iteration 2087, dc_loss: 0.016744667664170265, tv_loss: 0.017201881855726242\n",
      "iteration 2088, dc_loss: 0.016744643449783325, tv_loss: 0.017201580107212067\n",
      "iteration 2089, dc_loss: 0.01674460992217064, tv_loss: 0.0172013808041811\n",
      "iteration 2090, dc_loss: 0.01674458384513855, tv_loss: 0.0172011386603117\n",
      "iteration 2091, dc_loss: 0.01674455963075161, tv_loss: 0.01720087043941021\n",
      "iteration 2092, dc_loss: 0.016744542866945267, tv_loss: 0.017200613394379616\n",
      "iteration 2093, dc_loss: 0.016744470223784447, tv_loss: 0.01720057614147663\n",
      "iteration 2094, dc_loss: 0.01674433797597885, tv_loss: 0.017200294882059097\n",
      "iteration 2095, dc_loss: 0.016744187101721764, tv_loss: 0.017200224101543427\n",
      "iteration 2096, dc_loss: 0.01674407161772251, tv_loss: 0.017200062051415443\n",
      "iteration 2097, dc_loss: 0.01674400269985199, tv_loss: 0.017199749127030373\n",
      "iteration 2098, dc_loss: 0.016743972897529602, tv_loss: 0.017199572175741196\n",
      "iteration 2099, dc_loss: 0.01674400083720684, tv_loss: 0.017199374735355377\n",
      "iteration 2100, dc_loss: 0.01674402691423893, tv_loss: 0.017199035733938217\n",
      "iteration 2101, dc_loss: 0.016744045540690422, tv_loss: 0.017198704183101654\n",
      "iteration 2102, dc_loss: 0.01674400456249714, tv_loss: 0.01719842292368412\n",
      "iteration 2103, dc_loss: 0.01674393005669117, tv_loss: 0.01719828136265278\n",
      "iteration 2104, dc_loss: 0.016743816435337067, tv_loss: 0.01719830185174942\n",
      "iteration 2105, dc_loss: 0.01674371398985386, tv_loss: 0.017198117449879646\n",
      "iteration 2106, dc_loss: 0.0167436171323061, tv_loss: 0.017197856679558754\n",
      "iteration 2107, dc_loss: 0.016743527725338936, tv_loss: 0.01719752512872219\n",
      "iteration 2108, dc_loss: 0.01674346998333931, tv_loss: 0.017197543755173683\n",
      "iteration 2109, dc_loss: 0.01674344204366207, tv_loss: 0.017197558656334877\n",
      "iteration 2110, dc_loss: 0.016743415966629982, tv_loss: 0.017197074368596077\n",
      "iteration 2111, dc_loss: 0.016743425279855728, tv_loss: 0.0171967726200819\n",
      "iteration 2112, dc_loss: 0.016743415966629982, tv_loss: 0.01719658635556698\n",
      "iteration 2113, dc_loss: 0.0167433749884367, tv_loss: 0.01719638705253601\n",
      "iteration 2114, dc_loss: 0.01674327626824379, tv_loss: 0.017195967957377434\n",
      "iteration 2115, dc_loss: 0.016743144020438194, tv_loss: 0.017195895314216614\n",
      "iteration 2116, dc_loss: 0.016743041574954987, tv_loss: 0.017195837572216988\n",
      "iteration 2117, dc_loss: 0.016742946580052376, tv_loss: 0.0171955693513155\n",
      "iteration 2118, dc_loss: 0.016742905601859093, tv_loss: 0.0171954445540905\n",
      "iteration 2119, dc_loss: 0.016742827370762825, tv_loss: 0.017195241525769234\n",
      "iteration 2120, dc_loss: 0.01674273982644081, tv_loss: 0.01719505339860916\n",
      "iteration 2121, dc_loss: 0.016742678359150887, tv_loss: 0.017194731160998344\n",
      "iteration 2122, dc_loss: 0.016742685809731483, tv_loss: 0.01719452627003193\n",
      "iteration 2123, dc_loss: 0.01674272119998932, tv_loss: 0.017194317653775215\n",
      "iteration 2124, dc_loss: 0.016742665320634842, tv_loss: 0.017194144427776337\n",
      "iteration 2125, dc_loss: 0.016742555424571037, tv_loss: 0.017193937674164772\n",
      "iteration 2126, dc_loss: 0.016742466017603874, tv_loss: 0.017193708568811417\n",
      "iteration 2127, dc_loss: 0.01674243062734604, tv_loss: 0.017193466424942017\n",
      "iteration 2128, dc_loss: 0.01674240455031395, tv_loss: 0.017193349078297615\n",
      "iteration 2129, dc_loss: 0.016742359846830368, tv_loss: 0.01719316653907299\n",
      "iteration 2130, dc_loss: 0.01674230583012104, tv_loss: 0.017193017527461052\n",
      "iteration 2131, dc_loss: 0.016742268577218056, tv_loss: 0.017192630097270012\n",
      "iteration 2132, dc_loss: 0.016742216423153877, tv_loss: 0.017192404717206955\n",
      "iteration 2133, dc_loss: 0.016742166131734848, tv_loss: 0.017192265018820763\n",
      "iteration 2134, dc_loss: 0.016742099076509476, tv_loss: 0.017192086204886436\n",
      "iteration 2135, dc_loss: 0.01674206368625164, tv_loss: 0.017191773280501366\n",
      "iteration 2136, dc_loss: 0.016742004081606865, tv_loss: 0.017191698774695396\n",
      "iteration 2137, dc_loss: 0.016741929575800896, tv_loss: 0.017191600054502487\n",
      "iteration 2138, dc_loss: 0.01674182154238224, tv_loss: 0.017191430553793907\n",
      "iteration 2139, dc_loss: 0.016741741448640823, tv_loss: 0.017191214486956596\n",
      "iteration 2140, dc_loss: 0.016741665080189705, tv_loss: 0.017191000282764435\n",
      "iteration 2141, dc_loss: 0.01674160361289978, tv_loss: 0.017190974205732346\n",
      "iteration 2142, dc_loss: 0.016741584986448288, tv_loss: 0.017190754413604736\n",
      "iteration 2143, dc_loss: 0.016741562634706497, tv_loss: 0.017190556973218918\n",
      "iteration 2144, dc_loss: 0.016741544008255005, tv_loss: 0.01719020865857601\n",
      "iteration 2145, dc_loss: 0.016741516068577766, tv_loss: 0.017190054059028625\n",
      "iteration 2146, dc_loss: 0.01674146018922329, tv_loss: 0.017189811915159225\n",
      "iteration 2147, dc_loss: 0.016741406172513962, tv_loss: 0.01718936674296856\n",
      "iteration 2148, dc_loss: 0.016741350293159485, tv_loss: 0.017189234495162964\n",
      "iteration 2149, dc_loss: 0.016741309314966202, tv_loss: 0.0171892661601305\n",
      "iteration 2150, dc_loss: 0.016741260886192322, tv_loss: 0.01718907430768013\n",
      "iteration 2151, dc_loss: 0.016741175204515457, tv_loss: 0.017188675701618195\n",
      "iteration 2152, dc_loss: 0.016741102561354637, tv_loss: 0.017188414931297302\n",
      "iteration 2153, dc_loss: 0.016741042956709862, tv_loss: 0.01718844845890999\n",
      "iteration 2154, dc_loss: 0.016740983352065086, tv_loss: 0.01718810945749283\n",
      "iteration 2155, dc_loss: 0.016740920022130013, tv_loss: 0.01718779467046261\n",
      "iteration 2156, dc_loss: 0.016740849241614342, tv_loss: 0.017187729477882385\n",
      "iteration 2157, dc_loss: 0.016740798950195312, tv_loss: 0.017187518998980522\n",
      "iteration 2158, dc_loss: 0.01674072816967964, tv_loss: 0.01718740351498127\n",
      "iteration 2159, dc_loss: 0.016740642488002777, tv_loss: 0.01718742772936821\n",
      "iteration 2160, dc_loss: 0.016740581020712852, tv_loss: 0.01718706265091896\n",
      "iteration 2161, dc_loss: 0.01674053817987442, tv_loss: 0.017186764627695084\n",
      "iteration 2162, dc_loss: 0.016740508377552032, tv_loss: 0.017186706885695457\n",
      "iteration 2163, dc_loss: 0.016740478575229645, tv_loss: 0.017186470329761505\n",
      "iteration 2164, dc_loss: 0.01674041897058487, tv_loss: 0.017186185345053673\n",
      "iteration 2165, dc_loss: 0.016740357503294945, tv_loss: 0.017186108976602554\n",
      "iteration 2166, dc_loss: 0.016740290448069572, tv_loss: 0.017186082899570465\n",
      "iteration 2167, dc_loss: 0.016740228980779648, tv_loss: 0.01718573085963726\n",
      "iteration 2168, dc_loss: 0.016740188002586365, tv_loss: 0.01718556135892868\n",
      "iteration 2169, dc_loss: 0.016740135848522186, tv_loss: 0.017185380682349205\n",
      "iteration 2170, dc_loss: 0.016740072518587112, tv_loss: 0.017185240983963013\n",
      "iteration 2171, dc_loss: 0.01674000918865204, tv_loss: 0.017184995114803314\n",
      "iteration 2172, dc_loss: 0.01673997938632965, tv_loss: 0.017184779047966003\n",
      "iteration 2173, dc_loss: 0.016739971935749054, tv_loss: 0.017184535041451454\n",
      "iteration 2174, dc_loss: 0.01673993654549122, tv_loss: 0.017184315249323845\n",
      "iteration 2175, dc_loss: 0.016739891842007637, tv_loss: 0.017184065654873848\n",
      "iteration 2176, dc_loss: 0.016739828512072563, tv_loss: 0.017183853313326836\n",
      "iteration 2177, dc_loss: 0.016739750280976295, tv_loss: 0.017183827236294746\n",
      "iteration 2178, dc_loss: 0.016739673912525177, tv_loss: 0.01718367449939251\n",
      "iteration 2179, dc_loss: 0.016739601269364357, tv_loss: 0.017183437943458557\n",
      "iteration 2180, dc_loss: 0.01673954352736473, tv_loss: 0.01718323864042759\n",
      "iteration 2181, dc_loss: 0.01673949509859085, tv_loss: 0.017183076590299606\n",
      "iteration 2182, dc_loss: 0.016739478334784508, tv_loss: 0.0171828456223011\n",
      "iteration 2183, dc_loss: 0.016739418730139732, tv_loss: 0.017182549461722374\n",
      "iteration 2184, dc_loss: 0.016739360988140106, tv_loss: 0.01718241348862648\n",
      "iteration 2185, dc_loss: 0.016739318147301674, tv_loss: 0.01718228869140148\n",
      "iteration 2186, dc_loss: 0.01673932373523712, tv_loss: 0.01718199998140335\n",
      "iteration 2187, dc_loss: 0.01673933118581772, tv_loss: 0.017181582748889923\n",
      "iteration 2188, dc_loss: 0.016739290207624435, tv_loss: 0.017181396484375\n",
      "iteration 2189, dc_loss: 0.016739187762141228, tv_loss: 0.017181329429149628\n",
      "iteration 2190, dc_loss: 0.016739070415496826, tv_loss: 0.017181208357214928\n",
      "iteration 2191, dc_loss: 0.01673899218440056, tv_loss: 0.017181020230054855\n",
      "iteration 2192, dc_loss: 0.016738921403884888, tv_loss: 0.0171807911247015\n",
      "iteration 2193, dc_loss: 0.016738880425691605, tv_loss: 0.017180712893605232\n",
      "iteration 2194, dc_loss: 0.01673883944749832, tv_loss: 0.017180444672703743\n",
      "iteration 2195, dc_loss: 0.016738777980208397, tv_loss: 0.01718033291399479\n",
      "iteration 2196, dc_loss: 0.01673867180943489, tv_loss: 0.01718008704483509\n",
      "iteration 2197, dc_loss: 0.01673857308924198, tv_loss: 0.017179999500513077\n",
      "iteration 2198, dc_loss: 0.016738567501306534, tv_loss: 0.017179884016513824\n",
      "iteration 2199, dc_loss: 0.016738563776016235, tv_loss: 0.017179615795612335\n",
      "iteration 2200, dc_loss: 0.016738537698984146, tv_loss: 0.017179381102323532\n",
      "iteration 2201, dc_loss: 0.016738522797822952, tv_loss: 0.017179135233163834\n",
      "iteration 2202, dc_loss: 0.016738498583436012, tv_loss: 0.017178939655423164\n",
      "iteration 2203, dc_loss: 0.016738461330533028, tv_loss: 0.01717873476445675\n",
      "iteration 2204, dc_loss: 0.01673843152821064, tv_loss: 0.017178546637296677\n",
      "iteration 2205, dc_loss: 0.016738364472985268, tv_loss: 0.017178213223814964\n",
      "iteration 2206, dc_loss: 0.016738271340727806, tv_loss: 0.017178194597363472\n",
      "iteration 2207, dc_loss: 0.0167381688952446, tv_loss: 0.017178088426589966\n",
      "iteration 2208, dc_loss: 0.016738073900341988, tv_loss: 0.01717795617878437\n",
      "iteration 2209, dc_loss: 0.01673804223537445, tv_loss: 0.017177807167172432\n",
      "iteration 2210, dc_loss: 0.01673799380660057, tv_loss: 0.01717761717736721\n",
      "iteration 2211, dc_loss: 0.01673794910311699, tv_loss: 0.01717732846736908\n",
      "iteration 2212, dc_loss: 0.016737928614020348, tv_loss: 0.017177028581500053\n",
      "iteration 2213, dc_loss: 0.016737941652536392, tv_loss: 0.017176959663629532\n",
      "iteration 2214, dc_loss: 0.0167379230260849, tv_loss: 0.017176751047372818\n",
      "iteration 2215, dc_loss: 0.016737865284085274, tv_loss: 0.017176605761051178\n",
      "iteration 2216, dc_loss: 0.016737785190343857, tv_loss: 0.017176393419504166\n",
      "iteration 2217, dc_loss: 0.01673770323395729, tv_loss: 0.01717628724873066\n",
      "iteration 2218, dc_loss: 0.016737641766667366, tv_loss: 0.01717621460556984\n",
      "iteration 2219, dc_loss: 0.01673761010169983, tv_loss: 0.017175884917378426\n",
      "iteration 2220, dc_loss: 0.016737570986151695, tv_loss: 0.017175618559122086\n",
      "iteration 2221, dc_loss: 0.01673753373324871, tv_loss: 0.017175545915961266\n",
      "iteration 2222, dc_loss: 0.01673748530447483, tv_loss: 0.01717536151409149\n",
      "iteration 2223, dc_loss: 0.016737427562475204, tv_loss: 0.017175184562802315\n",
      "iteration 2224, dc_loss: 0.016737353056669235, tv_loss: 0.017175117507576942\n",
      "iteration 2225, dc_loss: 0.01673726737499237, tv_loss: 0.01717495173215866\n",
      "iteration 2226, dc_loss: 0.016737209632992744, tv_loss: 0.017174769192934036\n",
      "iteration 2227, dc_loss: 0.016737116500735283, tv_loss: 0.017174605280160904\n",
      "iteration 2228, dc_loss: 0.01673705317080021, tv_loss: 0.017174676060676575\n",
      "iteration 2229, dc_loss: 0.01673700287938118, tv_loss: 0.01717434450984001\n",
      "iteration 2230, dc_loss: 0.016736960038542747, tv_loss: 0.01717393845319748\n",
      "iteration 2231, dc_loss: 0.016736915335059166, tv_loss: 0.017173975706100464\n",
      "iteration 2232, dc_loss: 0.01673690788447857, tv_loss: 0.017173726111650467\n",
      "iteration 2233, dc_loss: 0.016736868768930435, tv_loss: 0.01717342622578144\n",
      "iteration 2234, dc_loss: 0.016736838966608047, tv_loss: 0.01717333123087883\n",
      "iteration 2235, dc_loss: 0.016736790537834167, tv_loss: 0.01717308722436428\n",
      "iteration 2236, dc_loss: 0.016736753284931183, tv_loss: 0.017172813415527344\n",
      "iteration 2237, dc_loss: 0.016736719757318497, tv_loss: 0.017172733321785927\n",
      "iteration 2238, dc_loss: 0.016736723482608795, tv_loss: 0.017172370105981827\n",
      "iteration 2239, dc_loss: 0.016736730933189392, tv_loss: 0.017172198742628098\n",
      "iteration 2240, dc_loss: 0.016736697405576706, tv_loss: 0.01717183366417885\n",
      "iteration 2241, dc_loss: 0.016736648976802826, tv_loss: 0.017171639949083328\n",
      "iteration 2242, dc_loss: 0.016736578196287155, tv_loss: 0.01717168651521206\n",
      "iteration 2243, dc_loss: 0.016736498102545738, tv_loss: 0.01717153936624527\n",
      "iteration 2244, dc_loss: 0.016736414283514023, tv_loss: 0.01717136986553669\n",
      "iteration 2245, dc_loss: 0.0167363453656435, tv_loss: 0.017171237617731094\n",
      "iteration 2246, dc_loss: 0.016736282035708427, tv_loss: 0.01717115007340908\n",
      "iteration 2247, dc_loss: 0.0167362242937088, tv_loss: 0.01717107556760311\n",
      "iteration 2248, dc_loss: 0.016736142337322235, tv_loss: 0.0171707421541214\n",
      "iteration 2249, dc_loss: 0.01673603616654873, tv_loss: 0.017170697450637817\n",
      "iteration 2250, dc_loss: 0.016735965386033058, tv_loss: 0.01717056892812252\n",
      "iteration 2251, dc_loss: 0.016735956072807312, tv_loss: 0.01717035286128521\n",
      "iteration 2252, dc_loss: 0.01673593930900097, tv_loss: 0.017170192673802376\n",
      "iteration 2253, dc_loss: 0.01673589088022709, tv_loss: 0.017169972881674767\n",
      "iteration 2254, dc_loss: 0.016735849902033806, tv_loss: 0.01716972142457962\n",
      "iteration 2255, dc_loss: 0.016735821962356567, tv_loss: 0.017169516533613205\n",
      "iteration 2256, dc_loss: 0.016735799610614777, tv_loss: 0.01716930791735649\n",
      "iteration 2257, dc_loss: 0.016735749319195747, tv_loss: 0.017169181257486343\n",
      "iteration 2258, dc_loss: 0.01673569157719612, tv_loss: 0.017169009894132614\n",
      "iteration 2259, dc_loss: 0.016735628247261047, tv_loss: 0.017168860882520676\n",
      "iteration 2260, dc_loss: 0.016735555604100227, tv_loss: 0.017168883234262466\n",
      "iteration 2261, dc_loss: 0.0167354978621006, tv_loss: 0.017168579623103142\n",
      "iteration 2262, dc_loss: 0.016735456883907318, tv_loss: 0.01716839335858822\n",
      "iteration 2263, dc_loss: 0.01673540286719799, tv_loss: 0.017168402671813965\n",
      "iteration 2264, dc_loss: 0.016735365614295006, tv_loss: 0.01716817542910576\n",
      "iteration 2265, dc_loss: 0.016735296696424484, tv_loss: 0.017167774960398674\n",
      "iteration 2266, dc_loss: 0.0167352594435215, tv_loss: 0.017167607322335243\n",
      "iteration 2267, dc_loss: 0.01673523336648941, tv_loss: 0.01716744340956211\n",
      "iteration 2268, dc_loss: 0.016735197976231575, tv_loss: 0.01716724783182144\n",
      "iteration 2269, dc_loss: 0.016735149547457695, tv_loss: 0.01716715656220913\n",
      "iteration 2270, dc_loss: 0.016735101118683815, tv_loss: 0.01716695912182331\n",
      "iteration 2271, dc_loss: 0.016735071316361427, tv_loss: 0.017166735604405403\n",
      "iteration 2272, dc_loss: 0.016735000535845757, tv_loss: 0.017166554927825928\n",
      "iteration 2273, dc_loss: 0.016734931617975235, tv_loss: 0.017166538164019585\n",
      "iteration 2274, dc_loss: 0.016734836623072624, tv_loss: 0.01716644875705242\n",
      "iteration 2275, dc_loss: 0.01673472486436367, tv_loss: 0.01716623641550541\n",
      "iteration 2276, dc_loss: 0.016734657809138298, tv_loss: 0.01716596633195877\n",
      "iteration 2277, dc_loss: 0.016734646633267403, tv_loss: 0.0171658992767334\n",
      "iteration 2278, dc_loss: 0.016734622418880463, tv_loss: 0.01716581918299198\n",
      "iteration 2279, dc_loss: 0.016734590753912926, tv_loss: 0.017165571451187134\n",
      "iteration 2280, dc_loss: 0.016734568402171135, tv_loss: 0.017165308818221092\n",
      "iteration 2281, dc_loss: 0.0167345330119133, tv_loss: 0.0171651653945446\n",
      "iteration 2282, dc_loss: 0.016734475269913673, tv_loss: 0.017164932563900948\n",
      "iteration 2283, dc_loss: 0.016734452918171883, tv_loss: 0.017164835706353188\n",
      "iteration 2284, dc_loss: 0.016734415665268898, tv_loss: 0.017164718359708786\n",
      "iteration 2285, dc_loss: 0.01673438772559166, tv_loss: 0.01716439612209797\n",
      "iteration 2286, dc_loss: 0.016734344884753227, tv_loss: 0.017164267599582672\n",
      "iteration 2287, dc_loss: 0.016734281554818153, tv_loss: 0.017164064571261406\n",
      "iteration 2288, dc_loss: 0.016734227538108826, tv_loss: 0.017164023593068123\n",
      "iteration 2289, dc_loss: 0.016734186559915543, tv_loss: 0.01716393232345581\n",
      "iteration 2290, dc_loss: 0.016734126955270767, tv_loss: 0.017163647338747978\n",
      "iteration 2291, dc_loss: 0.016734058037400246, tv_loss: 0.01716342568397522\n",
      "iteration 2292, dc_loss: 0.016734009608626366, tv_loss: 0.017163271084427834\n",
      "iteration 2293, dc_loss: 0.016733966767787933, tv_loss: 0.017163043841719627\n",
      "iteration 2294, dc_loss: 0.016733909025788307, tv_loss: 0.01716284453868866\n",
      "iteration 2295, dc_loss: 0.01673385500907898, tv_loss: 0.017162831500172615\n",
      "iteration 2296, dc_loss: 0.016733800992369652, tv_loss: 0.017162632197141647\n",
      "iteration 2297, dc_loss: 0.01673376001417637, tv_loss: 0.017162412405014038\n",
      "iteration 2298, dc_loss: 0.01673371158540249, tv_loss: 0.017162224277853966\n",
      "iteration 2299, dc_loss: 0.016733655706048012, tv_loss: 0.017162011936306953\n",
      "iteration 2300, dc_loss: 0.016733601689338684, tv_loss: 0.017161807045340538\n",
      "iteration 2301, dc_loss: 0.01673353835940361, tv_loss: 0.017161836847662926\n",
      "iteration 2302, dc_loss: 0.01673346757888794, tv_loss: 0.01716163009405136\n",
      "iteration 2303, dc_loss: 0.016733428463339806, tv_loss: 0.017161428928375244\n",
      "iteration 2304, dc_loss: 0.016733426600694656, tv_loss: 0.017161212861537933\n",
      "iteration 2305, dc_loss: 0.016733430325984955, tv_loss: 0.017161058261990547\n",
      "iteration 2306, dc_loss: 0.016733424738049507, tv_loss: 0.017160797491669655\n",
      "iteration 2307, dc_loss: 0.01673341915011406, tv_loss: 0.017160823568701744\n",
      "iteration 2308, dc_loss: 0.016733357682824135, tv_loss: 0.017160534858703613\n",
      "iteration 2309, dc_loss: 0.01673327013850212, tv_loss: 0.017160270363092422\n",
      "iteration 2310, dc_loss: 0.016733156517148018, tv_loss: 0.017160356044769287\n",
      "iteration 2311, dc_loss: 0.016733108088374138, tv_loss: 0.017160214483737946\n",
      "iteration 2312, dc_loss: 0.01673305779695511, tv_loss: 0.017159990966320038\n",
      "iteration 2313, dc_loss: 0.01673298329114914, tv_loss: 0.01715990900993347\n",
      "iteration 2314, dc_loss: 0.016732892021536827, tv_loss: 0.01715969853103161\n",
      "iteration 2315, dc_loss: 0.016732793301343918, tv_loss: 0.017159651964902878\n",
      "iteration 2316, dc_loss: 0.016732724383473396, tv_loss: 0.017159733921289444\n",
      "iteration 2317, dc_loss: 0.016732679679989815, tv_loss: 0.017159486189484596\n",
      "iteration 2318, dc_loss: 0.01673264615237713, tv_loss: 0.01715915836393833\n",
      "iteration 2319, dc_loss: 0.01673264428973198, tv_loss: 0.01715903729200363\n",
      "iteration 2320, dc_loss: 0.016732661053538322, tv_loss: 0.017158813774585724\n",
      "iteration 2321, dc_loss: 0.016732674092054367, tv_loss: 0.017158519476652145\n",
      "iteration 2322, dc_loss: 0.01673267036676407, tv_loss: 0.017158381640911102\n",
      "iteration 2323, dc_loss: 0.016732629388570786, tv_loss: 0.017158132046461105\n",
      "iteration 2324, dc_loss: 0.01673256978392601, tv_loss: 0.017157988622784615\n",
      "iteration 2325, dc_loss: 0.016732526943087578, tv_loss: 0.017157940194010735\n",
      "iteration 2326, dc_loss: 0.01673247292637825, tv_loss: 0.017157722264528275\n",
      "iteration 2327, dc_loss: 0.01673240028321743, tv_loss: 0.017157526686787605\n",
      "iteration 2328, dc_loss: 0.016732294112443924, tv_loss: 0.01715736649930477\n",
      "iteration 2329, dc_loss: 0.01673216186463833, tv_loss: 0.01715722493827343\n",
      "iteration 2330, dc_loss: 0.01673208363354206, tv_loss: 0.01715703494846821\n",
      "iteration 2331, dc_loss: 0.016732074320316315, tv_loss: 0.01715710014104843\n",
      "iteration 2332, dc_loss: 0.016732120886445045, tv_loss: 0.017156729474663734\n",
      "iteration 2333, dc_loss: 0.016732145100831985, tv_loss: 0.017156504094600677\n",
      "iteration 2334, dc_loss: 0.016732150688767433, tv_loss: 0.01715620420873165\n",
      "iteration 2335, dc_loss: 0.01673213578760624, tv_loss: 0.017156032845377922\n",
      "iteration 2336, dc_loss: 0.01673208549618721, tv_loss: 0.017156004905700684\n",
      "iteration 2337, dc_loss: 0.016732001677155495, tv_loss: 0.01715574786067009\n",
      "iteration 2338, dc_loss: 0.016731927171349525, tv_loss: 0.017155595123767853\n",
      "iteration 2339, dc_loss: 0.016731837764382362, tv_loss: 0.01715567149221897\n",
      "iteration 2340, dc_loss: 0.016731752082705498, tv_loss: 0.017155522480607033\n",
      "iteration 2341, dc_loss: 0.016731692478060722, tv_loss: 0.017155315726995468\n",
      "iteration 2342, dc_loss: 0.016731638461351395, tv_loss: 0.017155254259705544\n",
      "iteration 2343, dc_loss: 0.01673158071935177, tv_loss: 0.01715514250099659\n",
      "iteration 2344, dc_loss: 0.01673155091702938, tv_loss: 0.01715504564344883\n",
      "iteration 2345, dc_loss: 0.01673152483999729, tv_loss: 0.017154661938548088\n",
      "iteration 2346, dc_loss: 0.01673153042793274, tv_loss: 0.017154542729258537\n",
      "iteration 2347, dc_loss: 0.01673150062561035, tv_loss: 0.017154492437839508\n",
      "iteration 2348, dc_loss: 0.01673142798244953, tv_loss: 0.017154498025774956\n",
      "iteration 2349, dc_loss: 0.01673133298754692, tv_loss: 0.01715417392551899\n",
      "iteration 2350, dc_loss: 0.016731252893805504, tv_loss: 0.017153963446617126\n",
      "iteration 2351, dc_loss: 0.01673119328916073, tv_loss: 0.017153887078166008\n",
      "iteration 2352, dc_loss: 0.01673116348683834, tv_loss: 0.017153771594166756\n",
      "iteration 2353, dc_loss: 0.01673113740980625, tv_loss: 0.017153633758425713\n",
      "iteration 2354, dc_loss: 0.016731103882193565, tv_loss: 0.01715339720249176\n",
      "iteration 2355, dc_loss: 0.016731049865484238, tv_loss: 0.01715320348739624\n",
      "iteration 2356, dc_loss: 0.016731027513742447, tv_loss: 0.01715308427810669\n",
      "iteration 2357, dc_loss: 0.016731031239032745, tv_loss: 0.017152858898043633\n",
      "iteration 2358, dc_loss: 0.016731005162000656, tv_loss: 0.01715259440243244\n",
      "iteration 2359, dc_loss: 0.016730954870581627, tv_loss: 0.017152462154626846\n",
      "iteration 2360, dc_loss: 0.016730865463614464, tv_loss: 0.017152436077594757\n",
      "iteration 2361, dc_loss: 0.016730809584259987, tv_loss: 0.017152322456240654\n",
      "iteration 2362, dc_loss: 0.016730783507227898, tv_loss: 0.017152102664113045\n",
      "iteration 2363, dc_loss: 0.016730742529034615, tv_loss: 0.017152080312371254\n",
      "iteration 2364, dc_loss: 0.01673068106174469, tv_loss: 0.01715199463069439\n",
      "iteration 2365, dc_loss: 0.01673060655593872, tv_loss: 0.01715170405805111\n",
      "iteration 2366, dc_loss: 0.01673055998980999, tv_loss: 0.01715162768959999\n",
      "iteration 2367, dc_loss: 0.016730545088648796, tv_loss: 0.017151465639472008\n",
      "iteration 2368, dc_loss: 0.01673050969839096, tv_loss: 0.01715126447379589\n",
      "iteration 2369, dc_loss: 0.01673048920929432, tv_loss: 0.01715099811553955\n",
      "iteration 2370, dc_loss: 0.016730476170778275, tv_loss: 0.01715075969696045\n",
      "iteration 2371, dc_loss: 0.016730470582842827, tv_loss: 0.017150618135929108\n",
      "iteration 2372, dc_loss: 0.016730453819036484, tv_loss: 0.017150474712252617\n",
      "iteration 2373, dc_loss: 0.01673036627471447, tv_loss: 0.017150379717350006\n",
      "iteration 2374, dc_loss: 0.016730275005102158, tv_loss: 0.017150208353996277\n",
      "iteration 2375, dc_loss: 0.0167301706969738, tv_loss: 0.017150072380900383\n",
      "iteration 2376, dc_loss: 0.01673007197678089, tv_loss: 0.01714997924864292\n",
      "iteration 2377, dc_loss: 0.01673000678420067, tv_loss: 0.01714993640780449\n",
      "iteration 2378, dc_loss: 0.016729997470974922, tv_loss: 0.017149796709418297\n",
      "iteration 2379, dc_loss: 0.016729986295104027, tv_loss: 0.017149509862065315\n",
      "iteration 2380, dc_loss: 0.016729991883039474, tv_loss: 0.0171494297683239\n",
      "iteration 2381, dc_loss: 0.016729947179555893, tv_loss: 0.017149362713098526\n",
      "iteration 2382, dc_loss: 0.01672988012433052, tv_loss: 0.01714903861284256\n",
      "iteration 2383, dc_loss: 0.016729868948459625, tv_loss: 0.017148833721876144\n",
      "iteration 2384, dc_loss: 0.01672985777258873, tv_loss: 0.017148654907941818\n",
      "iteration 2385, dc_loss: 0.016729824244976044, tv_loss: 0.01714874431490898\n",
      "iteration 2386, dc_loss: 0.016729775816202164, tv_loss: 0.01714862324297428\n",
      "iteration 2387, dc_loss: 0.01672971434891224, tv_loss: 0.017148157581686974\n",
      "iteration 2388, dc_loss: 0.01672963611781597, tv_loss: 0.017148084938526154\n",
      "iteration 2389, dc_loss: 0.01672956347465515, tv_loss: 0.017148224636912346\n",
      "iteration 2390, dc_loss: 0.01672952063381672, tv_loss: 0.017148055136203766\n",
      "iteration 2391, dc_loss: 0.01672946661710739, tv_loss: 0.01714790239930153\n",
      "iteration 2392, dc_loss: 0.016729438677430153, tv_loss: 0.01714787259697914\n",
      "iteration 2393, dc_loss: 0.01672942563891411, tv_loss: 0.017147544771432877\n",
      "iteration 2394, dc_loss: 0.016729403287172318, tv_loss: 0.017147371545433998\n",
      "iteration 2395, dc_loss: 0.016729343682527542, tv_loss: 0.017147231847047806\n",
      "iteration 2396, dc_loss: 0.016729256138205528, tv_loss: 0.017147161066532135\n",
      "iteration 2397, dc_loss: 0.016729161143302917, tv_loss: 0.01714707911014557\n",
      "iteration 2398, dc_loss: 0.016729062423110008, tv_loss: 0.017146948724985123\n",
      "iteration 2399, dc_loss: 0.016728973016142845, tv_loss: 0.01714685745537281\n",
      "iteration 2400, dc_loss: 0.016728917136788368, tv_loss: 0.017146630212664604\n",
      "iteration 2401, dc_loss: 0.016728876158595085, tv_loss: 0.01714641973376274\n",
      "iteration 2402, dc_loss: 0.016728896647691727, tv_loss: 0.01714635081589222\n",
      "iteration 2403, dc_loss: 0.016728907823562622, tv_loss: 0.017146164551377296\n",
      "iteration 2404, dc_loss: 0.016728918999433517, tv_loss: 0.017145931720733643\n",
      "iteration 2405, dc_loss: 0.016728896647691727, tv_loss: 0.0171456690877676\n",
      "iteration 2406, dc_loss: 0.016728872433304787, tv_loss: 0.01714540831744671\n",
      "iteration 2407, dc_loss: 0.016728878021240234, tv_loss: 0.017145423218607903\n",
      "iteration 2408, dc_loss: 0.016728831455111504, tv_loss: 0.017145097255706787\n",
      "iteration 2409, dc_loss: 0.016728783026337624, tv_loss: 0.017144901677966118\n",
      "iteration 2410, dc_loss: 0.01672876439988613, tv_loss: 0.017144791781902313\n",
      "iteration 2411, dc_loss: 0.016728730872273445, tv_loss: 0.017144691199064255\n",
      "iteration 2412, dc_loss: 0.01672869175672531, tv_loss: 0.0171444620937109\n",
      "iteration 2413, dc_loss: 0.016728632152080536, tv_loss: 0.017144393175840378\n",
      "iteration 2414, dc_loss: 0.016728533431887627, tv_loss: 0.017144398763775826\n",
      "iteration 2415, dc_loss: 0.016728440299630165, tv_loss: 0.017144324257969856\n",
      "iteration 2416, dc_loss: 0.016728371381759644, tv_loss: 0.01714402437210083\n",
      "iteration 2417, dc_loss: 0.01672833412885666, tv_loss: 0.017143936827778816\n",
      "iteration 2418, dc_loss: 0.016728315502405167, tv_loss: 0.017143744975328445\n",
      "iteration 2419, dc_loss: 0.016728322952985764, tv_loss: 0.017143620178103447\n",
      "iteration 2420, dc_loss: 0.016728291288018227, tv_loss: 0.017143521457910538\n",
      "iteration 2421, dc_loss: 0.016728246584534645, tv_loss: 0.017143353819847107\n",
      "iteration 2422, dc_loss: 0.016728200018405914, tv_loss: 0.017143230885267258\n",
      "iteration 2423, dc_loss: 0.016728153452277184, tv_loss: 0.017143020406365395\n",
      "iteration 2424, dc_loss: 0.01672813668847084, tv_loss: 0.017142929136753082\n",
      "iteration 2425, dc_loss: 0.016728097572922707, tv_loss: 0.017143066972494125\n",
      "iteration 2426, dc_loss: 0.01672804355621338, tv_loss: 0.017142770811915398\n",
      "iteration 2427, dc_loss: 0.016727985814213753, tv_loss: 0.017142517492175102\n",
      "iteration 2428, dc_loss: 0.016727931797504425, tv_loss: 0.01714257337152958\n",
      "iteration 2429, dc_loss: 0.016727838665246964, tv_loss: 0.017142485827207565\n",
      "iteration 2430, dc_loss: 0.016727738082408905, tv_loss: 0.017142266035079956\n",
      "iteration 2431, dc_loss: 0.0167276319116354, tv_loss: 0.017142171040177345\n",
      "iteration 2432, dc_loss: 0.01672755740582943, tv_loss: 0.017142128199338913\n",
      "iteration 2433, dc_loss: 0.01672750525176525, tv_loss: 0.01714201457798481\n",
      "iteration 2434, dc_loss: 0.016727466136217117, tv_loss: 0.017141861841082573\n",
      "iteration 2435, dc_loss: 0.016727454960346222, tv_loss: 0.01714167185127735\n",
      "iteration 2436, dc_loss: 0.016727466136217117, tv_loss: 0.017141543328762054\n",
      "iteration 2437, dc_loss: 0.016727494075894356, tv_loss: 0.017141230404376984\n",
      "iteration 2438, dc_loss: 0.016727525740861893, tv_loss: 0.017141064628958702\n",
      "iteration 2439, dc_loss: 0.016727549955248833, tv_loss: 0.017140865325927734\n",
      "iteration 2440, dc_loss: 0.01672753132879734, tv_loss: 0.017140645533800125\n",
      "iteration 2441, dc_loss: 0.01672753132879734, tv_loss: 0.017140420153737068\n",
      "iteration 2442, dc_loss: 0.016727503389120102, tv_loss: 0.017140286043286324\n",
      "iteration 2443, dc_loss: 0.016727443784475327, tv_loss: 0.017139919102191925\n",
      "iteration 2444, dc_loss: 0.016727400943636894, tv_loss: 0.017139915376901627\n",
      "iteration 2445, dc_loss: 0.01672733761370182, tv_loss: 0.017139898613095284\n",
      "iteration 2446, dc_loss: 0.016727246344089508, tv_loss: 0.01713976077735424\n",
      "iteration 2447, dc_loss: 0.016727164387702942, tv_loss: 0.017139775678515434\n",
      "iteration 2448, dc_loss: 0.016727054491639137, tv_loss: 0.01713961362838745\n",
      "iteration 2449, dc_loss: 0.01672697439789772, tv_loss: 0.017139531672000885\n",
      "iteration 2450, dc_loss: 0.01672692410647869, tv_loss: 0.017139481380581856\n",
      "iteration 2451, dc_loss: 0.01672690361738205, tv_loss: 0.017139367759227753\n",
      "iteration 2452, dc_loss: 0.016726907342672348, tv_loss: 0.017139166593551636\n",
      "iteration 2453, dc_loss: 0.01672692783176899, tv_loss: 0.017138974741101265\n",
      "iteration 2454, dc_loss: 0.01672692410647869, tv_loss: 0.017138643190264702\n",
      "iteration 2455, dc_loss: 0.016726896166801453, tv_loss: 0.01713854819536209\n",
      "iteration 2456, dc_loss: 0.016726812347769737, tv_loss: 0.017138564959168434\n",
      "iteration 2457, dc_loss: 0.016726771369576454, tv_loss: 0.01713855378329754\n",
      "iteration 2458, dc_loss: 0.016726721078157425, tv_loss: 0.01713813655078411\n",
      "iteration 2459, dc_loss: 0.016726676374673843, tv_loss: 0.017137976363301277\n",
      "iteration 2460, dc_loss: 0.016726618632674217, tv_loss: 0.01713799312710762\n",
      "iteration 2461, dc_loss: 0.016726573929190636, tv_loss: 0.017137931659817696\n",
      "iteration 2462, dc_loss: 0.016726532950997353, tv_loss: 0.017137644812464714\n",
      "iteration 2463, dc_loss: 0.01672646403312683, tv_loss: 0.01713753491640091\n",
      "iteration 2464, dc_loss: 0.016726406291127205, tv_loss: 0.01713746041059494\n",
      "iteration 2465, dc_loss: 0.016726357862353325, tv_loss: 0.01713735982775688\n",
      "iteration 2466, dc_loss: 0.016726313158869743, tv_loss: 0.017137112095952034\n",
      "iteration 2467, dc_loss: 0.016726279631257057, tv_loss: 0.017136815935373306\n",
      "iteration 2468, dc_loss: 0.016726277768611908, tv_loss: 0.017136817798018456\n",
      "iteration 2469, dc_loss: 0.016726281493902206, tv_loss: 0.01713682897388935\n",
      "iteration 2470, dc_loss: 0.016726277768611908, tv_loss: 0.017136437818408012\n",
      "iteration 2471, dc_loss: 0.01672622747719288, tv_loss: 0.017136216163635254\n",
      "iteration 2472, dc_loss: 0.016726156696677208, tv_loss: 0.017136231064796448\n",
      "iteration 2473, dc_loss: 0.01672610267996788, tv_loss: 0.01713632419705391\n",
      "iteration 2474, dc_loss: 0.01672603189945221, tv_loss: 0.01713601127266884\n",
      "iteration 2475, dc_loss: 0.0167259369045496, tv_loss: 0.017135869711637497\n",
      "iteration 2476, dc_loss: 0.016725894063711166, tv_loss: 0.017135828733444214\n",
      "iteration 2477, dc_loss: 0.016725879162549973, tv_loss: 0.017135746777057648\n",
      "iteration 2478, dc_loss: 0.016725866124033928, tv_loss: 0.017135469242930412\n",
      "iteration 2479, dc_loss: 0.016725853085517883, tv_loss: 0.017135174944996834\n",
      "iteration 2480, dc_loss: 0.016725847497582436, tv_loss: 0.017135048285126686\n",
      "iteration 2481, dc_loss: 0.016725849360227585, tv_loss: 0.017134899273514748\n",
      "iteration 2482, dc_loss: 0.01672583632171154, tv_loss: 0.01713484339416027\n",
      "iteration 2483, dc_loss: 0.016725806519389153, tv_loss: 0.017134783789515495\n",
      "iteration 2484, dc_loss: 0.01672573760151863, tv_loss: 0.017134660854935646\n",
      "iteration 2485, dc_loss: 0.016725681722164154, tv_loss: 0.017134500667452812\n",
      "iteration 2486, dc_loss: 0.016725637018680573, tv_loss: 0.01713445410132408\n",
      "iteration 2487, dc_loss: 0.016725581139326096, tv_loss: 0.017134398221969604\n",
      "iteration 2488, dc_loss: 0.01672547124326229, tv_loss: 0.017134282737970352\n",
      "iteration 2489, dc_loss: 0.016725340858101845, tv_loss: 0.017134228721261024\n",
      "iteration 2490, dc_loss: 0.016725271940231323, tv_loss: 0.017134184017777443\n",
      "iteration 2491, dc_loss: 0.01672525145113468, tv_loss: 0.017134053632616997\n",
      "iteration 2492, dc_loss: 0.01672522909939289, tv_loss: 0.01713383197784424\n",
      "iteration 2493, dc_loss: 0.016725219786167145, tv_loss: 0.017133651301264763\n",
      "iteration 2494, dc_loss: 0.016725216060876846, tv_loss: 0.017133627086877823\n",
      "iteration 2495, dc_loss: 0.01672520861029625, tv_loss: 0.017133500427007675\n",
      "iteration 2496, dc_loss: 0.01672518625855446, tv_loss: 0.01713317446410656\n",
      "iteration 2497, dc_loss: 0.016725147143006325, tv_loss: 0.017133070155978203\n",
      "iteration 2498, dc_loss: 0.016725121065974236, tv_loss: 0.01713298074901104\n",
      "iteration 2499, dc_loss: 0.016725124791264534, tv_loss: 0.017132854089140892\n",
      "iteration 2500, dc_loss: 0.01672515831887722, tv_loss: 0.01713256724178791\n",
      "iteration 2501, dc_loss: 0.016725121065974236, tv_loss: 0.017132282257080078\n",
      "iteration 2502, dc_loss: 0.01672501489520073, tv_loss: 0.01713237166404724\n",
      "iteration 2503, dc_loss: 0.016724886372685432, tv_loss: 0.01713239774107933\n",
      "iteration 2504, dc_loss: 0.016724813729524612, tv_loss: 0.017132140696048737\n",
      "iteration 2505, dc_loss: 0.01672479510307312, tv_loss: 0.01713217794895172\n",
      "iteration 2506, dc_loss: 0.01672479696571827, tv_loss: 0.017132006585597992\n",
      "iteration 2507, dc_loss: 0.016724789515137672, tv_loss: 0.0171316247433424\n",
      "iteration 2508, dc_loss: 0.016724761575460434, tv_loss: 0.017131386324763298\n",
      "iteration 2509, dc_loss: 0.016724757850170135, tv_loss: 0.017131216824054718\n",
      "iteration 2510, dc_loss: 0.016724713146686554, tv_loss: 0.017131278291344643\n",
      "iteration 2511, dc_loss: 0.01672462746500969, tv_loss: 0.017131060361862183\n",
      "iteration 2512, dc_loss: 0.016724523156881332, tv_loss: 0.01713104546070099\n",
      "iteration 2513, dc_loss: 0.016724450513720512, tv_loss: 0.01713099703192711\n",
      "iteration 2514, dc_loss: 0.016724446788430214, tv_loss: 0.01713080145418644\n",
      "iteration 2515, dc_loss: 0.016724439337849617, tv_loss: 0.017130734398961067\n",
      "iteration 2516, dc_loss: 0.016724422574043274, tv_loss: 0.017130477353930473\n",
      "iteration 2517, dc_loss: 0.016724370419979095, tv_loss: 0.017130356281995773\n",
      "iteration 2518, dc_loss: 0.016724321991205215, tv_loss: 0.017130328342318535\n",
      "iteration 2519, dc_loss: 0.01672428846359253, tv_loss: 0.017130106687545776\n",
      "iteration 2520, dc_loss: 0.0167242381721735, tv_loss: 0.017129933461546898\n",
      "iteration 2521, dc_loss: 0.01672418974339962, tv_loss: 0.01712999865412712\n",
      "iteration 2522, dc_loss: 0.016724171116948128, tv_loss: 0.01712987944483757\n",
      "iteration 2523, dc_loss: 0.016724172979593277, tv_loss: 0.01712958700954914\n",
      "iteration 2524, dc_loss: 0.01672416739165783, tv_loss: 0.017129424959421158\n",
      "iteration 2525, dc_loss: 0.016724128276109695, tv_loss: 0.017129255458712578\n",
      "iteration 2526, dc_loss: 0.016724104061722755, tv_loss: 0.017129233106970787\n",
      "iteration 2527, dc_loss: 0.016724053770303726, tv_loss: 0.01712922751903534\n",
      "iteration 2528, dc_loss: 0.016723966225981712, tv_loss: 0.017129110172390938\n",
      "iteration 2529, dc_loss: 0.016723887994885445, tv_loss: 0.017129039391875267\n",
      "iteration 2530, dc_loss: 0.016723791137337685, tv_loss: 0.017128849402070045\n",
      "iteration 2531, dc_loss: 0.016723716631531715, tv_loss: 0.017128881067037582\n",
      "iteration 2532, dc_loss: 0.016723664477467537, tv_loss: 0.017128800973296165\n",
      "iteration 2533, dc_loss: 0.016723651438951492, tv_loss: 0.017128577455878258\n",
      "iteration 2534, dc_loss: 0.01672367937862873, tv_loss: 0.017128316685557365\n",
      "iteration 2535, dc_loss: 0.016723688691854477, tv_loss: 0.01712818071246147\n",
      "iteration 2536, dc_loss: 0.01672368124127388, tv_loss: 0.01712806709110737\n",
      "iteration 2537, dc_loss: 0.016723640263080597, tv_loss: 0.017128093168139458\n",
      "iteration 2538, dc_loss: 0.016723591834306717, tv_loss: 0.017127735540270805\n",
      "iteration 2539, dc_loss: 0.016723567619919777, tv_loss: 0.01712770387530327\n",
      "iteration 2540, dc_loss: 0.016723522916436195, tv_loss: 0.017127739265561104\n",
      "iteration 2541, dc_loss: 0.01672343909740448, tv_loss: 0.017127487808465958\n",
      "iteration 2542, dc_loss: 0.016723360866308212, tv_loss: 0.017127564176917076\n",
      "iteration 2543, dc_loss: 0.01672329753637314, tv_loss: 0.01712762378156185\n",
      "iteration 2544, dc_loss: 0.016723275184631348, tv_loss: 0.017127245664596558\n",
      "iteration 2545, dc_loss: 0.01672329194843769, tv_loss: 0.017126988619565964\n",
      "iteration 2546, dc_loss: 0.01672331988811493, tv_loss: 0.01712690107524395\n",
      "iteration 2547, dc_loss: 0.016723301261663437, tv_loss: 0.01712684892117977\n",
      "iteration 2548, dc_loss: 0.016723258420825005, tv_loss: 0.017126716673374176\n",
      "iteration 2549, dc_loss: 0.016723187640309334, tv_loss: 0.017126478254795074\n",
      "iteration 2550, dc_loss: 0.016723092645406723, tv_loss: 0.01712648756802082\n",
      "iteration 2551, dc_loss: 0.016722995787858963, tv_loss: 0.017126481980085373\n",
      "iteration 2552, dc_loss: 0.01672290824353695, tv_loss: 0.017126424238085747\n",
      "iteration 2553, dc_loss: 0.01672288402915001, tv_loss: 0.0171261765062809\n",
      "iteration 2554, dc_loss: 0.01672288402915001, tv_loss: 0.017126064747571945\n",
      "iteration 2555, dc_loss: 0.0167229026556015, tv_loss: 0.017125800251960754\n",
      "iteration 2556, dc_loss: 0.016722917556762695, tv_loss: 0.017125727608799934\n",
      "iteration 2557, dc_loss: 0.01672290824353695, tv_loss: 0.017125558108091354\n",
      "iteration 2558, dc_loss: 0.01672285608947277, tv_loss: 0.017125457525253296\n",
      "iteration 2559, dc_loss: 0.016722790896892548, tv_loss: 0.017125319689512253\n",
      "iteration 2560, dc_loss: 0.016722731292247772, tv_loss: 0.017125329002738\n",
      "iteration 2561, dc_loss: 0.016722694039344788, tv_loss: 0.017125239595770836\n",
      "iteration 2562, dc_loss: 0.0167226642370224, tv_loss: 0.01712501421570778\n",
      "iteration 2563, dc_loss: 0.0167226679623127, tv_loss: 0.01712465099990368\n",
      "iteration 2564, dc_loss: 0.01672268845140934, tv_loss: 0.017124533653259277\n",
      "iteration 2565, dc_loss: 0.016722623258829117, tv_loss: 0.01712452806532383\n",
      "iteration 2566, dc_loss: 0.01672251895070076, tv_loss: 0.017124494537711143\n",
      "iteration 2567, dc_loss: 0.016722427681088448, tv_loss: 0.017124401405453682\n",
      "iteration 2568, dc_loss: 0.01672235131263733, tv_loss: 0.0171241145581007\n",
      "iteration 2569, dc_loss: 0.0167223010212183, tv_loss: 0.017124012112617493\n",
      "iteration 2570, dc_loss: 0.016722338274121284, tv_loss: 0.017123915255069733\n",
      "iteration 2571, dc_loss: 0.016722422093153, tv_loss: 0.01712370663881302\n",
      "iteration 2572, dc_loss: 0.016722461208701134, tv_loss: 0.017123376950621605\n",
      "iteration 2573, dc_loss: 0.016722476109862328, tv_loss: 0.017123324796557426\n",
      "iteration 2574, dc_loss: 0.016722440719604492, tv_loss: 0.017123231664299965\n",
      "iteration 2575, dc_loss: 0.016722368076443672, tv_loss: 0.017123179510235786\n",
      "iteration 2576, dc_loss: 0.01672225631773472, tv_loss: 0.017123084515333176\n",
      "iteration 2577, dc_loss: 0.016722170636057854, tv_loss: 0.017123103141784668\n",
      "iteration 2578, dc_loss: 0.01672210358083248, tv_loss: 0.01712307333946228\n",
      "iteration 2579, dc_loss: 0.016722049564123154, tv_loss: 0.017123034223914146\n",
      "iteration 2580, dc_loss: 0.01672200672328472, tv_loss: 0.017122872173786163\n",
      "iteration 2581, dc_loss: 0.016721980646252632, tv_loss: 0.0171227864921093\n",
      "iteration 2582, dc_loss: 0.0167219378054142, tv_loss: 0.017122656106948853\n",
      "iteration 2583, dc_loss: 0.016721894964575768, tv_loss: 0.017122678458690643\n",
      "iteration 2584, dc_loss: 0.01672186702489853, tv_loss: 0.017122475430369377\n",
      "iteration 2585, dc_loss: 0.01672184281051159, tv_loss: 0.017122285440564156\n",
      "iteration 2586, dc_loss: 0.01672179624438286, tv_loss: 0.017122263088822365\n",
      "iteration 2587, dc_loss: 0.016721760854125023, tv_loss: 0.01712188869714737\n",
      "iteration 2588, dc_loss: 0.016721751540899277, tv_loss: 0.017121868208050728\n",
      "iteration 2589, dc_loss: 0.016721714287996292, tv_loss: 0.0171216893941164\n",
      "iteration 2590, dc_loss: 0.016721680760383606, tv_loss: 0.01712159439921379\n",
      "iteration 2591, dc_loss: 0.016721660271286964, tv_loss: 0.017121586948633194\n",
      "iteration 2592, dc_loss: 0.016721604391932487, tv_loss: 0.017121421173214912\n",
      "iteration 2593, dc_loss: 0.016721557825803757, tv_loss: 0.017121130600571632\n",
      "iteration 2594, dc_loss: 0.016721557825803757, tv_loss: 0.017121171578764915\n",
      "iteration 2595, dc_loss: 0.016721513122320175, tv_loss: 0.017121167853474617\n",
      "iteration 2596, dc_loss: 0.016721444204449654, tv_loss: 0.017121020704507828\n",
      "iteration 2597, dc_loss: 0.016721416264772415, tv_loss: 0.017120923846960068\n",
      "iteration 2598, dc_loss: 0.01672137714922428, tv_loss: 0.0171208418905735\n",
      "iteration 2599, dc_loss: 0.016721338033676147, tv_loss: 0.01712070219218731\n",
      "iteration 2600, dc_loss: 0.016721270978450775, tv_loss: 0.017120542004704475\n",
      "iteration 2601, dc_loss: 0.016721224412322044, tv_loss: 0.017120325937867165\n",
      "iteration 2602, dc_loss: 0.016721211373806, tv_loss: 0.01712031476199627\n",
      "iteration 2603, dc_loss: 0.016721194609999657, tv_loss: 0.017120297998189926\n",
      "iteration 2604, dc_loss: 0.016721166670322418, tv_loss: 0.017119918018579483\n",
      "iteration 2605, dc_loss: 0.016721131280064583, tv_loss: 0.017119770869612694\n",
      "iteration 2606, dc_loss: 0.01672108843922615, tv_loss: 0.01711977645754814\n",
      "iteration 2607, dc_loss: 0.01672106608748436, tv_loss: 0.017119651660323143\n",
      "iteration 2608, dc_loss: 0.016721051186323166, tv_loss: 0.01711937226355076\n",
      "iteration 2609, dc_loss: 0.016721034422516823, tv_loss: 0.01711946539580822\n",
      "iteration 2610, dc_loss: 0.016721006482839584, tv_loss: 0.017119193449616432\n",
      "iteration 2611, dc_loss: 0.016720950603485107, tv_loss: 0.017119022086262703\n",
      "iteration 2612, dc_loss: 0.016720913350582123, tv_loss: 0.017119012773036957\n",
      "iteration 2613, dc_loss: 0.016720851883292198, tv_loss: 0.01711893454194069\n",
      "iteration 2614, dc_loss: 0.016720810905098915, tv_loss: 0.01711885817348957\n",
      "iteration 2615, dc_loss: 0.01672079972922802, tv_loss: 0.0171186625957489\n",
      "iteration 2616, dc_loss: 0.016720782965421677, tv_loss: 0.017118513584136963\n",
      "iteration 2617, dc_loss: 0.016720779240131378, tv_loss: 0.017118507996201515\n",
      "iteration 2618, dc_loss: 0.016720743849873543, tv_loss: 0.01711825095117092\n",
      "iteration 2619, dc_loss: 0.016720660030841827, tv_loss: 0.01711805909872055\n",
      "iteration 2620, dc_loss: 0.016720615327358246, tv_loss: 0.017118142917752266\n",
      "iteration 2621, dc_loss: 0.01672060415148735, tv_loss: 0.017117856070399284\n",
      "iteration 2622, dc_loss: 0.016720565035939217, tv_loss: 0.017117749899625778\n",
      "iteration 2623, dc_loss: 0.016720492392778397, tv_loss: 0.017117751762270927\n",
      "iteration 2624, dc_loss: 0.01672043465077877, tv_loss: 0.01711762510240078\n",
      "iteration 2625, dc_loss: 0.016720375046133995, tv_loss: 0.017117489129304886\n",
      "iteration 2626, dc_loss: 0.016720373183488846, tv_loss: 0.017117533832788467\n",
      "iteration 2627, dc_loss: 0.016720367595553398, tv_loss: 0.017117442563176155\n",
      "iteration 2628, dc_loss: 0.016720309853553772, tv_loss: 0.017117181792855263\n",
      "iteration 2629, dc_loss: 0.016720261424779892, tv_loss: 0.017117230221629143\n",
      "iteration 2630, dc_loss: 0.01672021485865116, tv_loss: 0.017117096111178398\n",
      "iteration 2631, dc_loss: 0.016720188781619072, tv_loss: 0.017116939648985863\n",
      "iteration 2632, dc_loss: 0.016720151528716087, tv_loss: 0.01711687073111534\n",
      "iteration 2633, dc_loss: 0.01672009937465191, tv_loss: 0.01711682230234146\n",
      "iteration 2634, dc_loss: 0.016720080748200417, tv_loss: 0.01711658388376236\n",
      "iteration 2635, dc_loss: 0.01672007329761982, tv_loss: 0.017116405069828033\n",
      "iteration 2636, dc_loss: 0.016720101237297058, tv_loss: 0.0171163659542799\n",
      "iteration 2637, dc_loss: 0.016720101237297058, tv_loss: 0.017116155475378036\n",
      "iteration 2638, dc_loss: 0.016720062121748924, tv_loss: 0.017116092145442963\n",
      "iteration 2639, dc_loss: 0.01671999879181385, tv_loss: 0.017116129398345947\n",
      "iteration 2640, dc_loss: 0.016719913110136986, tv_loss: 0.017116012051701546\n",
      "iteration 2641, dc_loss: 0.016719797626137733, tv_loss: 0.01711590588092804\n",
      "iteration 2642, dc_loss: 0.016719747334718704, tv_loss: 0.017116006463766098\n",
      "iteration 2643, dc_loss: 0.01671973615884781, tv_loss: 0.01711588352918625\n",
      "iteration 2644, dc_loss: 0.0167197585105896, tv_loss: 0.01711561717092991\n",
      "iteration 2645, dc_loss: 0.016719769686460495, tv_loss: 0.017115460708737373\n",
      "iteration 2646, dc_loss: 0.016719726845622063, tv_loss: 0.017115265130996704\n",
      "iteration 2647, dc_loss: 0.01671968214213848, tv_loss: 0.017115307971835136\n",
      "iteration 2648, dc_loss: 0.016719626262784004, tv_loss: 0.017115049064159393\n",
      "iteration 2649, dc_loss: 0.016719616949558258, tv_loss: 0.017114920541644096\n",
      "iteration 2650, dc_loss: 0.01671961136162281, tv_loss: 0.017114801332354546\n",
      "iteration 2651, dc_loss: 0.01671959087252617, tv_loss: 0.017114605754613876\n",
      "iteration 2652, dc_loss: 0.016719548031687737, tv_loss: 0.017114531248807907\n",
      "iteration 2653, dc_loss: 0.016719510778784752, tv_loss: 0.01711449958384037\n",
      "iteration 2654, dc_loss: 0.016719462350010872, tv_loss: 0.017114536836743355\n",
      "iteration 2655, dc_loss: 0.016719402745366096, tv_loss: 0.017114469781517982\n",
      "iteration 2656, dc_loss: 0.016719359904527664, tv_loss: 0.017114244401454926\n",
      "iteration 2657, dc_loss: 0.016719283536076546, tv_loss: 0.017114119604229927\n",
      "iteration 2658, dc_loss: 0.016719205304980278, tv_loss: 0.017114169895648956\n",
      "iteration 2659, dc_loss: 0.01671915128827095, tv_loss: 0.01711410842835903\n",
      "iteration 2660, dc_loss: 0.016719091683626175, tv_loss: 0.017114030197262764\n",
      "iteration 2661, dc_loss: 0.01671907864511013, tv_loss: 0.017113855108618736\n",
      "iteration 2662, dc_loss: 0.01671908237040043, tv_loss: 0.0171137023717165\n",
      "iteration 2663, dc_loss: 0.016719067469239235, tv_loss: 0.017113659530878067\n",
      "iteration 2664, dc_loss: 0.016719067469239235, tv_loss: 0.017113547772169113\n",
      "iteration 2665, dc_loss: 0.01671907678246498, tv_loss: 0.017113296315073967\n",
      "iteration 2666, dc_loss: 0.01671905815601349, tv_loss: 0.017113199457526207\n",
      "iteration 2667, dc_loss: 0.0167190283536911, tv_loss: 0.017112936824560165\n",
      "iteration 2668, dc_loss: 0.016718987375497818, tv_loss: 0.017112817615270615\n",
      "iteration 2669, dc_loss: 0.016718914732336998, tv_loss: 0.01711290143430233\n",
      "iteration 2670, dc_loss: 0.016718829050660133, tv_loss: 0.017112838104367256\n",
      "iteration 2671, dc_loss: 0.01671876385807991, tv_loss: 0.01711270958185196\n",
      "iteration 2672, dc_loss: 0.016718722879886627, tv_loss: 0.017112595960497856\n",
      "iteration 2673, dc_loss: 0.016718680039048195, tv_loss: 0.017112648114562035\n",
      "iteration 2674, dc_loss: 0.016718680039048195, tv_loss: 0.017112569883465767\n",
      "iteration 2675, dc_loss: 0.016718706116080284, tv_loss: 0.017112301662564278\n",
      "iteration 2676, dc_loss: 0.016718702390789986, tv_loss: 0.017112160101532936\n",
      "iteration 2677, dc_loss: 0.016718704253435135, tv_loss: 0.01711202971637249\n",
      "iteration 2678, dc_loss: 0.016718676313757896, tv_loss: 0.017111914232373238\n",
      "iteration 2679, dc_loss: 0.016718609258532524, tv_loss: 0.017111878842115402\n",
      "iteration 2680, dc_loss: 0.016718579456210136, tv_loss: 0.017111776396632195\n",
      "iteration 2681, dc_loss: 0.016718555241823196, tv_loss: 0.01711166463792324\n",
      "iteration 2682, dc_loss: 0.016718503087759018, tv_loss: 0.01711166277527809\n",
      "iteration 2683, dc_loss: 0.016718436032533646, tv_loss: 0.017111577093601227\n",
      "iteration 2684, dc_loss: 0.016718367114663124, tv_loss: 0.017111537978053093\n",
      "iteration 2685, dc_loss: 0.01671830751001835, tv_loss: 0.017111340537667274\n",
      "iteration 2686, dc_loss: 0.01671828143298626, tv_loss: 0.017111290246248245\n",
      "iteration 2687, dc_loss: 0.01671827957034111, tv_loss: 0.01711127534508705\n",
      "iteration 2688, dc_loss: 0.01671830751001835, tv_loss: 0.01711108349263668\n",
      "iteration 2689, dc_loss: 0.016718314960598946, tv_loss: 0.017110833898186684\n",
      "iteration 2690, dc_loss: 0.016718290746212006, tv_loss: 0.017110580578446388\n",
      "iteration 2691, dc_loss: 0.016718227416276932, tv_loss: 0.017110640183091164\n",
      "iteration 2692, dc_loss: 0.01671815849840641, tv_loss: 0.017110643908381462\n",
      "iteration 2693, dc_loss: 0.016718123108148575, tv_loss: 0.0171103123575449\n",
      "iteration 2694, dc_loss: 0.016718091443181038, tv_loss: 0.01711035519838333\n",
      "iteration 2695, dc_loss: 0.016718056052923203, tv_loss: 0.017110317945480347\n",
      "iteration 2696, dc_loss: 0.016717983409762383, tv_loss: 0.01711009256541729\n",
      "iteration 2697, dc_loss: 0.01671792007982731, tv_loss: 0.017109999433159828\n",
      "iteration 2698, dc_loss: 0.016717905178666115, tv_loss: 0.017109910026192665\n",
      "iteration 2699, dc_loss: 0.016717934980988503, tv_loss: 0.017109869047999382\n",
      "iteration 2700, dc_loss: 0.01671796664595604, tv_loss: 0.01710973121225834\n",
      "iteration 2701, dc_loss: 0.016717955470085144, tv_loss: 0.01710948348045349\n",
      "iteration 2702, dc_loss: 0.01671791262924671, tv_loss: 0.017109358683228493\n",
      "iteration 2703, dc_loss: 0.016717830672860146, tv_loss: 0.01710943505167961\n",
      "iteration 2704, dc_loss: 0.01671774312853813, tv_loss: 0.017109455540776253\n",
      "iteration 2705, dc_loss: 0.016717638820409775, tv_loss: 0.017109302803874016\n",
      "iteration 2706, dc_loss: 0.016717582941055298, tv_loss: 0.01710929349064827\n",
      "iteration 2707, dc_loss: 0.01671760156750679, tv_loss: 0.017109233886003494\n",
      "iteration 2708, dc_loss: 0.01671762578189373, tv_loss: 0.017109043896198273\n",
      "iteration 2709, dc_loss: 0.01671760343015194, tv_loss: 0.017108798027038574\n",
      "iteration 2710, dc_loss: 0.016717564314603806, tv_loss: 0.01710866577923298\n",
      "iteration 2711, dc_loss: 0.016717521473765373, tv_loss: 0.017108725383877754\n",
      "iteration 2712, dc_loss: 0.01671747677028179, tv_loss: 0.017108583822846413\n",
      "iteration 2713, dc_loss: 0.016717419028282166, tv_loss: 0.017108537256717682\n",
      "iteration 2714, dc_loss: 0.01671741157770157, tv_loss: 0.01710839755833149\n",
      "iteration 2715, dc_loss: 0.016717378050088882, tv_loss: 0.01710820198059082\n",
      "iteration 2716, dc_loss: 0.016717366874217987, tv_loss: 0.017108188942074776\n",
      "iteration 2717, dc_loss: 0.016717368736863136, tv_loss: 0.01710788905620575\n",
      "iteration 2718, dc_loss: 0.016717366874217987, tv_loss: 0.017107747495174408\n",
      "iteration 2719, dc_loss: 0.016717340797185898, tv_loss: 0.017107749357819557\n",
      "iteration 2720, dc_loss: 0.016717292368412018, tv_loss: 0.017107637599110603\n",
      "iteration 2721, dc_loss: 0.016717268154025078, tv_loss: 0.017107579857110977\n",
      "iteration 2722, dc_loss: 0.016717206686735153, tv_loss: 0.017107581719756126\n",
      "iteration 2723, dc_loss: 0.016717107966542244, tv_loss: 0.017107471823692322\n",
      "iteration 2724, dc_loss: 0.016717037186026573, tv_loss: 0.017107414081692696\n",
      "iteration 2725, dc_loss: 0.01671702042222023, tv_loss: 0.017107320949435234\n",
      "iteration 2726, dc_loss: 0.016717012971639633, tv_loss: 0.017107173800468445\n",
      "iteration 2727, dc_loss: 0.01671697571873665, tv_loss: 0.017107071354985237\n",
      "iteration 2728, dc_loss: 0.016716932877898216, tv_loss: 0.017106911167502403\n",
      "iteration 2729, dc_loss: 0.016716891899704933, tv_loss: 0.017106754705309868\n",
      "iteration 2730, dc_loss: 0.016716845333576202, tv_loss: 0.01710684597492218\n",
      "iteration 2731, dc_loss: 0.016716815531253815, tv_loss: 0.017106888815760612\n",
      "iteration 2732, dc_loss: 0.01671680063009262, tv_loss: 0.017106642946600914\n",
      "iteration 2733, dc_loss: 0.016716832295060158, tv_loss: 0.01710633561015129\n",
      "iteration 2734, dc_loss: 0.01671685464680195, tv_loss: 0.01710619404911995\n",
      "iteration 2735, dc_loss: 0.01671682298183441, tv_loss: 0.017106305807828903\n",
      "iteration 2736, dc_loss: 0.016716793179512024, tv_loss: 0.017106246203184128\n",
      "iteration 2737, dc_loss: 0.01671675778925419, tv_loss: 0.01710590161383152\n",
      "iteration 2738, dc_loss: 0.016716649755835533, tv_loss: 0.017105933278799057\n",
      "iteration 2739, dc_loss: 0.01671655848622322, tv_loss: 0.0171060748398304\n",
      "iteration 2740, dc_loss: 0.016716470941901207, tv_loss: 0.017106011509895325\n",
      "iteration 2741, dc_loss: 0.016716372221708298, tv_loss: 0.017105966806411743\n",
      "iteration 2742, dc_loss: 0.01671633869409561, tv_loss: 0.01710587739944458\n",
      "iteration 2743, dc_loss: 0.016716357320547104, tv_loss: 0.017105791717767715\n",
      "iteration 2744, dc_loss: 0.016716400161385536, tv_loss: 0.017105571925640106\n",
      "iteration 2745, dc_loss: 0.01671641878783703, tv_loss: 0.01710541360080242\n",
      "iteration 2746, dc_loss: 0.016716405749320984, tv_loss: 0.017105335369706154\n",
      "iteration 2747, dc_loss: 0.016716374084353447, tv_loss: 0.017105283215641975\n",
      "iteration 2748, dc_loss: 0.01671636290848255, tv_loss: 0.01710514724254608\n",
      "iteration 2749, dc_loss: 0.016716357320547104, tv_loss: 0.017104893922805786\n",
      "iteration 2750, dc_loss: 0.016716351732611656, tv_loss: 0.017104599624872208\n",
      "iteration 2751, dc_loss: 0.016716361045837402, tv_loss: 0.017104610800743103\n",
      "iteration 2752, dc_loss: 0.016716351732611656, tv_loss: 0.017104793339967728\n",
      "iteration 2753, dc_loss: 0.016716286540031433, tv_loss: 0.01710452325642109\n",
      "iteration 2754, dc_loss: 0.01671622134745121, tv_loss: 0.01710432767868042\n",
      "iteration 2755, dc_loss: 0.01671614870429039, tv_loss: 0.017104245722293854\n",
      "iteration 2756, dc_loss: 0.01671612448990345, tv_loss: 0.017104385420680046\n",
      "iteration 2757, dc_loss: 0.016716115176677704, tv_loss: 0.017104310914874077\n",
      "iteration 2758, dc_loss: 0.01671612448990345, tv_loss: 0.01710405759513378\n",
      "iteration 2759, dc_loss: 0.016716068610548973, tv_loss: 0.017103835940361023\n",
      "iteration 2760, dc_loss: 0.016715992242097855, tv_loss: 0.0171038880944252\n",
      "iteration 2761, dc_loss: 0.016715891659259796, tv_loss: 0.01710408367216587\n",
      "iteration 2762, dc_loss: 0.01671580970287323, tv_loss: 0.017103983089327812\n",
      "iteration 2763, dc_loss: 0.016715744510293007, tv_loss: 0.01710379868745804\n",
      "iteration 2764, dc_loss: 0.016715699806809425, tv_loss: 0.01710372418165207\n",
      "iteration 2765, dc_loss: 0.016715649515390396, tv_loss: 0.01710374280810356\n",
      "iteration 2766, dc_loss: 0.01671564392745495, tv_loss: 0.01710350066423416\n",
      "iteration 2767, dc_loss: 0.01671568676829338, tv_loss: 0.017103292047977448\n",
      "iteration 2768, dc_loss: 0.016715725883841515, tv_loss: 0.0171031653881073\n",
      "iteration 2769, dc_loss: 0.01671570912003517, tv_loss: 0.017103096470236778\n",
      "iteration 2770, dc_loss: 0.016715651378035545, tv_loss: 0.01710309274494648\n",
      "iteration 2771, dc_loss: 0.016715606674551964, tv_loss: 0.01710299216210842\n",
      "iteration 2772, dc_loss: 0.016715573146939278, tv_loss: 0.017102833837270737\n",
      "iteration 2773, dc_loss: 0.016715535894036293, tv_loss: 0.017102783545851707\n",
      "iteration 2774, dc_loss: 0.0167155172675848, tv_loss: 0.01710258238017559\n",
      "iteration 2775, dc_loss: 0.016715509817004204, tv_loss: 0.017102552577853203\n",
      "iteration 2776, dc_loss: 0.016715511679649353, tv_loss: 0.01710251159965992\n",
      "iteration 2777, dc_loss: 0.016715478152036667, tv_loss: 0.017102336511015892\n",
      "iteration 2778, dc_loss: 0.01671541854739189, tv_loss: 0.01710214465856552\n",
      "iteration 2779, dc_loss: 0.016715379431843758, tv_loss: 0.01710219494998455\n",
      "iteration 2780, dc_loss: 0.016715358942747116, tv_loss: 0.017102185636758804\n",
      "iteration 2781, dc_loss: 0.01671534962952137, tv_loss: 0.01710215024650097\n",
      "iteration 2782, dc_loss: 0.01671532541513443, tv_loss: 0.017101839184761047\n",
      "iteration 2783, dc_loss: 0.016715314239263535, tv_loss: 0.0171017125248909\n",
      "iteration 2784, dc_loss: 0.01671530306339264, tv_loss: 0.01710171438753605\n",
      "iteration 2785, dc_loss: 0.016715262085199356, tv_loss: 0.017101509496569633\n",
      "iteration 2786, dc_loss: 0.016715215519070625, tv_loss: 0.01710151508450508\n",
      "iteration 2787, dc_loss: 0.01671517826616764, tv_loss: 0.01710132509469986\n",
      "iteration 2788, dc_loss: 0.016715142875909805, tv_loss: 0.0171012282371521\n",
      "iteration 2789, dc_loss: 0.016715075820684433, tv_loss: 0.017101189121603966\n",
      "iteration 2790, dc_loss: 0.016715005040168762, tv_loss: 0.01710125245153904\n",
      "iteration 2791, dc_loss: 0.016714949160814285, tv_loss: 0.01710137538611889\n",
      "iteration 2792, dc_loss: 0.01671491004526615, tv_loss: 0.01710105873644352\n",
      "iteration 2793, dc_loss: 0.016714924946427345, tv_loss: 0.017100945115089417\n",
      "iteration 2794, dc_loss: 0.016714954748749733, tv_loss: 0.017100853845477104\n",
      "iteration 2795, dc_loss: 0.016714978963136673, tv_loss: 0.017100833356380463\n",
      "iteration 2796, dc_loss: 0.016714969649910927, tv_loss: 0.017100634053349495\n",
      "iteration 2797, dc_loss: 0.01671495847404003, tv_loss: 0.017100460827350616\n",
      "iteration 2798, dc_loss: 0.01671493612229824, tv_loss: 0.017100468277931213\n",
      "iteration 2799, dc_loss: 0.016714878380298615, tv_loss: 0.017100384458899498\n",
      "iteration 2800, dc_loss: 0.016714833676815033, tv_loss: 0.01710011065006256\n",
      "iteration 2801, dc_loss: 0.016714779660105705, tv_loss: 0.017100058495998383\n",
      "iteration 2802, dc_loss: 0.016714710742235184, tv_loss: 0.017100151628255844\n",
      "iteration 2803, dc_loss: 0.016714641824364662, tv_loss: 0.017100032418966293\n",
      "iteration 2804, dc_loss: 0.01671460270881653, tv_loss: 0.017099907621741295\n",
      "iteration 2805, dc_loss: 0.016714582219719887, tv_loss: 0.01709992252290249\n",
      "iteration 2806, dc_loss: 0.016714582219719887, tv_loss: 0.017099780961871147\n",
      "iteration 2807, dc_loss: 0.016714556142687798, tv_loss: 0.017099637538194656\n",
      "iteration 2808, dc_loss: 0.016714518889784813, tv_loss: 0.017099520191550255\n",
      "iteration 2809, dc_loss: 0.016714518889784813, tv_loss: 0.017099427059292793\n",
      "iteration 2810, dc_loss: 0.01671450585126877, tv_loss: 0.017099348828196526\n",
      "iteration 2811, dc_loss: 0.016714489087462425, tv_loss: 0.017099155113101006\n",
      "iteration 2812, dc_loss: 0.016714461147785187, tv_loss: 0.017098896205425262\n",
      "iteration 2813, dc_loss: 0.016714435070753098, tv_loss: 0.017099011689424515\n",
      "iteration 2814, dc_loss: 0.016714369878172874, tv_loss: 0.01709897816181183\n",
      "iteration 2815, dc_loss: 0.016714291647076607, tv_loss: 0.017098966985940933\n",
      "iteration 2816, dc_loss: 0.016714222729206085, tv_loss: 0.017098717391490936\n",
      "iteration 2817, dc_loss: 0.01671416312456131, tv_loss: 0.01709871180355549\n",
      "iteration 2818, dc_loss: 0.01671416684985161, tv_loss: 0.0170988030731678\n",
      "iteration 2819, dc_loss: 0.016714192926883698, tv_loss: 0.01709863170981407\n",
      "iteration 2820, dc_loss: 0.016714202240109444, tv_loss: 0.017098410055041313\n",
      "iteration 2821, dc_loss: 0.016714204102754593, tv_loss: 0.017098315060138702\n",
      "iteration 2822, dc_loss: 0.01671418733894825, tv_loss: 0.01709824800491333\n",
      "iteration 2823, dc_loss: 0.016714153811335564, tv_loss: 0.01709803380072117\n",
      "iteration 2824, dc_loss: 0.016714097931981087, tv_loss: 0.017097877338528633\n",
      "iteration 2825, dc_loss: 0.016714032739400864, tv_loss: 0.01709786057472229\n",
      "iteration 2826, dc_loss: 0.01671401411294937, tv_loss: 0.017097804695367813\n",
      "iteration 2827, dc_loss: 0.016714006662368774, tv_loss: 0.01709776185452938\n",
      "iteration 2828, dc_loss: 0.016714032739400864, tv_loss: 0.01709754206240177\n",
      "iteration 2829, dc_loss: 0.01671406626701355, tv_loss: 0.01709742471575737\n",
      "iteration 2830, dc_loss: 0.016714056953787804, tv_loss: 0.017097368836402893\n",
      "iteration 2831, dc_loss: 0.01671399548649788, tv_loss: 0.017097288742661476\n",
      "iteration 2832, dc_loss: 0.016713937744498253, tv_loss: 0.01709718257188797\n",
      "iteration 2833, dc_loss: 0.016713907942175865, tv_loss: 0.017096970230340958\n",
      "iteration 2834, dc_loss: 0.01671384833753109, tv_loss: 0.017096949741244316\n",
      "iteration 2835, dc_loss: 0.01671374775469303, tv_loss: 0.01709708943963051\n",
      "iteration 2836, dc_loss: 0.01671365648508072, tv_loss: 0.017096828669309616\n",
      "iteration 2837, dc_loss: 0.0167135838419199, tv_loss: 0.017096849158406258\n",
      "iteration 2838, dc_loss: 0.01671353541314602, tv_loss: 0.017096813768148422\n",
      "iteration 2839, dc_loss: 0.016713518649339676, tv_loss: 0.01709674671292305\n",
      "iteration 2840, dc_loss: 0.016713541001081467, tv_loss: 0.017096757888793945\n",
      "iteration 2841, dc_loss: 0.0167135838419199, tv_loss: 0.01709664613008499\n",
      "iteration 2842, dc_loss: 0.016713639721274376, tv_loss: 0.01709635555744171\n",
      "iteration 2843, dc_loss: 0.016713693737983704, tv_loss: 0.017096249386668205\n",
      "iteration 2844, dc_loss: 0.016713684424757957, tv_loss: 0.017096344381570816\n",
      "iteration 2845, dc_loss: 0.016713624820113182, tv_loss: 0.017096081748604774\n",
      "iteration 2846, dc_loss: 0.016713572666049004, tv_loss: 0.017096105962991714\n",
      "iteration 2847, dc_loss: 0.016713501885533333, tv_loss: 0.017096109688282013\n",
      "iteration 2848, dc_loss: 0.0167134590446949, tv_loss: 0.017096029594540596\n",
      "iteration 2849, dc_loss: 0.016713405027985573, tv_loss: 0.017095763236284256\n",
      "iteration 2850, dc_loss: 0.016713328659534454, tv_loss: 0.017095765098929405\n",
      "iteration 2851, dc_loss: 0.016713278368115425, tv_loss: 0.017095698043704033\n",
      "iteration 2852, dc_loss: 0.016713231801986694, tv_loss: 0.017095645889639854\n",
      "iteration 2853, dc_loss: 0.016713213175535202, tv_loss: 0.017095467075705528\n",
      "iteration 2854, dc_loss: 0.016713229939341545, tv_loss: 0.017095476388931274\n",
      "iteration 2855, dc_loss: 0.01671323925256729, tv_loss: 0.0170952957123518\n",
      "iteration 2856, dc_loss: 0.016713198274374008, tv_loss: 0.017095232382416725\n",
      "iteration 2857, dc_loss: 0.016713162884116173, tv_loss: 0.01709507405757904\n",
      "iteration 2858, dc_loss: 0.016713129356503487, tv_loss: 0.017094897106289864\n",
      "iteration 2859, dc_loss: 0.016713066026568413, tv_loss: 0.01709497906267643\n",
      "iteration 2860, dc_loss: 0.016712980344891548, tv_loss: 0.017095109447836876\n",
      "iteration 2861, dc_loss: 0.016712898388504982, tv_loss: 0.017094867303967476\n",
      "iteration 2862, dc_loss: 0.016712870448827744, tv_loss: 0.017094939947128296\n",
      "iteration 2863, dc_loss: 0.016712864860892296, tv_loss: 0.017094843089580536\n",
      "iteration 2864, dc_loss: 0.016712844371795654, tv_loss: 0.01709451898932457\n",
      "iteration 2865, dc_loss: 0.016712862998247147, tv_loss: 0.01709463633596897\n",
      "iteration 2866, dc_loss: 0.01671287976205349, tv_loss: 0.017094409093260765\n",
      "iteration 2867, dc_loss: 0.016712889075279236, tv_loss: 0.01709437184035778\n",
      "iteration 2868, dc_loss: 0.01671287976205349, tv_loss: 0.017094293609261513\n",
      "iteration 2869, dc_loss: 0.016712846234440804, tv_loss: 0.01709420420229435\n",
      "iteration 2870, dc_loss: 0.016712849959731102, tv_loss: 0.0170940849930048\n",
      "iteration 2871, dc_loss: 0.016712889075279236, tv_loss: 0.01709395833313465\n",
      "iteration 2872, dc_loss: 0.016712922602891922, tv_loss: 0.017093755304813385\n",
      "iteration 2873, dc_loss: 0.01671290211379528, tv_loss: 0.017093660309910774\n",
      "iteration 2874, dc_loss: 0.016712846234440804, tv_loss: 0.017093535512685776\n",
      "iteration 2875, dc_loss: 0.016712728887796402, tv_loss: 0.017093634232878685\n",
      "iteration 2876, dc_loss: 0.01671261340379715, tv_loss: 0.017093736678361893\n",
      "iteration 2877, dc_loss: 0.016712579876184464, tv_loss: 0.01709362305700779\n",
      "iteration 2878, dc_loss: 0.016712555661797523, tv_loss: 0.017093516886234283\n",
      "iteration 2879, dc_loss: 0.016712510958313942, tv_loss: 0.017093371599912643\n",
      "iteration 2880, dc_loss: 0.016712481155991554, tv_loss: 0.01709342561662197\n",
      "iteration 2881, dc_loss: 0.016712479293346405, tv_loss: 0.017093360424041748\n",
      "iteration 2882, dc_loss: 0.01671246625483036, tv_loss: 0.017093205824494362\n",
      "iteration 2883, dc_loss: 0.016712432727217674, tv_loss: 0.01709304004907608\n",
      "iteration 2884, dc_loss: 0.01671239547431469, tv_loss: 0.01709289662539959\n",
      "iteration 2885, dc_loss: 0.01671237125992775, tv_loss: 0.01709287241101265\n",
      "iteration 2886, dc_loss: 0.0167123693972826, tv_loss: 0.017092768102884293\n",
      "iteration 2887, dc_loss: 0.01671234890818596, tv_loss: 0.017092589288949966\n",
      "iteration 2888, dc_loss: 0.016712309792637825, tv_loss: 0.017092566937208176\n",
      "iteration 2889, dc_loss: 0.016712259501218796, tv_loss: 0.01709272526204586\n",
      "iteration 2890, dc_loss: 0.0167122520506382, tv_loss: 0.01709248684346676\n",
      "iteration 2891, dc_loss: 0.0167122520506382, tv_loss: 0.017092444002628326\n",
      "iteration 2892, dc_loss: 0.0167122483253479, tv_loss: 0.01709231548011303\n",
      "iteration 2893, dc_loss: 0.016712233424186707, tv_loss: 0.017092090100049973\n",
      "iteration 2894, dc_loss: 0.01671220362186432, tv_loss: 0.017092030495405197\n",
      "iteration 2895, dc_loss: 0.016712205484509468, tv_loss: 0.01709209568798542\n",
      "iteration 2896, dc_loss: 0.01671220175921917, tv_loss: 0.01709195226430893\n",
      "iteration 2897, dc_loss: 0.016712168231606483, tv_loss: 0.017091860994696617\n",
      "iteration 2898, dc_loss: 0.016712093725800514, tv_loss: 0.017091689631342888\n",
      "iteration 2899, dc_loss: 0.016712037846446037, tv_loss: 0.017091689631342888\n",
      "iteration 2900, dc_loss: 0.01671198569238186, tv_loss: 0.01709168590605259\n",
      "iteration 2901, dc_loss: 0.016711972653865814, tv_loss: 0.01709161512553692\n",
      "iteration 2902, dc_loss: 0.016711946576833725, tv_loss: 0.01709148660302162\n",
      "iteration 2903, dc_loss: 0.01671193167567253, tv_loss: 0.017091404646635056\n",
      "iteration 2904, dc_loss: 0.016711916774511337, tv_loss: 0.017091426998376846\n",
      "iteration 2905, dc_loss: 0.016711868345737457, tv_loss: 0.017091257497668266\n",
      "iteration 2906, dc_loss: 0.01671178825199604, tv_loss: 0.017091166228055954\n",
      "iteration 2907, dc_loss: 0.016711732372641563, tv_loss: 0.017091190442442894\n",
      "iteration 2908, dc_loss: 0.016711698845028877, tv_loss: 0.017091214656829834\n",
      "iteration 2909, dc_loss: 0.016711721196770668, tv_loss: 0.017091108486056328\n",
      "iteration 2910, dc_loss: 0.016711756587028503, tv_loss: 0.017090920358896255\n",
      "iteration 2911, dc_loss: 0.016711752861738205, tv_loss: 0.017090804874897003\n",
      "iteration 2912, dc_loss: 0.016711724922060966, tv_loss: 0.017090769484639168\n",
      "iteration 2913, dc_loss: 0.016711698845028877, tv_loss: 0.017090685665607452\n",
      "iteration 2914, dc_loss: 0.016711732372641563, tv_loss: 0.017090437933802605\n",
      "iteration 2915, dc_loss: 0.016711710020899773, tv_loss: 0.017090365290641785\n",
      "iteration 2916, dc_loss: 0.016711654141545296, tv_loss: 0.017090298235416412\n",
      "iteration 2917, dc_loss: 0.016711590811610222, tv_loss: 0.01709039881825447\n",
      "iteration 2918, dc_loss: 0.01671152003109455, tv_loss: 0.01709028333425522\n",
      "iteration 2919, dc_loss: 0.01671142503619194, tv_loss: 0.017090218141674995\n",
      "iteration 2920, dc_loss: 0.016711389645934105, tv_loss: 0.017090320587158203\n",
      "iteration 2921, dc_loss: 0.0167114045470953, tv_loss: 0.017090175300836563\n",
      "iteration 2922, dc_loss: 0.016711441799998283, tv_loss: 0.01709011010825634\n",
      "iteration 2923, dc_loss: 0.016711488366127014, tv_loss: 0.01708972081542015\n",
      "iteration 2924, dc_loss: 0.016711527481675148, tv_loss: 0.017089741304516792\n",
      "iteration 2925, dc_loss: 0.016711488366127014, tv_loss: 0.017089711502194405\n",
      "iteration 2926, dc_loss: 0.016711430624127388, tv_loss: 0.0170895978808403\n",
      "iteration 2927, dc_loss: 0.01671137474477291, tv_loss: 0.017089752480387688\n",
      "iteration 2928, dc_loss: 0.01671130768954754, tv_loss: 0.017089638859033585\n",
      "iteration 2929, dc_loss: 0.016711261123418808, tv_loss: 0.017089324072003365\n",
      "iteration 2930, dc_loss: 0.016711214557290077, tv_loss: 0.017089346423745155\n",
      "iteration 2931, dc_loss: 0.016711166128516197, tv_loss: 0.017089327797293663\n",
      "iteration 2932, dc_loss: 0.016711145639419556, tv_loss: 0.017089232802391052\n",
      "iteration 2933, dc_loss: 0.016711125150322914, tv_loss: 0.01708909682929516\n",
      "iteration 2934, dc_loss: 0.016711102798581123, tv_loss: 0.017089135944843292\n",
      "iteration 2935, dc_loss: 0.01671108976006508, tv_loss: 0.017089076340198517\n",
      "iteration 2936, dc_loss: 0.016711069270968437, tv_loss: 0.017088942229747772\n",
      "iteration 2937, dc_loss: 0.016711030155420303, tv_loss: 0.017088912427425385\n",
      "iteration 2938, dc_loss: 0.016711024567484856, tv_loss: 0.017088914290070534\n",
      "iteration 2939, dc_loss: 0.01671098917722702, tv_loss: 0.017088718712329865\n",
      "iteration 2940, dc_loss: 0.016710955649614334, tv_loss: 0.017088724300265312\n",
      "iteration 2941, dc_loss: 0.016710922122001648, tv_loss: 0.017088651657104492\n",
      "iteration 2942, dc_loss: 0.01671091467142105, tv_loss: 0.01708853244781494\n",
      "iteration 2943, dc_loss: 0.01671089604496956, tv_loss: 0.017088470980525017\n",
      "iteration 2944, dc_loss: 0.01671086996793747, tv_loss: 0.017088301479816437\n",
      "iteration 2945, dc_loss: 0.016710849478840828, tv_loss: 0.01708817668259144\n",
      "iteration 2946, dc_loss: 0.01671084389090538, tv_loss: 0.017088187858462334\n",
      "iteration 2947, dc_loss: 0.016710832715034485, tv_loss: 0.017088156193494797\n",
      "iteration 2948, dc_loss: 0.016710825264453888, tv_loss: 0.017087973654270172\n",
      "iteration 2949, dc_loss: 0.016710784286260605, tv_loss: 0.01708792895078659\n",
      "iteration 2950, dc_loss: 0.01671072468161583, tv_loss: 0.0170879103243351\n",
      "iteration 2951, dc_loss: 0.016710707917809486, tv_loss: 0.01708797737956047\n",
      "iteration 2952, dc_loss: 0.016710679978132248, tv_loss: 0.017087725922465324\n",
      "iteration 2953, dc_loss: 0.01671062968671322, tv_loss: 0.017087699845433235\n",
      "iteration 2954, dc_loss: 0.016710584983229637, tv_loss: 0.017087619751691818\n",
      "iteration 2955, dc_loss: 0.01671053096652031, tv_loss: 0.017087705433368683\n",
      "iteration 2956, dc_loss: 0.016710525378584862, tv_loss: 0.01708756759762764\n",
      "iteration 2957, dc_loss: 0.016710523515939713, tv_loss: 0.017087357118725777\n",
      "iteration 2958, dc_loss: 0.01671048253774643, tv_loss: 0.01708720065653324\n",
      "iteration 2959, dc_loss: 0.016710452735424042, tv_loss: 0.017087267711758614\n",
      "iteration 2960, dc_loss: 0.016710473224520683, tv_loss: 0.017087189480662346\n",
      "iteration 2961, dc_loss: 0.01671050488948822, tv_loss: 0.01708691380918026\n",
      "iteration 2962, dc_loss: 0.016710512340068817, tv_loss: 0.01708686165511608\n",
      "iteration 2963, dc_loss: 0.01671050302684307, tv_loss: 0.01708674430847168\n",
      "iteration 2964, dc_loss: 0.016710501164197922, tv_loss: 0.017086656764149666\n",
      "iteration 2965, dc_loss: 0.01671050302684307, tv_loss: 0.01708664931356907\n",
      "iteration 2966, dc_loss: 0.016710465773940086, tv_loss: 0.017086446285247803\n",
      "iteration 2967, dc_loss: 0.016710396856069565, tv_loss: 0.01708633080124855\n",
      "iteration 2968, dc_loss: 0.016710305586457253, tv_loss: 0.017086390405893326\n",
      "iteration 2969, dc_loss: 0.016710208728909492, tv_loss: 0.017086492851376534\n",
      "iteration 2970, dc_loss: 0.016710124909877777, tv_loss: 0.017086338251829147\n",
      "iteration 2971, dc_loss: 0.016710078343749046, tv_loss: 0.0170863289386034\n",
      "iteration 2972, dc_loss: 0.016710102558135986, tv_loss: 0.017086340114474297\n",
      "iteration 2973, dc_loss: 0.01671013981103897, tv_loss: 0.01708616502583027\n",
      "iteration 2974, dc_loss: 0.016710150986909866, tv_loss: 0.017085902392864227\n",
      "iteration 2975, dc_loss: 0.016710175201296806, tv_loss: 0.017085837200284004\n",
      "iteration 2976, dc_loss: 0.01671016961336136, tv_loss: 0.017085716128349304\n",
      "iteration 2977, dc_loss: 0.016710156574845314, tv_loss: 0.017085695639252663\n",
      "iteration 2978, dc_loss: 0.016710130497813225, tv_loss: 0.017085658386349678\n",
      "iteration 2979, dc_loss: 0.016710087656974792, tv_loss: 0.017085526138544083\n",
      "iteration 2980, dc_loss: 0.016710050404071808, tv_loss: 0.017085473984479904\n",
      "iteration 2981, dc_loss: 0.01671002432703972, tv_loss: 0.01708550937473774\n",
      "iteration 2982, dc_loss: 0.01670996844768524, tv_loss: 0.017085522413253784\n",
      "iteration 2983, dc_loss: 0.016709890216588974, tv_loss: 0.017085248604416847\n",
      "iteration 2984, dc_loss: 0.01670985482633114, tv_loss: 0.01708517223596573\n",
      "iteration 2985, dc_loss: 0.016709845513105392, tv_loss: 0.017085334286093712\n",
      "iteration 2986, dc_loss: 0.01670984737575054, tv_loss: 0.01708514615893364\n",
      "iteration 2987, dc_loss: 0.016709845513105392, tv_loss: 0.017084820196032524\n",
      "iteration 2988, dc_loss: 0.016709832474589348, tv_loss: 0.017084769904613495\n",
      "iteration 2989, dc_loss: 0.016709808260202408, tv_loss: 0.017084935680031776\n",
      "iteration 2990, dc_loss: 0.016709769144654274, tv_loss: 0.01708495430648327\n",
      "iteration 2991, dc_loss: 0.016709761694073677, tv_loss: 0.017084645107388496\n",
      "iteration 2992, dc_loss: 0.016709772869944572, tv_loss: 0.017084617167711258\n",
      "iteration 2993, dc_loss: 0.01670977845788002, tv_loss: 0.01708446629345417\n",
      "iteration 2994, dc_loss: 0.016709746792912483, tv_loss: 0.017084570601582527\n",
      "iteration 2995, dc_loss: 0.016709676012396812, tv_loss: 0.017084572464227676\n",
      "iteration 2996, dc_loss: 0.016709594056010246, tv_loss: 0.017084427177906036\n",
      "iteration 2997, dc_loss: 0.016709519550204277, tv_loss: 0.01708431914448738\n",
      "iteration 2998, dc_loss: 0.0167094599455595, tv_loss: 0.017084414139389992\n",
      "iteration 2999, dc_loss: 0.01670943759381771, tv_loss: 0.017084496095776558\n",
      "iteration 3000, dc_loss: 0.0167094599455595, tv_loss: 0.017084281891584396\n",
      "iteration 3001, dc_loss: 0.016709500923752785, tv_loss: 0.017084019258618355\n",
      "iteration 3002, dc_loss: 0.01670951023697853, tv_loss: 0.017083993181586266\n",
      "iteration 3003, dc_loss: 0.016709502786397934, tv_loss: 0.01708386279642582\n",
      "iteration 3004, dc_loss: 0.016709499061107635, tv_loss: 0.017083754763007164\n",
      "iteration 3005, dc_loss: 0.016709472984075546, tv_loss: 0.0170836690813303\n",
      "iteration 3006, dc_loss: 0.01670943759381771, tv_loss: 0.017083797603845596\n",
      "iteration 3007, dc_loss: 0.01670938916504383, tv_loss: 0.017083745449781418\n",
      "iteration 3008, dc_loss: 0.016709353774785995, tv_loss: 0.017083633691072464\n",
      "iteration 3009, dc_loss: 0.016709309071302414, tv_loss: 0.017083583399653435\n",
      "iteration 3010, dc_loss: 0.01670927181839943, tv_loss: 0.017083508893847466\n",
      "iteration 3011, dc_loss: 0.016709232702851295, tv_loss: 0.017083389684557915\n",
      "iteration 3012, dc_loss: 0.016709191724658012, tv_loss: 0.017083298414945602\n",
      "iteration 3013, dc_loss: 0.01670917123556137, tv_loss: 0.017083313316106796\n",
      "iteration 3014, dc_loss: 0.01670914888381958, tv_loss: 0.0170831847935915\n",
      "iteration 3015, dc_loss: 0.01670910231769085, tv_loss: 0.017083272337913513\n",
      "iteration 3016, dc_loss: 0.016709035262465477, tv_loss: 0.017083149403333664\n",
      "iteration 3017, dc_loss: 0.016709012910723686, tv_loss: 0.017083052545785904\n",
      "iteration 3018, dc_loss: 0.01670902967453003, tv_loss: 0.01708286441862583\n",
      "iteration 3019, dc_loss: 0.016709081828594208, tv_loss: 0.017082706093788147\n",
      "iteration 3020, dc_loss: 0.01670912839472294, tv_loss: 0.01708265207707882\n",
      "iteration 3021, dc_loss: 0.016709160059690475, tv_loss: 0.017082596197724342\n",
      "iteration 3022, dc_loss: 0.016709154471755028, tv_loss: 0.01708246022462845\n",
      "iteration 3023, dc_loss: 0.016709106042981148, tv_loss: 0.01708228699862957\n",
      "iteration 3024, dc_loss: 0.016709011048078537, tv_loss: 0.01708238758146763\n",
      "iteration 3025, dc_loss: 0.016708901152014732, tv_loss: 0.01708248443901539\n",
      "iteration 3026, dc_loss: 0.01670883782207966, tv_loss: 0.017082540318369865\n",
      "iteration 3027, dc_loss: 0.016708800569176674, tv_loss: 0.017082277685403824\n",
      "iteration 3028, dc_loss: 0.016708755865693092, tv_loss: 0.017082247883081436\n",
      "iteration 3029, dc_loss: 0.016708729788661003, tv_loss: 0.017082249745726585\n",
      "iteration 3030, dc_loss: 0.016708731651306152, tv_loss: 0.01708230748772621\n",
      "iteration 3031, dc_loss: 0.0167087409645319, tv_loss: 0.01708212122321129\n",
      "iteration 3032, dc_loss: 0.016708780080080032, tv_loss: 0.017081940546631813\n",
      "iteration 3033, dc_loss: 0.016708824783563614, tv_loss: 0.017081908881664276\n",
      "iteration 3034, dc_loss: 0.016708776354789734, tv_loss: 0.017081867903470993\n",
      "iteration 3035, dc_loss: 0.016708729788661003, tv_loss: 0.01708178035914898\n",
      "iteration 3036, dc_loss: 0.016708703711628914, tv_loss: 0.017081720754504204\n",
      "iteration 3037, dc_loss: 0.01670868508517742, tv_loss: 0.017081670463085175\n",
      "iteration 3038, dc_loss: 0.016708634793758392, tv_loss: 0.01708156429231167\n",
      "iteration 3039, dc_loss: 0.016708604991436005, tv_loss: 0.0170814860612154\n",
      "iteration 3040, dc_loss: 0.016708580777049065, tv_loss: 0.017081422731280327\n",
      "iteration 3041, dc_loss: 0.016708586364984512, tv_loss: 0.01708134450018406\n",
      "iteration 3042, dc_loss: 0.016708610579371452, tv_loss: 0.017081156373023987\n",
      "iteration 3043, dc_loss: 0.016708629205822945, tv_loss: 0.01708102412521839\n",
      "iteration 3044, dc_loss: 0.0167086124420166, tv_loss: 0.01708095706999302\n",
      "iteration 3045, dc_loss: 0.016708575189113617, tv_loss: 0.01708095520734787\n",
      "iteration 3046, dc_loss: 0.016708509996533394, tv_loss: 0.017080793157219887\n",
      "iteration 3047, dc_loss: 0.016708457842469215, tv_loss: 0.01708075776696205\n",
      "iteration 3048, dc_loss: 0.016708428040146828, tv_loss: 0.01708068884909153\n",
      "iteration 3049, dc_loss: 0.0167083740234375, tv_loss: 0.017080621793866158\n",
      "iteration 3050, dc_loss: 0.0167083702981472, tv_loss: 0.0170805174857378\n",
      "iteration 3051, dc_loss: 0.016708388924598694, tv_loss: 0.017080415040254593\n",
      "iteration 3052, dc_loss: 0.01670835353434086, tv_loss: 0.017080217599868774\n",
      "iteration 3053, dc_loss: 0.016708310693502426, tv_loss: 0.017080336809158325\n",
      "iteration 3054, dc_loss: 0.01670827344059944, tv_loss: 0.017080459743738174\n",
      "iteration 3055, dc_loss: 0.016708286479115486, tv_loss: 0.017080163583159447\n",
      "iteration 3056, dc_loss: 0.016708308830857277, tv_loss: 0.017080003395676613\n",
      "iteration 3057, dc_loss: 0.01670828089118004, tv_loss: 0.017080027610063553\n",
      "iteration 3058, dc_loss: 0.016708210110664368, tv_loss: 0.017080096527934074\n",
      "iteration 3059, dc_loss: 0.016708143055438995, tv_loss: 0.017079945653676987\n",
      "iteration 3060, dc_loss: 0.016708049923181534, tv_loss: 0.017079953104257584\n",
      "iteration 3061, dc_loss: 0.016708027571439743, tv_loss: 0.017079923301935196\n",
      "iteration 3062, dc_loss: 0.016708027571439743, tv_loss: 0.017079723998904228\n",
      "iteration 3063, dc_loss: 0.016708051785826683, tv_loss: 0.017079578712582588\n",
      "iteration 3064, dc_loss: 0.016708090901374817, tv_loss: 0.017079660668969154\n",
      "iteration 3065, dc_loss: 0.016708068549633026, tv_loss: 0.017079632729291916\n",
      "iteration 3066, dc_loss: 0.016708042472600937, tv_loss: 0.017079288139939308\n",
      "iteration 3067, dc_loss: 0.016708029434084892, tv_loss: 0.017079254612326622\n",
      "iteration 3068, dc_loss: 0.01670801267027855, tv_loss: 0.01707933098077774\n",
      "iteration 3069, dc_loss: 0.01670799031853676, tv_loss: 0.017079263925552368\n",
      "iteration 3070, dc_loss: 0.016707953065633774, tv_loss: 0.017079133540391922\n",
      "iteration 3071, dc_loss: 0.0167078897356987, tv_loss: 0.017079133540391922\n",
      "iteration 3072, dc_loss: 0.01670781336724758, tv_loss: 0.01707899011671543\n",
      "iteration 3073, dc_loss: 0.01670772023499012, tv_loss: 0.017079215496778488\n",
      "iteration 3074, dc_loss: 0.016707660630345345, tv_loss: 0.01707904413342476\n",
      "iteration 3075, dc_loss: 0.016707655042409897, tv_loss: 0.017078781500458717\n",
      "iteration 3076, dc_loss: 0.016707688570022583, tv_loss: 0.01707865111529827\n",
      "iteration 3077, dc_loss: 0.016707738861441612, tv_loss: 0.017078489065170288\n",
      "iteration 3078, dc_loss: 0.016707703471183777, tv_loss: 0.01707863062620163\n",
      "iteration 3079, dc_loss: 0.016707653179764748, tv_loss: 0.017078464850783348\n",
      "iteration 3080, dc_loss: 0.01670762337744236, tv_loss: 0.017078356817364693\n",
      "iteration 3081, dc_loss: 0.016707638278603554, tv_loss: 0.0170784629881382\n",
      "iteration 3082, dc_loss: 0.016707677394151688, tv_loss: 0.017078397795557976\n",
      "iteration 3083, dc_loss: 0.016707709059119225, tv_loss: 0.0170780960470438\n",
      "iteration 3084, dc_loss: 0.01670769602060318, tv_loss: 0.017078014090657234\n",
      "iteration 3085, dc_loss: 0.016707642003893852, tv_loss: 0.017078129574656487\n",
      "iteration 3086, dc_loss: 0.016707541421055794, tv_loss: 0.017078198492527008\n",
      "iteration 3087, dc_loss: 0.016707472503185272, tv_loss: 0.017078058794140816\n",
      "iteration 3088, dc_loss: 0.01670740731060505, tv_loss: 0.017078006640076637\n",
      "iteration 3089, dc_loss: 0.016707424074411392, tv_loss: 0.017078105360269547\n",
      "iteration 3090, dc_loss: 0.016707442700862885, tv_loss: 0.01707797311246395\n",
      "iteration 3091, dc_loss: 0.016707440838217735, tv_loss: 0.017077673226594925\n",
      "iteration 3092, dc_loss: 0.01670745201408863, tv_loss: 0.01707754284143448\n",
      "iteration 3093, dc_loss: 0.016707487404346466, tv_loss: 0.01707751490175724\n",
      "iteration 3094, dc_loss: 0.016707489266991615, tv_loss: 0.017077378928661346\n",
      "iteration 3095, dc_loss: 0.01670745387673378, tv_loss: 0.017077479511499405\n",
      "iteration 3096, dc_loss: 0.016707373782992363, tv_loss: 0.017077447846531868\n",
      "iteration 3097, dc_loss: 0.016707327216863632, tv_loss: 0.01707734353840351\n",
      "iteration 3098, dc_loss: 0.016707303002476692, tv_loss: 0.017077600583434105\n",
      "iteration 3099, dc_loss: 0.016707269474864006, tv_loss: 0.017077339813113213\n",
      "iteration 3100, dc_loss: 0.016707269474864006, tv_loss: 0.01707720384001732\n",
      "iteration 3101, dc_loss: 0.01670726388692856, tv_loss: 0.017077190801501274\n",
      "iteration 3102, dc_loss: 0.01670725829899311, tv_loss: 0.017077110707759857\n",
      "iteration 3103, dc_loss: 0.01670723967254162, tv_loss: 0.017077025026082993\n",
      "iteration 3104, dc_loss: 0.01670721173286438, tv_loss: 0.01707705482840538\n",
      "iteration 3105, dc_loss: 0.016707174479961395, tv_loss: 0.017077019438147545\n",
      "iteration 3106, dc_loss: 0.0167071595788002, tv_loss: 0.01707681640982628\n",
      "iteration 3107, dc_loss: 0.01670714095234871, tv_loss: 0.017076849937438965\n",
      "iteration 3108, dc_loss: 0.01670711115002632, tv_loss: 0.017076866701245308\n",
      "iteration 3109, dc_loss: 0.01670707017183304, tv_loss: 0.017076751217246056\n",
      "iteration 3110, dc_loss: 0.01670706272125244, tv_loss: 0.017076635733246803\n",
      "iteration 3111, dc_loss: 0.016707034781575203, tv_loss: 0.017076579853892326\n",
      "iteration 3112, dc_loss: 0.016706984490156174, tv_loss: 0.0170765183866024\n",
      "iteration 3113, dc_loss: 0.016706950962543488, tv_loss: 0.01707659661769867\n",
      "iteration 3114, dc_loss: 0.01670694723725319, tv_loss: 0.017076678574085236\n",
      "iteration 3115, dc_loss: 0.01670694909989834, tv_loss: 0.017076490446925163\n",
      "iteration 3116, dc_loss: 0.016706952825188637, tv_loss: 0.017076294869184494\n",
      "iteration 3117, dc_loss: 0.016706975176930428, tv_loss: 0.017076056450605392\n",
      "iteration 3118, dc_loss: 0.01670699194073677, tv_loss: 0.017076224088668823\n",
      "iteration 3119, dc_loss: 0.016706980764865875, tv_loss: 0.01707613281905651\n",
      "iteration 3120, dc_loss: 0.0167069174349308, tv_loss: 0.017075933516025543\n",
      "iteration 3121, dc_loss: 0.016706882044672966, tv_loss: 0.017075907438993454\n",
      "iteration 3122, dc_loss: 0.016706839203834534, tv_loss: 0.017075983807444572\n",
      "iteration 3123, dc_loss: 0.016706766560673714, tv_loss: 0.017075875774025917\n",
      "iteration 3124, dc_loss: 0.01670670695602894, tv_loss: 0.017075764015316963\n",
      "iteration 3125, dc_loss: 0.016706697642803192, tv_loss: 0.017075665295124054\n",
      "iteration 3126, dc_loss: 0.01670668087899685, tv_loss: 0.017075635492801666\n",
      "iteration 3127, dc_loss: 0.016706686466932297, tv_loss: 0.01707562804222107\n",
      "iteration 3128, dc_loss: 0.016706664115190506, tv_loss: 0.01707545854151249\n",
      "iteration 3129, dc_loss: 0.016706600785255432, tv_loss: 0.01707538217306137\n",
      "iteration 3130, dc_loss: 0.016706557944417, tv_loss: 0.017075425013899803\n",
      "iteration 3131, dc_loss: 0.016706492751836777, tv_loss: 0.01707548275589943\n",
      "iteration 3132, dc_loss: 0.01670648157596588, tv_loss: 0.017075393348932266\n",
      "iteration 3133, dc_loss: 0.016706503927707672, tv_loss: 0.017075372859835625\n",
      "iteration 3134, dc_loss: 0.016706591472029686, tv_loss: 0.017075026407837868\n",
      "iteration 3135, dc_loss: 0.016706664115190506, tv_loss: 0.017074914649128914\n",
      "iteration 3136, dc_loss: 0.01670670323073864, tv_loss: 0.017074866220355034\n",
      "iteration 3137, dc_loss: 0.0167066790163517, tv_loss: 0.017074652016162872\n",
      "iteration 3138, dc_loss: 0.016706621274352074, tv_loss: 0.017074644565582275\n",
      "iteration 3139, dc_loss: 0.01670653186738491, tv_loss: 0.01707477681338787\n",
      "iteration 3140, dc_loss: 0.01670645736157894, tv_loss: 0.01707480289041996\n",
      "iteration 3141, dc_loss: 0.016706433147192, tv_loss: 0.017074694857001305\n",
      "iteration 3142, dc_loss: 0.01670641265809536, tv_loss: 0.01707458682358265\n",
      "iteration 3143, dc_loss: 0.016706397756934166, tv_loss: 0.017074495553970337\n",
      "iteration 3144, dc_loss: 0.016706427559256554, tv_loss: 0.01707470417022705\n",
      "iteration 3145, dc_loss: 0.016706425696611404, tv_loss: 0.017074523493647575\n",
      "iteration 3146, dc_loss: 0.016706394031643867, tv_loss: 0.017074234783649445\n",
      "iteration 3147, dc_loss: 0.016706354916095734, tv_loss: 0.017074227333068848\n",
      "iteration 3148, dc_loss: 0.016706334426999092, tv_loss: 0.017074309289455414\n",
      "iteration 3149, dc_loss: 0.0167063120752573, tv_loss: 0.017074154689908028\n",
      "iteration 3150, dc_loss: 0.01670626550912857, tv_loss: 0.017074165865778923\n",
      "iteration 3151, dc_loss: 0.016706209629774094, tv_loss: 0.017074162140488625\n",
      "iteration 3152, dc_loss: 0.016706155613064766, tv_loss: 0.017074162140488625\n",
      "iteration 3153, dc_loss: 0.01670614257454872, tv_loss: 0.01707390695810318\n",
      "iteration 3154, dc_loss: 0.016706133261322975, tv_loss: 0.01707412488758564\n",
      "iteration 3155, dc_loss: 0.01670612022280693, tv_loss: 0.017074059695005417\n",
      "iteration 3156, dc_loss: 0.016706101596355438, tv_loss: 0.0170737411826849\n",
      "iteration 3157, dc_loss: 0.016706079244613647, tv_loss: 0.017073601484298706\n",
      "iteration 3158, dc_loss: 0.016706055030226707, tv_loss: 0.017073744907975197\n",
      "iteration 3159, dc_loss: 0.016706053167581558, tv_loss: 0.017073659226298332\n",
      "iteration 3160, dc_loss: 0.016706060618162155, tv_loss: 0.017073502764105797\n",
      "iteration 3161, dc_loss: 0.016706036403775215, tv_loss: 0.017073387280106544\n",
      "iteration 3162, dc_loss: 0.016706014052033424, tv_loss: 0.01707330532371998\n",
      "iteration 3163, dc_loss: 0.016706004738807678, tv_loss: 0.01707342639565468\n",
      "iteration 3164, dc_loss: 0.016705967485904694, tv_loss: 0.017073314636945724\n",
      "iteration 3165, dc_loss: 0.016705945134162903, tv_loss: 0.0170731283724308\n",
      "iteration 3166, dc_loss: 0.01670595072209835, tv_loss: 0.01707300916314125\n",
      "iteration 3167, dc_loss: 0.016706012189388275, tv_loss: 0.01707295887172222\n",
      "iteration 3168, dc_loss: 0.016706055030226707, tv_loss: 0.0170728899538517\n",
      "iteration 3169, dc_loss: 0.016706066206097603, tv_loss: 0.017072560265660286\n",
      "iteration 3170, dc_loss: 0.016706082969903946, tv_loss: 0.017072442919015884\n",
      "iteration 3171, dc_loss: 0.01670605130493641, tv_loss: 0.01707271859049797\n",
      "iteration 3172, dc_loss: 0.016705945134162903, tv_loss: 0.01707250066101551\n",
      "iteration 3173, dc_loss: 0.016705818474292755, tv_loss: 0.01707257144153118\n",
      "iteration 3174, dc_loss: 0.016705695539712906, tv_loss: 0.017072616145014763\n",
      "iteration 3175, dc_loss: 0.016705598682165146, tv_loss: 0.017072658985853195\n",
      "iteration 3176, dc_loss: 0.016705559566617012, tv_loss: 0.017072705551981926\n",
      "iteration 3177, dc_loss: 0.016705583781003952, tv_loss: 0.017072655260562897\n",
      "iteration 3178, dc_loss: 0.016705647110939026, tv_loss: 0.01707233302295208\n",
      "iteration 3179, dc_loss: 0.01670571230351925, tv_loss: 0.017072230577468872\n",
      "iteration 3180, dc_loss: 0.01670578308403492, tv_loss: 0.0170721597969532\n",
      "iteration 3181, dc_loss: 0.01670578494668007, tv_loss: 0.017071973532438278\n",
      "iteration 3182, dc_loss: 0.01670573279261589, tv_loss: 0.017071954905986786\n",
      "iteration 3183, dc_loss: 0.016705650836229324, tv_loss: 0.017072059214115143\n",
      "iteration 3184, dc_loss: 0.016705557703971863, tv_loss: 0.017072059214115143\n",
      "iteration 3185, dc_loss: 0.016705505549907684, tv_loss: 0.017071962356567383\n",
      "iteration 3186, dc_loss: 0.016705498099327087, tv_loss: 0.017071828246116638\n",
      "iteration 3187, dc_loss: 0.01670551858842373, tv_loss: 0.017071766778826714\n",
      "iteration 3188, dc_loss: 0.016705553978681564, tv_loss: 0.017071599140763283\n",
      "iteration 3189, dc_loss: 0.016705594956874847, tv_loss: 0.017071625217795372\n",
      "iteration 3190, dc_loss: 0.016705622896552086, tv_loss: 0.017071381211280823\n",
      "iteration 3191, dc_loss: 0.016705604270100594, tv_loss: 0.01707138493657112\n",
      "iteration 3192, dc_loss: 0.016705552116036415, tv_loss: 0.017071537673473358\n",
      "iteration 3193, dc_loss: 0.01670548878610134, tv_loss: 0.017071284353733063\n",
      "iteration 3194, dc_loss: 0.016705438494682312, tv_loss: 0.017071200534701347\n",
      "iteration 3195, dc_loss: 0.016705425456166267, tv_loss: 0.01707136444747448\n",
      "iteration 3196, dc_loss: 0.016705399379134178, tv_loss: 0.017071226611733437\n",
      "iteration 3197, dc_loss: 0.016705375164747238, tv_loss: 0.017071198672056198\n",
      "iteration 3198, dc_loss: 0.016705326735973358, tv_loss: 0.0170710738748312\n",
      "iteration 3199, dc_loss: 0.01670529507100582, tv_loss: 0.017070947214961052\n",
      "iteration 3200, dc_loss: 0.016705261543393135, tv_loss: 0.017071112990379333\n",
      "iteration 3201, dc_loss: 0.01670520380139351, tv_loss: 0.017071090638637543\n",
      "iteration 3202, dc_loss: 0.01670515164732933, tv_loss: 0.01707085408270359\n",
      "iteration 3203, dc_loss: 0.016705162823200226, tv_loss: 0.01707085780799389\n",
      "iteration 3204, dc_loss: 0.016705216839909554, tv_loss: 0.017070893198251724\n",
      "iteration 3205, dc_loss: 0.016705244779586792, tv_loss: 0.017070749774575233\n",
      "iteration 3206, dc_loss: 0.016705268993973732, tv_loss: 0.01707051880657673\n",
      "iteration 3207, dc_loss: 0.016705283895134926, tv_loss: 0.017070535570383072\n",
      "iteration 3208, dc_loss: 0.016705268993973732, tv_loss: 0.017070570960640907\n",
      "iteration 3209, dc_loss: 0.01670522801578045, tv_loss: 0.017070384696125984\n",
      "iteration 3210, dc_loss: 0.016705133020877838, tv_loss: 0.017070289701223373\n",
      "iteration 3211, dc_loss: 0.016705049201846123, tv_loss: 0.017070449888706207\n",
      "iteration 3212, dc_loss: 0.016704954206943512, tv_loss: 0.017070403322577477\n",
      "iteration 3213, dc_loss: 0.01670488528907299, tv_loss: 0.01707044243812561\n",
      "iteration 3214, dc_loss: 0.016704896464943886, tv_loss: 0.01707044057548046\n",
      "iteration 3215, dc_loss: 0.01670493371784687, tv_loss: 0.0170701052993536\n",
      "iteration 3216, dc_loss: 0.016704963520169258, tv_loss: 0.017070084810256958\n",
      "iteration 3217, dc_loss: 0.016704997047781944, tv_loss: 0.017070068046450615\n",
      "iteration 3218, dc_loss: 0.016705002635717392, tv_loss: 0.017069850116968155\n",
      "iteration 3219, dc_loss: 0.01670500449836254, tv_loss: 0.01706966571509838\n",
      "iteration 3220, dc_loss: 0.016704987734556198, tv_loss: 0.017069734632968903\n",
      "iteration 3221, dc_loss: 0.01670496165752411, tv_loss: 0.017069777473807335\n",
      "iteration 3222, dc_loss: 0.016704916954040527, tv_loss: 0.017069555819034576\n",
      "iteration 3223, dc_loss: 0.016704849898815155, tv_loss: 0.017069516703486443\n",
      "iteration 3224, dc_loss: 0.016704769805073738, tv_loss: 0.017069658264517784\n",
      "iteration 3225, dc_loss: 0.016704706475138664, tv_loss: 0.017069663852453232\n",
      "iteration 3226, dc_loss: 0.016704659909009933, tv_loss: 0.01706957444548607\n",
      "iteration 3227, dc_loss: 0.016704626381397247, tv_loss: 0.017069634050130844\n",
      "iteration 3228, dc_loss: 0.016704663634300232, tv_loss: 0.017069531604647636\n",
      "iteration 3229, dc_loss: 0.016704721376299858, tv_loss: 0.01706932671368122\n",
      "iteration 3230, dc_loss: 0.01670481450855732, tv_loss: 0.017069149762392044\n",
      "iteration 3231, dc_loss: 0.016704868525266647, tv_loss: 0.017069149762392044\n",
      "iteration 3232, dc_loss: 0.016704928129911423, tv_loss: 0.017069032415747643\n",
      "iteration 3233, dc_loss: 0.01670490764081478, tv_loss: 0.01706884056329727\n",
      "iteration 3234, dc_loss: 0.016704797744750977, tv_loss: 0.017068952322006226\n",
      "iteration 3235, dc_loss: 0.016704685986042023, tv_loss: 0.017068959772586823\n",
      "iteration 3236, dc_loss: 0.016704615205526352, tv_loss: 0.01706889271736145\n",
      "iteration 3237, dc_loss: 0.016704538837075233, tv_loss: 0.017068885266780853\n",
      "iteration 3238, dc_loss: 0.016704505309462547, tv_loss: 0.017068900167942047\n",
      "iteration 3239, dc_loss: 0.016704468056559563, tv_loss: 0.017068935558199883\n",
      "iteration 3240, dc_loss: 0.016704445704817772, tv_loss: 0.01706894300878048\n",
      "iteration 3241, dc_loss: 0.016704434528946877, tv_loss: 0.017068829387426376\n",
      "iteration 3242, dc_loss: 0.016704438254237175, tv_loss: 0.017068756744265556\n",
      "iteration 3243, dc_loss: 0.016704430803656578, tv_loss: 0.017068680375814438\n",
      "iteration 3244, dc_loss: 0.01670444943010807, tv_loss: 0.017068594694137573\n",
      "iteration 3245, dc_loss: 0.016704490408301353, tv_loss: 0.01706862263381481\n",
      "iteration 3246, dc_loss: 0.016704529523849487, tv_loss: 0.017068395391106606\n",
      "iteration 3247, dc_loss: 0.016704531386494637, tv_loss: 0.0170682892203331\n",
      "iteration 3248, dc_loss: 0.016704505309462547, tv_loss: 0.01706836372613907\n",
      "iteration 3249, dc_loss: 0.016704466193914413, tv_loss: 0.01706829108297825\n",
      "iteration 3250, dc_loss: 0.01670444943010807, tv_loss: 0.017068088054656982\n",
      "iteration 3251, dc_loss: 0.016704412177205086, tv_loss: 0.01706802286207676\n",
      "iteration 3252, dc_loss: 0.016704382374882698, tv_loss: 0.017068084329366684\n",
      "iteration 3253, dc_loss: 0.016704337671399117, tv_loss: 0.017067916691303253\n",
      "iteration 3254, dc_loss: 0.016704289242625237, tv_loss: 0.01706785149872303\n",
      "iteration 3255, dc_loss: 0.01670425944030285, tv_loss: 0.01706778071820736\n",
      "iteration 3256, dc_loss: 0.016704224050045013, tv_loss: 0.017067909240722656\n",
      "iteration 3257, dc_loss: 0.01670418679714203, tv_loss: 0.01706794649362564\n",
      "iteration 3258, dc_loss: 0.016704121604561806, tv_loss: 0.017067627981305122\n",
      "iteration 3259, dc_loss: 0.01670410856604576, tv_loss: 0.017067648470401764\n",
      "iteration 3260, dc_loss: 0.016704104840755463, tv_loss: 0.017067603766918182\n",
      "iteration 3261, dc_loss: 0.01670411229133606, tv_loss: 0.017067529261112213\n",
      "iteration 3262, dc_loss: 0.016704142093658447, tv_loss: 0.017067434266209602\n",
      "iteration 3263, dc_loss: 0.016704177483916283, tv_loss: 0.017067205160856247\n",
      "iteration 3264, dc_loss: 0.016704188659787178, tv_loss: 0.017067192122340202\n",
      "iteration 3265, dc_loss: 0.01670416258275509, tv_loss: 0.017067085951566696\n",
      "iteration 3266, dc_loss: 0.016704130917787552, tv_loss: 0.017066938802599907\n",
      "iteration 3267, dc_loss: 0.016704071313142776, tv_loss: 0.017067043110728264\n",
      "iteration 3268, dc_loss: 0.01670403592288494, tv_loss: 0.017067238688468933\n",
      "iteration 3269, dc_loss: 0.0167040154337883, tv_loss: 0.01706703193485737\n",
      "iteration 3270, dc_loss: 0.016703981906175613, tv_loss: 0.01706690900027752\n",
      "iteration 3271, dc_loss: 0.01670396514236927, tv_loss: 0.017067063599824905\n",
      "iteration 3272, dc_loss: 0.016703912988305092, tv_loss: 0.01706702820956707\n",
      "iteration 3273, dc_loss: 0.01670384220778942, tv_loss: 0.01706690900027752\n",
      "iteration 3274, dc_loss: 0.016703829169273376, tv_loss: 0.01706691086292267\n",
      "iteration 3275, dc_loss: 0.016703831031918526, tv_loss: 0.017067106440663338\n",
      "iteration 3276, dc_loss: 0.016703883185982704, tv_loss: 0.017066769301891327\n",
      "iteration 3277, dc_loss: 0.016703953966498375, tv_loss: 0.01706659235060215\n",
      "iteration 3278, dc_loss: 0.016703980043530464, tv_loss: 0.017066609114408493\n",
      "iteration 3279, dc_loss: 0.016703946515917778, tv_loss: 0.017066430300474167\n",
      "iteration 3280, dc_loss: 0.016703905537724495, tv_loss: 0.01706630364060402\n",
      "iteration 3281, dc_loss: 0.016703829169273376, tv_loss: 0.01706641912460327\n",
      "iteration 3282, dc_loss: 0.0167037695646286, tv_loss: 0.017066478729248047\n",
      "iteration 3283, dc_loss: 0.016703728586435318, tv_loss: 0.017066368833184242\n",
      "iteration 3284, dc_loss: 0.016703693196177483, tv_loss: 0.01706627383828163\n",
      "iteration 3285, dc_loss: 0.01670370250940323, tv_loss: 0.01706620119512081\n",
      "iteration 3286, dc_loss: 0.01670372672379017, tv_loss: 0.017066212370991707\n",
      "iteration 3287, dc_loss: 0.016703767701983452, tv_loss: 0.01706620119512081\n",
      "iteration 3288, dc_loss: 0.016703756526112556, tv_loss: 0.017065996304154396\n",
      "iteration 3289, dc_loss: 0.016703715547919273, tv_loss: 0.01706603914499283\n",
      "iteration 3290, dc_loss: 0.016703691333532333, tv_loss: 0.017065972089767456\n",
      "iteration 3291, dc_loss: 0.0167036522179842, tv_loss: 0.01706582121551037\n",
      "iteration 3292, dc_loss: 0.016703616827726364, tv_loss: 0.017065906897187233\n",
      "iteration 3293, dc_loss: 0.01670362800359726, tv_loss: 0.017065849155187607\n",
      "iteration 3294, dc_loss: 0.016703646630048752, tv_loss: 0.017065631225705147\n",
      "iteration 3295, dc_loss: 0.016703633591532707, tv_loss: 0.01706565171480179\n",
      "iteration 3296, dc_loss: 0.016703613102436066, tv_loss: 0.01706560328602791\n",
      "iteration 3297, dc_loss: 0.016703596338629723, tv_loss: 0.0170653834939003\n",
      "iteration 3298, dc_loss: 0.016703587025403976, tv_loss: 0.017065400257706642\n",
      "iteration 3299, dc_loss: 0.016703572124242783, tv_loss: 0.017065489664673805\n",
      "iteration 3300, dc_loss: 0.016703534871339798, tv_loss: 0.017065217718482018\n",
      "iteration 3301, dc_loss: 0.016703492030501366, tv_loss: 0.01706516742706299\n",
      "iteration 3302, dc_loss: 0.016703465953469276, tv_loss: 0.01706533506512642\n",
      "iteration 3303, dc_loss: 0.016703452914953232, tv_loss: 0.01706521213054657\n",
      "iteration 3304, dc_loss: 0.01670346036553383, tv_loss: 0.017065072432160378\n",
      "iteration 3305, dc_loss: 0.016703447327017784, tv_loss: 0.017064986750483513\n",
      "iteration 3306, dc_loss: 0.016703449189662933, tv_loss: 0.017064876854419708\n",
      "iteration 3307, dc_loss: 0.01670345850288868, tv_loss: 0.01706501841545105\n",
      "iteration 3308, dc_loss: 0.01670345664024353, tv_loss: 0.017064888030290604\n",
      "iteration 3309, dc_loss: 0.016703404486179352, tv_loss: 0.01706472970545292\n",
      "iteration 3310, dc_loss: 0.016703320667147636, tv_loss: 0.01706470362842083\n",
      "iteration 3311, dc_loss: 0.016703251749277115, tv_loss: 0.01706465147435665\n",
      "iteration 3312, dc_loss: 0.016703233122825623, tv_loss: 0.017064642161130905\n",
      "iteration 3313, dc_loss: 0.016703210771083832, tv_loss: 0.0170647781342268\n",
      "iteration 3314, dc_loss: 0.01670321449637413, tv_loss: 0.017064647749066353\n",
      "iteration 3315, dc_loss: 0.01670326478779316, tv_loss: 0.017064377665519714\n",
      "iteration 3316, dc_loss: 0.016703316941857338, tv_loss: 0.01706414483487606\n",
      "iteration 3317, dc_loss: 0.01670336164534092, tv_loss: 0.017064331099390984\n",
      "iteration 3318, dc_loss: 0.01670333929359913, tv_loss: 0.01706419512629509\n",
      "iteration 3319, dc_loss: 0.016703328117728233, tv_loss: 0.017064012587070465\n",
      "iteration 3320, dc_loss: 0.016703302040696144, tv_loss: 0.017064141109585762\n",
      "iteration 3321, dc_loss: 0.016703221946954727, tv_loss: 0.017064165323972702\n",
      "iteration 3322, dc_loss: 0.01670311763882637, tv_loss: 0.01706397533416748\n",
      "iteration 3323, dc_loss: 0.01670304872095585, tv_loss: 0.017064044252038002\n",
      "iteration 3324, dc_loss: 0.016703041270375252, tv_loss: 0.017064008861780167\n",
      "iteration 3325, dc_loss: 0.016703031957149506, tv_loss: 0.01706397347152233\n",
      "iteration 3326, dc_loss: 0.01670306921005249, tv_loss: 0.017063943669199944\n",
      "iteration 3327, dc_loss: 0.016703087836503983, tv_loss: 0.017063762992620468\n",
      "iteration 3328, dc_loss: 0.016703112050890923, tv_loss: 0.017063790932297707\n",
      "iteration 3329, dc_loss: 0.016703126952052116, tv_loss: 0.01706371083855629\n",
      "iteration 3330, dc_loss: 0.016703100875020027, tv_loss: 0.017063476145267487\n",
      "iteration 3331, dc_loss: 0.016703061759471893, tv_loss: 0.017063433304429054\n",
      "iteration 3332, dc_loss: 0.016703015193343163, tv_loss: 0.01706351712346077\n",
      "iteration 3333, dc_loss: 0.01670297607779503, tv_loss: 0.017063414677977562\n",
      "iteration 3334, dc_loss: 0.016702942550182343, tv_loss: 0.017063379287719727\n",
      "iteration 3335, dc_loss: 0.01670289784669876, tv_loss: 0.01706336997449398\n",
      "iteration 3336, dc_loss: 0.016702871769666672, tv_loss: 0.01706341840326786\n",
      "iteration 3337, dc_loss: 0.01670285500586033, tv_loss: 0.017063280567526817\n",
      "iteration 3338, dc_loss: 0.016702858731150627, tv_loss: 0.017063364386558533\n",
      "iteration 3339, dc_loss: 0.016702858731150627, tv_loss: 0.017063280567526817\n",
      "iteration 3340, dc_loss: 0.016702843829989433, tv_loss: 0.017063094303011894\n",
      "iteration 3341, dc_loss: 0.01670282520353794, tv_loss: 0.017063096165657043\n",
      "iteration 3342, dc_loss: 0.016702815890312195, tv_loss: 0.017063239589333534\n",
      "iteration 3343, dc_loss: 0.01670283079147339, tv_loss: 0.01706305891275406\n",
      "iteration 3344, dc_loss: 0.01670290157198906, tv_loss: 0.017062826082110405\n",
      "iteration 3345, dc_loss: 0.016702931374311447, tv_loss: 0.017062772065401077\n",
      "iteration 3346, dc_loss: 0.016702916473150253, tv_loss: 0.017062846571207047\n",
      "iteration 3347, dc_loss: 0.016702858731150627, tv_loss: 0.017062628641724586\n",
      "iteration 3348, dc_loss: 0.01670275814831257, tv_loss: 0.017062775790691376\n",
      "iteration 3349, dc_loss: 0.01670267805457115, tv_loss: 0.017062926664948463\n",
      "iteration 3350, dc_loss: 0.016702640801668167, tv_loss: 0.017062870785593987\n",
      "iteration 3351, dc_loss: 0.01670262962579727, tv_loss: 0.017062798142433167\n",
      "iteration 3352, dc_loss: 0.016702646389603615, tv_loss: 0.01706264168024063\n",
      "iteration 3353, dc_loss: 0.016702737659215927, tv_loss: 0.017062515020370483\n",
      "iteration 3354, dc_loss: 0.016702819615602493, tv_loss: 0.017062503844499588\n",
      "iteration 3355, dc_loss: 0.016702841967344284, tv_loss: 0.017062369734048843\n",
      "iteration 3356, dc_loss: 0.016702815890312195, tv_loss: 0.017062319442629814\n",
      "iteration 3357, dc_loss: 0.016702765598893166, tv_loss: 0.017062148079276085\n",
      "iteration 3358, dc_loss: 0.01670270785689354, tv_loss: 0.017062263563275337\n",
      "iteration 3359, dc_loss: 0.016702625900506973, tv_loss: 0.017062393948435783\n",
      "iteration 3360, dc_loss: 0.016702501103281975, tv_loss: 0.01706245355308056\n",
      "iteration 3361, dc_loss: 0.01670241728425026, tv_loss: 0.01706230640411377\n",
      "iteration 3362, dc_loss: 0.01670239306986332, tv_loss: 0.01706230267882347\n",
      "iteration 3363, dc_loss: 0.016702424734830856, tv_loss: 0.017062341794371605\n",
      "iteration 3364, dc_loss: 0.016702454537153244, tv_loss: 0.017062069848179817\n",
      "iteration 3365, dc_loss: 0.016702495515346527, tv_loss: 0.017061937600374222\n",
      "iteration 3366, dc_loss: 0.01670250855386257, tv_loss: 0.017061946913599968\n",
      "iteration 3367, dc_loss: 0.016702476888895035, tv_loss: 0.017061645165085793\n",
      "iteration 3368, dc_loss: 0.016702448949217796, tv_loss: 0.017061658203601837\n",
      "iteration 3369, dc_loss: 0.016702456399798393, tv_loss: 0.017061816528439522\n",
      "iteration 3370, dc_loss: 0.016702454537153244, tv_loss: 0.017061535269021988\n",
      "iteration 3371, dc_loss: 0.01670246757566929, tv_loss: 0.017061416059732437\n",
      "iteration 3372, dc_loss: 0.01670246385037899, tv_loss: 0.017061468213796616\n",
      "iteration 3373, dc_loss: 0.016702428460121155, tv_loss: 0.0170613881200552\n",
      "iteration 3374, dc_loss: 0.01670237071812153, tv_loss: 0.017061395570635796\n",
      "iteration 3375, dc_loss: 0.016702307388186455, tv_loss: 0.017061421647667885\n",
      "iteration 3376, dc_loss: 0.016702264547348022, tv_loss: 0.017061421647667885\n",
      "iteration 3377, dc_loss: 0.01670224778354168, tv_loss: 0.017061300575733185\n",
      "iteration 3378, dc_loss: 0.01670227199792862, tv_loss: 0.01706131361424923\n",
      "iteration 3379, dc_loss: 0.01670227199792862, tv_loss: 0.01706133596599102\n",
      "iteration 3380, dc_loss: 0.01670227199792862, tv_loss: 0.01706118881702423\n",
      "iteration 3381, dc_loss: 0.016702281311154366, tv_loss: 0.01706109382212162\n",
      "iteration 3382, dc_loss: 0.016702281311154366, tv_loss: 0.017061198130249977\n",
      "iteration 3383, dc_loss: 0.016702214255928993, tv_loss: 0.017061233520507812\n",
      "iteration 3384, dc_loss: 0.01670215092599392, tv_loss: 0.01706114411354065\n",
      "iteration 3385, dc_loss: 0.01670209690928459, tv_loss: 0.017061088234186172\n",
      "iteration 3386, dc_loss: 0.016702119261026382, tv_loss: 0.01706121489405632\n",
      "iteration 3387, dc_loss: 0.016702134162187576, tv_loss: 0.01706097647547722\n",
      "iteration 3388, dc_loss: 0.01670212671160698, tv_loss: 0.017060957849025726\n",
      "iteration 3389, dc_loss: 0.016702115535736084, tv_loss: 0.01706097461283207\n",
      "iteration 3390, dc_loss: 0.01670207642018795, tv_loss: 0.01706085167825222\n",
      "iteration 3391, dc_loss: 0.016702085733413696, tv_loss: 0.017060687765479088\n",
      "iteration 3392, dc_loss: 0.016702061519026756, tv_loss: 0.01706056110560894\n",
      "iteration 3393, dc_loss: 0.01670202612876892, tv_loss: 0.017060576006770134\n",
      "iteration 3394, dc_loss: 0.016701987013220787, tv_loss: 0.017060620710253716\n",
      "iteration 3395, dc_loss: 0.016701990738511086, tv_loss: 0.017060505226254463\n",
      "iteration 3396, dc_loss: 0.016702013090252876, tv_loss: 0.017060456797480583\n",
      "iteration 3397, dc_loss: 0.016701998189091682, tv_loss: 0.017060432583093643\n",
      "iteration 3398, dc_loss: 0.016702020540833473, tv_loss: 0.017060376703739166\n",
      "iteration 3399, dc_loss: 0.016702035441994667, tv_loss: 0.017060130834579468\n",
      "iteration 3400, dc_loss: 0.016702087596058846, tv_loss: 0.017060095444321632\n",
      "iteration 3401, dc_loss: 0.016702083870768547, tv_loss: 0.017059816047549248\n",
      "iteration 3402, dc_loss: 0.016702016815543175, tv_loss: 0.01705998368561268\n",
      "iteration 3403, dc_loss: 0.01670192927122116, tv_loss: 0.017060108482837677\n",
      "iteration 3404, dc_loss: 0.01670185476541519, tv_loss: 0.017059966921806335\n",
      "iteration 3405, dc_loss: 0.01670178398489952, tv_loss: 0.01706000231206417\n",
      "iteration 3406, dc_loss: 0.016701750457286835, tv_loss: 0.017060091719031334\n",
      "iteration 3407, dc_loss: 0.016701744869351387, tv_loss: 0.01705997996032238\n",
      "iteration 3408, dc_loss: 0.016701767221093178, tv_loss: 0.017059829086065292\n",
      "iteration 3409, dc_loss: 0.016701802611351013, tv_loss: 0.017059827223420143\n",
      "iteration 3410, dc_loss: 0.016701778396964073, tv_loss: 0.01705987937748432\n",
      "iteration 3411, dc_loss: 0.016701744869351387, tv_loss: 0.017059823498129845\n",
      "iteration 3412, dc_loss: 0.016701752319931984, tv_loss: 0.01705963909626007\n",
      "iteration 3413, dc_loss: 0.016701774671673775, tv_loss: 0.017059512436389923\n",
      "iteration 3414, dc_loss: 0.016701748594641685, tv_loss: 0.017059575766324997\n",
      "iteration 3415, dc_loss: 0.01670166105031967, tv_loss: 0.017059577628970146\n",
      "iteration 3416, dc_loss: 0.01670162007212639, tv_loss: 0.01705966889858246\n",
      "iteration 3417, dc_loss: 0.016701590269804, tv_loss: 0.01705959066748619\n",
      "iteration 3418, dc_loss: 0.01670159213244915, tv_loss: 0.0170594472438097\n",
      "iteration 3419, dc_loss: 0.01670161262154579, tv_loss: 0.017059430480003357\n",
      "iteration 3420, dc_loss: 0.01670162007212639, tv_loss: 0.01705946959555149\n",
      "iteration 3421, dc_loss: 0.016701577231287956, tv_loss: 0.017059409990906715\n",
      "iteration 3422, dc_loss: 0.01670149527490139, tv_loss: 0.017059389501810074\n",
      "iteration 3423, dc_loss: 0.016701465472579002, tv_loss: 0.017059326171875\n",
      "iteration 3424, dc_loss: 0.016701482236385345, tv_loss: 0.017059193924069405\n",
      "iteration 3425, dc_loss: 0.016701530665159225, tv_loss: 0.017059171572327614\n",
      "iteration 3426, dc_loss: 0.01670153997838497, tv_loss: 0.017059193924069405\n",
      "iteration 3427, dc_loss: 0.016701526939868927, tv_loss: 0.01705898903310299\n",
      "iteration 3428, dc_loss: 0.016701508313417435, tv_loss: 0.017058994621038437\n",
      "iteration 3429, dc_loss: 0.01670149713754654, tv_loss: 0.017059041187167168\n",
      "iteration 3430, dc_loss: 0.016701476648449898, tv_loss: 0.01705905795097351\n",
      "iteration 3431, dc_loss: 0.01670141890645027, tv_loss: 0.017059005796909332\n",
      "iteration 3432, dc_loss: 0.01670142449438572, tv_loss: 0.017058804631233215\n",
      "iteration 3433, dc_loss: 0.016701437532901764, tv_loss: 0.01705871894955635\n",
      "iteration 3434, dc_loss: 0.01670146733522415, tv_loss: 0.01705878973007202\n",
      "iteration 3435, dc_loss: 0.016701435670256615, tv_loss: 0.0170588381588459\n",
      "iteration 3436, dc_loss: 0.016701361164450645, tv_loss: 0.017058702185750008\n",
      "iteration 3437, dc_loss: 0.016701320186257362, tv_loss: 0.017058594152331352\n",
      "iteration 3438, dc_loss: 0.016701312735676765, tv_loss: 0.01705870032310486\n",
      "iteration 3439, dc_loss: 0.01670128107070923, tv_loss: 0.017058581113815308\n",
      "iteration 3440, dc_loss: 0.016701260581612587, tv_loss: 0.01705845445394516\n",
      "iteration 3441, dc_loss: 0.016701266169548035, tv_loss: 0.017058467492461205\n",
      "iteration 3442, dc_loss: 0.01670127920806408, tv_loss: 0.017058318480849266\n",
      "iteration 3443, dc_loss: 0.01670125313103199, tv_loss: 0.01705826260149479\n",
      "iteration 3444, dc_loss: 0.0167012307792902, tv_loss: 0.017058227211236954\n",
      "iteration 3445, dc_loss: 0.016701210290193558, tv_loss: 0.01705825887620449\n",
      "iteration 3446, dc_loss: 0.01670118421316147, tv_loss: 0.017058299854397774\n",
      "iteration 3447, dc_loss: 0.01670117676258087, tv_loss: 0.017058109864592552\n",
      "iteration 3448, dc_loss: 0.016701171174645424, tv_loss: 0.017057951539754868\n",
      "iteration 3449, dc_loss: 0.016701189801096916, tv_loss: 0.017057903110980988\n",
      "iteration 3450, dc_loss: 0.016701284795999527, tv_loss: 0.017057890072464943\n",
      "iteration 3451, dc_loss: 0.01670134998857975, tv_loss: 0.017057765275239944\n",
      "iteration 3452, dc_loss: 0.0167013481259346, tv_loss: 0.017057571560144424\n",
      "iteration 3453, dc_loss: 0.01670132577419281, tv_loss: 0.017057528719305992\n",
      "iteration 3454, dc_loss: 0.016701290383934975, tv_loss: 0.01705760881304741\n",
      "iteration 3455, dc_loss: 0.01670120842754841, tv_loss: 0.0170576348900795\n",
      "iteration 3456, dc_loss: 0.01670113392174244, tv_loss: 0.01705758087337017\n",
      "iteration 3457, dc_loss: 0.01670106314122677, tv_loss: 0.017057685181498528\n",
      "iteration 3458, dc_loss: 0.016700997948646545, tv_loss: 0.017057636752724648\n",
      "iteration 3459, dc_loss: 0.01670101284980774, tv_loss: 0.017057431861758232\n",
      "iteration 3460, dc_loss: 0.01670103333890438, tv_loss: 0.01705731637775898\n",
      "iteration 3461, dc_loss: 0.016701042652130127, tv_loss: 0.017057320103049278\n",
      "iteration 3462, dc_loss: 0.01670101098716259, tv_loss: 0.01705721765756607\n",
      "iteration 3463, dc_loss: 0.016700971871614456, tv_loss: 0.017057090997695923\n",
      "iteration 3464, dc_loss: 0.016700968146324158, tv_loss: 0.017057055607438087\n",
      "iteration 3465, dc_loss: 0.016700923442840576, tv_loss: 0.01705712452530861\n",
      "iteration 3466, dc_loss: 0.01670093648135662, tv_loss: 0.01705711893737316\n",
      "iteration 3467, dc_loss: 0.016700977459549904, tv_loss: 0.017056889832019806\n",
      "iteration 3468, dc_loss: 0.01670098677277565, tv_loss: 0.01705670729279518\n",
      "iteration 3469, dc_loss: 0.01670096628367901, tv_loss: 0.017056934535503387\n",
      "iteration 3470, dc_loss: 0.01670093461871147, tv_loss: 0.017056748270988464\n",
      "iteration 3471, dc_loss: 0.01670088618993759, tv_loss: 0.017056504264473915\n",
      "iteration 3472, dc_loss: 0.016700822860002518, tv_loss: 0.017056630924344063\n",
      "iteration 3473, dc_loss: 0.016700809821486473, tv_loss: 0.017056725919246674\n",
      "iteration 3474, dc_loss: 0.01670081540942192, tv_loss: 0.017056630924344063\n",
      "iteration 3475, dc_loss: 0.016700858250260353, tv_loss: 0.017056554555892944\n",
      "iteration 3476, dc_loss: 0.016700921580195427, tv_loss: 0.01705639436841011\n",
      "iteration 3477, dc_loss: 0.016700906679034233, tv_loss: 0.017056329175829887\n",
      "iteration 3478, dc_loss: 0.01670083962380886, tv_loss: 0.017056353390216827\n",
      "iteration 3479, dc_loss: 0.016700750216841698, tv_loss: 0.01705656200647354\n",
      "iteration 3480, dc_loss: 0.016700688749551773, tv_loss: 0.017056496813893318\n",
      "iteration 3481, dc_loss: 0.016700662672519684, tv_loss: 0.01705631986260414\n",
      "iteration 3482, dc_loss: 0.016700655221939087, tv_loss: 0.017056358978152275\n",
      "iteration 3483, dc_loss: 0.01670064590871334, tv_loss: 0.017056278884410858\n",
      "iteration 3484, dc_loss: 0.016700662672519684, tv_loss: 0.0170561783015728\n",
      "iteration 3485, dc_loss: 0.016700731590390205, tv_loss: 0.01705612801015377\n",
      "iteration 3486, dc_loss: 0.016700774431228638, tv_loss: 0.017055973410606384\n",
      "iteration 3487, dc_loss: 0.01670077256858349, tv_loss: 0.01705591194331646\n",
      "iteration 3488, dc_loss: 0.016700763255357742, tv_loss: 0.01705595664680004\n",
      "iteration 3489, dc_loss: 0.01670069806277752, tv_loss: 0.017055919393897057\n",
      "iteration 3490, dc_loss: 0.016700640320777893, tv_loss: 0.017055874690413475\n",
      "iteration 3491, dc_loss: 0.016700563952326775, tv_loss: 0.0170560572296381\n",
      "iteration 3492, dc_loss: 0.016700541600584984, tv_loss: 0.017056141048669815\n",
      "iteration 3493, dc_loss: 0.01670057885348797, tv_loss: 0.017055919393897057\n",
      "iteration 3494, dc_loss: 0.01670062728226185, tv_loss: 0.017055809497833252\n",
      "iteration 3495, dc_loss: 0.016700632870197296, tv_loss: 0.017055658623576164\n",
      "iteration 3496, dc_loss: 0.01670064590871334, tv_loss: 0.017055556178092957\n",
      "iteration 3497, dc_loss: 0.01670064404606819, tv_loss: 0.017055438831448555\n",
      "iteration 3498, dc_loss: 0.016700632870197296, tv_loss: 0.01705530658364296\n",
      "iteration 3499, dc_loss: 0.01670059747993946, tv_loss: 0.017055343836545944\n",
      "iteration 3500, dc_loss: 0.016700536012649536, tv_loss: 0.017055274918675423\n",
      "iteration 3501, dc_loss: 0.016700493171811104, tv_loss: 0.017055248841643333\n",
      "iteration 3502, dc_loss: 0.016700472682714462, tv_loss: 0.017055369913578033\n",
      "iteration 3503, dc_loss: 0.01670042984187603, tv_loss: 0.01705530844628811\n",
      "iteration 3504, dc_loss: 0.016700390726327896, tv_loss: 0.017055319622159004\n",
      "iteration 3505, dc_loss: 0.016700387001037598, tv_loss: 0.017055390402674675\n",
      "iteration 3506, dc_loss: 0.01670037768781185, tv_loss: 0.017055246978998184\n",
      "iteration 3507, dc_loss: 0.016700344160199165, tv_loss: 0.017055246978998184\n",
      "iteration 3508, dc_loss: 0.01670030876994133, tv_loss: 0.017055269330739975\n",
      "iteration 3509, dc_loss: 0.01670028269290924, tv_loss: 0.017055191099643707\n",
      "iteration 3510, dc_loss: 0.01670028641819954, tv_loss: 0.017055189236998558\n",
      "iteration 3511, dc_loss: 0.016700292006134987, tv_loss: 0.0170550886541605\n",
      "iteration 3512, dc_loss: 0.01670030690729618, tv_loss: 0.0170549638569355\n",
      "iteration 3513, dc_loss: 0.016700273379683495, tv_loss: 0.01705506071448326\n",
      "iteration 3514, dc_loss: 0.016700226813554764, tv_loss: 0.01705499365925789\n",
      "iteration 3515, dc_loss: 0.016700221225619316, tv_loss: 0.01705486886203289\n",
      "iteration 3516, dc_loss: 0.016700245440006256, tv_loss: 0.017054971307516098\n",
      "iteration 3517, dc_loss: 0.016700273379683495, tv_loss: 0.017054997384548187\n",
      "iteration 3518, dc_loss: 0.016700277104973793, tv_loss: 0.017054688185453415\n",
      "iteration 3519, dc_loss: 0.0167002622038126, tv_loss: 0.017054522410035133\n",
      "iteration 3520, dc_loss: 0.016700228676199913, tv_loss: 0.017054492607712746\n",
      "iteration 3521, dc_loss: 0.01670021191239357, tv_loss: 0.017054716125130653\n",
      "iteration 3522, dc_loss: 0.016700202599167824, tv_loss: 0.01705458015203476\n",
      "iteration 3523, dc_loss: 0.016700172796845436, tv_loss: 0.017054464668035507\n",
      "iteration 3524, dc_loss: 0.016700154170393944, tv_loss: 0.017054501920938492\n",
      "iteration 3525, dc_loss: 0.016700156033039093, tv_loss: 0.017054442316293716\n",
      "iteration 3526, dc_loss: 0.016700154170393944, tv_loss: 0.01705414429306984\n",
      "iteration 3527, dc_loss: 0.01670018956065178, tv_loss: 0.0170541200786829\n",
      "iteration 3528, dc_loss: 0.016700226813554764, tv_loss: 0.017054196447134018\n",
      "iteration 3529, dc_loss: 0.01670021563768387, tv_loss: 0.017054174095392227\n",
      "iteration 3530, dc_loss: 0.016700206324458122, tv_loss: 0.017053985968232155\n",
      "iteration 3531, dc_loss: 0.01670018583536148, tv_loss: 0.017053991556167603\n",
      "iteration 3532, dc_loss: 0.01670015975832939, tv_loss: 0.0170539990067482\n",
      "iteration 3533, dc_loss: 0.016700129956007004, tv_loss: 0.01705385372042656\n",
      "iteration 3534, dc_loss: 0.01670009084045887, tv_loss: 0.01705382578074932\n",
      "iteration 3535, dc_loss: 0.016700079664587975, tv_loss: 0.017053915187716484\n",
      "iteration 3536, dc_loss: 0.016700047999620438, tv_loss: 0.01705387607216835\n",
      "iteration 3537, dc_loss: 0.016700025647878647, tv_loss: 0.01705361343920231\n",
      "iteration 3538, dc_loss: 0.016700047999620438, tv_loss: 0.017053531482815742\n",
      "iteration 3539, dc_loss: 0.01670004054903984, tv_loss: 0.017053626477718353\n",
      "iteration 3540, dc_loss: 0.016700049862265587, tv_loss: 0.017053628340363503\n",
      "iteration 3541, dc_loss: 0.016700053587555885, tv_loss: 0.017053578048944473\n",
      "iteration 3542, dc_loss: 0.016700033098459244, tv_loss: 0.01705344207584858\n",
      "iteration 3543, dc_loss: 0.01669999025762081, tv_loss: 0.01705341227352619\n",
      "iteration 3544, dc_loss: 0.016699956730008125, tv_loss: 0.017053471878170967\n",
      "iteration 3545, dc_loss: 0.016699934378266335, tv_loss: 0.01705346815288067\n",
      "iteration 3546, dc_loss: 0.016699889674782753, tv_loss: 0.017053352668881416\n",
      "iteration 3547, dc_loss: 0.016699818894267082, tv_loss: 0.01705338805913925\n",
      "iteration 3548, dc_loss: 0.016699790954589844, tv_loss: 0.01705348864197731\n",
      "iteration 3549, dc_loss: 0.016699807718396187, tv_loss: 0.01705343835055828\n",
      "iteration 3550, dc_loss: 0.016699856147170067, tv_loss: 0.017053283751010895\n",
      "iteration 3551, dc_loss: 0.016699889674782753, tv_loss: 0.017053164541721344\n",
      "iteration 3552, dc_loss: 0.016699912026524544, tv_loss: 0.017053041607141495\n",
      "iteration 3553, dc_loss: 0.016699902713298798, tv_loss: 0.017053106799721718\n",
      "iteration 3554, dc_loss: 0.016699835658073425, tv_loss: 0.017053036019206047\n",
      "iteration 3555, dc_loss: 0.016699781641364098, tv_loss: 0.017052944749593735\n",
      "iteration 3556, dc_loss: 0.016699720174074173, tv_loss: 0.017052950337529182\n",
      "iteration 3557, dc_loss: 0.016699684783816338, tv_loss: 0.0170530304312706\n",
      "iteration 3558, dc_loss: 0.016699718311429024, tv_loss: 0.017052963376045227\n",
      "iteration 3559, dc_loss: 0.016699759289622307, tv_loss: 0.01705276407301426\n",
      "iteration 3560, dc_loss: 0.01669972948729992, tv_loss: 0.0170526672154665\n",
      "iteration 3561, dc_loss: 0.01669965498149395, tv_loss: 0.01705276407301426\n",
      "iteration 3562, dc_loss: 0.016699587926268578, tv_loss: 0.017052842304110527\n",
      "iteration 3563, dc_loss: 0.01669955439865589, tv_loss: 0.017052730545401573\n",
      "iteration 3564, dc_loss: 0.01669953018426895, tv_loss: 0.017052670940756798\n",
      "iteration 3565, dc_loss: 0.016699519008398056, tv_loss: 0.017052780836820602\n",
      "iteration 3566, dc_loss: 0.016699520871043205, tv_loss: 0.01705276407301426\n",
      "iteration 3567, dc_loss: 0.016699563711881638, tv_loss: 0.017052454873919487\n",
      "iteration 3568, dc_loss: 0.016699599102139473, tv_loss: 0.017052462324500084\n",
      "iteration 3569, dc_loss: 0.016699597239494324, tv_loss: 0.017052555456757545\n",
      "iteration 3570, dc_loss: 0.016699571162462234, tv_loss: 0.017052458599209785\n",
      "iteration 3571, dc_loss: 0.0166995357722044, tv_loss: 0.017052389681339264\n",
      "iteration 3572, dc_loss: 0.01669951155781746, tv_loss: 0.01705225557088852\n",
      "iteration 3573, dc_loss: 0.016699491068720818, tv_loss: 0.01705232262611389\n",
      "iteration 3574, dc_loss: 0.01669950969517231, tv_loss: 0.017052259296178818\n",
      "iteration 3575, dc_loss: 0.0166995357722044, tv_loss: 0.017052054405212402\n",
      "iteration 3576, dc_loss: 0.01669955626130104, tv_loss: 0.017052026465535164\n",
      "iteration 3577, dc_loss: 0.01669953390955925, tv_loss: 0.017051981762051582\n",
      "iteration 3578, dc_loss: 0.016699498519301414, tv_loss: 0.017051946371793747\n",
      "iteration 3579, dc_loss: 0.016699450090527534, tv_loss: 0.01705211028456688\n",
      "iteration 3580, dc_loss: 0.016699405387043953, tv_loss: 0.01705203391611576\n",
      "iteration 3581, dc_loss: 0.016699358820915222, tv_loss: 0.017051979899406433\n",
      "iteration 3582, dc_loss: 0.01669934391975403, tv_loss: 0.017052041366696358\n",
      "iteration 3583, dc_loss: 0.01669938676059246, tv_loss: 0.01705213263630867\n",
      "iteration 3584, dc_loss: 0.016699420288205147, tv_loss: 0.017051909118890762\n",
      "iteration 3585, dc_loss: 0.01669943518936634, tv_loss: 0.017051681876182556\n",
      "iteration 3586, dc_loss: 0.01669939048588276, tv_loss: 0.017051761969923973\n",
      "iteration 3587, dc_loss: 0.01669936813414097, tv_loss: 0.017051784321665764\n",
      "iteration 3588, dc_loss: 0.016699369996786118, tv_loss: 0.017051704227924347\n",
      "iteration 3589, dc_loss: 0.016699397936463356, tv_loss: 0.017051665112376213\n",
      "iteration 3590, dc_loss: 0.016699424013495445, tv_loss: 0.0170514527708292\n",
      "iteration 3591, dc_loss: 0.01669945940375328, tv_loss: 0.017051534727215767\n",
      "iteration 3592, dc_loss: 0.01669943332672119, tv_loss: 0.017051542177796364\n",
      "iteration 3593, dc_loss: 0.016699358820915222, tv_loss: 0.017051508650183678\n",
      "iteration 3594, dc_loss: 0.016699280589818954, tv_loss: 0.01705149933695793\n",
      "iteration 3595, dc_loss: 0.016699232161045074, tv_loss: 0.017051467671990395\n",
      "iteration 3596, dc_loss: 0.016699187457561493, tv_loss: 0.017051663249731064\n",
      "iteration 3597, dc_loss: 0.016699180006980896, tv_loss: 0.017051614820957184\n",
      "iteration 3598, dc_loss: 0.01669919677078724, tv_loss: 0.017051445320248604\n",
      "iteration 3599, dc_loss: 0.016699176281690598, tv_loss: 0.01705128513276577\n",
      "iteration 3600, dc_loss: 0.01669917069375515, tv_loss: 0.017051292583346367\n",
      "iteration 3601, dc_loss: 0.016699153929948807, tv_loss: 0.01705138385295868\n",
      "iteration 3602, dc_loss: 0.01669914275407791, tv_loss: 0.017051156610250473\n",
      "iteration 3603, dc_loss: 0.01669914461672306, tv_loss: 0.01705099828541279\n",
      "iteration 3604, dc_loss: 0.016699155792593956, tv_loss: 0.01705090142786503\n",
      "iteration 3605, dc_loss: 0.01669916883111, tv_loss: 0.01705085299909115\n",
      "iteration 3606, dc_loss: 0.016699185594916344, tv_loss: 0.01705070212483406\n",
      "iteration 3607, dc_loss: 0.016699204221367836, tv_loss: 0.01705075427889824\n",
      "iteration 3608, dc_loss: 0.01669919118285179, tv_loss: 0.017050717025995255\n",
      "iteration 3609, dc_loss: 0.01669921912252903, tv_loss: 0.017050622031092644\n",
      "iteration 3610, dc_loss: 0.016699228435754776, tv_loss: 0.017050541937351227\n",
      "iteration 3611, dc_loss: 0.016699209809303284, tv_loss: 0.017050549387931824\n",
      "iteration 3612, dc_loss: 0.016699155792593956, tv_loss: 0.017050402238965034\n",
      "iteration 3613, dc_loss: 0.016699109226465225, tv_loss: 0.01705046556890011\n",
      "iteration 3614, dc_loss: 0.016699034720659256, tv_loss: 0.017050592228770256\n",
      "iteration 3615, dc_loss: 0.01669894903898239, tv_loss: 0.01705053448677063\n",
      "iteration 3616, dc_loss: 0.0166989266872406, tv_loss: 0.017050469294190407\n",
      "iteration 3617, dc_loss: 0.01669890247285366, tv_loss: 0.017050379887223244\n",
      "iteration 3618, dc_loss: 0.01669885776937008, tv_loss: 0.01705051213502884\n",
      "iteration 3619, dc_loss: 0.01669885590672493, tv_loss: 0.017050568014383316\n",
      "iteration 3620, dc_loss: 0.016698896884918213, tv_loss: 0.017050383612513542\n",
      "iteration 3621, dc_loss: 0.016698943451046944, tv_loss: 0.017050154507160187\n",
      "iteration 3622, dc_loss: 0.01669897511601448, tv_loss: 0.017050225287675858\n",
      "iteration 3623, dc_loss: 0.016698969528079033, tv_loss: 0.017050178721547127\n",
      "iteration 3624, dc_loss: 0.016698934137821198, tv_loss: 0.017050065100193024\n",
      "iteration 3625, dc_loss: 0.016698921099305153, tv_loss: 0.01705007255077362\n",
      "iteration 3626, dc_loss: 0.016698909923434258, tv_loss: 0.017050141468644142\n",
      "iteration 3627, dc_loss: 0.016698917374014854, tv_loss: 0.017049899324774742\n",
      "iteration 3628, dc_loss: 0.01669890061020851, tv_loss: 0.017049865797162056\n",
      "iteration 3629, dc_loss: 0.016698861494660378, tv_loss: 0.017049942165613174\n",
      "iteration 3630, dc_loss: 0.016698846593499184, tv_loss: 0.01704990863800049\n",
      "iteration 3631, dc_loss: 0.016698867082595825, tv_loss: 0.017049970105290413\n",
      "iteration 3632, dc_loss: 0.016698872670531273, tv_loss: 0.017049886286258698\n",
      "iteration 3633, dc_loss: 0.016698863357305527, tv_loss: 0.01704961620271206\n",
      "iteration 3634, dc_loss: 0.016698848456144333, tv_loss: 0.01704980991780758\n",
      "iteration 3635, dc_loss: 0.016698837280273438, tv_loss: 0.01704985275864601\n",
      "iteration 3636, dc_loss: 0.016698814928531647, tv_loss: 0.017049767076969147\n",
      "iteration 3637, dc_loss: 0.016698764637112617, tv_loss: 0.017049673944711685\n",
      "iteration 3638, dc_loss: 0.016698703169822693, tv_loss: 0.01704953797161579\n",
      "iteration 3639, dc_loss: 0.01669866032898426, tv_loss: 0.017049608752131462\n",
      "iteration 3640, dc_loss: 0.016698669642210007, tv_loss: 0.01704971306025982\n",
      "iteration 3641, dc_loss: 0.016698678955435753, tv_loss: 0.01704956404864788\n",
      "iteration 3642, dc_loss: 0.016698703169822693, tv_loss: 0.017049461603164673\n",
      "iteration 3643, dc_loss: 0.016698751598596573, tv_loss: 0.01704944670200348\n",
      "iteration 3644, dc_loss: 0.016698786988854408, tv_loss: 0.01704932563006878\n",
      "iteration 3645, dc_loss: 0.01669878326356411, tv_loss: 0.01704929582774639\n",
      "iteration 3646, dc_loss: 0.016698744148015976, tv_loss: 0.017049431800842285\n",
      "iteration 3647, dc_loss: 0.016698677092790604, tv_loss: 0.017049208283424377\n",
      "iteration 3648, dc_loss: 0.01669863611459732, tv_loss: 0.017049167305231094\n",
      "iteration 3649, dc_loss: 0.01669858582317829, tv_loss: 0.017049336805939674\n",
      "iteration 3650, dc_loss: 0.01669853739440441, tv_loss: 0.01704932563006878\n",
      "iteration 3651, dc_loss: 0.01669849269092083, tv_loss: 0.017049496993422508\n",
      "iteration 3652, dc_loss: 0.016698457300662994, tv_loss: 0.017049282789230347\n",
      "iteration 3653, dc_loss: 0.016698433086276054, tv_loss: 0.017049245536327362\n",
      "iteration 3654, dc_loss: 0.016698405146598816, tv_loss: 0.017049530521035194\n",
      "iteration 3655, dc_loss: 0.016698390245437622, tv_loss: 0.017049312591552734\n",
      "iteration 3656, dc_loss: 0.016698412597179413, tv_loss: 0.017049051821231842\n",
      "iteration 3657, dc_loss: 0.01669846475124359, tv_loss: 0.017049171030521393\n",
      "iteration 3658, dc_loss: 0.016698535531759262, tv_loss: 0.017048968002200127\n",
      "iteration 3659, dc_loss: 0.01669859141111374, tv_loss: 0.01704878732562065\n",
      "iteration 3660, dc_loss: 0.01669863611459732, tv_loss: 0.017048746347427368\n",
      "iteration 3661, dc_loss: 0.016698628664016724, tv_loss: 0.017048632726073265\n",
      "iteration 3662, dc_loss: 0.01669859141111374, tv_loss: 0.017048589885234833\n",
      "iteration 3663, dc_loss: 0.01669854111969471, tv_loss: 0.01704850234091282\n",
      "iteration 3664, dc_loss: 0.01669851876795292, tv_loss: 0.01704876497387886\n",
      "iteration 3665, dc_loss: 0.01669846661388874, tv_loss: 0.01704862155020237\n",
      "iteration 3666, dc_loss: 0.016698401421308517, tv_loss: 0.017048576846718788\n",
      "iteration 3667, dc_loss: 0.01669834554195404, tv_loss: 0.017048610374331474\n",
      "iteration 3668, dc_loss: 0.016698302701115608, tv_loss: 0.017048530280590057\n",
      "iteration 3669, dc_loss: 0.016698304563760757, tv_loss: 0.017048532143235207\n",
      "iteration 3670, dc_loss: 0.016698302701115608, tv_loss: 0.017048578709363937\n",
      "iteration 3671, dc_loss: 0.016698312014341354, tv_loss: 0.017048362642526627\n",
      "iteration 3672, dc_loss: 0.01669831946492195, tv_loss: 0.017048358917236328\n",
      "iteration 3673, dc_loss: 0.016698317602276802, tv_loss: 0.01704828441143036\n",
      "iteration 3674, dc_loss: 0.016698326915502548, tv_loss: 0.01704813353717327\n",
      "iteration 3675, dc_loss: 0.01669834926724434, tv_loss: 0.017048213630914688\n",
      "iteration 3676, dc_loss: 0.016698379069566727, tv_loss: 0.017048103734850883\n",
      "iteration 3677, dc_loss: 0.01669837161898613, tv_loss: 0.0170479454100132\n",
      "iteration 3678, dc_loss: 0.01669832319021225, tv_loss: 0.017047908157110214\n",
      "iteration 3679, dc_loss: 0.016698278486728668, tv_loss: 0.017047960311174393\n",
      "iteration 3680, dc_loss: 0.016698230057954788, tv_loss: 0.017048031091690063\n",
      "iteration 3681, dc_loss: 0.01669817604124546, tv_loss: 0.01704804040491581\n",
      "iteration 3682, dc_loss: 0.01669815368950367, tv_loss: 0.017048120498657227\n",
      "iteration 3683, dc_loss: 0.016698161140084267, tv_loss: 0.017047792673110962\n",
      "iteration 3684, dc_loss: 0.01669814996421337, tv_loss: 0.017047833651304245\n",
      "iteration 3685, dc_loss: 0.016698209568858147, tv_loss: 0.01704786904156208\n",
      "iteration 3686, dc_loss: 0.016698265448212624, tv_loss: 0.017047632485628128\n",
      "iteration 3687, dc_loss: 0.016698282212018967, tv_loss: 0.017047472298145294\n",
      "iteration 3688, dc_loss: 0.01669825240969658, tv_loss: 0.017047429457306862\n",
      "iteration 3689, dc_loss: 0.016698189079761505, tv_loss: 0.017047597095370293\n",
      "iteration 3690, dc_loss: 0.01669810712337494, tv_loss: 0.017047563567757607\n",
      "iteration 3691, dc_loss: 0.016698017716407776, tv_loss: 0.017047638073563576\n",
      "iteration 3692, dc_loss: 0.016697952523827553, tv_loss: 0.017047643661499023\n",
      "iteration 3693, dc_loss: 0.016697948798537254, tv_loss: 0.017047561705112457\n",
      "iteration 3694, dc_loss: 0.016697997227311134, tv_loss: 0.0170475784689188\n",
      "iteration 3695, dc_loss: 0.01669810526072979, tv_loss: 0.017047323286533356\n",
      "iteration 3696, dc_loss: 0.016698161140084267, tv_loss: 0.017047185450792313\n",
      "iteration 3697, dc_loss: 0.016698142513632774, tv_loss: 0.017047176137566566\n",
      "iteration 3698, dc_loss: 0.016698142513632774, tv_loss: 0.017047220841050148\n",
      "iteration 3699, dc_loss: 0.016698166728019714, tv_loss: 0.01704716868698597\n",
      "iteration 3700, dc_loss: 0.016698148101568222, tv_loss: 0.017047014087438583\n",
      "iteration 3701, dc_loss: 0.01669808104634285, tv_loss: 0.017047081142663956\n",
      "iteration 3702, dc_loss: 0.016698025166988373, tv_loss: 0.017047137022018433\n",
      "iteration 3703, dc_loss: 0.016697987914085388, tv_loss: 0.017047256231307983\n",
      "iteration 3704, dc_loss: 0.016697950661182404, tv_loss: 0.01704719103872776\n",
      "iteration 3705, dc_loss: 0.016697904095053673, tv_loss: 0.01704731583595276\n",
      "iteration 3706, dc_loss: 0.01669788919389248, tv_loss: 0.017047202214598656\n",
      "iteration 3707, dc_loss: 0.016697915270924568, tv_loss: 0.01704697124660015\n",
      "iteration 3708, dc_loss: 0.016697971150279045, tv_loss: 0.017047066241502762\n",
      "iteration 3709, dc_loss: 0.016697995364665985, tv_loss: 0.01704704947769642\n",
      "iteration 3710, dc_loss: 0.016697974875569344, tv_loss: 0.01704675331711769\n",
      "iteration 3711, dc_loss: 0.016697930172085762, tv_loss: 0.017046824097633362\n",
      "iteration 3712, dc_loss: 0.016697939485311508, tv_loss: 0.017046868801116943\n",
      "iteration 3713, dc_loss: 0.016697952523827553, tv_loss: 0.017046742141246796\n",
      "iteration 3714, dc_loss: 0.016697926446795464, tv_loss: 0.017046760767698288\n",
      "iteration 3715, dc_loss: 0.01669788919389248, tv_loss: 0.017046896740794182\n",
      "iteration 3716, dc_loss: 0.01669786497950554, tv_loss: 0.017046794295310974\n",
      "iteration 3717, dc_loss: 0.0166978407651186, tv_loss: 0.017046764492988586\n",
      "iteration 3718, dc_loss: 0.016697848215699196, tv_loss: 0.017046624794602394\n",
      "iteration 3719, dc_loss: 0.016697870567440987, tv_loss: 0.01704658567905426\n",
      "iteration 3720, dc_loss: 0.016697850078344345, tv_loss: 0.017046578228473663\n",
      "iteration 3721, dc_loss: 0.016697855666279793, tv_loss: 0.017046509310603142\n",
      "iteration 3722, dc_loss: 0.016697799786925316, tv_loss: 0.01704639568924904\n",
      "iteration 3723, dc_loss: 0.01669776812195778, tv_loss: 0.01704651676118374\n",
      "iteration 3724, dc_loss: 0.01669776253402233, tv_loss: 0.01704655960202217\n",
      "iteration 3725, dc_loss: 0.016697723418474197, tv_loss: 0.017046436667442322\n",
      "iteration 3726, dc_loss: 0.016697721555829048, tv_loss: 0.017046259716153145\n",
      "iteration 3727, dc_loss: 0.0166977196931839, tv_loss: 0.017046194523572922\n",
      "iteration 3728, dc_loss: 0.016697745770215988, tv_loss: 0.01704617217183113\n",
      "iteration 3729, dc_loss: 0.016697747632861137, tv_loss: 0.01704615168273449\n",
      "iteration 3730, dc_loss: 0.016697727143764496, tv_loss: 0.0170461293309927\n",
      "iteration 3731, dc_loss: 0.016697686165571213, tv_loss: 0.017046133056282997\n",
      "iteration 3732, dc_loss: 0.01669766753911972, tv_loss: 0.017045944929122925\n",
      "iteration 3733, dc_loss: 0.016697615385055542, tv_loss: 0.017046036198735237\n",
      "iteration 3734, dc_loss: 0.01669759675860405, tv_loss: 0.017046140506863594\n",
      "iteration 3735, dc_loss: 0.016697585582733154, tv_loss: 0.01704607903957367\n",
      "iteration 3736, dc_loss: 0.016697634011507034, tv_loss: 0.01704603247344494\n",
      "iteration 3737, dc_loss: 0.016697680577635765, tv_loss: 0.017045961692929268\n",
      "iteration 3738, dc_loss: 0.016697702929377556, tv_loss: 0.017046011984348297\n",
      "iteration 3739, dc_loss: 0.01669768989086151, tv_loss: 0.017045842483639717\n",
      "iteration 3740, dc_loss: 0.016697680577635765, tv_loss: 0.017045678570866585\n",
      "iteration 3741, dc_loss: 0.016697663813829422, tv_loss: 0.017045723274350166\n",
      "iteration 3742, dc_loss: 0.01669761724770069, tv_loss: 0.017045773565769196\n",
      "iteration 3743, dc_loss: 0.016697539016604424, tv_loss: 0.017045674845576286\n",
      "iteration 3744, dc_loss: 0.016697527840733528, tv_loss: 0.017045600339770317\n",
      "iteration 3745, dc_loss: 0.016697540879249573, tv_loss: 0.017045587301254272\n",
      "iteration 3746, dc_loss: 0.016697533428668976, tv_loss: 0.01704549603164196\n",
      "iteration 3747, dc_loss: 0.016697535291314125, tv_loss: 0.017045404762029648\n",
      "iteration 3748, dc_loss: 0.016697531566023827, tv_loss: 0.0170454028993845\n",
      "iteration 3749, dc_loss: 0.016697559505701065, tv_loss: 0.017045464366674423\n",
      "iteration 3750, dc_loss: 0.016697587445378304, tv_loss: 0.01704520359635353\n",
      "iteration 3751, dc_loss: 0.016697563230991364, tv_loss: 0.017045073211193085\n",
      "iteration 3752, dc_loss: 0.01669750176370144, tv_loss: 0.017045343294739723\n",
      "iteration 3753, dc_loss: 0.016697455197572708, tv_loss: 0.01704537868499756\n",
      "iteration 3754, dc_loss: 0.016697408631443977, tv_loss: 0.017045313492417336\n",
      "iteration 3755, dc_loss: 0.016697391867637634, tv_loss: 0.017045293003320694\n",
      "iteration 3756, dc_loss: 0.016697386279702187, tv_loss: 0.017045164480805397\n",
      "iteration 3757, dc_loss: 0.01669740304350853, tv_loss: 0.017045293003320694\n",
      "iteration 3758, dc_loss: 0.016697421669960022, tv_loss: 0.017045313492417336\n",
      "iteration 3759, dc_loss: 0.01669740304350853, tv_loss: 0.017045140266418457\n",
      "iteration 3760, dc_loss: 0.016697410494089127, tv_loss: 0.017045147716999054\n",
      "iteration 3761, dc_loss: 0.01669740304350853, tv_loss: 0.0170451570302248\n",
      "iteration 3762, dc_loss: 0.016697410494089127, tv_loss: 0.017045050859451294\n",
      "iteration 3763, dc_loss: 0.016697421669960022, tv_loss: 0.01704498752951622\n",
      "iteration 3764, dc_loss: 0.01669744960963726, tv_loss: 0.017044909298419952\n",
      "iteration 3765, dc_loss: 0.016697464510798454, tv_loss: 0.01704493723809719\n",
      "iteration 3766, dc_loss: 0.016697419807314873, tv_loss: 0.017044857144355774\n",
      "iteration 3767, dc_loss: 0.016697390004992485, tv_loss: 0.017044812440872192\n",
      "iteration 3768, dc_loss: 0.0166973527520895, tv_loss: 0.01704474724829197\n",
      "iteration 3769, dc_loss: 0.016697337850928307, tv_loss: 0.017044803127646446\n",
      "iteration 3770, dc_loss: 0.016697291284799576, tv_loss: 0.017044896259903908\n",
      "iteration 3771, dc_loss: 0.016697226092219353, tv_loss: 0.017044909298419952\n",
      "iteration 3772, dc_loss: 0.016697170212864876, tv_loss: 0.01704481616616249\n",
      "iteration 3773, dc_loss: 0.016697121784090996, tv_loss: 0.017044808715581894\n",
      "iteration 3774, dc_loss: 0.016697118058800697, tv_loss: 0.017044872045516968\n",
      "iteration 3775, dc_loss: 0.016697153449058533, tv_loss: 0.01704472117125988\n",
      "iteration 3776, dc_loss: 0.016697203740477562, tv_loss: 0.017044557258486748\n",
      "iteration 3777, dc_loss: 0.016697239130735397, tv_loss: 0.017044400796294212\n",
      "iteration 3778, dc_loss: 0.016697298735380173, tv_loss: 0.01704445108771324\n",
      "iteration 3779, dc_loss: 0.016697319224476814, tv_loss: 0.017044411972165108\n",
      "iteration 3780, dc_loss: 0.016697274520993233, tv_loss: 0.01704423688352108\n",
      "iteration 3781, dc_loss: 0.016697240993380547, tv_loss: 0.017044253647327423\n",
      "iteration 3782, dc_loss: 0.016697213053703308, tv_loss: 0.017044225707650185\n",
      "iteration 3783, dc_loss: 0.016697144135832787, tv_loss: 0.017044231295585632\n",
      "iteration 3784, dc_loss: 0.016697082668542862, tv_loss: 0.017044154927134514\n",
      "iteration 3785, dc_loss: 0.01669703610241413, tv_loss: 0.017044179141521454\n",
      "iteration 3786, dc_loss: 0.016697021201252937, tv_loss: 0.017044221982359886\n",
      "iteration 3787, dc_loss: 0.016697043552994728, tv_loss: 0.01704423688352108\n",
      "iteration 3788, dc_loss: 0.01669708453118801, tv_loss: 0.017043842002749443\n",
      "iteration 3789, dc_loss: 0.016697103157639503, tv_loss: 0.01704379729926586\n",
      "iteration 3790, dc_loss: 0.016697121784090996, tv_loss: 0.01704380288720131\n",
      "iteration 3791, dc_loss: 0.016697131097316742, tv_loss: 0.01704377681016922\n",
      "iteration 3792, dc_loss: 0.016697149723768234, tv_loss: 0.017043868079781532\n",
      "iteration 3793, dc_loss: 0.01669713482260704, tv_loss: 0.017043884843587875\n",
      "iteration 3794, dc_loss: 0.01669713668525219, tv_loss: 0.017043743282556534\n",
      "iteration 3795, dc_loss: 0.01669713668525219, tv_loss: 0.01704355701804161\n",
      "iteration 3796, dc_loss: 0.016697095707058907, tv_loss: 0.01704365573823452\n",
      "iteration 3797, dc_loss: 0.01669704169034958, tv_loss: 0.01704370230436325\n",
      "iteration 3798, dc_loss: 0.016697000712156296, tv_loss: 0.017043637111783028\n",
      "iteration 3799, dc_loss: 0.016696959733963013, tv_loss: 0.017043620347976685\n",
      "iteration 3800, dc_loss: 0.01669694110751152, tv_loss: 0.017043644562363625\n",
      "iteration 3801, dc_loss: 0.016696961596608162, tv_loss: 0.017043620347976685\n",
      "iteration 3802, dc_loss: 0.016696976497769356, tv_loss: 0.017043540254235268\n",
      "iteration 3803, dc_loss: 0.016696983948349953, tv_loss: 0.017043456435203552\n",
      "iteration 3804, dc_loss: 0.016696948558092117, tv_loss: 0.01704339310526848\n",
      "iteration 3805, dc_loss: 0.01669694297015667, tv_loss: 0.017043348401784897\n",
      "iteration 3806, dc_loss: 0.016696929931640625, tv_loss: 0.017043447121977806\n",
      "iteration 3807, dc_loss: 0.01669691875576973, tv_loss: 0.01704348810017109\n",
      "iteration 3808, dc_loss: 0.01669689267873764, tv_loss: 0.017043454572558403\n",
      "iteration 3809, dc_loss: 0.016696887090802193, tv_loss: 0.017043383792042732\n",
      "iteration 3810, dc_loss: 0.016696903854608536, tv_loss: 0.017043279483914375\n",
      "iteration 3811, dc_loss: 0.01669691503047943, tv_loss: 0.017043160274624825\n",
      "iteration 3812, dc_loss: 0.016696931794285774, tv_loss: 0.017043182626366615\n",
      "iteration 3813, dc_loss: 0.016696937382221222, tv_loss: 0.0170432198792696\n",
      "iteration 3814, dc_loss: 0.016696933656930923, tv_loss: 0.017043158411979675\n",
      "iteration 3815, dc_loss: 0.01669692061841488, tv_loss: 0.017043115571141243\n",
      "iteration 3816, dc_loss: 0.016696877777576447, tv_loss: 0.01704321801662445\n",
      "iteration 3817, dc_loss: 0.01669684238731861, tv_loss: 0.017043188214302063\n",
      "iteration 3818, dc_loss: 0.016696812584996223, tv_loss: 0.017043063417077065\n",
      "iteration 3819, dc_loss: 0.01669682189822197, tv_loss: 0.0170427393168211\n",
      "iteration 3820, dc_loss: 0.016696803271770477, tv_loss: 0.017042888328433037\n",
      "iteration 3821, dc_loss: 0.01669677160680294, tv_loss: 0.01704305410385132\n",
      "iteration 3822, dc_loss: 0.01669674925506115, tv_loss: 0.017042940482497215\n",
      "iteration 3823, dc_loss: 0.01669670082628727, tv_loss: 0.01704285852611065\n",
      "iteration 3824, dc_loss: 0.016696663573384285, tv_loss: 0.017042776569724083\n",
      "iteration 3825, dc_loss: 0.016696641221642494, tv_loss: 0.01704283244907856\n",
      "iteration 3826, dc_loss: 0.016696656122803688, tv_loss: 0.01704270765185356\n",
      "iteration 3827, dc_loss: 0.016696657985448837, tv_loss: 0.017042813822627068\n",
      "iteration 3828, dc_loss: 0.016696687787771225, tv_loss: 0.01704275608062744\n",
      "iteration 3829, dc_loss: 0.016696719452738762, tv_loss: 0.017042620107531548\n",
      "iteration 3830, dc_loss: 0.016696713864803314, tv_loss: 0.017042577266693115\n",
      "iteration 3831, dc_loss: 0.016696713864803314, tv_loss: 0.01704249531030655\n",
      "iteration 3832, dc_loss: 0.016696736216545105, tv_loss: 0.017042510211467743\n",
      "iteration 3833, dc_loss: 0.016696730628609657, tv_loss: 0.01704246923327446\n",
      "iteration 3834, dc_loss: 0.016696693375706673, tv_loss: 0.017042377963662148\n",
      "iteration 3835, dc_loss: 0.01669667474925518, tv_loss: 0.017042361199855804\n",
      "iteration 3836, dc_loss: 0.01669662445783615, tv_loss: 0.017042173072695732\n",
      "iteration 3837, dc_loss: 0.016696643084287643, tv_loss: 0.017042072489857674\n",
      "iteration 3838, dc_loss: 0.01669672317802906, tv_loss: 0.017042143270373344\n",
      "iteration 3839, dc_loss: 0.016696788370609283, tv_loss: 0.017041988670825958\n",
      "iteration 3840, dc_loss: 0.01669677346944809, tv_loss: 0.01704205386340618\n",
      "iteration 3841, dc_loss: 0.016696691513061523, tv_loss: 0.017042068764567375\n",
      "iteration 3842, dc_loss: 0.016696587204933167, tv_loss: 0.017041971907019615\n",
      "iteration 3843, dc_loss: 0.016696523874998093, tv_loss: 0.01704210601747036\n",
      "iteration 3844, dc_loss: 0.01669648103415966, tv_loss: 0.01704220287501812\n",
      "iteration 3845, dc_loss: 0.016696462407708168, tv_loss: 0.017042167484760284\n",
      "iteration 3846, dc_loss: 0.016696486622095108, tv_loss: 0.01704215072095394\n",
      "iteration 3847, dc_loss: 0.016696542501449585, tv_loss: 0.017041940242052078\n",
      "iteration 3848, dc_loss: 0.016696585342288017, tv_loss: 0.01704181730747223\n",
      "iteration 3849, dc_loss: 0.016696583479642868, tv_loss: 0.017041945829987526\n",
      "iteration 3850, dc_loss: 0.016696540638804436, tv_loss: 0.01704183779656887\n",
      "iteration 3851, dc_loss: 0.016696499660611153, tv_loss: 0.017041951417922974\n",
      "iteration 3852, dc_loss: 0.016696453094482422, tv_loss: 0.017041737213730812\n",
      "iteration 3853, dc_loss: 0.01669643446803093, tv_loss: 0.017041783779859543\n",
      "iteration 3854, dc_loss: 0.016696447506546974, tv_loss: 0.017041826620697975\n",
      "iteration 3855, dc_loss: 0.016696494072675705, tv_loss: 0.017041746526956558\n",
      "iteration 3856, dc_loss: 0.016696494072675705, tv_loss: 0.017041651532053947\n",
      "iteration 3857, dc_loss: 0.016696494072675705, tv_loss: 0.017041651532053947\n",
      "iteration 3858, dc_loss: 0.0166965089738369, tv_loss: 0.017041655257344246\n",
      "iteration 3859, dc_loss: 0.016696499660611153, tv_loss: 0.017041532322764397\n",
      "iteration 3860, dc_loss: 0.016696510836482048, tv_loss: 0.017041420564055443\n",
      "iteration 3861, dc_loss: 0.01669648289680481, tv_loss: 0.017041591927409172\n",
      "iteration 3862, dc_loss: 0.016696425154805183, tv_loss: 0.017041627317667007\n",
      "iteration 3863, dc_loss: 0.016696369275450706, tv_loss: 0.01704140193760395\n",
      "iteration 3864, dc_loss: 0.016696350648999214, tv_loss: 0.017041483893990517\n",
      "iteration 3865, dc_loss: 0.016696354374289513, tv_loss: 0.017041608691215515\n",
      "iteration 3866, dc_loss: 0.016696352511644363, tv_loss: 0.017041418701410294\n",
      "iteration 3867, dc_loss: 0.016696330159902573, tv_loss: 0.017041435465216637\n",
      "iteration 3868, dc_loss: 0.016696322709321976, tv_loss: 0.017041344195604324\n",
      "iteration 3869, dc_loss: 0.016696343198418617, tv_loss: 0.017041174694895744\n",
      "iteration 3870, dc_loss: 0.016696374863386154, tv_loss: 0.0170411616563797\n",
      "iteration 3871, dc_loss: 0.016696346923708916, tv_loss: 0.017041249200701714\n",
      "iteration 3872, dc_loss: 0.016696300357580185, tv_loss: 0.01704111509025097\n",
      "iteration 3873, dc_loss: 0.016696281731128693, tv_loss: 0.01704115979373455\n",
      "iteration 3874, dc_loss: 0.01669626124203205, tv_loss: 0.017041070386767387\n",
      "iteration 3875, dc_loss: 0.01669626496732235, tv_loss: 0.017041215673089027\n",
      "iteration 3876, dc_loss: 0.016696274280548096, tv_loss: 0.01704101264476776\n",
      "iteration 3877, dc_loss: 0.016696246340870857, tv_loss: 0.01704092137515545\n",
      "iteration 3878, dc_loss: 0.016696210950613022, tv_loss: 0.017041057348251343\n",
      "iteration 3879, dc_loss: 0.016696199774742126, tv_loss: 0.01704104244709015\n",
      "iteration 3880, dc_loss: 0.01669619232416153, tv_loss: 0.017041008919477463\n",
      "iteration 3881, dc_loss: 0.01669618859887123, tv_loss: 0.017040912061929703\n",
      "iteration 3882, dc_loss: 0.016696223989129066, tv_loss: 0.017040899023413658\n",
      "iteration 3883, dc_loss: 0.016696227714419365, tv_loss: 0.017040912061929703\n",
      "iteration 3884, dc_loss: 0.016696270555257797, tv_loss: 0.017040710896253586\n",
      "iteration 3885, dc_loss: 0.01669633947312832, tv_loss: 0.017040591686964035\n",
      "iteration 3886, dc_loss: 0.016696369275450706, tv_loss: 0.017040513455867767\n",
      "iteration 3887, dc_loss: 0.016696345061063766, tv_loss: 0.01704048551619053\n",
      "iteration 3888, dc_loss: 0.01669626496732235, tv_loss: 0.017040515318512917\n",
      "iteration 3889, dc_loss: 0.016696162521839142, tv_loss: 0.017040541395545006\n",
      "iteration 3890, dc_loss: 0.016696061939001083, tv_loss: 0.017040567472577095\n",
      "iteration 3891, dc_loss: 0.016695981845259666, tv_loss: 0.017040682956576347\n",
      "iteration 3892, dc_loss: 0.016695937141776085, tv_loss: 0.017040686681866646\n",
      "iteration 3893, dc_loss: 0.016695940867066383, tv_loss: 0.017040617763996124\n",
      "iteration 3894, dc_loss: 0.016695979982614517, tv_loss: 0.017040608450770378\n",
      "iteration 3895, dc_loss: 0.0166960209608078, tv_loss: 0.01704053394496441\n",
      "iteration 3896, dc_loss: 0.016696026548743248, tv_loss: 0.017040442675352097\n",
      "iteration 3897, dc_loss: 0.01669601909816265, tv_loss: 0.01704062707722187\n",
      "iteration 3898, dc_loss: 0.016695978119969368, tv_loss: 0.01704058237373829\n",
      "iteration 3899, dc_loss: 0.016695979982614517, tv_loss: 0.017040342092514038\n",
      "iteration 3900, dc_loss: 0.016696006059646606, tv_loss: 0.017040405422449112\n",
      "iteration 3901, dc_loss: 0.016696013510227203, tv_loss: 0.017040418460965157\n",
      "iteration 3902, dc_loss: 0.016696030274033546, tv_loss: 0.0170403141528368\n",
      "iteration 3903, dc_loss: 0.016696033999323845, tv_loss: 0.01704031601548195\n",
      "iteration 3904, dc_loss: 0.016696041449904442, tv_loss: 0.01704014465212822\n",
      "iteration 3905, dc_loss: 0.01669606752693653, tv_loss: 0.017040004953742027\n",
      "iteration 3906, dc_loss: 0.016696088016033173, tv_loss: 0.017040172591805458\n",
      "iteration 3907, dc_loss: 0.016696130856871605, tv_loss: 0.01704011857509613\n",
      "iteration 3908, dc_loss: 0.01669611595571041, tv_loss: 0.017039945349097252\n",
      "iteration 3909, dc_loss: 0.016696061939001083, tv_loss: 0.01704012230038643\n",
      "iteration 3910, dc_loss: 0.016695944592356682, tv_loss: 0.017040135338902473\n",
      "iteration 3911, dc_loss: 0.016695870086550713, tv_loss: 0.01704011671245098\n",
      "iteration 3912, dc_loss: 0.01669580489397049, tv_loss: 0.017040200531482697\n",
      "iteration 3913, dc_loss: 0.01669572852551937, tv_loss: 0.017040198668837547\n",
      "iteration 3914, dc_loss: 0.016695721074938774, tv_loss: 0.017040085047483444\n",
      "iteration 3915, dc_loss: 0.016695749014616013, tv_loss: 0.01704009436070919\n",
      "iteration 3916, dc_loss: 0.01669580489397049, tv_loss: 0.01703989878296852\n",
      "iteration 3917, dc_loss: 0.016695870086550713, tv_loss: 0.017039811238646507\n",
      "iteration 3918, dc_loss: 0.01669587939977646, tv_loss: 0.01703987456858158\n",
      "iteration 3919, dc_loss: 0.016695857048034668, tv_loss: 0.01703973487019539\n",
      "iteration 3920, dc_loss: 0.016695834696292877, tv_loss: 0.01703963801264763\n",
      "iteration 3921, dc_loss: 0.01669585146009922, tv_loss: 0.017039669677615166\n",
      "iteration 3922, dc_loss: 0.01669585146009922, tv_loss: 0.017039567232131958\n",
      "iteration 3923, dc_loss: 0.01669587567448616, tv_loss: 0.01703966036438942\n",
      "iteration 3924, dc_loss: 0.01669589802622795, tv_loss: 0.017039403319358826\n",
      "iteration 3925, dc_loss: 0.016695916652679443, tv_loss: 0.0170392245054245\n",
      "iteration 3926, dc_loss: 0.016695931553840637, tv_loss: 0.017039233818650246\n",
      "iteration 3927, dc_loss: 0.01669589802622795, tv_loss: 0.017039379104971886\n",
      "iteration 3928, dc_loss: 0.01669583097100258, tv_loss: 0.017039412632584572\n",
      "iteration 3929, dc_loss: 0.016695814207196236, tv_loss: 0.01703927479684353\n",
      "iteration 3930, dc_loss: 0.016695819795131683, tv_loss: 0.017039217054843903\n",
      "iteration 3931, dc_loss: 0.016695819795131683, tv_loss: 0.017039328813552856\n",
      "iteration 3932, dc_loss: 0.016695775091648102, tv_loss: 0.01703936979174614\n",
      "iteration 3933, dc_loss: 0.016695745289325714, tv_loss: 0.017039084807038307\n",
      "iteration 3934, dc_loss: 0.016695735976099968, tv_loss: 0.01703924499452114\n",
      "iteration 3935, dc_loss: 0.01669570989906788, tv_loss: 0.017039263620972633\n",
      "iteration 3936, dc_loss: 0.016695661470294, tv_loss: 0.01703931950032711\n",
      "iteration 3937, dc_loss: 0.01669563725590706, tv_loss: 0.01703925058245659\n",
      "iteration 3938, dc_loss: 0.01669565960764885, tv_loss: 0.017039256170392036\n",
      "iteration 3939, dc_loss: 0.01669568195939064, tv_loss: 0.017039118334650993\n",
      "iteration 3940, dc_loss: 0.01669573038816452, tv_loss: 0.017038915306329727\n",
      "iteration 3941, dc_loss: 0.01669575646519661, tv_loss: 0.01703910529613495\n",
      "iteration 3942, dc_loss: 0.01669573225080967, tv_loss: 0.017039118334650993\n",
      "iteration 3943, dc_loss: 0.016695689409971237, tv_loss: 0.01703883521258831\n",
      "iteration 3944, dc_loss: 0.016695626080036163, tv_loss: 0.017038827762007713\n",
      "iteration 3945, dc_loss: 0.016695590689778328, tv_loss: 0.01703898049890995\n",
      "iteration 3946, dc_loss: 0.01669556088745594, tv_loss: 0.017038989812135696\n",
      "iteration 3947, dc_loss: 0.016695547848939896, tv_loss: 0.01703891158103943\n",
      "iteration 3948, dc_loss: 0.016695573925971985, tv_loss: 0.017038898542523384\n",
      "iteration 3949, dc_loss: 0.016695590689778328, tv_loss: 0.017038874328136444\n",
      "iteration 3950, dc_loss: 0.016695598140358925, tv_loss: 0.017038976773619652\n",
      "iteration 3951, dc_loss: 0.01669558696448803, tv_loss: 0.017038919031620026\n",
      "iteration 3952, dc_loss: 0.016695555299520493, tv_loss: 0.017038732767105103\n",
      "iteration 3953, dc_loss: 0.01669556088745594, tv_loss: 0.017038753256201744\n",
      "iteration 3954, dc_loss: 0.016695525497198105, tv_loss: 0.017038719728589058\n",
      "iteration 3955, dc_loss: 0.01669549010694027, tv_loss: 0.017038656398653984\n",
      "iteration 3956, dc_loss: 0.016695478931069374, tv_loss: 0.017038723453879356\n",
      "iteration 3957, dc_loss: 0.01669544354081154, tv_loss: 0.017038708552718163\n",
      "iteration 3958, dc_loss: 0.01669544354081154, tv_loss: 0.017038675025105476\n",
      "iteration 3959, dc_loss: 0.01669548638164997, tv_loss: 0.017038816586136818\n",
      "iteration 3960, dc_loss: 0.016695521771907806, tv_loss: 0.01703861728310585\n",
      "iteration 3961, dc_loss: 0.01669551432132721, tv_loss: 0.017038492485880852\n",
      "iteration 3962, dc_loss: 0.01669546216726303, tv_loss: 0.017038438469171524\n",
      "iteration 3963, dc_loss: 0.016695383936166763, tv_loss: 0.017038585618138313\n",
      "iteration 3964, dc_loss: 0.016695363447070122, tv_loss: 0.017038539052009583\n",
      "iteration 3965, dc_loss: 0.016695376485586166, tv_loss: 0.017038432881236076\n",
      "iteration 3966, dc_loss: 0.016695424914360046, tv_loss: 0.01703840307891369\n",
      "iteration 3967, dc_loss: 0.016695525497198105, tv_loss: 0.017038313671946526\n",
      "iteration 3968, dc_loss: 0.016695620492100716, tv_loss: 0.017038097605109215\n",
      "iteration 3969, dc_loss: 0.016695650294423103, tv_loss: 0.01703811064362526\n",
      "iteration 3970, dc_loss: 0.016695626080036163, tv_loss: 0.01703810505568981\n",
      "iteration 3971, dc_loss: 0.016695525497198105, tv_loss: 0.017038213089108467\n",
      "iteration 3972, dc_loss: 0.01669538952410221, tv_loss: 0.0170383732765913\n",
      "iteration 3973, dc_loss: 0.016695279628038406, tv_loss: 0.01703849993646145\n",
      "iteration 3974, dc_loss: 0.01669522561132908, tv_loss: 0.01703830435872078\n",
      "iteration 3975, dc_loss: 0.016695234924554825, tv_loss: 0.017038101330399513\n",
      "iteration 3976, dc_loss: 0.016695281490683556, tv_loss: 0.01703815348446369\n",
      "iteration 3977, dc_loss: 0.016695348545908928, tv_loss: 0.01703830249607563\n",
      "iteration 3978, dc_loss: 0.016695398837327957, tv_loss: 0.017037950456142426\n",
      "iteration 3979, dc_loss: 0.01669544167816639, tv_loss: 0.017037931829690933\n",
      "iteration 3980, dc_loss: 0.016695445403456688, tv_loss: 0.017038093879818916\n",
      "iteration 3981, dc_loss: 0.01669541373848915, tv_loss: 0.017037875950336456\n",
      "iteration 3982, dc_loss: 0.016695348545908928, tv_loss: 0.017037706449627876\n",
      "iteration 3983, dc_loss: 0.016695279628038406, tv_loss: 0.01703803800046444\n",
      "iteration 3984, dc_loss: 0.01669524796307087, tv_loss: 0.017038004472851753\n",
      "iteration 3985, dc_loss: 0.016695218160748482, tv_loss: 0.01703796163201332\n",
      "iteration 3986, dc_loss: 0.016695231199264526, tv_loss: 0.017037974670529366\n",
      "iteration 3987, dc_loss: 0.016695303842425346, tv_loss: 0.017037758603692055\n",
      "iteration 3988, dc_loss: 0.016695357859134674, tv_loss: 0.017037680372595787\n",
      "iteration 3989, dc_loss: 0.016695355996489525, tv_loss: 0.017037665471434593\n",
      "iteration 3990, dc_loss: 0.016695307567715645, tv_loss: 0.017037713900208473\n",
      "iteration 3991, dc_loss: 0.016695288941264153, tv_loss: 0.017037708312273026\n",
      "iteration 3992, dc_loss: 0.016695277765393257, tv_loss: 0.01703769341111183\n",
      "iteration 3993, dc_loss: 0.016695290803909302, tv_loss: 0.017037639394402504\n",
      "iteration 3994, dc_loss: 0.01669527217745781, tv_loss: 0.01703754998743534\n",
      "iteration 3995, dc_loss: 0.01669520139694214, tv_loss: 0.01703767664730549\n",
      "iteration 3996, dc_loss: 0.016695134341716766, tv_loss: 0.01703774183988571\n",
      "iteration 3997, dc_loss: 0.016695046797394753, tv_loss: 0.017037643119692802\n",
      "iteration 3998, dc_loss: 0.016694987192749977, tv_loss: 0.017037764191627502\n",
      "iteration 3999, dc_loss: 0.01669500023126602, tv_loss: 0.01703774370253086\n",
      "iteration 4000, dc_loss: 0.01669508032500744, tv_loss: 0.017037590965628624\n",
      "PSNR Value mt1: 33.09335996435637\n",
      "SSIM Value mt1: 0.7467458261349804\n",
      "tensor([[[6.0409e-04, 7.4484e-05]]]) tensor([[[1.2870, 1.2171]]])\n",
      "torch.Size([1, 1, 2]) torch.Size([1, 1, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| params.keys(): dict_keys(['grid'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "iteration 1, dc_loss: 2.9717695713043213, tv_loss: 0.0\n",
      "iteration 2, dc_loss: 2.911811113357544, tv_loss: 0.0002560284920036793\n",
      "iteration 3, dc_loss: 2.8529512882232666, tv_loss: 0.0005029813037253916\n",
      "iteration 4, dc_loss: 2.7951695919036865, tv_loss: 0.0007473558653146029\n",
      "iteration 5, dc_loss: 2.738449811935425, tv_loss: 0.0009907493367791176\n",
      "iteration 6, dc_loss: 2.682776927947998, tv_loss: 0.0012332746991887689\n",
      "iteration 7, dc_loss: 2.628136157989502, tv_loss: 0.0014739499893039465\n",
      "iteration 8, dc_loss: 2.5745139122009277, tv_loss: 0.0017143618315458298\n",
      "iteration 9, dc_loss: 2.52189564704895, tv_loss: 0.0019502121722325683\n",
      "iteration 10, dc_loss: 2.470266580581665, tv_loss: 0.0021846273448318243\n",
      "iteration 11, dc_loss: 2.4196126461029053, tv_loss: 0.0024153287522494793\n",
      "iteration 12, dc_loss: 2.3699188232421875, tv_loss: 0.0026421924121677876\n",
      "iteration 13, dc_loss: 2.3211708068847656, tv_loss: 0.002866727067157626\n",
      "iteration 14, dc_loss: 2.273355007171631, tv_loss: 0.0030872891657054424\n",
      "iteration 15, dc_loss: 2.226456880569458, tv_loss: 0.0033049490302801132\n",
      "iteration 16, dc_loss: 2.1804630756378174, tv_loss: 0.003518634708598256\n",
      "iteration 17, dc_loss: 2.135359287261963, tv_loss: 0.003728908021003008\n",
      "iteration 18, dc_loss: 2.0911333560943604, tv_loss: 0.003937370143830776\n",
      "iteration 19, dc_loss: 2.0477707386016846, tv_loss: 0.00414241524413228\n",
      "iteration 20, dc_loss: 2.005258798599243, tv_loss: 0.004344334360212088\n",
      "iteration 21, dc_loss: 1.9635837078094482, tv_loss: 0.004542990121990442\n",
      "iteration 22, dc_loss: 1.9227321147918701, tv_loss: 0.004737925250083208\n",
      "iteration 23, dc_loss: 1.8826907873153687, tv_loss: 0.004930706694722176\n",
      "iteration 24, dc_loss: 1.8434467315673828, tv_loss: 0.005121857859194279\n",
      "iteration 25, dc_loss: 1.8049864768981934, tv_loss: 0.005310004111379385\n",
      "iteration 26, dc_loss: 1.7672966718673706, tv_loss: 0.005493167322129011\n",
      "iteration 27, dc_loss: 1.7303647994995117, tv_loss: 0.0056753032840788364\n",
      "iteration 28, dc_loss: 1.694177508354187, tv_loss: 0.005854254122823477\n",
      "iteration 29, dc_loss: 1.6587218046188354, tv_loss: 0.00603024335578084\n",
      "iteration 30, dc_loss: 1.6239851713180542, tv_loss: 0.006202810909599066\n",
      "iteration 31, dc_loss: 1.5899546146392822, tv_loss: 0.006373939570039511\n",
      "iteration 32, dc_loss: 1.5566176176071167, tv_loss: 0.006542516872286797\n",
      "iteration 33, dc_loss: 1.523961067199707, tv_loss: 0.006708499509841204\n",
      "iteration 34, dc_loss: 1.4919729232788086, tv_loss: 0.0068711694329977036\n",
      "iteration 35, dc_loss: 1.460640788078308, tv_loss: 0.007031463086605072\n",
      "iteration 36, dc_loss: 1.4299521446228027, tv_loss: 0.007190539501607418\n",
      "iteration 37, dc_loss: 1.3998953104019165, tv_loss: 0.0073472801595926285\n",
      "iteration 38, dc_loss: 1.3704584836959839, tv_loss: 0.007501103915274143\n",
      "iteration 39, dc_loss: 1.3416298627853394, tv_loss: 0.0076523153111338615\n",
      "iteration 40, dc_loss: 1.313397765159607, tv_loss: 0.007801895961165428\n",
      "iteration 41, dc_loss: 1.2857513427734375, tv_loss: 0.00794984307140112\n",
      "iteration 42, dc_loss: 1.2586790323257446, tv_loss: 0.008095636032521725\n",
      "iteration 43, dc_loss: 1.2321698665618896, tv_loss: 0.008239579387009144\n",
      "iteration 44, dc_loss: 1.2062134742736816, tv_loss: 0.008382335305213928\n",
      "iteration 45, dc_loss: 1.1807986497879028, tv_loss: 0.00852289330214262\n",
      "iteration 46, dc_loss: 1.1559147834777832, tv_loss: 0.008661202155053616\n",
      "iteration 47, dc_loss: 1.1315518617630005, tv_loss: 0.008796930313110352\n",
      "iteration 48, dc_loss: 1.1076996326446533, tv_loss: 0.008932121098041534\n",
      "iteration 49, dc_loss: 1.0843477249145508, tv_loss: 0.009065493009984493\n",
      "iteration 50, dc_loss: 1.0614863634109497, tv_loss: 0.00919691938906908\n",
      "iteration 51, dc_loss: 1.039105772972107, tv_loss: 0.009325915016233921\n",
      "iteration 52, dc_loss: 1.0171964168548584, tv_loss: 0.009452261961996555\n",
      "iteration 53, dc_loss: 0.9957486391067505, tv_loss: 0.009578756056725979\n",
      "iteration 54, dc_loss: 0.9747532606124878, tv_loss: 0.009704766795039177\n",
      "iteration 55, dc_loss: 0.9542011618614197, tv_loss: 0.009828750044107437\n",
      "iteration 56, dc_loss: 0.934083104133606, tv_loss: 0.009949611499905586\n",
      "iteration 57, dc_loss: 0.914390504360199, tv_loss: 0.010069512762129307\n",
      "iteration 58, dc_loss: 0.8951146006584167, tv_loss: 0.010188251733779907\n",
      "iteration 59, dc_loss: 0.8762468695640564, tv_loss: 0.01030552014708519\n",
      "iteration 60, dc_loss: 0.857778787612915, tv_loss: 0.01042134128510952\n",
      "iteration 61, dc_loss: 0.839702308177948, tv_loss: 0.010536202229559422\n",
      "iteration 62, dc_loss: 0.8220094442367554, tv_loss: 0.010648969560861588\n",
      "iteration 63, dc_loss: 0.8046919107437134, tv_loss: 0.0107596879824996\n",
      "iteration 64, dc_loss: 0.7877423167228699, tv_loss: 0.010870483703911304\n",
      "iteration 65, dc_loss: 0.7711527943611145, tv_loss: 0.010979575105011463\n",
      "iteration 66, dc_loss: 0.7549158930778503, tv_loss: 0.011085434816777706\n",
      "iteration 67, dc_loss: 0.73902428150177, tv_loss: 0.011192049831151962\n",
      "iteration 68, dc_loss: 0.7234706282615662, tv_loss: 0.011297527700662613\n",
      "iteration 69, dc_loss: 0.7082480788230896, tv_loss: 0.011400973424315453\n",
      "iteration 70, dc_loss: 0.6933494210243225, tv_loss: 0.01150151900947094\n",
      "iteration 71, dc_loss: 0.6787680983543396, tv_loss: 0.011603603139519691\n",
      "iteration 72, dc_loss: 0.6644973754882812, tv_loss: 0.011704031378030777\n",
      "iteration 73, dc_loss: 0.6505306959152222, tv_loss: 0.011802181601524353\n",
      "iteration 74, dc_loss: 0.6368616819381714, tv_loss: 0.01189857255667448\n",
      "iteration 75, dc_loss: 0.6234840154647827, tv_loss: 0.011995143257081509\n",
      "iteration 76, dc_loss: 0.6103916168212891, tv_loss: 0.012091336771845818\n",
      "iteration 77, dc_loss: 0.5975784063339233, tv_loss: 0.01218461710959673\n",
      "iteration 78, dc_loss: 0.5850385427474976, tv_loss: 0.01227659173309803\n",
      "iteration 79, dc_loss: 0.5727663636207581, tv_loss: 0.01236817892640829\n",
      "iteration 80, dc_loss: 0.5607560276985168, tv_loss: 0.0124592250213027\n",
      "iteration 81, dc_loss: 0.5490021705627441, tv_loss: 0.01254922803491354\n",
      "iteration 82, dc_loss: 0.5374993681907654, tv_loss: 0.01263761892914772\n",
      "iteration 83, dc_loss: 0.5262421369552612, tv_loss: 0.012723703868687153\n",
      "iteration 84, dc_loss: 0.5152254700660706, tv_loss: 0.012809629552066326\n",
      "iteration 85, dc_loss: 0.5044442415237427, tv_loss: 0.012896415777504444\n",
      "iteration 86, dc_loss: 0.49389350414276123, tv_loss: 0.012980482541024685\n",
      "iteration 87, dc_loss: 0.48356831073760986, tv_loss: 0.013063417747616768\n",
      "iteration 88, dc_loss: 0.4734640121459961, tv_loss: 0.013145196251571178\n",
      "iteration 89, dc_loss: 0.4635757505893707, tv_loss: 0.013225323520600796\n",
      "iteration 90, dc_loss: 0.4538991451263428, tv_loss: 0.013306704349815845\n",
      "iteration 91, dc_loss: 0.4444296061992645, tv_loss: 0.013386654667556286\n",
      "iteration 92, dc_loss: 0.4351627826690674, tv_loss: 0.013464237563312054\n",
      "iteration 93, dc_loss: 0.4260943531990051, tv_loss: 0.013540877029299736\n",
      "iteration 94, dc_loss: 0.41722017526626587, tv_loss: 0.013616538606584072\n",
      "iteration 95, dc_loss: 0.40853604674339294, tv_loss: 0.013692278414964676\n",
      "iteration 96, dc_loss: 0.40003806352615356, tv_loss: 0.013767597265541553\n",
      "iteration 97, dc_loss: 0.3917221426963806, tv_loss: 0.01384096872061491\n",
      "iteration 98, dc_loss: 0.38358449935913086, tv_loss: 0.013912573456764221\n",
      "iteration 99, dc_loss: 0.37562134861946106, tv_loss: 0.013984750024974346\n",
      "iteration 100, dc_loss: 0.36782896518707275, tv_loss: 0.01405565906316042\n",
      "iteration 101, dc_loss: 0.3602038323879242, tv_loss: 0.014125368557870388\n",
      "iteration 102, dc_loss: 0.3527422845363617, tv_loss: 0.01419379934668541\n",
      "iteration 103, dc_loss: 0.34544092416763306, tv_loss: 0.014261567033827305\n",
      "iteration 104, dc_loss: 0.3382962644100189, tv_loss: 0.014329314231872559\n",
      "iteration 105, dc_loss: 0.3313050866127014, tv_loss: 0.014395641162991524\n",
      "iteration 106, dc_loss: 0.3244640827178955, tv_loss: 0.014459789730608463\n",
      "iteration 107, dc_loss: 0.3177700936794281, tv_loss: 0.014524189755320549\n",
      "iteration 108, dc_loss: 0.3112199604511261, tv_loss: 0.014587773941457272\n",
      "iteration 109, dc_loss: 0.30481064319610596, tv_loss: 0.014650980941951275\n",
      "iteration 110, dc_loss: 0.2985392212867737, tv_loss: 0.014711737632751465\n",
      "iteration 111, dc_loss: 0.29240262508392334, tv_loss: 0.014772983267903328\n",
      "iteration 112, dc_loss: 0.2863980531692505, tv_loss: 0.014833655208349228\n",
      "iteration 113, dc_loss: 0.28052273392677307, tv_loss: 0.014892633073031902\n",
      "iteration 114, dc_loss: 0.27477389574050903, tv_loss: 0.014949863776564598\n",
      "iteration 115, dc_loss: 0.26914888620376587, tv_loss: 0.015008349902927876\n",
      "iteration 116, dc_loss: 0.2636450231075287, tv_loss: 0.015066523104906082\n",
      "iteration 117, dc_loss: 0.25825977325439453, tv_loss: 0.015121634118258953\n",
      "iteration 118, dc_loss: 0.25299057364463806, tv_loss: 0.015175266191363335\n",
      "iteration 119, dc_loss: 0.24783490598201752, tv_loss: 0.015232215635478497\n",
      "iteration 120, dc_loss: 0.24279047548770905, tv_loss: 0.015286932699382305\n",
      "iteration 121, dc_loss: 0.23785485327243805, tv_loss: 0.015338723547756672\n",
      "iteration 122, dc_loss: 0.23302577435970306, tv_loss: 0.015390044078230858\n",
      "iteration 123, dc_loss: 0.22830095887184143, tv_loss: 0.015442831441760063\n",
      "iteration 124, dc_loss: 0.22367815673351288, tv_loss: 0.015494751743972301\n",
      "iteration 125, dc_loss: 0.21915526688098907, tv_loss: 0.01554430928081274\n",
      "iteration 126, dc_loss: 0.21473011374473572, tv_loss: 0.015593254007399082\n",
      "iteration 127, dc_loss: 0.21040059626102448, tv_loss: 0.015641679987311363\n",
      "iteration 128, dc_loss: 0.20616468787193298, tv_loss: 0.015691114589571953\n",
      "iteration 129, dc_loss: 0.20202039182186127, tv_loss: 0.015738926827907562\n",
      "iteration 130, dc_loss: 0.1979658156633377, tv_loss: 0.015785781666636467\n",
      "iteration 131, dc_loss: 0.1939990520477295, tv_loss: 0.01583130471408367\n",
      "iteration 132, dc_loss: 0.19011816382408142, tv_loss: 0.015875784680247307\n",
      "iteration 133, dc_loss: 0.18632137775421143, tv_loss: 0.01592179574072361\n",
      "iteration 134, dc_loss: 0.18260686099529266, tv_loss: 0.0159661415964365\n",
      "iteration 135, dc_loss: 0.17897288501262665, tv_loss: 0.01600891351699829\n",
      "iteration 136, dc_loss: 0.17541775107383728, tv_loss: 0.016051381826400757\n",
      "iteration 137, dc_loss: 0.17193977534770966, tv_loss: 0.01609383150935173\n",
      "iteration 138, dc_loss: 0.16853725910186768, tv_loss: 0.01613478921353817\n",
      "iteration 139, dc_loss: 0.1652085930109024, tv_loss: 0.016176290810108185\n",
      "iteration 140, dc_loss: 0.16195222735404968, tv_loss: 0.016216883435845375\n",
      "iteration 141, dc_loss: 0.15876665711402893, tv_loss: 0.016257120296359062\n",
      "iteration 142, dc_loss: 0.15565036237239838, tv_loss: 0.01629561558365822\n",
      "iteration 143, dc_loss: 0.15260182321071625, tv_loss: 0.01633373647928238\n",
      "iteration 144, dc_loss: 0.14961959421634674, tv_loss: 0.016372131183743477\n",
      "iteration 145, dc_loss: 0.1467023491859436, tv_loss: 0.016410114243626595\n",
      "iteration 146, dc_loss: 0.14384862780570984, tv_loss: 0.01644696481525898\n",
      "iteration 147, dc_loss: 0.141057088971138, tv_loss: 0.016482559964060783\n",
      "iteration 148, dc_loss: 0.13832639157772064, tv_loss: 0.016519227996468544\n",
      "iteration 149, dc_loss: 0.1356552392244339, tv_loss: 0.016554519534111023\n",
      "iteration 150, dc_loss: 0.1330423653125763, tv_loss: 0.01658945344388485\n",
      "iteration 151, dc_loss: 0.13048651814460754, tv_loss: 0.016623690724372864\n",
      "iteration 152, dc_loss: 0.12798653542995453, tv_loss: 0.016657397150993347\n",
      "iteration 153, dc_loss: 0.12554116547107697, tv_loss: 0.016690976917743683\n",
      "iteration 154, dc_loss: 0.12314924597740173, tv_loss: 0.01672366075217724\n",
      "iteration 155, dc_loss: 0.12080967426300049, tv_loss: 0.016756214201450348\n",
      "iteration 156, dc_loss: 0.11852126568555832, tv_loss: 0.016788046807050705\n",
      "iteration 157, dc_loss: 0.11628293246030807, tv_loss: 0.01681949570775032\n",
      "iteration 158, dc_loss: 0.11409363895654678, tv_loss: 0.01685110107064247\n",
      "iteration 159, dc_loss: 0.11195234209299088, tv_loss: 0.016881786286830902\n",
      "iteration 160, dc_loss: 0.10985799878835678, tv_loss: 0.01691160351037979\n",
      "iteration 161, dc_loss: 0.10780956596136093, tv_loss: 0.016941577196121216\n",
      "iteration 162, dc_loss: 0.10580610483884811, tv_loss: 0.016970448195934296\n",
      "iteration 163, dc_loss: 0.10384662449359894, tv_loss: 0.01699950359761715\n",
      "iteration 164, dc_loss: 0.10193021595478058, tv_loss: 0.01702813431620598\n",
      "iteration 165, dc_loss: 0.10005595535039902, tv_loss: 0.017055945470929146\n",
      "iteration 166, dc_loss: 0.09822291135787964, tv_loss: 0.017083585262298584\n",
      "iteration 167, dc_loss: 0.0964302271604538, tv_loss: 0.017110222950577736\n",
      "iteration 168, dc_loss: 0.09467702358961105, tv_loss: 0.017137113958597183\n",
      "iteration 169, dc_loss: 0.09296246618032455, tv_loss: 0.017163293436169624\n",
      "iteration 170, dc_loss: 0.09128569811582565, tv_loss: 0.01718871109187603\n",
      "iteration 171, dc_loss: 0.08964591473340988, tv_loss: 0.017213981598615646\n",
      "iteration 172, dc_loss: 0.08804229646921158, tv_loss: 0.017238935455679893\n",
      "iteration 173, dc_loss: 0.08647411316633224, tv_loss: 0.0172637440264225\n",
      "iteration 174, dc_loss: 0.084940604865551, tv_loss: 0.017287278547883034\n",
      "iteration 175, dc_loss: 0.08344101905822754, tv_loss: 0.01731061190366745\n",
      "iteration 176, dc_loss: 0.08197463303804398, tv_loss: 0.017334366217255592\n",
      "iteration 177, dc_loss: 0.08054066449403763, tv_loss: 0.017357051372528076\n",
      "iteration 178, dc_loss: 0.07913847267627716, tv_loss: 0.017378946766257286\n",
      "iteration 179, dc_loss: 0.07776731997728348, tv_loss: 0.017401423305273056\n",
      "iteration 180, dc_loss: 0.07642660290002823, tv_loss: 0.01742391474545002\n",
      "iteration 181, dc_loss: 0.0751156210899353, tv_loss: 0.017445271834731102\n",
      "iteration 182, dc_loss: 0.07383372634649277, tv_loss: 0.017465872690081596\n",
      "iteration 183, dc_loss: 0.07258031517267227, tv_loss: 0.017486512660980225\n",
      "iteration 184, dc_loss: 0.07135476171970367, tv_loss: 0.017507070675492287\n",
      "iteration 185, dc_loss: 0.07015646994113922, tv_loss: 0.017527474090456963\n",
      "iteration 186, dc_loss: 0.06898485124111176, tv_loss: 0.017547346651554108\n",
      "iteration 187, dc_loss: 0.06783932447433472, tv_loss: 0.017566725611686707\n",
      "iteration 188, dc_loss: 0.06671933829784393, tv_loss: 0.017586221918463707\n",
      "iteration 189, dc_loss: 0.06562429666519165, tv_loss: 0.017604881897568703\n",
      "iteration 190, dc_loss: 0.06455370783805847, tv_loss: 0.01762338913977146\n",
      "iteration 191, dc_loss: 0.06350699812173843, tv_loss: 0.017642317339777946\n",
      "iteration 192, dc_loss: 0.06248366832733154, tv_loss: 0.01766081713140011\n",
      "iteration 193, dc_loss: 0.061483196914196014, tv_loss: 0.017677947878837585\n",
      "iteration 194, dc_loss: 0.06050512194633484, tv_loss: 0.017695140093564987\n",
      "iteration 195, dc_loss: 0.05954896658658981, tv_loss: 0.01771324686706066\n",
      "iteration 196, dc_loss: 0.058614205569028854, tv_loss: 0.017730500549077988\n",
      "iteration 197, dc_loss: 0.057700369507074356, tv_loss: 0.01774657517671585\n",
      "iteration 198, dc_loss: 0.05680704489350319, tv_loss: 0.017762362957000732\n",
      "iteration 199, dc_loss: 0.05593377351760864, tv_loss: 0.01777874492108822\n",
      "iteration 200, dc_loss: 0.05508013069629669, tv_loss: 0.01779475435614586\n",
      "iteration 201, dc_loss: 0.054245639592409134, tv_loss: 0.01780991069972515\n",
      "iteration 202, dc_loss: 0.053429897874593735, tv_loss: 0.01782464236021042\n",
      "iteration 203, dc_loss: 0.05263251066207886, tv_loss: 0.01783956214785576\n",
      "iteration 204, dc_loss: 0.051853079348802567, tv_loss: 0.017855364829301834\n",
      "iteration 205, dc_loss: 0.051091209053993225, tv_loss: 0.01786995865404606\n",
      "iteration 206, dc_loss: 0.0503465011715889, tv_loss: 0.01788312941789627\n",
      "iteration 207, dc_loss: 0.049618590623140335, tv_loss: 0.017896657809615135\n",
      "iteration 208, dc_loss: 0.04890710115432739, tv_loss: 0.017910869792103767\n",
      "iteration 209, dc_loss: 0.04821166396141052, tv_loss: 0.01792418770492077\n",
      "iteration 210, dc_loss: 0.04753192514181137, tv_loss: 0.017936132848262787\n",
      "iteration 211, dc_loss: 0.04686753824353218, tv_loss: 0.01794895902276039\n",
      "iteration 212, dc_loss: 0.04621817544102669, tv_loss: 0.017962399870157242\n",
      "iteration 213, dc_loss: 0.045583512634038925, tv_loss: 0.017974328249692917\n",
      "iteration 214, dc_loss: 0.044963251799345016, tv_loss: 0.017985686659812927\n",
      "iteration 215, dc_loss: 0.0443570576608181, tv_loss: 0.017998240888118744\n",
      "iteration 216, dc_loss: 0.0437646359205246, tv_loss: 0.018010636791586876\n",
      "iteration 217, dc_loss: 0.04318564757704735, tv_loss: 0.018021663650870323\n",
      "iteration 218, dc_loss: 0.04261978715658188, tv_loss: 0.018032345920801163\n",
      "iteration 219, dc_loss: 0.042066752910614014, tv_loss: 0.018043605610728264\n",
      "iteration 220, dc_loss: 0.041526298969984055, tv_loss: 0.018054932355880737\n",
      "iteration 221, dc_loss: 0.040998127311468124, tv_loss: 0.0180653166025877\n",
      "iteration 222, dc_loss: 0.04048198089003563, tv_loss: 0.018075494095683098\n",
      "iteration 223, dc_loss: 0.03997761011123657, tv_loss: 0.018085770308971405\n",
      "iteration 224, dc_loss: 0.03948470950126648, tv_loss: 0.018096109852194786\n",
      "iteration 225, dc_loss: 0.03900301828980446, tv_loss: 0.018105771392583847\n",
      "iteration 226, dc_loss: 0.0385323241353035, tv_loss: 0.01811516098678112\n",
      "iteration 227, dc_loss: 0.0380723737180233, tv_loss: 0.018125025555491447\n",
      "iteration 228, dc_loss: 0.03762292116880417, tv_loss: 0.018134010955691338\n",
      "iteration 229, dc_loss: 0.03718375787138939, tv_loss: 0.01814323477447033\n",
      "iteration 230, dc_loss: 0.03675462678074837, tv_loss: 0.01815234310925007\n",
      "iteration 231, dc_loss: 0.03633531183004379, tv_loss: 0.01816137693822384\n",
      "iteration 232, dc_loss: 0.03592555597424507, tv_loss: 0.018169915303587914\n",
      "iteration 233, dc_loss: 0.03552519157528877, tv_loss: 0.018178502097725868\n",
      "iteration 234, dc_loss: 0.03513401374220848, tv_loss: 0.018187006935477257\n",
      "iteration 235, dc_loss: 0.034751810133457184, tv_loss: 0.018195100128650665\n",
      "iteration 236, dc_loss: 0.034378379583358765, tv_loss: 0.018202899023890495\n",
      "iteration 237, dc_loss: 0.03401351720094681, tv_loss: 0.018210334703326225\n",
      "iteration 238, dc_loss: 0.0336570143699646, tv_loss: 0.018218424171209335\n",
      "iteration 239, dc_loss: 0.033308710902929306, tv_loss: 0.018225861713290215\n",
      "iteration 240, dc_loss: 0.032968390733003616, tv_loss: 0.018232975155115128\n",
      "iteration 241, dc_loss: 0.032635897397994995, tv_loss: 0.018240181729197502\n",
      "iteration 242, dc_loss: 0.03231107071042061, tv_loss: 0.018247343599796295\n",
      "iteration 243, dc_loss: 0.031993698328733444, tv_loss: 0.01825438253581524\n",
      "iteration 244, dc_loss: 0.03168362379074097, tv_loss: 0.018260786309838295\n",
      "iteration 245, dc_loss: 0.031380679458379745, tv_loss: 0.018267741426825523\n",
      "iteration 246, dc_loss: 0.031084690243005753, tv_loss: 0.018274404108524323\n",
      "iteration 247, dc_loss: 0.030795499682426453, tv_loss: 0.018280109390616417\n",
      "iteration 248, dc_loss: 0.030512964352965355, tv_loss: 0.018286673352122307\n",
      "iteration 249, dc_loss: 0.030236931517720222, tv_loss: 0.01829289086163044\n",
      "iteration 250, dc_loss: 0.02996726892888546, tv_loss: 0.018298758193850517\n",
      "iteration 251, dc_loss: 0.02970385178923607, tv_loss: 0.01830451749265194\n",
      "iteration 252, dc_loss: 0.029446538537740707, tv_loss: 0.018310166895389557\n",
      "iteration 253, dc_loss: 0.029195211827754974, tv_loss: 0.018315454944968224\n",
      "iteration 254, dc_loss: 0.028949711471796036, tv_loss: 0.01832086592912674\n",
      "iteration 255, dc_loss: 0.02870986983180046, tv_loss: 0.018326200544834137\n",
      "iteration 256, dc_loss: 0.028475560247898102, tv_loss: 0.0183307733386755\n",
      "iteration 257, dc_loss: 0.02824665792286396, tv_loss: 0.018336094915866852\n",
      "iteration 258, dc_loss: 0.028023043647408485, tv_loss: 0.018341105431318283\n",
      "iteration 259, dc_loss: 0.027804594486951828, tv_loss: 0.018345829099416733\n",
      "iteration 260, dc_loss: 0.02759118191897869, tv_loss: 0.01835007220506668\n",
      "iteration 261, dc_loss: 0.02738269977271557, tv_loss: 0.01835460215806961\n",
      "iteration 262, dc_loss: 0.027179032564163208, tv_loss: 0.018359417095780373\n",
      "iteration 263, dc_loss: 0.0269800815731287, tv_loss: 0.018363654613494873\n",
      "iteration 264, dc_loss: 0.02678573690354824, tv_loss: 0.018367744982242584\n",
      "iteration 265, dc_loss: 0.02659589797258377, tv_loss: 0.01837165653705597\n",
      "iteration 266, dc_loss: 0.026410458609461784, tv_loss: 0.018375974148511887\n",
      "iteration 267, dc_loss: 0.02622932940721512, tv_loss: 0.018379824236035347\n",
      "iteration 268, dc_loss: 0.026052430272102356, tv_loss: 0.01838270202279091\n",
      "iteration 269, dc_loss: 0.02587965317070484, tv_loss: 0.018386559560894966\n",
      "iteration 270, dc_loss: 0.02571086771786213, tv_loss: 0.018390342593193054\n",
      "iteration 271, dc_loss: 0.025545958429574966, tv_loss: 0.01839376427233219\n",
      "iteration 272, dc_loss: 0.025384875014424324, tv_loss: 0.018396807834506035\n",
      "iteration 273, dc_loss: 0.02522752806544304, tv_loss: 0.018400058150291443\n",
      "iteration 274, dc_loss: 0.025073843076825142, tv_loss: 0.01840350404381752\n",
      "iteration 275, dc_loss: 0.024923723191022873, tv_loss: 0.018406599760055542\n",
      "iteration 276, dc_loss: 0.024777067825198174, tv_loss: 0.018409384414553642\n",
      "iteration 277, dc_loss: 0.024633804336190224, tv_loss: 0.01841249130666256\n",
      "iteration 278, dc_loss: 0.024493860080838203, tv_loss: 0.0184152964502573\n",
      "iteration 279, dc_loss: 0.02435716986656189, tv_loss: 0.018418174237012863\n",
      "iteration 280, dc_loss: 0.024223661050200462, tv_loss: 0.018420696258544922\n",
      "iteration 281, dc_loss: 0.024093255400657654, tv_loss: 0.018423182889819145\n",
      "iteration 282, dc_loss: 0.02396586909890175, tv_loss: 0.018425557762384415\n",
      "iteration 283, dc_loss: 0.023841431364417076, tv_loss: 0.018427740782499313\n",
      "iteration 284, dc_loss: 0.02371986024081707, tv_loss: 0.01843010075390339\n",
      "iteration 285, dc_loss: 0.023601077497005463, tv_loss: 0.018432771787047386\n",
      "iteration 286, dc_loss: 0.023485038429498672, tv_loss: 0.01843484863638878\n",
      "iteration 287, dc_loss: 0.023371702060103416, tv_loss: 0.018436366692185402\n",
      "iteration 288, dc_loss: 0.023261001333594322, tv_loss: 0.018438635393977165\n",
      "iteration 289, dc_loss: 0.023152874782681465, tv_loss: 0.018440743908286095\n",
      "iteration 290, dc_loss: 0.023047272115945816, tv_loss: 0.01844276674091816\n",
      "iteration 291, dc_loss: 0.02294410765171051, tv_loss: 0.018444355577230453\n",
      "iteration 292, dc_loss: 0.022843338549137115, tv_loss: 0.018445787951350212\n",
      "iteration 293, dc_loss: 0.022744912654161453, tv_loss: 0.018447792157530785\n",
      "iteration 294, dc_loss: 0.022648757323622704, tv_loss: 0.01844942942261696\n",
      "iteration 295, dc_loss: 0.0225547906011343, tv_loss: 0.018450597301125526\n",
      "iteration 296, dc_loss: 0.022462990134954453, tv_loss: 0.01845180056989193\n",
      "iteration 297, dc_loss: 0.02237330749630928, tv_loss: 0.018453463912010193\n",
      "iteration 298, dc_loss: 0.022285696119070053, tv_loss: 0.018454737961292267\n",
      "iteration 299, dc_loss: 0.02220010757446289, tv_loss: 0.018456168472766876\n",
      "iteration 300, dc_loss: 0.022116487845778465, tv_loss: 0.018457133322954178\n",
      "iteration 301, dc_loss: 0.022034792229533195, tv_loss: 0.018458135426044464\n",
      "iteration 302, dc_loss: 0.02195497415959835, tv_loss: 0.01845918409526348\n",
      "iteration 303, dc_loss: 0.021877000108361244, tv_loss: 0.018460383638739586\n",
      "iteration 304, dc_loss: 0.021800843998789787, tv_loss: 0.018461264669895172\n",
      "iteration 305, dc_loss: 0.021726442500948906, tv_loss: 0.018462127074599266\n",
      "iteration 306, dc_loss: 0.021653741598129272, tv_loss: 0.018462812528014183\n",
      "iteration 307, dc_loss: 0.0215827114880085, tv_loss: 0.018463458865880966\n",
      "iteration 308, dc_loss: 0.02151329256594181, tv_loss: 0.01846417598426342\n",
      "iteration 309, dc_loss: 0.02144544944167137, tv_loss: 0.01846497133374214\n",
      "iteration 310, dc_loss: 0.02137918397784233, tv_loss: 0.018465660512447357\n",
      "iteration 311, dc_loss: 0.021314434707164764, tv_loss: 0.018466003239154816\n",
      "iteration 312, dc_loss: 0.02125115692615509, tv_loss: 0.01846652291715145\n",
      "iteration 313, dc_loss: 0.021189337596297264, tv_loss: 0.01846694014966488\n",
      "iteration 314, dc_loss: 0.021128928288817406, tv_loss: 0.01846766658127308\n",
      "iteration 315, dc_loss: 0.021069884300231934, tv_loss: 0.01846807636320591\n",
      "iteration 316, dc_loss: 0.02101217955350876, tv_loss: 0.018468009307980537\n",
      "iteration 317, dc_loss: 0.020955778658390045, tv_loss: 0.01846836507320404\n",
      "iteration 318, dc_loss: 0.02090066485106945, tv_loss: 0.018468936905264854\n",
      "iteration 319, dc_loss: 0.02084679715335369, tv_loss: 0.018469154834747314\n",
      "iteration 320, dc_loss: 0.020794125273823738, tv_loss: 0.01846907287836075\n",
      "iteration 321, dc_loss: 0.02074265480041504, tv_loss: 0.018469402566552162\n",
      "iteration 322, dc_loss: 0.0206923671066761, tv_loss: 0.018469806760549545\n",
      "iteration 323, dc_loss: 0.020643247291445732, tv_loss: 0.018469613045454025\n",
      "iteration 324, dc_loss: 0.020595263689756393, tv_loss: 0.01846919395029545\n",
      "iteration 325, dc_loss: 0.020548369735479355, tv_loss: 0.018468936905264854\n",
      "iteration 326, dc_loss: 0.02050253003835678, tv_loss: 0.018468840047717094\n",
      "iteration 327, dc_loss: 0.020457718521356583, tv_loss: 0.018469007685780525\n",
      "iteration 328, dc_loss: 0.020413894206285477, tv_loss: 0.018468515947461128\n",
      "iteration 329, dc_loss: 0.020371008664369583, tv_loss: 0.018468061462044716\n",
      "iteration 330, dc_loss: 0.0203290656208992, tv_loss: 0.018468154594302177\n",
      "iteration 331, dc_loss: 0.02028806321322918, tv_loss: 0.01846800558269024\n",
      "iteration 332, dc_loss: 0.02024800516664982, tv_loss: 0.018467608839273453\n",
      "iteration 333, dc_loss: 0.020208828151226044, tv_loss: 0.018467221409082413\n",
      "iteration 334, dc_loss: 0.020170513540506363, tv_loss: 0.01846686378121376\n",
      "iteration 335, dc_loss: 0.020133059471845627, tv_loss: 0.018466470763087273\n",
      "iteration 336, dc_loss: 0.02009645663201809, tv_loss: 0.018465667963027954\n",
      "iteration 337, dc_loss: 0.020060671493411064, tv_loss: 0.018465127795934677\n",
      "iteration 338, dc_loss: 0.020025691017508507, tv_loss: 0.01846468076109886\n",
      "iteration 339, dc_loss: 0.019991489127278328, tv_loss: 0.018464097753167152\n",
      "iteration 340, dc_loss: 0.019958049058914185, tv_loss: 0.018463164567947388\n",
      "iteration 341, dc_loss: 0.01992534101009369, tv_loss: 0.018462160602211952\n",
      "iteration 342, dc_loss: 0.01989334262907505, tv_loss: 0.01846182532608509\n",
      "iteration 343, dc_loss: 0.019862022250890732, tv_loss: 0.018460992723703384\n",
      "iteration 344, dc_loss: 0.019831383600831032, tv_loss: 0.01846008375287056\n",
      "iteration 345, dc_loss: 0.019801411777734756, tv_loss: 0.018459366634488106\n",
      "iteration 346, dc_loss: 0.01977209933102131, tv_loss: 0.018458746373653412\n",
      "iteration 347, dc_loss: 0.019743435084819794, tv_loss: 0.018457653000950813\n",
      "iteration 348, dc_loss: 0.01971539668738842, tv_loss: 0.018456781283020973\n",
      "iteration 349, dc_loss: 0.0196879543364048, tv_loss: 0.0184561125934124\n",
      "iteration 350, dc_loss: 0.019661108031868935, tv_loss: 0.018455343320965767\n",
      "iteration 351, dc_loss: 0.019634854048490524, tv_loss: 0.01845436543226242\n",
      "iteration 352, dc_loss: 0.01960917003452778, tv_loss: 0.018453324213624\n",
      "iteration 353, dc_loss: 0.01958402246236801, tv_loss: 0.018452497199177742\n",
      "iteration 354, dc_loss: 0.019559402018785477, tv_loss: 0.018451588228344917\n",
      "iteration 355, dc_loss: 0.01953529566526413, tv_loss: 0.018450645729899406\n",
      "iteration 356, dc_loss: 0.019511686637997627, tv_loss: 0.018449513241648674\n",
      "iteration 357, dc_loss: 0.019488580524921417, tv_loss: 0.018448468297719955\n",
      "iteration 358, dc_loss: 0.019465969875454903, tv_loss: 0.01844719797372818\n",
      "iteration 359, dc_loss: 0.01944386214017868, tv_loss: 0.018446091562509537\n",
      "iteration 360, dc_loss: 0.019422221928834915, tv_loss: 0.018444901332259178\n",
      "iteration 361, dc_loss: 0.019401032477617264, tv_loss: 0.018443774431943893\n",
      "iteration 362, dc_loss: 0.01938028819859028, tv_loss: 0.018442625179886818\n",
      "iteration 363, dc_loss: 0.019359957426786423, tv_loss: 0.018441522493958473\n",
      "iteration 364, dc_loss: 0.019340040162205696, tv_loss: 0.01844036392867565\n",
      "iteration 365, dc_loss: 0.019320538267493248, tv_loss: 0.01843922771513462\n",
      "iteration 366, dc_loss: 0.01930142380297184, tv_loss: 0.018437959253787994\n",
      "iteration 367, dc_loss: 0.019282711669802666, tv_loss: 0.018436873331665993\n",
      "iteration 368, dc_loss: 0.019264401867985725, tv_loss: 0.01843547448515892\n",
      "iteration 369, dc_loss: 0.019246486946940422, tv_loss: 0.01843424141407013\n",
      "iteration 370, dc_loss: 0.019228938966989517, tv_loss: 0.018432971090078354\n",
      "iteration 371, dc_loss: 0.019211741164326668, tv_loss: 0.018431510776281357\n",
      "iteration 372, dc_loss: 0.01919490285217762, tv_loss: 0.018430201336741447\n",
      "iteration 373, dc_loss: 0.01917840912938118, tv_loss: 0.018428951501846313\n",
      "iteration 374, dc_loss: 0.01916223019361496, tv_loss: 0.018427729606628418\n",
      "iteration 375, dc_loss: 0.019146360456943512, tv_loss: 0.018425920978188515\n",
      "iteration 376, dc_loss: 0.01913081668317318, tv_loss: 0.01842460408806801\n",
      "iteration 377, dc_loss: 0.01911557838320732, tv_loss: 0.018423382192850113\n",
      "iteration 378, dc_loss: 0.019100632518529892, tv_loss: 0.018422003835439682\n",
      "iteration 379, dc_loss: 0.019085997715592384, tv_loss: 0.018420521169900894\n",
      "iteration 380, dc_loss: 0.01907166838645935, tv_loss: 0.0184190534055233\n",
      "iteration 381, dc_loss: 0.01905760169029236, tv_loss: 0.018417440354824066\n",
      "iteration 382, dc_loss: 0.01904379203915596, tv_loss: 0.018416017293930054\n",
      "iteration 383, dc_loss: 0.019030241295695305, tv_loss: 0.018414560705423355\n",
      "iteration 384, dc_loss: 0.01901695504784584, tv_loss: 0.01841309666633606\n",
      "iteration 385, dc_loss: 0.019003940746188164, tv_loss: 0.018411414697766304\n",
      "iteration 386, dc_loss: 0.018991194665431976, tv_loss: 0.018409941345453262\n",
      "iteration 387, dc_loss: 0.018978700041770935, tv_loss: 0.01840832643210888\n",
      "iteration 388, dc_loss: 0.01896645873785019, tv_loss: 0.01840704120695591\n",
      "iteration 389, dc_loss: 0.0189544465392828, tv_loss: 0.018405532464385033\n",
      "iteration 390, dc_loss: 0.018942631781101227, tv_loss: 0.018403755500912666\n",
      "iteration 391, dc_loss: 0.018931036815047264, tv_loss: 0.01840210147202015\n",
      "iteration 392, dc_loss: 0.01891966536641121, tv_loss: 0.018400559201836586\n",
      "iteration 393, dc_loss: 0.018908491358160973, tv_loss: 0.018399111926555634\n",
      "iteration 394, dc_loss: 0.018897535279393196, tv_loss: 0.018397508189082146\n",
      "iteration 395, dc_loss: 0.018886784091591835, tv_loss: 0.018395954743027687\n",
      "iteration 396, dc_loss: 0.018876226618885994, tv_loss: 0.018393922597169876\n",
      "iteration 397, dc_loss: 0.01886589452624321, tv_loss: 0.018392672762274742\n",
      "iteration 398, dc_loss: 0.018855735659599304, tv_loss: 0.018391182646155357\n",
      "iteration 399, dc_loss: 0.018845753744244576, tv_loss: 0.018389403820037842\n",
      "iteration 400, dc_loss: 0.01883595623075962, tv_loss: 0.01838754117488861\n",
      "iteration 401, dc_loss: 0.018826346844434738, tv_loss: 0.018386006355285645\n",
      "iteration 402, dc_loss: 0.018816912546753883, tv_loss: 0.018384192138910294\n",
      "iteration 403, dc_loss: 0.01880764402449131, tv_loss: 0.01838233508169651\n",
      "iteration 404, dc_loss: 0.018798546865582466, tv_loss: 0.018380777910351753\n",
      "iteration 405, dc_loss: 0.018789608031511307, tv_loss: 0.01837928220629692\n",
      "iteration 406, dc_loss: 0.018780814483761787, tv_loss: 0.018377192318439484\n",
      "iteration 407, dc_loss: 0.018772179260849953, tv_loss: 0.018375474959611893\n",
      "iteration 408, dc_loss: 0.01876370795071125, tv_loss: 0.018373891711235046\n",
      "iteration 409, dc_loss: 0.018755381926894188, tv_loss: 0.018372012302279472\n",
      "iteration 410, dc_loss: 0.018747175112366676, tv_loss: 0.01837037317454815\n",
      "iteration 411, dc_loss: 0.018739109858870506, tv_loss: 0.01836879923939705\n",
      "iteration 412, dc_loss: 0.018731197342276573, tv_loss: 0.01836695894598961\n",
      "iteration 413, dc_loss: 0.018723437562584877, tv_loss: 0.018365101888775826\n",
      "iteration 414, dc_loss: 0.01871580444276333, tv_loss: 0.018363339826464653\n",
      "iteration 415, dc_loss: 0.01870829425752163, tv_loss: 0.018361717462539673\n",
      "iteration 416, dc_loss: 0.018700866028666496, tv_loss: 0.018359921872615814\n",
      "iteration 417, dc_loss: 0.01869354583323002, tv_loss: 0.01835823431611061\n",
      "iteration 418, dc_loss: 0.01868635229766369, tv_loss: 0.0183565691113472\n",
      "iteration 419, dc_loss: 0.018679287284612656, tv_loss: 0.01835496537387371\n",
      "iteration 420, dc_loss: 0.018672337755560875, tv_loss: 0.018353179097175598\n",
      "iteration 421, dc_loss: 0.018665511161088943, tv_loss: 0.01835150271654129\n",
      "iteration 422, dc_loss: 0.018658826127648354, tv_loss: 0.018349753692746162\n",
      "iteration 423, dc_loss: 0.018652264028787613, tv_loss: 0.01834792271256447\n",
      "iteration 424, dc_loss: 0.01864580623805523, tv_loss: 0.01834615133702755\n",
      "iteration 425, dc_loss: 0.018639426678419113, tv_loss: 0.01834437996149063\n",
      "iteration 426, dc_loss: 0.018633157014846802, tv_loss: 0.018342677503824234\n",
      "iteration 427, dc_loss: 0.018626989796757698, tv_loss: 0.018340757116675377\n",
      "iteration 428, dc_loss: 0.01862087845802307, tv_loss: 0.018338914960622787\n",
      "iteration 429, dc_loss: 0.018614863976836205, tv_loss: 0.018337218090891838\n",
      "iteration 430, dc_loss: 0.01860896311700344, tv_loss: 0.01833539828658104\n",
      "iteration 431, dc_loss: 0.018603159114718437, tv_loss: 0.018333489075303078\n",
      "iteration 432, dc_loss: 0.01859740912914276, tv_loss: 0.01833183318376541\n",
      "iteration 433, dc_loss: 0.018591759726405144, tv_loss: 0.018330048769712448\n",
      "iteration 434, dc_loss: 0.018586190417408943, tv_loss: 0.018328404054045677\n",
      "iteration 435, dc_loss: 0.018580738455057144, tv_loss: 0.01832653395831585\n",
      "iteration 436, dc_loss: 0.01857534795999527, tv_loss: 0.018324555829167366\n",
      "iteration 437, dc_loss: 0.01857004314661026, tv_loss: 0.018322909250855446\n",
      "iteration 438, dc_loss: 0.018564801663160324, tv_loss: 0.0183212049305439\n",
      "iteration 439, dc_loss: 0.01855960115790367, tv_loss: 0.018319308757781982\n",
      "iteration 440, dc_loss: 0.018554488196969032, tv_loss: 0.018317677080631256\n",
      "iteration 441, dc_loss: 0.01854945905506611, tv_loss: 0.018315890803933144\n",
      "iteration 442, dc_loss: 0.018544543534517288, tv_loss: 0.018313977867364883\n",
      "iteration 443, dc_loss: 0.01853971555829048, tv_loss: 0.018312186002731323\n",
      "iteration 444, dc_loss: 0.018534954637289047, tv_loss: 0.01831020973622799\n",
      "iteration 445, dc_loss: 0.018530230969190598, tv_loss: 0.0183084849268198\n",
      "iteration 446, dc_loss: 0.01852555386722088, tv_loss: 0.018306994810700417\n",
      "iteration 447, dc_loss: 0.018520928919315338, tv_loss: 0.018305033445358276\n",
      "iteration 448, dc_loss: 0.018516363576054573, tv_loss: 0.01830301061272621\n",
      "iteration 449, dc_loss: 0.018511846661567688, tv_loss: 0.01830133982002735\n",
      "iteration 450, dc_loss: 0.018507415428757668, tv_loss: 0.018299710005521774\n",
      "iteration 451, dc_loss: 0.01850307174026966, tv_loss: 0.01829778589308262\n",
      "iteration 452, dc_loss: 0.018498795107007027, tv_loss: 0.01829584501683712\n",
      "iteration 453, dc_loss: 0.018494578078389168, tv_loss: 0.018293865025043488\n",
      "iteration 454, dc_loss: 0.018490415066480637, tv_loss: 0.018292110413312912\n",
      "iteration 455, dc_loss: 0.01848628930747509, tv_loss: 0.01829025335609913\n",
      "iteration 456, dc_loss: 0.018482202664017677, tv_loss: 0.018288539722561836\n",
      "iteration 457, dc_loss: 0.018478156998753548, tv_loss: 0.018286515027284622\n",
      "iteration 458, dc_loss: 0.018474159762263298, tv_loss: 0.018284885212779045\n",
      "iteration 459, dc_loss: 0.01847023330628872, tv_loss: 0.01828298531472683\n",
      "iteration 460, dc_loss: 0.018466364592313766, tv_loss: 0.018281184136867523\n",
      "iteration 461, dc_loss: 0.01846254989504814, tv_loss: 0.018279433250427246\n",
      "iteration 462, dc_loss: 0.018458785489201546, tv_loss: 0.01827760599553585\n",
      "iteration 463, dc_loss: 0.018455058336257935, tv_loss: 0.018275702372193336\n",
      "iteration 464, dc_loss: 0.018451357260346413, tv_loss: 0.018273858353495598\n",
      "iteration 465, dc_loss: 0.018447723239660263, tv_loss: 0.018272139132022858\n",
      "iteration 466, dc_loss: 0.018444137647747993, tv_loss: 0.0182702224701643\n",
      "iteration 467, dc_loss: 0.018440594896674156, tv_loss: 0.018268506973981857\n",
      "iteration 468, dc_loss: 0.018437057733535767, tv_loss: 0.018266843631863594\n",
      "iteration 469, dc_loss: 0.018433552235364914, tv_loss: 0.01826494373381138\n",
      "iteration 470, dc_loss: 0.018430113792419434, tv_loss: 0.018263017758727074\n",
      "iteration 471, dc_loss: 0.018426738679409027, tv_loss: 0.01826118491590023\n",
      "iteration 472, dc_loss: 0.018423402681946754, tv_loss: 0.018259523436427116\n",
      "iteration 473, dc_loss: 0.018420079723000526, tv_loss: 0.01825771853327751\n",
      "iteration 474, dc_loss: 0.018416810780763626, tv_loss: 0.018255671486258507\n",
      "iteration 475, dc_loss: 0.018413584679365158, tv_loss: 0.01825384423136711\n",
      "iteration 476, dc_loss: 0.018410364165902138, tv_loss: 0.018252357840538025\n",
      "iteration 477, dc_loss: 0.018407205119729042, tv_loss: 0.018250538036227226\n",
      "iteration 478, dc_loss: 0.018404072150588036, tv_loss: 0.01824873499572277\n",
      "iteration 479, dc_loss: 0.01840098388493061, tv_loss: 0.018246877938508987\n",
      "iteration 480, dc_loss: 0.01839793287217617, tv_loss: 0.018245089799165726\n",
      "iteration 481, dc_loss: 0.018394920974969864, tv_loss: 0.01824342831969261\n",
      "iteration 482, dc_loss: 0.018391922116279602, tv_loss: 0.01824156381189823\n",
      "iteration 483, dc_loss: 0.01838894747197628, tv_loss: 0.01823972351849079\n",
      "iteration 484, dc_loss: 0.01838599145412445, tv_loss: 0.018237989395856857\n",
      "iteration 485, dc_loss: 0.018383091315627098, tv_loss: 0.018236298114061356\n",
      "iteration 486, dc_loss: 0.018380239605903625, tv_loss: 0.018234578892588615\n",
      "iteration 487, dc_loss: 0.018377387896180153, tv_loss: 0.01823287643492222\n",
      "iteration 488, dc_loss: 0.018374577164649963, tv_loss: 0.01823120005428791\n",
      "iteration 489, dc_loss: 0.018371812999248505, tv_loss: 0.018229123204946518\n",
      "iteration 490, dc_loss: 0.01836908981204033, tv_loss: 0.018227292224764824\n",
      "iteration 491, dc_loss: 0.018366387113928795, tv_loss: 0.01822582259774208\n",
      "iteration 492, dc_loss: 0.018363701179623604, tv_loss: 0.01822412572801113\n",
      "iteration 493, dc_loss: 0.018361009657382965, tv_loss: 0.01822226494550705\n",
      "iteration 494, dc_loss: 0.018358344212174416, tv_loss: 0.01822044886648655\n",
      "iteration 495, dc_loss: 0.01835574209690094, tv_loss: 0.01821873150765896\n",
      "iteration 496, dc_loss: 0.01835315302014351, tv_loss: 0.01821698062121868\n",
      "iteration 497, dc_loss: 0.01835058629512787, tv_loss: 0.018215280026197433\n",
      "iteration 498, dc_loss: 0.018348027020692825, tv_loss: 0.01821356825530529\n",
      "iteration 499, dc_loss: 0.018345501273870468, tv_loss: 0.01821187138557434\n",
      "iteration 500, dc_loss: 0.01834300346672535, tv_loss: 0.018210308626294136\n",
      "iteration 501, dc_loss: 0.018340542912483215, tv_loss: 0.018208777531981468\n",
      "iteration 502, dc_loss: 0.018338138237595558, tv_loss: 0.018206702545285225\n",
      "iteration 503, dc_loss: 0.018335752189159393, tv_loss: 0.018204815685749054\n",
      "iteration 504, dc_loss: 0.018333394080400467, tv_loss: 0.018203262239694595\n",
      "iteration 505, dc_loss: 0.018331030383706093, tv_loss: 0.018201543018221855\n",
      "iteration 506, dc_loss: 0.018328683450818062, tv_loss: 0.018199702724814415\n",
      "iteration 507, dc_loss: 0.01832636445760727, tv_loss: 0.018197989091277122\n",
      "iteration 508, dc_loss: 0.01832403987646103, tv_loss: 0.018196342512965202\n",
      "iteration 509, dc_loss: 0.018321722745895386, tv_loss: 0.018194764852523804\n",
      "iteration 510, dc_loss: 0.01831945963203907, tv_loss: 0.01819300465285778\n",
      "iteration 511, dc_loss: 0.018317220732569695, tv_loss: 0.018191102892160416\n",
      "iteration 512, dc_loss: 0.01831500418484211, tv_loss: 0.018189510330557823\n",
      "iteration 513, dc_loss: 0.018312834203243256, tv_loss: 0.018187785521149635\n",
      "iteration 514, dc_loss: 0.018310673534870148, tv_loss: 0.01818600669503212\n",
      "iteration 515, dc_loss: 0.018308527767658234, tv_loss: 0.018184421584010124\n",
      "iteration 516, dc_loss: 0.018306365236639977, tv_loss: 0.018182745203375816\n",
      "iteration 517, dc_loss: 0.018304254859685898, tv_loss: 0.018180986866354942\n",
      "iteration 518, dc_loss: 0.01830214075744152, tv_loss: 0.018179338425397873\n",
      "iteration 519, dc_loss: 0.01830006018280983, tv_loss: 0.018177542835474014\n",
      "iteration 520, dc_loss: 0.01829799823462963, tv_loss: 0.018175914883613586\n",
      "iteration 521, dc_loss: 0.018295973539352417, tv_loss: 0.01817435957491398\n",
      "iteration 522, dc_loss: 0.0182939600199461, tv_loss: 0.0181724950671196\n",
      "iteration 523, dc_loss: 0.01829192414879799, tv_loss: 0.018170703202486038\n",
      "iteration 524, dc_loss: 0.018289905041456223, tv_loss: 0.01816924475133419\n",
      "iteration 525, dc_loss: 0.018287919461727142, tv_loss: 0.01816771924495697\n",
      "iteration 526, dc_loss: 0.01828594319522381, tv_loss: 0.01816582679748535\n",
      "iteration 527, dc_loss: 0.01828400045633316, tv_loss: 0.018164020031690598\n",
      "iteration 528, dc_loss: 0.0182820912450552, tv_loss: 0.018162598833441734\n",
      "iteration 529, dc_loss: 0.018280189484357834, tv_loss: 0.018160877749323845\n",
      "iteration 530, dc_loss: 0.018278276547789574, tv_loss: 0.01815907098352909\n",
      "iteration 531, dc_loss: 0.018276397138834, tv_loss: 0.01815742440521717\n",
      "iteration 532, dc_loss: 0.018274519592523575, tv_loss: 0.01815570890903473\n",
      "iteration 533, dc_loss: 0.018272640183568, tv_loss: 0.01815410889685154\n",
      "iteration 534, dc_loss: 0.018270764499902725, tv_loss: 0.018152637407183647\n",
      "iteration 535, dc_loss: 0.018268896266818047, tv_loss: 0.01815093867480755\n",
      "iteration 536, dc_loss: 0.018267059698700905, tv_loss: 0.018149320036172867\n",
      "iteration 537, dc_loss: 0.018265249207615852, tv_loss: 0.01814769208431244\n",
      "iteration 538, dc_loss: 0.018263472244143486, tv_loss: 0.018146110698580742\n",
      "iteration 539, dc_loss: 0.018261723220348358, tv_loss: 0.018144411966204643\n",
      "iteration 540, dc_loss: 0.01825997419655323, tv_loss: 0.018142692744731903\n",
      "iteration 541, dc_loss: 0.01825825311243534, tv_loss: 0.018141061067581177\n",
      "iteration 542, dc_loss: 0.018256550654768944, tv_loss: 0.01813948154449463\n",
      "iteration 543, dc_loss: 0.01825484074652195, tv_loss: 0.018137861043214798\n",
      "iteration 544, dc_loss: 0.018253127112984657, tv_loss: 0.018136316910386086\n",
      "iteration 545, dc_loss: 0.01825142651796341, tv_loss: 0.01813466101884842\n",
      "iteration 546, dc_loss: 0.018249742686748505, tv_loss: 0.0181331317871809\n",
      "iteration 547, dc_loss: 0.018248064443469048, tv_loss: 0.018131528049707413\n",
      "iteration 548, dc_loss: 0.018246417865157127, tv_loss: 0.01812976412475109\n",
      "iteration 549, dc_loss: 0.018244799226522446, tv_loss: 0.01812816970050335\n",
      "iteration 550, dc_loss: 0.01824316941201687, tv_loss: 0.01812674291431904\n",
      "iteration 551, dc_loss: 0.01824154332280159, tv_loss: 0.018125183880329132\n",
      "iteration 552, dc_loss: 0.018239924684166908, tv_loss: 0.018123576417565346\n",
      "iteration 553, dc_loss: 0.01823829673230648, tv_loss: 0.01812201924622059\n",
      "iteration 554, dc_loss: 0.018236665055155754, tv_loss: 0.0181204155087471\n",
      "iteration 555, dc_loss: 0.018235070630908012, tv_loss: 0.018118951469659805\n",
      "iteration 556, dc_loss: 0.018233513459563255, tv_loss: 0.018117349594831467\n",
      "iteration 557, dc_loss: 0.018231963738799095, tv_loss: 0.018115755170583725\n",
      "iteration 558, dc_loss: 0.018230421468615532, tv_loss: 0.018114013597369194\n",
      "iteration 559, dc_loss: 0.018228910863399506, tv_loss: 0.018112434074282646\n",
      "iteration 560, dc_loss: 0.018227413296699524, tv_loss: 0.018111005425453186\n",
      "iteration 561, dc_loss: 0.018225915729999542, tv_loss: 0.018109265714883804\n",
      "iteration 562, dc_loss: 0.018224431201815605, tv_loss: 0.01810772903263569\n",
      "iteration 563, dc_loss: 0.01822293922305107, tv_loss: 0.01810595765709877\n",
      "iteration 564, dc_loss: 0.01822146400809288, tv_loss: 0.018104461953043938\n",
      "iteration 565, dc_loss: 0.01821998693048954, tv_loss: 0.01810307428240776\n",
      "iteration 566, dc_loss: 0.018218500539660454, tv_loss: 0.018101489171385765\n",
      "iteration 567, dc_loss: 0.01821700483560562, tv_loss: 0.01809985563158989\n",
      "iteration 568, dc_loss: 0.01821553334593773, tv_loss: 0.018098289147019386\n",
      "iteration 569, dc_loss: 0.018214086070656776, tv_loss: 0.0180968027561903\n",
      "iteration 570, dc_loss: 0.018212661147117615, tv_loss: 0.018095411360263824\n",
      "iteration 571, dc_loss: 0.018211258575320244, tv_loss: 0.018093841150403023\n",
      "iteration 572, dc_loss: 0.01820990815758705, tv_loss: 0.01809217780828476\n",
      "iteration 573, dc_loss: 0.018208596855401993, tv_loss: 0.01809064857661724\n",
      "iteration 574, dc_loss: 0.018207240849733353, tv_loss: 0.018088851124048233\n",
      "iteration 575, dc_loss: 0.01820589043200016, tv_loss: 0.018087344244122505\n",
      "iteration 576, dc_loss: 0.018204547464847565, tv_loss: 0.01808587647974491\n",
      "iteration 577, dc_loss: 0.01820318214595318, tv_loss: 0.018084555864334106\n",
      "iteration 578, dc_loss: 0.01820180006325245, tv_loss: 0.01808283105492592\n",
      "iteration 579, dc_loss: 0.018200412392616272, tv_loss: 0.01808137074112892\n",
      "iteration 580, dc_loss: 0.01819906197488308, tv_loss: 0.01807982847094536\n",
      "iteration 581, dc_loss: 0.018197696655988693, tv_loss: 0.018078314140439034\n",
      "iteration 582, dc_loss: 0.01819634810090065, tv_loss: 0.018077034503221512\n",
      "iteration 583, dc_loss: 0.018195055425167084, tv_loss: 0.01807532273232937\n",
      "iteration 584, dc_loss: 0.018193764612078667, tv_loss: 0.018073711544275284\n",
      "iteration 585, dc_loss: 0.018192483112215996, tv_loss: 0.018072403967380524\n",
      "iteration 586, dc_loss: 0.018191225826740265, tv_loss: 0.018071163445711136\n",
      "iteration 587, dc_loss: 0.018189985305070877, tv_loss: 0.018069442361593246\n",
      "iteration 588, dc_loss: 0.018188755959272385, tv_loss: 0.01806780695915222\n",
      "iteration 589, dc_loss: 0.018187524750828743, tv_loss: 0.018066419288516045\n",
      "iteration 590, dc_loss: 0.018186258152127266, tv_loss: 0.018064897507429123\n",
      "iteration 591, dc_loss: 0.018184995278716087, tv_loss: 0.01806332729756832\n",
      "iteration 592, dc_loss: 0.018183773383498192, tv_loss: 0.018061816692352295\n",
      "iteration 593, dc_loss: 0.01818256266415119, tv_loss: 0.01806044578552246\n",
      "iteration 594, dc_loss: 0.018181340768933296, tv_loss: 0.01805895008146763\n",
      "iteration 595, dc_loss: 0.018180109560489655, tv_loss: 0.018057432025671005\n",
      "iteration 596, dc_loss: 0.018178874626755714, tv_loss: 0.018055906519293785\n",
      "iteration 597, dc_loss: 0.01817765273153782, tv_loss: 0.018054664134979248\n",
      "iteration 598, dc_loss: 0.018176455050706863, tv_loss: 0.018053162842988968\n",
      "iteration 599, dc_loss: 0.018175285309553146, tv_loss: 0.018051665276288986\n",
      "iteration 600, dc_loss: 0.018174130469560623, tv_loss: 0.018050149083137512\n",
      "iteration 601, dc_loss: 0.018172994256019592, tv_loss: 0.018048636615276337\n",
      "iteration 602, dc_loss: 0.018171828240156174, tv_loss: 0.018047314137220383\n",
      "iteration 603, dc_loss: 0.018170656636357307, tv_loss: 0.018045969307422638\n",
      "iteration 604, dc_loss: 0.018169498071074486, tv_loss: 0.018044475466012955\n",
      "iteration 605, dc_loss: 0.018168354406952858, tv_loss: 0.018042994663119316\n",
      "iteration 606, dc_loss: 0.018167201429605484, tv_loss: 0.018041636794805527\n",
      "iteration 607, dc_loss: 0.018166083842515945, tv_loss: 0.01804017648100853\n",
      "iteration 608, dc_loss: 0.018164990469813347, tv_loss: 0.01803876832127571\n",
      "iteration 609, dc_loss: 0.018163900822401047, tv_loss: 0.018037330359220505\n",
      "iteration 610, dc_loss: 0.018162820488214493, tv_loss: 0.018035750836133957\n",
      "iteration 611, dc_loss: 0.01816174015402794, tv_loss: 0.01803424581885338\n",
      "iteration 612, dc_loss: 0.01816064491868019, tv_loss: 0.01803283579647541\n",
      "iteration 613, dc_loss: 0.01815950870513916, tv_loss: 0.018031524494290352\n",
      "iteration 614, dc_loss: 0.01815834455192089, tv_loss: 0.01803022250533104\n",
      "iteration 615, dc_loss: 0.01815720833837986, tv_loss: 0.018028752878308296\n",
      "iteration 616, dc_loss: 0.018156155943870544, tv_loss: 0.018027326092123985\n",
      "iteration 617, dc_loss: 0.018155155703425407, tv_loss: 0.01802581548690796\n",
      "iteration 618, dc_loss: 0.018154149875044823, tv_loss: 0.018024269491434097\n",
      "iteration 619, dc_loss: 0.01815311424434185, tv_loss: 0.018022984266281128\n",
      "iteration 620, dc_loss: 0.01815204881131649, tv_loss: 0.01802150532603264\n",
      "iteration 621, dc_loss: 0.01815098337829113, tv_loss: 0.01802016608417034\n",
      "iteration 622, dc_loss: 0.018149951472878456, tv_loss: 0.01801891066133976\n",
      "iteration 623, dc_loss: 0.018148919567465782, tv_loss: 0.01801740750670433\n",
      "iteration 624, dc_loss: 0.018147891387343407, tv_loss: 0.018016038462519646\n",
      "iteration 625, dc_loss: 0.018146900460124016, tv_loss: 0.01801479607820511\n",
      "iteration 626, dc_loss: 0.01814592443406582, tv_loss: 0.01801331155002117\n",
      "iteration 627, dc_loss: 0.018144920468330383, tv_loss: 0.018011746928095818\n",
      "iteration 628, dc_loss: 0.018143922090530396, tv_loss: 0.018010368570685387\n",
      "iteration 629, dc_loss: 0.018142933025956154, tv_loss: 0.018009129911661148\n",
      "iteration 630, dc_loss: 0.018141912296414375, tv_loss: 0.018007762730121613\n",
      "iteration 631, dc_loss: 0.018140897154808044, tv_loss: 0.018006419762969017\n",
      "iteration 632, dc_loss: 0.018139922991394997, tv_loss: 0.018005015328526497\n",
      "iteration 633, dc_loss: 0.018138961866497993, tv_loss: 0.01800377108156681\n",
      "iteration 634, dc_loss: 0.018138008192181587, tv_loss: 0.018002523109316826\n",
      "iteration 635, dc_loss: 0.018137067556381226, tv_loss: 0.018001234158873558\n",
      "iteration 636, dc_loss: 0.018136117607355118, tv_loss: 0.017999576404690742\n",
      "iteration 637, dc_loss: 0.01813517138361931, tv_loss: 0.01799815334379673\n",
      "iteration 638, dc_loss: 0.0181342251598835, tv_loss: 0.01799682341516018\n",
      "iteration 639, dc_loss: 0.018133295699954033, tv_loss: 0.017995623871684074\n",
      "iteration 640, dc_loss: 0.018132392317056656, tv_loss: 0.01799420826137066\n",
      "iteration 641, dc_loss: 0.018131470307707787, tv_loss: 0.017992721870541573\n",
      "iteration 642, dc_loss: 0.018130537122488022, tv_loss: 0.017991449683904648\n",
      "iteration 643, dc_loss: 0.01812959834933281, tv_loss: 0.017990324646234512\n",
      "iteration 644, dc_loss: 0.018128657713532448, tv_loss: 0.01798880286514759\n",
      "iteration 645, dc_loss: 0.01812770776450634, tv_loss: 0.017987556755542755\n",
      "iteration 646, dc_loss: 0.01812678575515747, tv_loss: 0.017986271530389786\n",
      "iteration 647, dc_loss: 0.0181258711963892, tv_loss: 0.017984986305236816\n",
      "iteration 648, dc_loss: 0.018124956637620926, tv_loss: 0.017983725294470787\n",
      "iteration 649, dc_loss: 0.01812405325472355, tv_loss: 0.01798228919506073\n",
      "iteration 650, dc_loss: 0.018123187124729156, tv_loss: 0.01798075996339321\n",
      "iteration 651, dc_loss: 0.0181223526597023, tv_loss: 0.0179794542491436\n",
      "iteration 652, dc_loss: 0.01812153123319149, tv_loss: 0.017978228628635406\n",
      "iteration 653, dc_loss: 0.018120696768164635, tv_loss: 0.01797681860625744\n",
      "iteration 654, dc_loss: 0.018119821324944496, tv_loss: 0.017975516617298126\n",
      "iteration 655, dc_loss: 0.01811893656849861, tv_loss: 0.017974166199564934\n",
      "iteration 656, dc_loss: 0.018118074163794518, tv_loss: 0.017972834408283234\n",
      "iteration 657, dc_loss: 0.01811721920967102, tv_loss: 0.01797168143093586\n",
      "iteration 658, dc_loss: 0.018116360530257225, tv_loss: 0.017970332875847816\n",
      "iteration 659, dc_loss: 0.018115488812327385, tv_loss: 0.01796877756714821\n",
      "iteration 660, dc_loss: 0.01811463013291359, tv_loss: 0.017967689782381058\n",
      "iteration 661, dc_loss: 0.01811378262937069, tv_loss: 0.017966579645872116\n",
      "iteration 662, dc_loss: 0.018112938851118088, tv_loss: 0.01796526089310646\n",
      "iteration 663, dc_loss: 0.01811208948493004, tv_loss: 0.017963780090212822\n",
      "iteration 664, dc_loss: 0.018111249431967735, tv_loss: 0.01796274445950985\n",
      "iteration 665, dc_loss: 0.018110426142811775, tv_loss: 0.017961535602808\n",
      "iteration 666, dc_loss: 0.018109602853655815, tv_loss: 0.017960112541913986\n",
      "iteration 667, dc_loss: 0.018108773976564407, tv_loss: 0.017958959564566612\n",
      "iteration 668, dc_loss: 0.018107960000634193, tv_loss: 0.017957711592316628\n",
      "iteration 669, dc_loss: 0.018107181414961815, tv_loss: 0.017956357449293137\n",
      "iteration 670, dc_loss: 0.01810639724135399, tv_loss: 0.017955083400011063\n",
      "iteration 671, dc_loss: 0.018105611205101013, tv_loss: 0.017953932285308838\n",
      "iteration 672, dc_loss: 0.01810482144355774, tv_loss: 0.01795264147222042\n",
      "iteration 673, dc_loss: 0.018104029819369316, tv_loss: 0.017951108515262604\n",
      "iteration 674, dc_loss: 0.018103230744600296, tv_loss: 0.017949968576431274\n",
      "iteration 675, dc_loss: 0.018102429807186127, tv_loss: 0.017948826774954796\n",
      "iteration 676, dc_loss: 0.01810164749622345, tv_loss: 0.01794765517115593\n",
      "iteration 677, dc_loss: 0.01810084469616413, tv_loss: 0.017946390435099602\n",
      "iteration 678, dc_loss: 0.018100034445524216, tv_loss: 0.01794513501226902\n",
      "iteration 679, dc_loss: 0.018099242821335793, tv_loss: 0.017943914979696274\n",
      "iteration 680, dc_loss: 0.01809844747185707, tv_loss: 0.017942626029253006\n",
      "iteration 681, dc_loss: 0.018097665160894394, tv_loss: 0.017941443249583244\n",
      "iteration 682, dc_loss: 0.0180969201028347, tv_loss: 0.017940018326044083\n",
      "iteration 683, dc_loss: 0.018096160143613815, tv_loss: 0.017938634380698204\n",
      "iteration 684, dc_loss: 0.018095416948199272, tv_loss: 0.017937570810317993\n",
      "iteration 685, dc_loss: 0.018094705417752266, tv_loss: 0.01793629676103592\n",
      "iteration 686, dc_loss: 0.01809399202466011, tv_loss: 0.017934897914528847\n",
      "iteration 687, dc_loss: 0.018093276768922806, tv_loss: 0.017933756113052368\n",
      "iteration 688, dc_loss: 0.018092526122927666, tv_loss: 0.017932375892996788\n",
      "iteration 689, dc_loss: 0.018091760575771332, tv_loss: 0.017931390553712845\n",
      "iteration 690, dc_loss: 0.01809101179242134, tv_loss: 0.017930341884493828\n",
      "iteration 691, dc_loss: 0.018090268597006798, tv_loss: 0.017929021269083023\n",
      "iteration 692, dc_loss: 0.018089523538947105, tv_loss: 0.01792776584625244\n",
      "iteration 693, dc_loss: 0.01808881387114525, tv_loss: 0.017926525324583054\n",
      "iteration 694, dc_loss: 0.018088072538375854, tv_loss: 0.017925379797816277\n",
      "iteration 695, dc_loss: 0.018087320029735565, tv_loss: 0.01792418770492077\n",
      "iteration 696, dc_loss: 0.018086595460772514, tv_loss: 0.01792285032570362\n",
      "iteration 697, dc_loss: 0.0180859062820673, tv_loss: 0.01792161539196968\n",
      "iteration 698, dc_loss: 0.01808524690568447, tv_loss: 0.017920441925525665\n",
      "iteration 699, dc_loss: 0.018084585666656494, tv_loss: 0.01791941002011299\n",
      "iteration 700, dc_loss: 0.018083874136209488, tv_loss: 0.017918121069669724\n",
      "iteration 701, dc_loss: 0.018083179369568825, tv_loss: 0.01791684329509735\n",
      "iteration 702, dc_loss: 0.01808246783912182, tv_loss: 0.017915625125169754\n",
      "iteration 703, dc_loss: 0.01808176375925541, tv_loss: 0.017914338037371635\n",
      "iteration 704, dc_loss: 0.018081054091453552, tv_loss: 0.017913367599248886\n",
      "iteration 705, dc_loss: 0.01808036118745804, tv_loss: 0.01791207864880562\n",
      "iteration 706, dc_loss: 0.01807968132197857, tv_loss: 0.01791086606681347\n",
      "iteration 707, dc_loss: 0.01807900331914425, tv_loss: 0.017909623682498932\n",
      "iteration 708, dc_loss: 0.018078304827213287, tv_loss: 0.017908424139022827\n",
      "iteration 709, dc_loss: 0.01807759329676628, tv_loss: 0.017907332628965378\n",
      "iteration 710, dc_loss: 0.01807689666748047, tv_loss: 0.017906151711940765\n",
      "iteration 711, dc_loss: 0.018076198175549507, tv_loss: 0.017905017361044884\n",
      "iteration 712, dc_loss: 0.018075518310070038, tv_loss: 0.017903931438922882\n",
      "iteration 713, dc_loss: 0.01807486265897751, tv_loss: 0.01790274865925312\n",
      "iteration 714, dc_loss: 0.018074244260787964, tv_loss: 0.017901526764035225\n",
      "iteration 715, dc_loss: 0.018073612824082375, tv_loss: 0.01790039613842964\n",
      "iteration 716, dc_loss: 0.018072983250021935, tv_loss: 0.017899204045534134\n",
      "iteration 717, dc_loss: 0.018072351813316345, tv_loss: 0.0178980752825737\n",
      "iteration 718, dc_loss: 0.018071725964546204, tv_loss: 0.01789696328341961\n",
      "iteration 719, dc_loss: 0.018071087077260017, tv_loss: 0.017895767465233803\n",
      "iteration 720, dc_loss: 0.01807042770087719, tv_loss: 0.017894595861434937\n",
      "iteration 721, dc_loss: 0.018069762736558914, tv_loss: 0.01789339818060398\n",
      "iteration 722, dc_loss: 0.018069075420498848, tv_loss: 0.017892414703965187\n",
      "iteration 723, dc_loss: 0.018068432807922363, tv_loss: 0.017891298979520798\n",
      "iteration 724, dc_loss: 0.018067823722958565, tv_loss: 0.01788998395204544\n",
      "iteration 725, dc_loss: 0.018067192286252975, tv_loss: 0.0178887527436018\n",
      "iteration 726, dc_loss: 0.018066564574837685, tv_loss: 0.017887668684124947\n",
      "iteration 727, dc_loss: 0.018065935000777245, tv_loss: 0.017886411398649216\n",
      "iteration 728, dc_loss: 0.018065301701426506, tv_loss: 0.017885342240333557\n",
      "iteration 729, dc_loss: 0.01806465908885002, tv_loss: 0.017884090542793274\n",
      "iteration 730, dc_loss: 0.01806400530040264, tv_loss: 0.017883026972413063\n",
      "iteration 731, dc_loss: 0.018063385039567947, tv_loss: 0.017881933599710464\n",
      "iteration 732, dc_loss: 0.018062783405184746, tv_loss: 0.017880816012620926\n",
      "iteration 733, dc_loss: 0.018062183633446693, tv_loss: 0.01787973754107952\n",
      "iteration 734, dc_loss: 0.01806158386170864, tv_loss: 0.01787847839295864\n",
      "iteration 735, dc_loss: 0.0180609580129385, tv_loss: 0.017877385020256042\n",
      "iteration 736, dc_loss: 0.018060341477394104, tv_loss: 0.017876336351037025\n",
      "iteration 737, dc_loss: 0.018059752881526947, tv_loss: 0.017875181511044502\n",
      "iteration 738, dc_loss: 0.018059171736240387, tv_loss: 0.017874078825116158\n",
      "iteration 739, dc_loss: 0.018058553338050842, tv_loss: 0.01787303201854229\n",
      "iteration 740, dc_loss: 0.018057914450764656, tv_loss: 0.017871778458356857\n",
      "iteration 741, dc_loss: 0.01805732399225235, tv_loss: 0.01787063106894493\n",
      "iteration 742, dc_loss: 0.018056750297546387, tv_loss: 0.017869561910629272\n",
      "iteration 743, dc_loss: 0.018056174740195274, tv_loss: 0.017868518829345703\n",
      "iteration 744, dc_loss: 0.018055584281682968, tv_loss: 0.01786724478006363\n",
      "iteration 745, dc_loss: 0.018054982647299767, tv_loss: 0.017866123467683792\n",
      "iteration 746, dc_loss: 0.01805441454052925, tv_loss: 0.017865251749753952\n",
      "iteration 747, dc_loss: 0.01805383712053299, tv_loss: 0.017864016816020012\n",
      "iteration 748, dc_loss: 0.01805325783789158, tv_loss: 0.017862820997834206\n",
      "iteration 749, dc_loss: 0.018052687868475914, tv_loss: 0.01786193996667862\n",
      "iteration 750, dc_loss: 0.018052123486995697, tv_loss: 0.01786082051694393\n",
      "iteration 751, dc_loss: 0.018051572144031525, tv_loss: 0.01785953901708126\n",
      "iteration 752, dc_loss: 0.01805099844932556, tv_loss: 0.017858464270830154\n",
      "iteration 753, dc_loss: 0.018050407990813255, tv_loss: 0.017857568338513374\n",
      "iteration 754, dc_loss: 0.018049797043204308, tv_loss: 0.017856333404779434\n",
      "iteration 755, dc_loss: 0.018049189820885658, tv_loss: 0.01785530149936676\n",
      "iteration 756, dc_loss: 0.018048597499728203, tv_loss: 0.017854340374469757\n",
      "iteration 757, dc_loss: 0.018048042431473732, tv_loss: 0.01785331591963768\n",
      "iteration 758, dc_loss: 0.018047476187348366, tv_loss: 0.017852069810032845\n",
      "iteration 759, dc_loss: 0.018046965822577477, tv_loss: 0.017850816249847412\n",
      "iteration 760, dc_loss: 0.018046438694000244, tv_loss: 0.017849816009402275\n",
      "iteration 761, dc_loss: 0.018045878037810326, tv_loss: 0.01784871704876423\n",
      "iteration 762, dc_loss: 0.01804536208510399, tv_loss: 0.01784747838973999\n",
      "iteration 763, dc_loss: 0.018044810742139816, tv_loss: 0.017846450209617615\n",
      "iteration 764, dc_loss: 0.01804422214627266, tv_loss: 0.01784549281001091\n",
      "iteration 765, dc_loss: 0.018043629825115204, tv_loss: 0.017844490706920624\n",
      "iteration 766, dc_loss: 0.018043052405118942, tv_loss: 0.01784336194396019\n",
      "iteration 767, dc_loss: 0.018042510375380516, tv_loss: 0.017842287197709084\n",
      "iteration 768, dc_loss: 0.018042007461190224, tv_loss: 0.01784134842455387\n",
      "iteration 769, dc_loss: 0.018041523173451424, tv_loss: 0.017840201035141945\n",
      "iteration 770, dc_loss: 0.018041010946035385, tv_loss: 0.017839159816503525\n",
      "iteration 771, dc_loss: 0.018040480092167854, tv_loss: 0.017838113009929657\n",
      "iteration 772, dc_loss: 0.018039949238300323, tv_loss: 0.017837094143033028\n",
      "iteration 773, dc_loss: 0.018039388582110405, tv_loss: 0.017836032435297966\n",
      "iteration 774, dc_loss: 0.01803882233798504, tv_loss: 0.017835067585110664\n",
      "iteration 775, dc_loss: 0.01803826168179512, tv_loss: 0.01783410832285881\n",
      "iteration 776, dc_loss: 0.018037740141153336, tv_loss: 0.017832927405834198\n",
      "iteration 777, dc_loss: 0.018037224188447, tv_loss: 0.017831766977906227\n",
      "iteration 778, dc_loss: 0.01803673431277275, tv_loss: 0.01783088967204094\n",
      "iteration 779, dc_loss: 0.018036222085356712, tv_loss: 0.017829854041337967\n",
      "iteration 780, dc_loss: 0.018035700544714928, tv_loss: 0.017828691750764847\n",
      "iteration 781, dc_loss: 0.018035192042589188, tv_loss: 0.0178277175873518\n",
      "iteration 782, dc_loss: 0.018034677952528, tv_loss: 0.017826734110713005\n",
      "iteration 783, dc_loss: 0.01803416572511196, tv_loss: 0.017825596034526825\n",
      "iteration 784, dc_loss: 0.018033644184470177, tv_loss: 0.01782449334859848\n",
      "iteration 785, dc_loss: 0.01803315430879593, tv_loss: 0.01782347448170185\n",
      "iteration 786, dc_loss: 0.018032677471637726, tv_loss: 0.01782255806028843\n",
      "iteration 787, dc_loss: 0.018032187595963478, tv_loss: 0.017821550369262695\n",
      "iteration 788, dc_loss: 0.018031684681773186, tv_loss: 0.017820503562688828\n",
      "iteration 789, dc_loss: 0.018031181767582893, tv_loss: 0.017819318920373917\n",
      "iteration 790, dc_loss: 0.018030676990747452, tv_loss: 0.017818523570895195\n",
      "iteration 791, dc_loss: 0.018030162900686264, tv_loss: 0.017817558720707893\n",
      "iteration 792, dc_loss: 0.018029624596238136, tv_loss: 0.017816318199038506\n",
      "iteration 793, dc_loss: 0.018029136583209038, tv_loss: 0.01781553402543068\n",
      "iteration 794, dc_loss: 0.01802865043282509, tv_loss: 0.017814645543694496\n",
      "iteration 795, dc_loss: 0.018028192222118378, tv_loss: 0.017813468351960182\n",
      "iteration 796, dc_loss: 0.01802770607173443, tv_loss: 0.017812291160225868\n",
      "iteration 797, dc_loss: 0.018027210608124733, tv_loss: 0.017811408266425133\n",
      "iteration 798, dc_loss: 0.01802670769393444, tv_loss: 0.017810393124818802\n",
      "iteration 799, dc_loss: 0.01802622154355049, tv_loss: 0.017809398472309113\n",
      "iteration 800, dc_loss: 0.01802574098110199, tv_loss: 0.01780848018825054\n",
      "iteration 801, dc_loss: 0.018025260418653488, tv_loss: 0.017807381227612495\n",
      "iteration 802, dc_loss: 0.01802479289472103, tv_loss: 0.017806276679039\n",
      "iteration 803, dc_loss: 0.018024316057562828, tv_loss: 0.017805306240916252\n",
      "iteration 804, dc_loss: 0.018023822456598282, tv_loss: 0.017804322764277458\n",
      "iteration 805, dc_loss: 0.018023362383246422, tv_loss: 0.01780330203473568\n",
      "iteration 806, dc_loss: 0.01802290976047516, tv_loss: 0.017802340909838676\n",
      "iteration 807, dc_loss: 0.018022434785962105, tv_loss: 0.017801208421587944\n",
      "iteration 808, dc_loss: 0.018021952360868454, tv_loss: 0.01780020073056221\n",
      "iteration 809, dc_loss: 0.018021462485194206, tv_loss: 0.01779947802424431\n",
      "iteration 810, dc_loss: 0.018020987510681152, tv_loss: 0.01779835671186447\n",
      "iteration 811, dc_loss: 0.018020551651716232, tv_loss: 0.017797378823161125\n",
      "iteration 812, dc_loss: 0.018020110204815865, tv_loss: 0.01779640093445778\n",
      "iteration 813, dc_loss: 0.018019653856754303, tv_loss: 0.017795301973819733\n",
      "iteration 814, dc_loss: 0.018019219860434532, tv_loss: 0.017794346436858177\n",
      "iteration 815, dc_loss: 0.01801878958940506, tv_loss: 0.01779339462518692\n",
      "iteration 816, dc_loss: 0.018018338829278946, tv_loss: 0.017792366445064545\n",
      "iteration 817, dc_loss: 0.0180178452283144, tv_loss: 0.017791371792554855\n",
      "iteration 818, dc_loss: 0.018017344176769257, tv_loss: 0.01779048889875412\n",
      "iteration 819, dc_loss: 0.018016858026385307, tv_loss: 0.017789585515856743\n",
      "iteration 820, dc_loss: 0.018016409128904343, tv_loss: 0.017788536846637726\n",
      "iteration 821, dc_loss: 0.01801597885787487, tv_loss: 0.017787549644708633\n",
      "iteration 822, dc_loss: 0.018015548586845398, tv_loss: 0.017786718904972076\n",
      "iteration 823, dc_loss: 0.01801510713994503, tv_loss: 0.017785659059882164\n",
      "iteration 824, dc_loss: 0.01801467128098011, tv_loss: 0.0177846048027277\n",
      "iteration 825, dc_loss: 0.018014254048466682, tv_loss: 0.017783669754862785\n",
      "iteration 826, dc_loss: 0.018013846129179, tv_loss: 0.017782654613256454\n",
      "iteration 827, dc_loss: 0.01801341585814953, tv_loss: 0.01778140664100647\n",
      "iteration 828, dc_loss: 0.01801297813653946, tv_loss: 0.01778058335185051\n",
      "iteration 829, dc_loss: 0.0180125180631876, tv_loss: 0.017779724672436714\n",
      "iteration 830, dc_loss: 0.018012050539255142, tv_loss: 0.01777857169508934\n",
      "iteration 831, dc_loss: 0.018011603504419327, tv_loss: 0.017777713015675545\n",
      "iteration 832, dc_loss: 0.018011150881648064, tv_loss: 0.017776867374777794\n",
      "iteration 833, dc_loss: 0.0180106982588768, tv_loss: 0.017775963991880417\n",
      "iteration 834, dc_loss: 0.018010253086686134, tv_loss: 0.01777512952685356\n",
      "iteration 835, dc_loss: 0.01800982840359211, tv_loss: 0.0177739430218935\n",
      "iteration 836, dc_loss: 0.018009383231401443, tv_loss: 0.017773117870092392\n",
      "iteration 837, dc_loss: 0.018008965998888016, tv_loss: 0.017772231251001358\n",
      "iteration 838, dc_loss: 0.018008561804890633, tv_loss: 0.017771290615200996\n",
      "iteration 839, dc_loss: 0.018008144572377205, tv_loss: 0.017770282924175262\n",
      "iteration 840, dc_loss: 0.018007759004831314, tv_loss: 0.017769329249858856\n",
      "iteration 841, dc_loss: 0.01800737902522087, tv_loss: 0.017768343910574913\n",
      "iteration 842, dc_loss: 0.018006999045610428, tv_loss: 0.01776743121445179\n",
      "iteration 843, dc_loss: 0.018006576225161552, tv_loss: 0.017766684293746948\n",
      "iteration 844, dc_loss: 0.018006155267357826, tv_loss: 0.01776539348065853\n",
      "iteration 845, dc_loss: 0.018005717545747757, tv_loss: 0.01776469126343727\n",
      "iteration 846, dc_loss: 0.018005279824137688, tv_loss: 0.017763959243893623\n",
      "iteration 847, dc_loss: 0.01800484210252762, tv_loss: 0.01776285655796528\n",
      "iteration 848, dc_loss: 0.018004417419433594, tv_loss: 0.017761949449777603\n",
      "iteration 849, dc_loss: 0.01800401322543621, tv_loss: 0.017761025577783585\n",
      "iteration 850, dc_loss: 0.018003618344664574, tv_loss: 0.017759935930371284\n",
      "iteration 851, dc_loss: 0.01800319366157055, tv_loss: 0.017758959904313087\n",
      "iteration 852, dc_loss: 0.018002765253186226, tv_loss: 0.017758261412382126\n",
      "iteration 853, dc_loss: 0.018002359196543694, tv_loss: 0.017757214605808258\n",
      "iteration 854, dc_loss: 0.018001986667513847, tv_loss: 0.017756221815943718\n",
      "iteration 855, dc_loss: 0.018001602962613106, tv_loss: 0.017755283042788506\n",
      "iteration 856, dc_loss: 0.018001213669776917, tv_loss: 0.017754333093762398\n",
      "iteration 857, dc_loss: 0.018000779673457146, tv_loss: 0.017753498628735542\n",
      "iteration 858, dc_loss: 0.01800033450126648, tv_loss: 0.017752762883901596\n",
      "iteration 859, dc_loss: 0.01799989491701126, tv_loss: 0.017751796171069145\n",
      "iteration 860, dc_loss: 0.017999496310949326, tv_loss: 0.017750846222043037\n",
      "iteration 861, dc_loss: 0.017999140545725822, tv_loss: 0.017750071361660957\n",
      "iteration 862, dc_loss: 0.01799881085753441, tv_loss: 0.017749009653925896\n",
      "iteration 863, dc_loss: 0.01799846440553665, tv_loss: 0.017748067155480385\n",
      "iteration 864, dc_loss: 0.017998110502958298, tv_loss: 0.017747173085808754\n",
      "iteration 865, dc_loss: 0.01799771375954151, tv_loss: 0.017746251076459885\n",
      "iteration 866, dc_loss: 0.01799730770289898, tv_loss: 0.01774527132511139\n",
      "iteration 867, dc_loss: 0.017996875569224358, tv_loss: 0.017744334414601326\n",
      "iteration 868, dc_loss: 0.017996419221162796, tv_loss: 0.017743607982993126\n",
      "iteration 869, dc_loss: 0.017995959147810936, tv_loss: 0.017742853611707687\n",
      "iteration 870, dc_loss: 0.017995521426200867, tv_loss: 0.017741773277521133\n",
      "iteration 871, dc_loss: 0.01799512282013893, tv_loss: 0.017740992829203606\n",
      "iteration 872, dc_loss: 0.017994767054915428, tv_loss: 0.017740070819854736\n",
      "iteration 873, dc_loss: 0.017994429916143417, tv_loss: 0.017739159986376762\n",
      "iteration 874, dc_loss: 0.0179941076785326, tv_loss: 0.01773812249302864\n",
      "iteration 875, dc_loss: 0.01799376867711544, tv_loss: 0.017737220972776413\n",
      "iteration 876, dc_loss: 0.01799336075782776, tv_loss: 0.01773647777736187\n",
      "iteration 877, dc_loss: 0.017992937937378883, tv_loss: 0.01773562654852867\n",
      "iteration 878, dc_loss: 0.017992528155446053, tv_loss: 0.01773456484079361\n",
      "iteration 879, dc_loss: 0.01799214817583561, tv_loss: 0.017733776941895485\n",
      "iteration 880, dc_loss: 0.017991771921515465, tv_loss: 0.017733046784996986\n",
      "iteration 881, dc_loss: 0.017991380766034126, tv_loss: 0.017732039093971252\n",
      "iteration 882, dc_loss: 0.01799100451171398, tv_loss: 0.017731204628944397\n",
      "iteration 883, dc_loss: 0.01799064502120018, tv_loss: 0.01773051545023918\n",
      "iteration 884, dc_loss: 0.01799030415713787, tv_loss: 0.017729531973600388\n",
      "iteration 885, dc_loss: 0.01798996515572071, tv_loss: 0.017728449776768684\n",
      "iteration 886, dc_loss: 0.017989611253142357, tv_loss: 0.017727449536323547\n",
      "iteration 887, dc_loss: 0.017989253625273705, tv_loss: 0.017726866528391838\n",
      "iteration 888, dc_loss: 0.017988864332437515, tv_loss: 0.017726020887494087\n",
      "iteration 889, dc_loss: 0.017988456413149834, tv_loss: 0.01772507093846798\n",
      "iteration 890, dc_loss: 0.017988067120313644, tv_loss: 0.017724227160215378\n",
      "iteration 891, dc_loss: 0.01798771694302559, tv_loss: 0.017723411321640015\n",
      "iteration 892, dc_loss: 0.017987344413995743, tv_loss: 0.017722468823194504\n",
      "iteration 893, dc_loss: 0.017986958846449852, tv_loss: 0.01772161014378071\n",
      "iteration 894, dc_loss: 0.017986584454774857, tv_loss: 0.017720680683851242\n",
      "iteration 895, dc_loss: 0.017986219376325607, tv_loss: 0.017719697207212448\n",
      "iteration 896, dc_loss: 0.01798585243523121, tv_loss: 0.01771889440715313\n",
      "iteration 897, dc_loss: 0.017985492944717407, tv_loss: 0.01771814189851284\n",
      "iteration 898, dc_loss: 0.017985153943300247, tv_loss: 0.017717212438583374\n",
      "iteration 899, dc_loss: 0.017984848469495773, tv_loss: 0.01771627739071846\n",
      "iteration 900, dc_loss: 0.017984526231884956, tv_loss: 0.01771547459065914\n",
      "iteration 901, dc_loss: 0.017984187230467796, tv_loss: 0.017714399844408035\n",
      "iteration 902, dc_loss: 0.017983850091695786, tv_loss: 0.017713630571961403\n",
      "iteration 903, dc_loss: 0.017983490601181984, tv_loss: 0.01771285943686962\n",
      "iteration 904, dc_loss: 0.017983129248023033, tv_loss: 0.017711983993649483\n",
      "iteration 905, dc_loss: 0.017982762306928635, tv_loss: 0.01771116629242897\n",
      "iteration 906, dc_loss: 0.017982356250286102, tv_loss: 0.017710452899336815\n",
      "iteration 907, dc_loss: 0.017981944605708122, tv_loss: 0.017709573730826378\n",
      "iteration 908, dc_loss: 0.017981575801968575, tv_loss: 0.017708798870444298\n",
      "iteration 909, dc_loss: 0.017981238663196564, tv_loss: 0.017707888036966324\n",
      "iteration 910, dc_loss: 0.01798093132674694, tv_loss: 0.017707059159874916\n",
      "iteration 911, dc_loss: 0.01798062026500702, tv_loss: 0.017706232145428658\n",
      "iteration 912, dc_loss: 0.017980290576815605, tv_loss: 0.017705438658595085\n",
      "iteration 913, dc_loss: 0.017979944124817848, tv_loss: 0.017704661935567856\n",
      "iteration 914, dc_loss: 0.017979592084884644, tv_loss: 0.017703860998153687\n",
      "iteration 915, dc_loss: 0.017979217693209648, tv_loss: 0.017703047022223473\n",
      "iteration 916, dc_loss: 0.01797882653772831, tv_loss: 0.01770218275487423\n",
      "iteration 917, dc_loss: 0.01797846332192421, tv_loss: 0.017701519653201103\n",
      "iteration 918, dc_loss: 0.017978113144636154, tv_loss: 0.017700782045722008\n",
      "iteration 919, dc_loss: 0.01797778345644474, tv_loss: 0.017699943855404854\n",
      "iteration 920, dc_loss: 0.01797747053205967, tv_loss: 0.01769891194999218\n",
      "iteration 921, dc_loss: 0.017977125942707062, tv_loss: 0.017698118463158607\n",
      "iteration 922, dc_loss: 0.017976779490709305, tv_loss: 0.017697295174002647\n",
      "iteration 923, dc_loss: 0.017976459115743637, tv_loss: 0.01769663579761982\n",
      "iteration 924, dc_loss: 0.017976151779294014, tv_loss: 0.0176959540694952\n",
      "iteration 925, dc_loss: 0.017975859344005585, tv_loss: 0.017694855108857155\n",
      "iteration 926, dc_loss: 0.01797555573284626, tv_loss: 0.01769380085170269\n",
      "iteration 927, dc_loss: 0.01797524280846119, tv_loss: 0.017693234607577324\n",
      "iteration 928, dc_loss: 0.017974914982914925, tv_loss: 0.017692357301712036\n",
      "iteration 929, dc_loss: 0.01797456480562687, tv_loss: 0.017691466957330704\n",
      "iteration 930, dc_loss: 0.017974210903048515, tv_loss: 0.017690973356366158\n",
      "iteration 931, dc_loss: 0.017973843961954117, tv_loss: 0.01768997684121132\n",
      "iteration 932, dc_loss: 0.01797347143292427, tv_loss: 0.017689187079668045\n",
      "iteration 933, dc_loss: 0.01797311194241047, tv_loss: 0.01768852397799492\n",
      "iteration 934, dc_loss: 0.017972776666283607, tv_loss: 0.01768762618303299\n",
      "iteration 935, dc_loss: 0.017972460016608238, tv_loss: 0.017686914652585983\n",
      "iteration 936, dc_loss: 0.017972148954868317, tv_loss: 0.01768600381910801\n",
      "iteration 937, dc_loss: 0.01797187514603138, tv_loss: 0.017685111612081528\n",
      "iteration 938, dc_loss: 0.017971577122807503, tv_loss: 0.01768428273499012\n",
      "iteration 939, dc_loss: 0.017971239984035492, tv_loss: 0.017683614045381546\n",
      "iteration 940, dc_loss: 0.017970886081457138, tv_loss: 0.017682870849967003\n",
      "iteration 941, dc_loss: 0.01797054335474968, tv_loss: 0.017681963741779327\n",
      "iteration 942, dc_loss: 0.017970221117138863, tv_loss: 0.017681147903203964\n",
      "iteration 943, dc_loss: 0.01796991005539894, tv_loss: 0.017680417746305466\n",
      "iteration 944, dc_loss: 0.017969608306884766, tv_loss: 0.017679523676633835\n",
      "iteration 945, dc_loss: 0.017969295382499695, tv_loss: 0.017678748816251755\n",
      "iteration 946, dc_loss: 0.017968999221920967, tv_loss: 0.017678020521998405\n",
      "iteration 947, dc_loss: 0.017968719825148582, tv_loss: 0.017677066847682\n",
      "iteration 948, dc_loss: 0.01796843111515045, tv_loss: 0.017676370218396187\n",
      "iteration 949, dc_loss: 0.01796812377870083, tv_loss: 0.01767544448375702\n",
      "iteration 950, dc_loss: 0.01796778291463852, tv_loss: 0.01767456717789173\n",
      "iteration 951, dc_loss: 0.01796743832528591, tv_loss: 0.017673924565315247\n",
      "iteration 952, dc_loss: 0.01796713098883629, tv_loss: 0.017673205584287643\n",
      "iteration 953, dc_loss: 0.017966825515031815, tv_loss: 0.017672594636678696\n",
      "iteration 954, dc_loss: 0.017966505140066147, tv_loss: 0.017671575769782066\n",
      "iteration 955, dc_loss: 0.017966195940971375, tv_loss: 0.01767062582075596\n",
      "iteration 956, dc_loss: 0.0179658941924572, tv_loss: 0.01767009310424328\n",
      "iteration 957, dc_loss: 0.017965583130717278, tv_loss: 0.0176693145185709\n",
      "iteration 958, dc_loss: 0.017965275794267654, tv_loss: 0.017668496817350388\n",
      "iteration 959, dc_loss: 0.017964979633688927, tv_loss: 0.017667820677161217\n",
      "iteration 960, dc_loss: 0.017964676022529602, tv_loss: 0.01766703836619854\n",
      "iteration 961, dc_loss: 0.017964376136660576, tv_loss: 0.01766618900001049\n",
      "iteration 962, dc_loss: 0.017964089289307594, tv_loss: 0.0176654364913702\n",
      "iteration 963, dc_loss: 0.017963774502277374, tv_loss: 0.017664624378085136\n",
      "iteration 964, dc_loss: 0.01796346716582775, tv_loss: 0.0176638625562191\n",
      "iteration 965, dc_loss: 0.01796317845582962, tv_loss: 0.01766304299235344\n",
      "iteration 966, dc_loss: 0.01796288974583149, tv_loss: 0.017662329599261284\n",
      "iteration 967, dc_loss: 0.017962584272027016, tv_loss: 0.017661651596426964\n",
      "iteration 968, dc_loss: 0.01796225644648075, tv_loss: 0.017660772427916527\n",
      "iteration 969, dc_loss: 0.01796192117035389, tv_loss: 0.017659902572631836\n",
      "iteration 970, dc_loss: 0.017961617559194565, tv_loss: 0.017659509554505348\n",
      "iteration 971, dc_loss: 0.01796133443713188, tv_loss: 0.017658649012446404\n",
      "iteration 972, dc_loss: 0.01796102523803711, tv_loss: 0.01765783131122589\n",
      "iteration 973, dc_loss: 0.017960740253329277, tv_loss: 0.01765700802206993\n",
      "iteration 974, dc_loss: 0.01796049252152443, tv_loss: 0.017656244337558746\n",
      "iteration 975, dc_loss: 0.017960239201784134, tv_loss: 0.017655568197369576\n",
      "iteration 976, dc_loss: 0.017959970980882645, tv_loss: 0.01765461452305317\n",
      "iteration 977, dc_loss: 0.01795966736972332, tv_loss: 0.01765374466776848\n",
      "iteration 978, dc_loss: 0.01795935444533825, tv_loss: 0.01765316165983677\n",
      "iteration 979, dc_loss: 0.01795903407037258, tv_loss: 0.017652451992034912\n",
      "iteration 980, dc_loss: 0.017958732321858406, tv_loss: 0.017651846632361412\n",
      "iteration 981, dc_loss: 0.017958424985408783, tv_loss: 0.01765104942023754\n",
      "iteration 982, dc_loss: 0.017958134412765503, tv_loss: 0.01765027455985546\n",
      "iteration 983, dc_loss: 0.017957860603928566, tv_loss: 0.017649594694375992\n",
      "iteration 984, dc_loss: 0.01795760914683342, tv_loss: 0.017648721113801003\n",
      "iteration 985, dc_loss: 0.017957353964447975, tv_loss: 0.017648030072450638\n",
      "iteration 986, dc_loss: 0.01795707456767559, tv_loss: 0.01764713227748871\n",
      "iteration 987, dc_loss: 0.017956765368580818, tv_loss: 0.017646489664912224\n",
      "iteration 988, dc_loss: 0.017956411466002464, tv_loss: 0.01764582470059395\n",
      "iteration 989, dc_loss: 0.017956089228391647, tv_loss: 0.017645038664340973\n",
      "iteration 990, dc_loss: 0.017955783754587173, tv_loss: 0.017644325271248817\n",
      "iteration 991, dc_loss: 0.017955485731363297, tv_loss: 0.017643703147768974\n",
      "iteration 992, dc_loss: 0.017955198884010315, tv_loss: 0.017642917111516\n",
      "iteration 993, dc_loss: 0.017954936251044273, tv_loss: 0.01764221303164959\n",
      "iteration 994, dc_loss: 0.01795470155775547, tv_loss: 0.017641445621848106\n",
      "iteration 995, dc_loss: 0.01795443706214428, tv_loss: 0.017640598118305206\n",
      "iteration 996, dc_loss: 0.017954159528017044, tv_loss: 0.017639925703406334\n",
      "iteration 997, dc_loss: 0.017953895032405853, tv_loss: 0.017639227211475372\n",
      "iteration 998, dc_loss: 0.01795363612473011, tv_loss: 0.017638318240642548\n",
      "iteration 999, dc_loss: 0.017953353002667427, tv_loss: 0.01763763464987278\n",
      "iteration 1000, dc_loss: 0.01795308291912079, tv_loss: 0.01763700693845749\n",
      "iteration 1001, dc_loss: 0.017952799797058105, tv_loss: 0.01763620786368847\n",
      "iteration 1002, dc_loss: 0.01795251853764057, tv_loss: 0.017635557800531387\n",
      "iteration 1003, dc_loss: 0.017952237278223038, tv_loss: 0.017634952440857887\n",
      "iteration 1004, dc_loss: 0.017951974645256996, tv_loss: 0.0176341962069273\n",
      "iteration 1005, dc_loss: 0.017951693385839462, tv_loss: 0.017633574083447456\n",
      "iteration 1006, dc_loss: 0.017951400950551033, tv_loss: 0.017632871866226196\n",
      "iteration 1007, dc_loss: 0.017951130867004395, tv_loss: 0.017632000148296356\n",
      "iteration 1008, dc_loss: 0.017950862646102905, tv_loss: 0.017631428316235542\n",
      "iteration 1009, dc_loss: 0.017950600013136864, tv_loss: 0.01763073354959488\n",
      "iteration 1010, dc_loss: 0.017950328066945076, tv_loss: 0.01762978732585907\n",
      "iteration 1011, dc_loss: 0.017950067296624184, tv_loss: 0.017629073932766914\n",
      "iteration 1012, dc_loss: 0.017949819564819336, tv_loss: 0.017628328874707222\n",
      "iteration 1013, dc_loss: 0.017949551343917847, tv_loss: 0.017627639696002007\n",
      "iteration 1014, dc_loss: 0.017949266359210014, tv_loss: 0.017626911401748657\n",
      "iteration 1015, dc_loss: 0.01794898882508278, tv_loss: 0.017626313492655754\n",
      "iteration 1016, dc_loss: 0.017948713153600693, tv_loss: 0.01762566715478897\n",
      "iteration 1017, dc_loss: 0.017948418855667114, tv_loss: 0.01762501150369644\n",
      "iteration 1018, dc_loss: 0.017948120832443237, tv_loss: 0.017624275758862495\n",
      "iteration 1019, dc_loss: 0.017947839573025703, tv_loss: 0.017623452469706535\n",
      "iteration 1020, dc_loss: 0.017947571352124214, tv_loss: 0.017622780054807663\n",
      "iteration 1021, dc_loss: 0.01794731430709362, tv_loss: 0.01762210950255394\n",
      "iteration 1022, dc_loss: 0.01794707030057907, tv_loss: 0.017621412873268127\n",
      "iteration 1023, dc_loss: 0.01794683374464512, tv_loss: 0.01762068271636963\n",
      "iteration 1024, dc_loss: 0.01794656179845333, tv_loss: 0.017620069906115532\n",
      "iteration 1025, dc_loss: 0.0179462768137455, tv_loss: 0.017619261518120766\n",
      "iteration 1026, dc_loss: 0.01794600673019886, tv_loss: 0.01761840470135212\n",
      "iteration 1027, dc_loss: 0.017945732921361923, tv_loss: 0.017617924138903618\n",
      "iteration 1028, dc_loss: 0.017945460975170135, tv_loss: 0.017617270350456238\n",
      "iteration 1029, dc_loss: 0.01794520393013954, tv_loss: 0.017616620287299156\n",
      "iteration 1030, dc_loss: 0.017944958060979843, tv_loss: 0.017615823075175285\n",
      "iteration 1031, dc_loss: 0.017944710329174995, tv_loss: 0.017615104094147682\n",
      "iteration 1032, dc_loss: 0.01794443465769291, tv_loss: 0.017614590004086494\n",
      "iteration 1033, dc_loss: 0.01794419251382351, tv_loss: 0.017613958567380905\n",
      "iteration 1034, dc_loss: 0.017943955957889557, tv_loss: 0.01761317066848278\n",
      "iteration 1035, dc_loss: 0.017943717539310455, tv_loss: 0.017612256109714508\n",
      "iteration 1036, dc_loss: 0.01794346608221531, tv_loss: 0.017611701041460037\n",
      "iteration 1037, dc_loss: 0.017943179234862328, tv_loss: 0.017611030489206314\n",
      "iteration 1038, dc_loss: 0.017942918464541435, tv_loss: 0.017610300332307816\n",
      "iteration 1039, dc_loss: 0.017942678183317184, tv_loss: 0.01760958693921566\n",
      "iteration 1040, dc_loss: 0.017942436039447784, tv_loss: 0.017608776688575745\n",
      "iteration 1041, dc_loss: 0.017942190170288086, tv_loss: 0.017608102411031723\n",
      "iteration 1042, dc_loss: 0.01794193871319294, tv_loss: 0.01760750263929367\n",
      "iteration 1043, dc_loss: 0.017941676080226898, tv_loss: 0.017606742680072784\n",
      "iteration 1044, dc_loss: 0.017941433936357498, tv_loss: 0.017605915665626526\n",
      "iteration 1045, dc_loss: 0.0179411843419075, tv_loss: 0.017605308443307877\n",
      "iteration 1046, dc_loss: 0.01794094778597355, tv_loss: 0.017604682594537735\n",
      "iteration 1047, dc_loss: 0.017940703779459, tv_loss: 0.01760401949286461\n",
      "iteration 1048, dc_loss: 0.017940450459718704, tv_loss: 0.017603116109967232\n",
      "iteration 1049, dc_loss: 0.017940163612365723, tv_loss: 0.017602408304810524\n",
      "iteration 1050, dc_loss: 0.017939887940883636, tv_loss: 0.01760200783610344\n",
      "iteration 1051, dc_loss: 0.017939629033207893, tv_loss: 0.017601361498236656\n",
      "iteration 1052, dc_loss: 0.017939377576112747, tv_loss: 0.017600590363144875\n",
      "iteration 1053, dc_loss: 0.01793910562992096, tv_loss: 0.017599863931536674\n",
      "iteration 1054, dc_loss: 0.017938848584890366, tv_loss: 0.01759924180805683\n",
      "iteration 1055, dc_loss: 0.017938638105988503, tv_loss: 0.017598595470190048\n",
      "iteration 1056, dc_loss: 0.017938438802957535, tv_loss: 0.017597774043679237\n",
      "iteration 1057, dc_loss: 0.017938245087862015, tv_loss: 0.017597122117877007\n",
      "iteration 1058, dc_loss: 0.01793801039457321, tv_loss: 0.01759677194058895\n",
      "iteration 1059, dc_loss: 0.01793777011334896, tv_loss: 0.017595838755369186\n",
      "iteration 1060, dc_loss: 0.01793745532631874, tv_loss: 0.01759507693350315\n",
      "iteration 1061, dc_loss: 0.017937134951353073, tv_loss: 0.017594918608665466\n",
      "iteration 1062, dc_loss: 0.017936844378709793, tv_loss: 0.017594248056411743\n",
      "iteration 1063, dc_loss: 0.017936604097485542, tv_loss: 0.01759333722293377\n",
      "iteration 1064, dc_loss: 0.017936406657099724, tv_loss: 0.01759263314306736\n",
      "iteration 1065, dc_loss: 0.017936183139681816, tv_loss: 0.017592113465070724\n",
      "iteration 1066, dc_loss: 0.017935950309038162, tv_loss: 0.017591465264558792\n",
      "iteration 1067, dc_loss: 0.01793571189045906, tv_loss: 0.017590781673789024\n",
      "iteration 1068, dc_loss: 0.01793546974658966, tv_loss: 0.017589986324310303\n",
      "iteration 1069, dc_loss: 0.017935210838913918, tv_loss: 0.017589468508958817\n",
      "iteration 1070, dc_loss: 0.01793499104678631, tv_loss: 0.017588699236512184\n",
      "iteration 1071, dc_loss: 0.017934786155819893, tv_loss: 0.017588023096323013\n",
      "iteration 1072, dc_loss: 0.017934564501047134, tv_loss: 0.017587369307875633\n",
      "iteration 1073, dc_loss: 0.01793431118130684, tv_loss: 0.017586782574653625\n",
      "iteration 1074, dc_loss: 0.01793406717479229, tv_loss: 0.017586130648851395\n",
      "iteration 1075, dc_loss: 0.017933834344148636, tv_loss: 0.017585573717951775\n",
      "iteration 1076, dc_loss: 0.0179335605353117, tv_loss: 0.017585014924407005\n",
      "iteration 1077, dc_loss: 0.017933299764990807, tv_loss: 0.017584364861249924\n",
      "iteration 1078, dc_loss: 0.01793307438492775, tv_loss: 0.017583657056093216\n",
      "iteration 1079, dc_loss: 0.017932884395122528, tv_loss: 0.017582999542355537\n",
      "iteration 1080, dc_loss: 0.017932670190930367, tv_loss: 0.01758234016597271\n",
      "iteration 1081, dc_loss: 0.01793244667351246, tv_loss: 0.017581652849912643\n",
      "iteration 1082, dc_loss: 0.017932187765836716, tv_loss: 0.01758101023733616\n",
      "iteration 1083, dc_loss: 0.017931923270225525, tv_loss: 0.017580362036824226\n",
      "iteration 1084, dc_loss: 0.01793166995048523, tv_loss: 0.017579836770892143\n",
      "iteration 1085, dc_loss: 0.01793144829571247, tv_loss: 0.017579268664121628\n",
      "iteration 1086, dc_loss: 0.017931252717971802, tv_loss: 0.017578499391674995\n",
      "iteration 1087, dc_loss: 0.017931055277585983, tv_loss: 0.01757781393826008\n",
      "iteration 1088, dc_loss: 0.017930854111909866, tv_loss: 0.01757713034749031\n",
      "iteration 1089, dc_loss: 0.01793063059449196, tv_loss: 0.017576418817043304\n",
      "iteration 1090, dc_loss: 0.01793042942881584, tv_loss: 0.017575688660144806\n",
      "iteration 1091, dc_loss: 0.017930177971720695, tv_loss: 0.017575088888406754\n",
      "iteration 1092, dc_loss: 0.017929917201399803, tv_loss: 0.01757446490228176\n",
      "iteration 1093, dc_loss: 0.01792965829372406, tv_loss: 0.017573969438672066\n",
      "iteration 1094, dc_loss: 0.017929399386048317, tv_loss: 0.017573313787579536\n",
      "iteration 1095, dc_loss: 0.01792914979159832, tv_loss: 0.0175725519657135\n",
      "iteration 1096, dc_loss: 0.01792890578508377, tv_loss: 0.01757202111184597\n",
      "iteration 1097, dc_loss: 0.017928676679730415, tv_loss: 0.017571497708559036\n",
      "iteration 1098, dc_loss: 0.017928471788764, tv_loss: 0.01757064461708069\n",
      "iteration 1099, dc_loss: 0.017928309738636017, tv_loss: 0.0175700131803751\n",
      "iteration 1100, dc_loss: 0.01792812906205654, tv_loss: 0.01756936125457287\n",
      "iteration 1101, dc_loss: 0.017927905544638634, tv_loss: 0.017568519338965416\n",
      "iteration 1102, dc_loss: 0.017927680164575577, tv_loss: 0.01756802201271057\n",
      "iteration 1103, dc_loss: 0.017927471548318863, tv_loss: 0.017567502334713936\n",
      "iteration 1104, dc_loss: 0.01792726293206215, tv_loss: 0.017566751688718796\n",
      "iteration 1105, dc_loss: 0.017927050590515137, tv_loss: 0.017566179856657982\n",
      "iteration 1106, dc_loss: 0.01792682334780693, tv_loss: 0.017565496265888214\n",
      "iteration 1107, dc_loss: 0.017926611006259918, tv_loss: 0.017564846202731133\n",
      "iteration 1108, dc_loss: 0.01792636699974537, tv_loss: 0.017564304172992706\n",
      "iteration 1109, dc_loss: 0.017926136031746864, tv_loss: 0.017563823610544205\n",
      "iteration 1110, dc_loss: 0.01792590320110321, tv_loss: 0.017563287168741226\n",
      "iteration 1111, dc_loss: 0.01792565919458866, tv_loss: 0.017562774941325188\n",
      "iteration 1112, dc_loss: 0.017925413325428963, tv_loss: 0.017562029883265495\n",
      "iteration 1113, dc_loss: 0.017925193533301353, tv_loss: 0.017561467364430428\n",
      "iteration 1114, dc_loss: 0.017924951389431953, tv_loss: 0.017560668289661407\n",
      "iteration 1115, dc_loss: 0.017924712970852852, tv_loss: 0.017560144886374474\n",
      "iteration 1116, dc_loss: 0.017924489453434944, tv_loss: 0.017559774219989777\n",
      "iteration 1117, dc_loss: 0.017924301326274872, tv_loss: 0.017558977007865906\n",
      "iteration 1118, dc_loss: 0.017924131825566292, tv_loss: 0.017558101564645767\n",
      "iteration 1119, dc_loss: 0.01792399398982525, tv_loss: 0.01755739003419876\n",
      "iteration 1120, dc_loss: 0.017923815175890923, tv_loss: 0.01755686290562153\n",
      "iteration 1121, dc_loss: 0.017923589795827866, tv_loss: 0.017556270584464073\n",
      "iteration 1122, dc_loss: 0.01792333647608757, tv_loss: 0.017555592581629753\n",
      "iteration 1123, dc_loss: 0.01792306639254093, tv_loss: 0.017555175349116325\n",
      "iteration 1124, dc_loss: 0.017922822386026382, tv_loss: 0.017554614692926407\n",
      "iteration 1125, dc_loss: 0.017922619357705116, tv_loss: 0.017553791403770447\n",
      "iteration 1126, dc_loss: 0.017922429367899895, tv_loss: 0.017553243786096573\n",
      "iteration 1127, dc_loss: 0.01792222075164318, tv_loss: 0.017552874982357025\n",
      "iteration 1128, dc_loss: 0.017922017723321915, tv_loss: 0.017552101984620094\n",
      "iteration 1129, dc_loss: 0.01792178489267826, tv_loss: 0.01755143702030182\n",
      "iteration 1130, dc_loss: 0.01792152225971222, tv_loss: 0.017551038414239883\n",
      "iteration 1131, dc_loss: 0.01792127639055252, tv_loss: 0.017550628632307053\n",
      "iteration 1132, dc_loss: 0.017921082675457, tv_loss: 0.017549892887473106\n",
      "iteration 1133, dc_loss: 0.017920874059200287, tv_loss: 0.017549294978380203\n",
      "iteration 1134, dc_loss: 0.01792067661881447, tv_loss: 0.017548812553286552\n",
      "iteration 1135, dc_loss: 0.017920494079589844, tv_loss: 0.017548177391290665\n",
      "iteration 1136, dc_loss: 0.017920322716236115, tv_loss: 0.017547255381941795\n",
      "iteration 1137, dc_loss: 0.017920155078172684, tv_loss: 0.017546743154525757\n",
      "iteration 1138, dc_loss: 0.017919957637786865, tv_loss: 0.017546167597174644\n",
      "iteration 1139, dc_loss: 0.01791972853243351, tv_loss: 0.017545554786920547\n",
      "iteration 1140, dc_loss: 0.017919519916176796, tv_loss: 0.01754487119615078\n",
      "iteration 1141, dc_loss: 0.017919348552823067, tv_loss: 0.017544524744153023\n",
      "iteration 1142, dc_loss: 0.01791914366185665, tv_loss: 0.01754392869770527\n",
      "iteration 1143, dc_loss: 0.01791893132030964, tv_loss: 0.017543021589517593\n",
      "iteration 1144, dc_loss: 0.017918722704052925, tv_loss: 0.017542574554681778\n",
      "iteration 1145, dc_loss: 0.017918499186635017, tv_loss: 0.017542310059070587\n",
      "iteration 1146, dc_loss: 0.0179183017462492, tv_loss: 0.01754165254533291\n",
      "iteration 1147, dc_loss: 0.017918093129992485, tv_loss: 0.01754101924598217\n",
      "iteration 1148, dc_loss: 0.01791786029934883, tv_loss: 0.01754044182598591\n",
      "iteration 1149, dc_loss: 0.01791762374341488, tv_loss: 0.017539821565151215\n",
      "iteration 1150, dc_loss: 0.017917422577738762, tv_loss: 0.01753930188715458\n",
      "iteration 1151, dc_loss: 0.017917226999998093, tv_loss: 0.017538662999868393\n",
      "iteration 1152, dc_loss: 0.017917023971676826, tv_loss: 0.017537865787744522\n",
      "iteration 1153, dc_loss: 0.017916817218065262, tv_loss: 0.01753736473619938\n",
      "iteration 1154, dc_loss: 0.01791662536561489, tv_loss: 0.017536861822009087\n",
      "iteration 1155, dc_loss: 0.017916440963745117, tv_loss: 0.017536351457238197\n",
      "iteration 1156, dc_loss: 0.01791628636419773, tv_loss: 0.017535477876663208\n",
      "iteration 1157, dc_loss: 0.01791609823703766, tv_loss: 0.017534799873828888\n",
      "iteration 1158, dc_loss: 0.017915857955813408, tv_loss: 0.01753445528447628\n",
      "iteration 1159, dc_loss: 0.01791563257575035, tv_loss: 0.01753382571041584\n",
      "iteration 1160, dc_loss: 0.01791541278362274, tv_loss: 0.017533354461193085\n",
      "iteration 1161, dc_loss: 0.017915228381752968, tv_loss: 0.017532849684357643\n",
      "iteration 1162, dc_loss: 0.017915094271302223, tv_loss: 0.017532052472233772\n",
      "iteration 1163, dc_loss: 0.01791495457291603, tv_loss: 0.017531419172883034\n",
      "iteration 1164, dc_loss: 0.017914794385433197, tv_loss: 0.017530951648950577\n",
      "iteration 1165, dc_loss: 0.017914587631821632, tv_loss: 0.017530325800180435\n",
      "iteration 1166, dc_loss: 0.017914364114403725, tv_loss: 0.017529793083667755\n",
      "iteration 1167, dc_loss: 0.017914142459630966, tv_loss: 0.017528986558318138\n",
      "iteration 1168, dc_loss: 0.017913931980729103, tv_loss: 0.01752840355038643\n",
      "iteration 1169, dc_loss: 0.017913714051246643, tv_loss: 0.017528127878904343\n",
      "iteration 1170, dc_loss: 0.017913496121764183, tv_loss: 0.017527451738715172\n",
      "iteration 1171, dc_loss: 0.01791328378021717, tv_loss: 0.017526740208268166\n",
      "iteration 1172, dc_loss: 0.0179130919277668, tv_loss: 0.017526263371109962\n",
      "iteration 1173, dc_loss: 0.017912913113832474, tv_loss: 0.017525827512145042\n",
      "iteration 1174, dc_loss: 0.01791275106370449, tv_loss: 0.017525160685181618\n",
      "iteration 1175, dc_loss: 0.01791257970035076, tv_loss: 0.017524557188153267\n",
      "iteration 1176, dc_loss: 0.01791238971054554, tv_loss: 0.017523912712931633\n",
      "iteration 1177, dc_loss: 0.01791221648454666, tv_loss: 0.017523501068353653\n",
      "iteration 1178, dc_loss: 0.017911987379193306, tv_loss: 0.017522921785712242\n",
      "iteration 1179, dc_loss: 0.01791171170771122, tv_loss: 0.01752224564552307\n",
      "iteration 1180, dc_loss: 0.017911460250616074, tv_loss: 0.01752186194062233\n",
      "iteration 1181, dc_loss: 0.017911236733198166, tv_loss: 0.017521468922495842\n",
      "iteration 1182, dc_loss: 0.017911018803715706, tv_loss: 0.017520859837532043\n",
      "iteration 1183, dc_loss: 0.017910830676555634, tv_loss: 0.017520194873213768\n",
      "iteration 1184, dc_loss: 0.017910677939653397, tv_loss: 0.017519613727927208\n",
      "iteration 1185, dc_loss: 0.01791052334010601, tv_loss: 0.017519038170576096\n",
      "iteration 1186, dc_loss: 0.017910383641719818, tv_loss: 0.01751839555799961\n",
      "iteration 1187, dc_loss: 0.017910242080688477, tv_loss: 0.017517944797873497\n",
      "iteration 1188, dc_loss: 0.017910074442625046, tv_loss: 0.017517277970910072\n",
      "iteration 1189, dc_loss: 0.017909929156303406, tv_loss: 0.017516588792204857\n",
      "iteration 1190, dc_loss: 0.017909739166498184, tv_loss: 0.017516201362013817\n",
      "iteration 1191, dc_loss: 0.01790951006114483, tv_loss: 0.017515653744339943\n",
      "iteration 1192, dc_loss: 0.01790926232933998, tv_loss: 0.01751507632434368\n",
      "iteration 1193, dc_loss: 0.017909035086631775, tv_loss: 0.017514530569314957\n",
      "iteration 1194, dc_loss: 0.01790882647037506, tv_loss: 0.017514007166028023\n",
      "iteration 1195, dc_loss: 0.01790866069495678, tv_loss: 0.017513571307063103\n",
      "iteration 1196, dc_loss: 0.017908500507473946, tv_loss: 0.017513034865260124\n",
      "iteration 1197, dc_loss: 0.0179083701223135, tv_loss: 0.01751238852739334\n",
      "iteration 1198, dc_loss: 0.01790822669863701, tv_loss: 0.017511676996946335\n",
      "iteration 1199, dc_loss: 0.017908038571476936, tv_loss: 0.017511241137981415\n",
      "iteration 1200, dc_loss: 0.017907842993736267, tv_loss: 0.01751060038805008\n",
      "iteration 1201, dc_loss: 0.01790766604244709, tv_loss: 0.017510028555989265\n",
      "iteration 1202, dc_loss: 0.017907503992319107, tv_loss: 0.017509566619992256\n",
      "iteration 1203, dc_loss: 0.01790732704102993, tv_loss: 0.017508912831544876\n",
      "iteration 1204, dc_loss: 0.017907125875353813, tv_loss: 0.017508454620838165\n",
      "iteration 1205, dc_loss: 0.0179069172590971, tv_loss: 0.017508000135421753\n",
      "iteration 1206, dc_loss: 0.01790669746696949, tv_loss: 0.01750762388110161\n",
      "iteration 1207, dc_loss: 0.017906486988067627, tv_loss: 0.017507022246718407\n",
      "iteration 1208, dc_loss: 0.01790631376206875, tv_loss: 0.017506510019302368\n",
      "iteration 1209, dc_loss: 0.01790613867342472, tv_loss: 0.017505960538983345\n",
      "iteration 1210, dc_loss: 0.01790597289800644, tv_loss: 0.01750526949763298\n",
      "iteration 1211, dc_loss: 0.017905816435813904, tv_loss: 0.01750466227531433\n",
      "iteration 1212, dc_loss: 0.017905639484524727, tv_loss: 0.01750415936112404\n",
      "iteration 1213, dc_loss: 0.0179054606705904, tv_loss: 0.01750369183719158\n",
      "iteration 1214, dc_loss: 0.017905259504914284, tv_loss: 0.01750321313738823\n",
      "iteration 1215, dc_loss: 0.017905035987496376, tv_loss: 0.017502877861261368\n",
      "iteration 1216, dc_loss: 0.017904825508594513, tv_loss: 0.017502233386039734\n",
      "iteration 1217, dc_loss: 0.01790463738143444, tv_loss: 0.0175015851855278\n",
      "iteration 1218, dc_loss: 0.017904501408338547, tv_loss: 0.017500977963209152\n",
      "iteration 1219, dc_loss: 0.01790434494614601, tv_loss: 0.017500553280115128\n",
      "iteration 1220, dc_loss: 0.017904145643115044, tv_loss: 0.01750006154179573\n",
      "iteration 1221, dc_loss: 0.017903953790664673, tv_loss: 0.017499631270766258\n",
      "iteration 1222, dc_loss: 0.017903756350278854, tv_loss: 0.01749907061457634\n",
      "iteration 1223, dc_loss: 0.017903564497828484, tv_loss: 0.01749848574399948\n",
      "iteration 1224, dc_loss: 0.01790338009595871, tv_loss: 0.017498014494776726\n",
      "iteration 1225, dc_loss: 0.017903219908475876, tv_loss: 0.01749747060239315\n",
      "iteration 1226, dc_loss: 0.017903035506606102, tv_loss: 0.017496997490525246\n",
      "iteration 1227, dc_loss: 0.01790284365415573, tv_loss: 0.01749628596007824\n",
      "iteration 1228, dc_loss: 0.01790265366435051, tv_loss: 0.017495816573500633\n",
      "iteration 1229, dc_loss: 0.01790248602628708, tv_loss: 0.017495376989245415\n",
      "iteration 1230, dc_loss: 0.017902344465255737, tv_loss: 0.017494751140475273\n",
      "iteration 1231, dc_loss: 0.017902204766869545, tv_loss: 0.017494123429059982\n",
      "iteration 1232, dc_loss: 0.01790205016732216, tv_loss: 0.017493529245257378\n",
      "iteration 1233, dc_loss: 0.017901860177516937, tv_loss: 0.01749306358397007\n",
      "iteration 1234, dc_loss: 0.017901692539453506, tv_loss: 0.017492692917585373\n",
      "iteration 1235, dc_loss: 0.017901532351970673, tv_loss: 0.017492106184363365\n",
      "iteration 1236, dc_loss: 0.01790132001042366, tv_loss: 0.017491541802883148\n",
      "iteration 1237, dc_loss: 0.017901122570037842, tv_loss: 0.01749100722372532\n",
      "iteration 1238, dc_loss: 0.017900945618748665, tv_loss: 0.017490457743406296\n",
      "iteration 1239, dc_loss: 0.01790080964565277, tv_loss: 0.01749001070857048\n",
      "iteration 1240, dc_loss: 0.01790066435933113, tv_loss: 0.017489489167928696\n",
      "iteration 1241, dc_loss: 0.017900509759783745, tv_loss: 0.017488835379481316\n",
      "iteration 1242, dc_loss: 0.017900314182043076, tv_loss: 0.01748829148709774\n",
      "iteration 1243, dc_loss: 0.01790010742843151, tv_loss: 0.017487891018390656\n",
      "iteration 1244, dc_loss: 0.017899896949529648, tv_loss: 0.01748748868703842\n",
      "iteration 1245, dc_loss: 0.017899708822369576, tv_loss: 0.0174869317561388\n",
      "iteration 1246, dc_loss: 0.017899522557854652, tv_loss: 0.017486369237303734\n",
      "iteration 1247, dc_loss: 0.017899351194500923, tv_loss: 0.017485935240983963\n",
      "iteration 1248, dc_loss: 0.017899198457598686, tv_loss: 0.017485452815890312\n",
      "iteration 1249, dc_loss: 0.017899062484502792, tv_loss: 0.0174849983304739\n",
      "iteration 1250, dc_loss: 0.01789890229701996, tv_loss: 0.01748446747660637\n",
      "iteration 1251, dc_loss: 0.017898736521601677, tv_loss: 0.017483795061707497\n",
      "iteration 1252, dc_loss: 0.017898520454764366, tv_loss: 0.017483577132225037\n",
      "iteration 1253, dc_loss: 0.017898311838507652, tv_loss: 0.017483048141002655\n",
      "iteration 1254, dc_loss: 0.017898110672831535, tv_loss: 0.01748233288526535\n",
      "iteration 1255, dc_loss: 0.0178979504853487, tv_loss: 0.017481869086623192\n",
      "iteration 1256, dc_loss: 0.01789780706167221, tv_loss: 0.01748141646385193\n",
      "iteration 1257, dc_loss: 0.01789768598973751, tv_loss: 0.01748085580766201\n",
      "iteration 1258, dc_loss: 0.017897574231028557, tv_loss: 0.01748032122850418\n",
      "iteration 1259, dc_loss: 0.017897415906190872, tv_loss: 0.017479628324508667\n",
      "iteration 1260, dc_loss: 0.017897190526127815, tv_loss: 0.017479199916124344\n",
      "iteration 1261, dc_loss: 0.01789695955812931, tv_loss: 0.01747891679406166\n",
      "iteration 1262, dc_loss: 0.0178967472165823, tv_loss: 0.017478443682193756\n",
      "iteration 1263, dc_loss: 0.01789657585322857, tv_loss: 0.017477795481681824\n",
      "iteration 1264, dc_loss: 0.01789643056690693, tv_loss: 0.017477452754974365\n",
      "iteration 1265, dc_loss: 0.017896296456456184, tv_loss: 0.01747686043381691\n",
      "iteration 1266, dc_loss: 0.017896153032779694, tv_loss: 0.017476258799433708\n",
      "iteration 1267, dc_loss: 0.017896004021167755, tv_loss: 0.01747581548988819\n",
      "iteration 1268, dc_loss: 0.017895841971039772, tv_loss: 0.01747536100447178\n",
      "iteration 1269, dc_loss: 0.01789567433297634, tv_loss: 0.01747475005686283\n",
      "iteration 1270, dc_loss: 0.01789551042020321, tv_loss: 0.017474308609962463\n",
      "iteration 1271, dc_loss: 0.017895352095365524, tv_loss: 0.017473775893449783\n",
      "iteration 1272, dc_loss: 0.017895210534334183, tv_loss: 0.017473267391324043\n",
      "iteration 1273, dc_loss: 0.017895054072141647, tv_loss: 0.01747284084558487\n",
      "iteration 1274, dc_loss: 0.01789490319788456, tv_loss: 0.01747225783765316\n",
      "iteration 1275, dc_loss: 0.017894720658659935, tv_loss: 0.017471875995397568\n",
      "iteration 1276, dc_loss: 0.01789456233382225, tv_loss: 0.0174713134765625\n",
      "iteration 1277, dc_loss: 0.01789439283311367, tv_loss: 0.017470773309469223\n",
      "iteration 1278, dc_loss: 0.017894228920340538, tv_loss: 0.017470499500632286\n",
      "iteration 1279, dc_loss: 0.017894065007567406, tv_loss: 0.017470018938183784\n",
      "iteration 1280, dc_loss: 0.017893891781568527, tv_loss: 0.01746942475438118\n",
      "iteration 1281, dc_loss: 0.01789371483027935, tv_loss: 0.01746911182999611\n",
      "iteration 1282, dc_loss: 0.017893530428409576, tv_loss: 0.017468485981225967\n",
      "iteration 1283, dc_loss: 0.017893346026539803, tv_loss: 0.017468130216002464\n",
      "iteration 1284, dc_loss: 0.017893165349960327, tv_loss: 0.017467837780714035\n",
      "iteration 1285, dc_loss: 0.017893005162477493, tv_loss: 0.017467154189944267\n",
      "iteration 1286, dc_loss: 0.0178928691893816, tv_loss: 0.017466571182012558\n",
      "iteration 1287, dc_loss: 0.01789274625480175, tv_loss: 0.01746612973511219\n",
      "iteration 1288, dc_loss: 0.017892612144351006, tv_loss: 0.01746569760143757\n",
      "iteration 1289, dc_loss: 0.01789248175919056, tv_loss: 0.01746503822505474\n",
      "iteration 1290, dc_loss: 0.017892319709062576, tv_loss: 0.01746477745473385\n",
      "iteration 1291, dc_loss: 0.017892155796289444, tv_loss: 0.017464442178606987\n",
      "iteration 1292, dc_loss: 0.01789199374616146, tv_loss: 0.017463844269514084\n",
      "iteration 1293, dc_loss: 0.01789180375635624, tv_loss: 0.017463279888033867\n",
      "iteration 1294, dc_loss: 0.017891617491841316, tv_loss: 0.017463019117712975\n",
      "iteration 1295, dc_loss: 0.017891446128487587, tv_loss: 0.01746240258216858\n",
      "iteration 1296, dc_loss: 0.017891304567456245, tv_loss: 0.0174618698656559\n",
      "iteration 1297, dc_loss: 0.017891183495521545, tv_loss: 0.017461499199271202\n",
      "iteration 1298, dc_loss: 0.017891034483909607, tv_loss: 0.017460841685533524\n",
      "iteration 1299, dc_loss: 0.01789088360965252, tv_loss: 0.01746034435927868\n",
      "iteration 1300, dc_loss: 0.01789073273539543, tv_loss: 0.017459938302636147\n",
      "iteration 1301, dc_loss: 0.017890581861138344, tv_loss: 0.017459435388445854\n",
      "iteration 1302, dc_loss: 0.01789041794836521, tv_loss: 0.017459016293287277\n",
      "iteration 1303, dc_loss: 0.017890239134430885, tv_loss: 0.017458414658904076\n",
      "iteration 1304, dc_loss: 0.01789005845785141, tv_loss: 0.017457975074648857\n",
      "iteration 1305, dc_loss: 0.017889881506562233, tv_loss: 0.0174576323479414\n",
      "iteration 1306, dc_loss: 0.017889702692627907, tv_loss: 0.017457161098718643\n",
      "iteration 1307, dc_loss: 0.017889540642499924, tv_loss: 0.017456669360399246\n",
      "iteration 1308, dc_loss: 0.01788940094411373, tv_loss: 0.017455995082855225\n",
      "iteration 1309, dc_loss: 0.01788925565779209, tv_loss: 0.01745559833943844\n",
      "iteration 1310, dc_loss: 0.01788911782205105, tv_loss: 0.01745518110692501\n",
      "iteration 1311, dc_loss: 0.01788896508514881, tv_loss: 0.017454609274864197\n",
      "iteration 1312, dc_loss: 0.017888806760311127, tv_loss: 0.017454147338867188\n",
      "iteration 1313, dc_loss: 0.017888646572828293, tv_loss: 0.017453771084547043\n",
      "iteration 1314, dc_loss: 0.017888493835926056, tv_loss: 0.01745312288403511\n",
      "iteration 1315, dc_loss: 0.017888320609927177, tv_loss: 0.017452649772167206\n",
      "iteration 1316, dc_loss: 0.01788816601037979, tv_loss: 0.017452510073781013\n",
      "iteration 1317, dc_loss: 0.017888018861413002, tv_loss: 0.017451833933591843\n",
      "iteration 1318, dc_loss: 0.017887892201542854, tv_loss: 0.017451303079724312\n",
      "iteration 1319, dc_loss: 0.01788775622844696, tv_loss: 0.017450720071792603\n",
      "iteration 1320, dc_loss: 0.01788761280477047, tv_loss: 0.01745043322443962\n",
      "iteration 1321, dc_loss: 0.01788748800754547, tv_loss: 0.017450008541345596\n",
      "iteration 1322, dc_loss: 0.01788732409477234, tv_loss: 0.017449522390961647\n",
      "iteration 1323, dc_loss: 0.017887115478515625, tv_loss: 0.017449019476771355\n",
      "iteration 1324, dc_loss: 0.017886916175484657, tv_loss: 0.01744864322245121\n",
      "iteration 1325, dc_loss: 0.017886746674776077, tv_loss: 0.017448393628001213\n",
      "iteration 1326, dc_loss: 0.01788661628961563, tv_loss: 0.01744779944419861\n",
      "iteration 1327, dc_loss: 0.017886504530906677, tv_loss: 0.01744719408452511\n",
      "iteration 1328, dc_loss: 0.017886390909552574, tv_loss: 0.01744670979678631\n",
      "iteration 1329, dc_loss: 0.017886262387037277, tv_loss: 0.017446162179112434\n",
      "iteration 1330, dc_loss: 0.017886124551296234, tv_loss: 0.017445819452404976\n",
      "iteration 1331, dc_loss: 0.017885979264974594, tv_loss: 0.017445499077439308\n",
      "iteration 1332, dc_loss: 0.017885809764266014, tv_loss: 0.0174449123442173\n",
      "iteration 1333, dc_loss: 0.017885632812976837, tv_loss: 0.017444562166929245\n",
      "iteration 1334, dc_loss: 0.017885470762848854, tv_loss: 0.017444103956222534\n",
      "iteration 1335, dc_loss: 0.017885299399495125, tv_loss: 0.017443617805838585\n",
      "iteration 1336, dc_loss: 0.017885150387883186, tv_loss: 0.01744326762855053\n",
      "iteration 1337, dc_loss: 0.017884986475110054, tv_loss: 0.017443029209971428\n",
      "iteration 1338, dc_loss: 0.017884807661175728, tv_loss: 0.01744248904287815\n",
      "iteration 1339, dc_loss: 0.0178846288472414, tv_loss: 0.017441852018237114\n",
      "iteration 1340, dc_loss: 0.017884470522403717, tv_loss: 0.0174415223300457\n",
      "iteration 1341, dc_loss: 0.017884312197566032, tv_loss: 0.017441045492887497\n",
      "iteration 1342, dc_loss: 0.01788417436182499, tv_loss: 0.017440546303987503\n",
      "iteration 1343, dc_loss: 0.017884058877825737, tv_loss: 0.017440158873796463\n",
      "iteration 1344, dc_loss: 0.01788395084440708, tv_loss: 0.017439646646380424\n",
      "iteration 1345, dc_loss: 0.017883840948343277, tv_loss: 0.01743917353451252\n",
      "iteration 1346, dc_loss: 0.017883749678730965, tv_loss: 0.017438659444451332\n",
      "iteration 1347, dc_loss: 0.01788364350795746, tv_loss: 0.017438244074583054\n",
      "iteration 1348, dc_loss: 0.01788349263370037, tv_loss: 0.017437724396586418\n",
      "iteration 1349, dc_loss: 0.017883334308862686, tv_loss: 0.01743726059794426\n",
      "iteration 1350, dc_loss: 0.01788315549492836, tv_loss: 0.017436902970075607\n",
      "iteration 1351, dc_loss: 0.017882991582155228, tv_loss: 0.0174365546554327\n",
      "iteration 1352, dc_loss: 0.017882823944091797, tv_loss: 0.017435945570468903\n",
      "iteration 1353, dc_loss: 0.017882656306028366, tv_loss: 0.01743548922240734\n",
      "iteration 1354, dc_loss: 0.017882511019706726, tv_loss: 0.017435025423765182\n",
      "iteration 1355, dc_loss: 0.01788236014544964, tv_loss: 0.017434710636734962\n",
      "iteration 1356, dc_loss: 0.0178822111338377, tv_loss: 0.017434267327189445\n",
      "iteration 1357, dc_loss: 0.017882058396935463, tv_loss: 0.017433740198612213\n",
      "iteration 1358, dc_loss: 0.01788192428648472, tv_loss: 0.017433296889066696\n",
      "iteration 1359, dc_loss: 0.017881805077195168, tv_loss: 0.017432773485779762\n",
      "iteration 1360, dc_loss: 0.01788167655467987, tv_loss: 0.017432386055588722\n",
      "iteration 1361, dc_loss: 0.017881548032164574, tv_loss: 0.017431922256946564\n",
      "iteration 1362, dc_loss: 0.017881428822875023, tv_loss: 0.01743156462907791\n",
      "iteration 1363, dc_loss: 0.01788128726184368, tv_loss: 0.017430933192372322\n",
      "iteration 1364, dc_loss: 0.017881128937005997, tv_loss: 0.017430534586310387\n",
      "iteration 1365, dc_loss: 0.01788097433745861, tv_loss: 0.017430076375603676\n",
      "iteration 1366, dc_loss: 0.01788082905113697, tv_loss: 0.017429878935217857\n",
      "iteration 1367, dc_loss: 0.017880672588944435, tv_loss: 0.017429469153285027\n",
      "iteration 1368, dc_loss: 0.017880506813526154, tv_loss: 0.017428867518901825\n",
      "iteration 1369, dc_loss: 0.01788036711513996, tv_loss: 0.017428455874323845\n",
      "iteration 1370, dc_loss: 0.017880260944366455, tv_loss: 0.01742812618613243\n",
      "iteration 1371, dc_loss: 0.017880138009786606, tv_loss: 0.017427632585167885\n",
      "iteration 1372, dc_loss: 0.017879996448755264, tv_loss: 0.017427224665880203\n",
      "iteration 1373, dc_loss: 0.01787983626127243, tv_loss: 0.01742679253220558\n",
      "iteration 1374, dc_loss: 0.017879663035273552, tv_loss: 0.017426474019885063\n",
      "iteration 1375, dc_loss: 0.01787952333688736, tv_loss: 0.017426077276468277\n",
      "iteration 1376, dc_loss: 0.017879392951726913, tv_loss: 0.017425695434212685\n",
      "iteration 1377, dc_loss: 0.017879270017147064, tv_loss: 0.01742534153163433\n",
      "iteration 1378, dc_loss: 0.01787913776934147, tv_loss: 0.017424937337636948\n",
      "iteration 1379, dc_loss: 0.017879020422697067, tv_loss: 0.017424335703253746\n",
      "iteration 1380, dc_loss: 0.01787889190018177, tv_loss: 0.017423836514353752\n",
      "iteration 1381, dc_loss: 0.01787872239947319, tv_loss: 0.017423441633582115\n",
      "iteration 1382, dc_loss: 0.01787855476140976, tv_loss: 0.017423076555132866\n",
      "iteration 1383, dc_loss: 0.017878388985991478, tv_loss: 0.017422573640942574\n",
      "iteration 1384, dc_loss: 0.017878254875540733, tv_loss: 0.017422201111912727\n",
      "iteration 1385, dc_loss: 0.017878130078315735, tv_loss: 0.017421744763851166\n",
      "iteration 1386, dc_loss: 0.01787799783051014, tv_loss: 0.017421258613467216\n",
      "iteration 1387, dc_loss: 0.017877863720059395, tv_loss: 0.017421023920178413\n",
      "iteration 1388, dc_loss: 0.01787772960960865, tv_loss: 0.017420539632439613\n",
      "iteration 1389, dc_loss: 0.017877589911222458, tv_loss: 0.017419986426830292\n",
      "iteration 1390, dc_loss: 0.01787743717432022, tv_loss: 0.01741950400173664\n",
      "iteration 1391, dc_loss: 0.01787727326154709, tv_loss: 0.01741921156644821\n",
      "iteration 1392, dc_loss: 0.01787712052464485, tv_loss: 0.01741885207593441\n",
      "iteration 1393, dc_loss: 0.01787697523832321, tv_loss: 0.017418190836906433\n",
      "iteration 1394, dc_loss: 0.017876844853162766, tv_loss: 0.017417842522263527\n",
      "iteration 1395, dc_loss: 0.017876747995615005, tv_loss: 0.01741752028465271\n",
      "iteration 1396, dc_loss: 0.017876610159873962, tv_loss: 0.017417067661881447\n",
      "iteration 1397, dc_loss: 0.01787647418677807, tv_loss: 0.017416642978787422\n",
      "iteration 1398, dc_loss: 0.01787634566426277, tv_loss: 0.01741621643304825\n",
      "iteration 1399, dc_loss: 0.01787620224058628, tv_loss: 0.017415955662727356\n",
      "iteration 1400, dc_loss: 0.017876040190458298, tv_loss: 0.017415547743439674\n",
      "iteration 1401, dc_loss: 0.01787586137652397, tv_loss: 0.017415061593055725\n",
      "iteration 1402, dc_loss: 0.017875690013170242, tv_loss: 0.017414715141057968\n",
      "iteration 1403, dc_loss: 0.01787552237510681, tv_loss: 0.017414318397641182\n",
      "iteration 1404, dc_loss: 0.017875419929623604, tv_loss: 0.017414014786481857\n",
      "iteration 1405, dc_loss: 0.017875347286462784, tv_loss: 0.01741342805325985\n",
      "iteration 1406, dc_loss: 0.017875246703624725, tv_loss: 0.017412951216101646\n",
      "iteration 1407, dc_loss: 0.017875149846076965, tv_loss: 0.017412591725587845\n",
      "iteration 1408, dc_loss: 0.017875052988529205, tv_loss: 0.017412088811397552\n",
      "iteration 1409, dc_loss: 0.017874950543045998, tv_loss: 0.0174116063863039\n",
      "iteration 1410, dc_loss: 0.017874792218208313, tv_loss: 0.01741117425262928\n",
      "iteration 1411, dc_loss: 0.01787465065717697, tv_loss: 0.0174107626080513\n",
      "iteration 1412, dc_loss: 0.01787453144788742, tv_loss: 0.017410410568118095\n",
      "iteration 1413, dc_loss: 0.017874376848340034, tv_loss: 0.0174101572483778\n",
      "iteration 1414, dc_loss: 0.01787419617176056, tv_loss: 0.017409799620509148\n",
      "iteration 1415, dc_loss: 0.017874019220471382, tv_loss: 0.017409389838576317\n",
      "iteration 1416, dc_loss: 0.01787387765944004, tv_loss: 0.017408927902579308\n",
      "iteration 1417, dc_loss: 0.01787373796105385, tv_loss: 0.017408691346645355\n",
      "iteration 1418, dc_loss: 0.017873594537377357, tv_loss: 0.017408296465873718\n",
      "iteration 1419, dc_loss: 0.017873432487249374, tv_loss: 0.01740778610110283\n",
      "iteration 1420, dc_loss: 0.01787332072854042, tv_loss: 0.017407376319169998\n",
      "iteration 1421, dc_loss: 0.01787319779396057, tv_loss: 0.01740695722401142\n",
      "iteration 1422, dc_loss: 0.017873072996735573, tv_loss: 0.017406539991497993\n",
      "iteration 1423, dc_loss: 0.017872970551252365, tv_loss: 0.017406195402145386\n",
      "iteration 1424, dc_loss: 0.017872842028737068, tv_loss: 0.01740577258169651\n",
      "iteration 1425, dc_loss: 0.017872700467705727, tv_loss: 0.017405351623892784\n",
      "iteration 1426, dc_loss: 0.017872534692287445, tv_loss: 0.017405014485120773\n",
      "iteration 1427, dc_loss: 0.017872408032417297, tv_loss: 0.01740455999970436\n",
      "iteration 1428, dc_loss: 0.017872294411063194, tv_loss: 0.01740410551428795\n",
      "iteration 1429, dc_loss: 0.017872178927063942, tv_loss: 0.017403703182935715\n",
      "iteration 1430, dc_loss: 0.017872057855129242, tv_loss: 0.017403392121195793\n",
      "iteration 1431, dc_loss: 0.017871912568807602, tv_loss: 0.017402831465005875\n",
      "iteration 1432, dc_loss: 0.017871785908937454, tv_loss: 0.01740257628262043\n",
      "iteration 1433, dc_loss: 0.017871636897325516, tv_loss: 0.017402200028300285\n",
      "iteration 1434, dc_loss: 0.017871471121907234, tv_loss: 0.017401767894625664\n",
      "iteration 1435, dc_loss: 0.017871350049972534, tv_loss: 0.01740136183798313\n",
      "iteration 1436, dc_loss: 0.017871243879199028, tv_loss: 0.017400991171598434\n",
      "iteration 1437, dc_loss: 0.017871150746941566, tv_loss: 0.017400609329342842\n",
      "iteration 1438, dc_loss: 0.017871035262942314, tv_loss: 0.01740032248198986\n",
      "iteration 1439, dc_loss: 0.017870884388685226, tv_loss: 0.017399922013282776\n",
      "iteration 1440, dc_loss: 0.017870722338557243, tv_loss: 0.017399515956640244\n",
      "iteration 1441, dc_loss: 0.017870554700493813, tv_loss: 0.01739906705915928\n",
      "iteration 1442, dc_loss: 0.01787041872739792, tv_loss: 0.01739872433245182\n",
      "iteration 1443, dc_loss: 0.017870280891656876, tv_loss: 0.017398390918970108\n",
      "iteration 1444, dc_loss: 0.01787014864385128, tv_loss: 0.017397981137037277\n",
      "iteration 1445, dc_loss: 0.017870040610432625, tv_loss: 0.017397552728652954\n",
      "iteration 1446, dc_loss: 0.017869964241981506, tv_loss: 0.017397074028849602\n",
      "iteration 1447, dc_loss: 0.01786988042294979, tv_loss: 0.017396682873368263\n",
      "iteration 1448, dc_loss: 0.017869766801595688, tv_loss: 0.017396390438079834\n",
      "iteration 1449, dc_loss: 0.017869649454951286, tv_loss: 0.01739598624408245\n",
      "iteration 1450, dc_loss: 0.017869530245661736, tv_loss: 0.017395582050085068\n",
      "iteration 1451, dc_loss: 0.0178693700581789, tv_loss: 0.017395231872797012\n",
      "iteration 1452, dc_loss: 0.017869217321276665, tv_loss: 0.017394812777638435\n",
      "iteration 1453, dc_loss: 0.017869051545858383, tv_loss: 0.017394524067640305\n",
      "iteration 1454, dc_loss: 0.017868919298052788, tv_loss: 0.01739424839615822\n",
      "iteration 1455, dc_loss: 0.017868833616375923, tv_loss: 0.01739378832280636\n",
      "iteration 1456, dc_loss: 0.01786874234676361, tv_loss: 0.01739327795803547\n",
      "iteration 1457, dc_loss: 0.01786862313747406, tv_loss: 0.017392907291650772\n",
      "iteration 1458, dc_loss: 0.01786847412586212, tv_loss: 0.017392421141266823\n",
      "iteration 1459, dc_loss: 0.01786833442747593, tv_loss: 0.017392102628946304\n",
      "iteration 1460, dc_loss: 0.017868174239993095, tv_loss: 0.017391948029398918\n",
      "iteration 1461, dc_loss: 0.017868027091026306, tv_loss: 0.017391517758369446\n",
      "iteration 1462, dc_loss: 0.017867883667349815, tv_loss: 0.017391037195920944\n",
      "iteration 1463, dc_loss: 0.017867758870124817, tv_loss: 0.017390817403793335\n",
      "iteration 1464, dc_loss: 0.017867635935544968, tv_loss: 0.017390375956892967\n",
      "iteration 1465, dc_loss: 0.017867496237158775, tv_loss: 0.01739002950489521\n",
      "iteration 1466, dc_loss: 0.01786736398935318, tv_loss: 0.017389755696058273\n",
      "iteration 1467, dc_loss: 0.017867261543869972, tv_loss: 0.01738923229277134\n",
      "iteration 1468, dc_loss: 0.017867164686322212, tv_loss: 0.01738864555954933\n",
      "iteration 1469, dc_loss: 0.017867082729935646, tv_loss: 0.017388321459293365\n",
      "iteration 1470, dc_loss: 0.017866987735033035, tv_loss: 0.01738797314465046\n",
      "iteration 1471, dc_loss: 0.017866898328065872, tv_loss: 0.017387568950653076\n",
      "iteration 1472, dc_loss: 0.017866801470518112, tv_loss: 0.017387177795171738\n",
      "iteration 1473, dc_loss: 0.017866667360067368, tv_loss: 0.01738678477704525\n",
      "iteration 1474, dc_loss: 0.017866503447294235, tv_loss: 0.017386533319950104\n",
      "iteration 1475, dc_loss: 0.01786632090806961, tv_loss: 0.01738600619137287\n",
      "iteration 1476, dc_loss: 0.017866147682070732, tv_loss: 0.017385734245181084\n",
      "iteration 1477, dc_loss: 0.01786600425839424, tv_loss: 0.01738562621176243\n",
      "iteration 1478, dc_loss: 0.017865914851427078, tv_loss: 0.01738523133099079\n",
      "iteration 1479, dc_loss: 0.017865844070911407, tv_loss: 0.017384730279445648\n",
      "iteration 1480, dc_loss: 0.017865771427750587, tv_loss: 0.017384180799126625\n",
      "iteration 1481, dc_loss: 0.017865680158138275, tv_loss: 0.017383718863129616\n",
      "iteration 1482, dc_loss: 0.017865562811493874, tv_loss: 0.01738356053829193\n",
      "iteration 1483, dc_loss: 0.017865408211946487, tv_loss: 0.017383255064487457\n",
      "iteration 1484, dc_loss: 0.01786523498594761, tv_loss: 0.017382802441716194\n",
      "iteration 1485, dc_loss: 0.017865097150206566, tv_loss: 0.01738244667649269\n",
      "iteration 1486, dc_loss: 0.01786498911678791, tv_loss: 0.01738222874701023\n",
      "iteration 1487, dc_loss: 0.01786489598453045, tv_loss: 0.017381781712174416\n",
      "iteration 1488, dc_loss: 0.017864806577563286, tv_loss: 0.01738128438591957\n",
      "iteration 1489, dc_loss: 0.017864705994725227, tv_loss: 0.017381012439727783\n",
      "iteration 1490, dc_loss: 0.017864549532532692, tv_loss: 0.017380602657794952\n",
      "iteration 1491, dc_loss: 0.0178644061088562, tv_loss: 0.01738004945218563\n",
      "iteration 1492, dc_loss: 0.01786426268517971, tv_loss: 0.01737985759973526\n",
      "iteration 1493, dc_loss: 0.01786412112414837, tv_loss: 0.01737958937883377\n",
      "iteration 1494, dc_loss: 0.017863985151052475, tv_loss: 0.017379330471158028\n",
      "iteration 1495, dc_loss: 0.017863869667053223, tv_loss: 0.01737876422703266\n",
      "iteration 1496, dc_loss: 0.01786377839744091, tv_loss: 0.017378415912389755\n",
      "iteration 1497, dc_loss: 0.01786370947957039, tv_loss: 0.01737801730632782\n",
      "iteration 1498, dc_loss: 0.017863597720861435, tv_loss: 0.017377683892846107\n",
      "iteration 1499, dc_loss: 0.01786348596215248, tv_loss: 0.017377443611621857\n",
      "iteration 1500, dc_loss: 0.01786334440112114, tv_loss: 0.01737695001065731\n",
      "iteration 1501, dc_loss: 0.017863236367702484, tv_loss: 0.017376476898789406\n",
      "iteration 1502, dc_loss: 0.017863137647509575, tv_loss: 0.01737627573311329\n",
      "iteration 1503, dc_loss: 0.017863018438220024, tv_loss: 0.017375927418470383\n",
      "iteration 1504, dc_loss: 0.017862915992736816, tv_loss: 0.017375539988279343\n",
      "iteration 1505, dc_loss: 0.017862800508737564, tv_loss: 0.017375225201249123\n",
      "iteration 1506, dc_loss: 0.017862653359770775, tv_loss: 0.017374752089381218\n",
      "iteration 1507, dc_loss: 0.017862506210803986, tv_loss: 0.01737440936267376\n",
      "iteration 1508, dc_loss: 0.0178623478859663, tv_loss: 0.017373953014612198\n",
      "iteration 1509, dc_loss: 0.01786222867667675, tv_loss: 0.017373748123645782\n",
      "iteration 1510, dc_loss: 0.017862122505903244, tv_loss: 0.017373478040099144\n",
      "iteration 1511, dc_loss: 0.017862021923065186, tv_loss: 0.017372850328683853\n",
      "iteration 1512, dc_loss: 0.01786193624138832, tv_loss: 0.017372507601976395\n",
      "iteration 1513, dc_loss: 0.01786183752119541, tv_loss: 0.017372267320752144\n",
      "iteration 1514, dc_loss: 0.01786172203719616, tv_loss: 0.017371831461787224\n",
      "iteration 1515, dc_loss: 0.01786157861351967, tv_loss: 0.017371421679854393\n",
      "iteration 1516, dc_loss: 0.017861440777778625, tv_loss: 0.017371203750371933\n",
      "iteration 1517, dc_loss: 0.01786130480468273, tv_loss: 0.01737087406218052\n",
      "iteration 1518, dc_loss: 0.017861170694231987, tv_loss: 0.017370430752635002\n",
      "iteration 1519, dc_loss: 0.017861075699329376, tv_loss: 0.01737016811966896\n",
      "iteration 1520, dc_loss: 0.01786099374294281, tv_loss: 0.01736990176141262\n",
      "iteration 1521, dc_loss: 0.01786091737449169, tv_loss: 0.01736939325928688\n",
      "iteration 1522, dc_loss: 0.01786084659397602, tv_loss: 0.01736891083419323\n",
      "iteration 1523, dc_loss: 0.017860738560557365, tv_loss: 0.01736854761838913\n",
      "iteration 1524, dc_loss: 0.017860641703009605, tv_loss: 0.017368106171488762\n",
      "iteration 1525, dc_loss: 0.017860477790236473, tv_loss: 0.017367860302329063\n",
      "iteration 1526, dc_loss: 0.01786031946539879, tv_loss: 0.01736755482852459\n",
      "iteration 1527, dc_loss: 0.01786019280552864, tv_loss: 0.017367228865623474\n",
      "iteration 1528, dc_loss: 0.01786009781062603, tv_loss: 0.017366819083690643\n",
      "iteration 1529, dc_loss: 0.01786000281572342, tv_loss: 0.017366409301757812\n",
      "iteration 1530, dc_loss: 0.017859848216176033, tv_loss: 0.01736612804234028\n",
      "iteration 1531, dc_loss: 0.01785968244075775, tv_loss: 0.017365995794534683\n",
      "iteration 1532, dc_loss: 0.0178595632314682, tv_loss: 0.017365485429763794\n",
      "iteration 1533, dc_loss: 0.01785944774746895, tv_loss: 0.01736520230770111\n",
      "iteration 1534, dc_loss: 0.01785937137901783, tv_loss: 0.017364999279379845\n",
      "iteration 1535, dc_loss: 0.017859280109405518, tv_loss: 0.017364658415317535\n",
      "iteration 1536, dc_loss: 0.017859172075986862, tv_loss: 0.017364151775836945\n",
      "iteration 1537, dc_loss: 0.017859049141407013, tv_loss: 0.01736382581293583\n",
      "iteration 1538, dc_loss: 0.017858941107988358, tv_loss: 0.017363520339131355\n",
      "iteration 1539, dc_loss: 0.017858853563666344, tv_loss: 0.017363280057907104\n",
      "iteration 1540, dc_loss: 0.01785876229405403, tv_loss: 0.01736292615532875\n",
      "iteration 1541, dc_loss: 0.017858676612377167, tv_loss: 0.017362473532557487\n",
      "iteration 1542, dc_loss: 0.017858581617474556, tv_loss: 0.017362108454108238\n",
      "iteration 1543, dc_loss: 0.017858482897281647, tv_loss: 0.017361948266625404\n",
      "iteration 1544, dc_loss: 0.01785835064947605, tv_loss: 0.017361551523208618\n",
      "iteration 1545, dc_loss: 0.017858227714896202, tv_loss: 0.017361167818307877\n",
      "iteration 1546, dc_loss: 0.017858115956187248, tv_loss: 0.01736082322895527\n",
      "iteration 1547, dc_loss: 0.017857985571026802, tv_loss: 0.017360521480441093\n",
      "iteration 1548, dc_loss: 0.017857879400253296, tv_loss: 0.017360184341669083\n",
      "iteration 1549, dc_loss: 0.017857760190963745, tv_loss: 0.017359672114253044\n",
      "iteration 1550, dc_loss: 0.01785767264664173, tv_loss: 0.017359493300318718\n",
      "iteration 1551, dc_loss: 0.01785758137702942, tv_loss: 0.01735905557870865\n",
      "iteration 1552, dc_loss: 0.01785745657980442, tv_loss: 0.017358586192131042\n",
      "iteration 1553, dc_loss: 0.017857324331998825, tv_loss: 0.01735842227935791\n",
      "iteration 1554, dc_loss: 0.017857220023870468, tv_loss: 0.01735818199813366\n",
      "iteration 1555, dc_loss: 0.017857111990451813, tv_loss: 0.017357684671878815\n",
      "iteration 1556, dc_loss: 0.0178570244461298, tv_loss: 0.017357302829623222\n",
      "iteration 1557, dc_loss: 0.017856936901807785, tv_loss: 0.017357025295495987\n",
      "iteration 1558, dc_loss: 0.01785685308277607, tv_loss: 0.01735682412981987\n",
      "iteration 1559, dc_loss: 0.017856720834970474, tv_loss: 0.017356375232338905\n",
      "iteration 1560, dc_loss: 0.017856549471616745, tv_loss: 0.017355987802147865\n",
      "iteration 1561, dc_loss: 0.01785638928413391, tv_loss: 0.017355840653181076\n",
      "iteration 1562, dc_loss: 0.017856242135167122, tv_loss: 0.017355618998408318\n",
      "iteration 1563, dc_loss: 0.01785614900290966, tv_loss: 0.01735518127679825\n",
      "iteration 1564, dc_loss: 0.017856091260910034, tv_loss: 0.017354751005768776\n",
      "iteration 1565, dc_loss: 0.017856011167168617, tv_loss: 0.017354387789964676\n",
      "iteration 1566, dc_loss: 0.017855919897556305, tv_loss: 0.017354190349578857\n",
      "iteration 1567, dc_loss: 0.017855796962976456, tv_loss: 0.017353776842355728\n",
      "iteration 1568, dc_loss: 0.017855694517493248, tv_loss: 0.017353372648358345\n",
      "iteration 1569, dc_loss: 0.017855610698461533, tv_loss: 0.0173530001193285\n",
      "iteration 1570, dc_loss: 0.017855528742074966, tv_loss: 0.017352746799588203\n",
      "iteration 1571, dc_loss: 0.01785542257130146, tv_loss: 0.01735241338610649\n",
      "iteration 1572, dc_loss: 0.01785530336201191, tv_loss: 0.01735210232436657\n",
      "iteration 1573, dc_loss: 0.017855193465948105, tv_loss: 0.017351826652884483\n",
      "iteration 1574, dc_loss: 0.017855100333690643, tv_loss: 0.017351297661662102\n",
      "iteration 1575, dc_loss: 0.017854999750852585, tv_loss: 0.01735101081430912\n",
      "iteration 1576, dc_loss: 0.01785486936569214, tv_loss: 0.017350906506180763\n",
      "iteration 1577, dc_loss: 0.017854714766144753, tv_loss: 0.017350507900118828\n",
      "iteration 1578, dc_loss: 0.017854580655694008, tv_loss: 0.017350168898701668\n",
      "iteration 1579, dc_loss: 0.017854496836662292, tv_loss: 0.01734985038638115\n",
      "iteration 1580, dc_loss: 0.017854414880275726, tv_loss: 0.01734953187406063\n",
      "iteration 1581, dc_loss: 0.01785433106124401, tv_loss: 0.017349103465676308\n",
      "iteration 1582, dc_loss: 0.017854247242212296, tv_loss: 0.017348796129226685\n",
      "iteration 1583, dc_loss: 0.01785414107143879, tv_loss: 0.017348581925034523\n",
      "iteration 1584, dc_loss: 0.017854001373052597, tv_loss: 0.01734805293381214\n",
      "iteration 1585, dc_loss: 0.017853865399956703, tv_loss: 0.01734788902103901\n",
      "iteration 1586, dc_loss: 0.0178537480533123, tv_loss: 0.017347466200590134\n",
      "iteration 1587, dc_loss: 0.017853664234280586, tv_loss: 0.01734725385904312\n",
      "iteration 1588, dc_loss: 0.017853615805506706, tv_loss: 0.017346812412142754\n",
      "iteration 1589, dc_loss: 0.017853567376732826, tv_loss: 0.017346441745758057\n",
      "iteration 1590, dc_loss: 0.01785350777208805, tv_loss: 0.0173459704965353\n",
      "iteration 1591, dc_loss: 0.017853422090411186, tv_loss: 0.017345620319247246\n",
      "iteration 1592, dc_loss: 0.01785328797996044, tv_loss: 0.01734527014195919\n",
      "iteration 1593, dc_loss: 0.017853157594799995, tv_loss: 0.017344960942864418\n",
      "iteration 1594, dc_loss: 0.01785302720963955, tv_loss: 0.017344655469059944\n",
      "iteration 1595, dc_loss: 0.017852870747447014, tv_loss: 0.01734442450106144\n",
      "iteration 1596, dc_loss: 0.017852723598480225, tv_loss: 0.017344100400805473\n",
      "iteration 1597, dc_loss: 0.01785260997712612, tv_loss: 0.017343910411000252\n",
      "iteration 1598, dc_loss: 0.017852501943707466, tv_loss: 0.017343342304229736\n",
      "iteration 1599, dc_loss: 0.01785244233906269, tv_loss: 0.017343057319521904\n",
      "iteration 1600, dc_loss: 0.017852410674095154, tv_loss: 0.017342757433652878\n",
      "iteration 1601, dc_loss: 0.017852341756224632, tv_loss: 0.017342448234558105\n",
      "iteration 1602, dc_loss: 0.017852243036031723, tv_loss: 0.017342133447527885\n",
      "iteration 1603, dc_loss: 0.01785210147500038, tv_loss: 0.017341922968626022\n",
      "iteration 1604, dc_loss: 0.017851971089839935, tv_loss: 0.017341608181595802\n",
      "iteration 1605, dc_loss: 0.01785185933113098, tv_loss: 0.017341170459985733\n",
      "iteration 1606, dc_loss: 0.017851751297712326, tv_loss: 0.01734098233282566\n",
      "iteration 1607, dc_loss: 0.017851663753390312, tv_loss: 0.01734057068824768\n",
      "iteration 1608, dc_loss: 0.01785161904990673, tv_loss: 0.017340127378702164\n",
      "iteration 1609, dc_loss: 0.017851542681455612, tv_loss: 0.017339937388896942\n",
      "iteration 1610, dc_loss: 0.01785142347216606, tv_loss: 0.01733960397541523\n",
      "iteration 1611, dc_loss: 0.01785128004848957, tv_loss: 0.01733921840786934\n",
      "iteration 1612, dc_loss: 0.017851144075393677, tv_loss: 0.01733909733593464\n",
      "iteration 1613, dc_loss: 0.01785101927816868, tv_loss: 0.01733877882361412\n",
      "iteration 1614, dc_loss: 0.017850924283266068, tv_loss: 0.0173384677618742\n",
      "iteration 1615, dc_loss: 0.017850816249847412, tv_loss: 0.01733820140361786\n",
      "iteration 1616, dc_loss: 0.0178507249802351, tv_loss: 0.017337802797555923\n",
      "iteration 1617, dc_loss: 0.017850659787654877, tv_loss: 0.017337441444396973\n",
      "iteration 1618, dc_loss: 0.017850583419203758, tv_loss: 0.017337091267108917\n",
      "iteration 1619, dc_loss: 0.017850499600172043, tv_loss: 0.017336931079626083\n",
      "iteration 1620, dc_loss: 0.017850400879979134, tv_loss: 0.017336446791887283\n",
      "iteration 1621, dc_loss: 0.017850322648882866, tv_loss: 0.017336232587695122\n",
      "iteration 1622, dc_loss: 0.017850223928689957, tv_loss: 0.017336128279566765\n",
      "iteration 1623, dc_loss: 0.017850104719400406, tv_loss: 0.017335742712020874\n",
      "iteration 1624, dc_loss: 0.017849957570433617, tv_loss: 0.017335236072540283\n",
      "iteration 1625, dc_loss: 0.017849812284111977, tv_loss: 0.017335081472992897\n",
      "iteration 1626, dc_loss: 0.017849678173661232, tv_loss: 0.017334945499897003\n",
      "iteration 1627, dc_loss: 0.017849549651145935, tv_loss: 0.017334528267383575\n",
      "iteration 1628, dc_loss: 0.017849501222372055, tv_loss: 0.017334191128611565\n",
      "iteration 1629, dc_loss: 0.01784946769475937, tv_loss: 0.01733381673693657\n",
      "iteration 1630, dc_loss: 0.01784941926598549, tv_loss: 0.01733332686126232\n",
      "iteration 1631, dc_loss: 0.01784936711192131, tv_loss: 0.017333071678876877\n",
      "iteration 1632, dc_loss: 0.01784929446876049, tv_loss: 0.017332719638943672\n",
      "iteration 1633, dc_loss: 0.01784922368824482, tv_loss: 0.01733226142823696\n",
      "iteration 1634, dc_loss: 0.01784912310540676, tv_loss: 0.01733214221894741\n",
      "iteration 1635, dc_loss: 0.017848987132310867, tv_loss: 0.017331955954432487\n",
      "iteration 1636, dc_loss: 0.017848847433924675, tv_loss: 0.017331548035144806\n",
      "iteration 1637, dc_loss: 0.01784871518611908, tv_loss: 0.0173311959952116\n",
      "iteration 1638, dc_loss: 0.017848597839474678, tv_loss: 0.017331229522824287\n",
      "iteration 1639, dc_loss: 0.01784849353134632, tv_loss: 0.017330892384052277\n",
      "iteration 1640, dc_loss: 0.01784837804734707, tv_loss: 0.017330465838313103\n",
      "iteration 1641, dc_loss: 0.017848284915089607, tv_loss: 0.017330056056380272\n",
      "iteration 1642, dc_loss: 0.017848214134573936, tv_loss: 0.01732981950044632\n",
      "iteration 1643, dc_loss: 0.017848143354058266, tv_loss: 0.01732960157096386\n",
      "iteration 1644, dc_loss: 0.017848031595349312, tv_loss: 0.01732916198670864\n",
      "iteration 1645, dc_loss: 0.017847910523414612, tv_loss: 0.017328783869743347\n",
      "iteration 1646, dc_loss: 0.01784779503941536, tv_loss: 0.017328660935163498\n",
      "iteration 1647, dc_loss: 0.017847709357738495, tv_loss: 0.017328353598713875\n",
      "iteration 1648, dc_loss: 0.017847644165158272, tv_loss: 0.017327802255749702\n",
      "iteration 1649, dc_loss: 0.01784760318696499, tv_loss: 0.017327388748526573\n",
      "iteration 1650, dc_loss: 0.017847560346126556, tv_loss: 0.017327290028333664\n",
      "iteration 1651, dc_loss: 0.017847472801804543, tv_loss: 0.01732700876891613\n",
      "iteration 1652, dc_loss: 0.017847368493676186, tv_loss: 0.017326371744275093\n",
      "iteration 1653, dc_loss: 0.017847295850515366, tv_loss: 0.01732635498046875\n",
      "iteration 1654, dc_loss: 0.01784721575677395, tv_loss: 0.017326021566987038\n",
      "iteration 1655, dc_loss: 0.017847079783678055, tv_loss: 0.017325637862086296\n",
      "iteration 1656, dc_loss: 0.017846910282969475, tv_loss: 0.017325349152088165\n",
      "iteration 1657, dc_loss: 0.017846787348389626, tv_loss: 0.017325252294540405\n",
      "iteration 1658, dc_loss: 0.017846686765551567, tv_loss: 0.017324917018413544\n",
      "iteration 1659, dc_loss: 0.017846614122390747, tv_loss: 0.017324671149253845\n",
      "iteration 1660, dc_loss: 0.01784651167690754, tv_loss: 0.017324524000287056\n",
      "iteration 1661, dc_loss: 0.01784641481935978, tv_loss: 0.017324157059192657\n",
      "iteration 1662, dc_loss: 0.017846301198005676, tv_loss: 0.017323823645710945\n",
      "iteration 1663, dc_loss: 0.017846187576651573, tv_loss: 0.017323771491646767\n",
      "iteration 1664, dc_loss: 0.01784610003232956, tv_loss: 0.01732339896261692\n",
      "iteration 1665, dc_loss: 0.017846008762717247, tv_loss: 0.01732306182384491\n",
      "iteration 1666, dc_loss: 0.017845915630459785, tv_loss: 0.017322663217782974\n",
      "iteration 1667, dc_loss: 0.017845842987298965, tv_loss: 0.01732245460152626\n",
      "iteration 1668, dc_loss: 0.017845790833234787, tv_loss: 0.017322134226560593\n",
      "iteration 1669, dc_loss: 0.01784573495388031, tv_loss: 0.017321692779660225\n",
      "iteration 1670, dc_loss: 0.017845680937170982, tv_loss: 0.01732131652534008\n",
      "iteration 1671, dc_loss: 0.017845606431365013, tv_loss: 0.017321156337857246\n",
      "iteration 1672, dc_loss: 0.0178455151617527, tv_loss: 0.01732080616056919\n",
      "iteration 1673, dc_loss: 0.017845429480075836, tv_loss: 0.017320457845926285\n",
      "iteration 1674, dc_loss: 0.017845334485173225, tv_loss: 0.017320292070508003\n",
      "iteration 1675, dc_loss: 0.01784520410001278, tv_loss: 0.017320027574896812\n",
      "iteration 1676, dc_loss: 0.017845071852207184, tv_loss: 0.017319638282060623\n",
      "iteration 1677, dc_loss: 0.017844917252659798, tv_loss: 0.017319442704319954\n",
      "iteration 1678, dc_loss: 0.01784478686749935, tv_loss: 0.01731923036277294\n",
      "iteration 1679, dc_loss: 0.017844704911112785, tv_loss: 0.017318934202194214\n",
      "iteration 1680, dc_loss: 0.017844652757048607, tv_loss: 0.017318500205874443\n",
      "iteration 1681, dc_loss: 0.017844589427113533, tv_loss: 0.017318183556199074\n",
      "iteration 1682, dc_loss: 0.017844529822468758, tv_loss: 0.017317937687039375\n",
      "iteration 1683, dc_loss: 0.01784444786608219, tv_loss: 0.017317524179816246\n",
      "iteration 1684, dc_loss: 0.017844367772340775, tv_loss: 0.017317278310656548\n",
      "iteration 1685, dc_loss: 0.017844246700406075, tv_loss: 0.017317192628979683\n",
      "iteration 1686, dc_loss: 0.01784413866698742, tv_loss: 0.017316831275820732\n",
      "iteration 1687, dc_loss: 0.017844051122665405, tv_loss: 0.017316482961177826\n",
      "iteration 1688, dc_loss: 0.01784399151802063, tv_loss: 0.017316114157438278\n",
      "iteration 1689, dc_loss: 0.01784393936395645, tv_loss: 0.017315862700343132\n",
      "iteration 1690, dc_loss: 0.01784391514956951, tv_loss: 0.017315497621893883\n",
      "iteration 1691, dc_loss: 0.017843861132860184, tv_loss: 0.01731516234576702\n",
      "iteration 1692, dc_loss: 0.01784375123679638, tv_loss: 0.017314841970801353\n",
      "iteration 1693, dc_loss: 0.01784360408782959, tv_loss: 0.01731453463435173\n",
      "iteration 1694, dc_loss: 0.017843468114733696, tv_loss: 0.017314249649643898\n",
      "iteration 1695, dc_loss: 0.01784333772957325, tv_loss: 0.01731419749557972\n",
      "iteration 1696, dc_loss: 0.01784321665763855, tv_loss: 0.01731392927467823\n",
      "iteration 1697, dc_loss: 0.01784316450357437, tv_loss: 0.017313478514552116\n",
      "iteration 1698, dc_loss: 0.01784317009150982, tv_loss: 0.017313087359070778\n",
      "iteration 1699, dc_loss: 0.017843151465058327, tv_loss: 0.017312942072749138\n",
      "iteration 1700, dc_loss: 0.017843078821897507, tv_loss: 0.017312616109848022\n",
      "iteration 1701, dc_loss: 0.01784294843673706, tv_loss: 0.017312100157141685\n",
      "iteration 1702, dc_loss: 0.017842797562479973, tv_loss: 0.01731189899146557\n",
      "iteration 1703, dc_loss: 0.017842689529061317, tv_loss: 0.017311839386820793\n",
      "iteration 1704, dc_loss: 0.017842616885900497, tv_loss: 0.017311526462435722\n",
      "iteration 1705, dc_loss: 0.01784255914390087, tv_loss: 0.01731107383966446\n",
      "iteration 1706, dc_loss: 0.01784248650074005, tv_loss: 0.017310835421085358\n",
      "iteration 1707, dc_loss: 0.017842400819063187, tv_loss: 0.01731063611805439\n",
      "iteration 1708, dc_loss: 0.017842290922999382, tv_loss: 0.017310524359345436\n",
      "iteration 1709, dc_loss: 0.017842158675193787, tv_loss: 0.017310120165348053\n",
      "iteration 1710, dc_loss: 0.017842043191194534, tv_loss: 0.017309805378317833\n",
      "iteration 1711, dc_loss: 0.01784197986125946, tv_loss: 0.017309503629803658\n",
      "iteration 1712, dc_loss: 0.017841920256614685, tv_loss: 0.01730930432677269\n",
      "iteration 1713, dc_loss: 0.017841849476099014, tv_loss: 0.017308995127677917\n",
      "iteration 1714, dc_loss: 0.017841758206486702, tv_loss: 0.017308710142970085\n",
      "iteration 1715, dc_loss: 0.017841655761003494, tv_loss: 0.0173083133995533\n",
      "iteration 1716, dc_loss: 0.01784159243106842, tv_loss: 0.017308056354522705\n",
      "iteration 1717, dc_loss: 0.017841501161456108, tv_loss: 0.01730780489742756\n",
      "iteration 1718, dc_loss: 0.017841389402747154, tv_loss: 0.017307505011558533\n",
      "iteration 1719, dc_loss: 0.017841285094618797, tv_loss: 0.017307288944721222\n",
      "iteration 1720, dc_loss: 0.017841191962361336, tv_loss: 0.01730707660317421\n",
      "iteration 1721, dc_loss: 0.01784113235771656, tv_loss: 0.017306536436080933\n",
      "iteration 1722, dc_loss: 0.017841091379523277, tv_loss: 0.017306357622146606\n",
      "iteration 1723, dc_loss: 0.017841055989265442, tv_loss: 0.017306072637438774\n",
      "iteration 1724, dc_loss: 0.017840998247265816, tv_loss: 0.01730584166944027\n",
      "iteration 1725, dc_loss: 0.017840905115008354, tv_loss: 0.017305463552474976\n",
      "iteration 1726, dc_loss: 0.017840798944234848, tv_loss: 0.017305094748735428\n",
      "iteration 1727, dc_loss: 0.017840689048171043, tv_loss: 0.01730496436357498\n",
      "iteration 1728, dc_loss: 0.017840566113591194, tv_loss: 0.017304779961705208\n",
      "iteration 1729, dc_loss: 0.01784045435488224, tv_loss: 0.017304416745901108\n",
      "iteration 1730, dc_loss: 0.017840351909399033, tv_loss: 0.017304060980677605\n",
      "iteration 1731, dc_loss: 0.01784026063978672, tv_loss: 0.017303790897130966\n",
      "iteration 1732, dc_loss: 0.017840158194303513, tv_loss: 0.017303675413131714\n",
      "iteration 1733, dc_loss: 0.017840059474110603, tv_loss: 0.017303358763456345\n",
      "iteration 1734, dc_loss: 0.017839988693594933, tv_loss: 0.017303112894296646\n",
      "iteration 1735, dc_loss: 0.01783992163836956, tv_loss: 0.017302904278039932\n",
      "iteration 1736, dc_loss: 0.017839841544628143, tv_loss: 0.017302656546235085\n",
      "iteration 1737, dc_loss: 0.017839744687080383, tv_loss: 0.01730230823159218\n",
      "iteration 1738, dc_loss: 0.017839640378952026, tv_loss: 0.01730211265385151\n",
      "iteration 1739, dc_loss: 0.01783958449959755, tv_loss: 0.01730172708630562\n",
      "iteration 1740, dc_loss: 0.01783956028521061, tv_loss: 0.017301484942436218\n",
      "iteration 1741, dc_loss: 0.017839517444372177, tv_loss: 0.017301099374890327\n",
      "iteration 1742, dc_loss: 0.017839420586824417, tv_loss: 0.017300736159086227\n",
      "iteration 1743, dc_loss: 0.01783931627869606, tv_loss: 0.01730046421289444\n",
      "iteration 1744, dc_loss: 0.01783919148147106, tv_loss: 0.0173003152012825\n",
      "iteration 1745, dc_loss: 0.017839085310697556, tv_loss: 0.017300112172961235\n",
      "iteration 1746, dc_loss: 0.017839014530181885, tv_loss: 0.017299847677350044\n",
      "iteration 1747, dc_loss: 0.017838967964053154, tv_loss: 0.01729942113161087\n",
      "iteration 1748, dc_loss: 0.017838917672634125, tv_loss: 0.017299097031354904\n",
      "iteration 1749, dc_loss: 0.017838839441537857, tv_loss: 0.017298772931098938\n",
      "iteration 1750, dc_loss: 0.01783875562250614, tv_loss: 0.0172985028475523\n",
      "iteration 1751, dc_loss: 0.017838628962635994, tv_loss: 0.017298266291618347\n",
      "iteration 1752, dc_loss: 0.017838522791862488, tv_loss: 0.0172980185598135\n",
      "iteration 1753, dc_loss: 0.01783844083547592, tv_loss: 0.017297977581620216\n",
      "iteration 1754, dc_loss: 0.01783837005496025, tv_loss: 0.017297562211751938\n",
      "iteration 1755, dc_loss: 0.01783827878534794, tv_loss: 0.01729709282517433\n",
      "iteration 1756, dc_loss: 0.017838168889284134, tv_loss: 0.017296839505434036\n",
      "iteration 1757, dc_loss: 0.017838124185800552, tv_loss: 0.017296696081757545\n",
      "iteration 1758, dc_loss: 0.017838075757026672, tv_loss: 0.01729634590446949\n",
      "iteration 1759, dc_loss: 0.017838003113865852, tv_loss: 0.017295969650149345\n",
      "iteration 1760, dc_loss: 0.017837904393672943, tv_loss: 0.01729585789144039\n",
      "iteration 1761, dc_loss: 0.017837779596447945, tv_loss: 0.01729566603899002\n",
      "iteration 1762, dc_loss: 0.017837677150964737, tv_loss: 0.01729523576796055\n",
      "iteration 1763, dc_loss: 0.01783759891986847, tv_loss: 0.01729484274983406\n",
      "iteration 1764, dc_loss: 0.01783749833703041, tv_loss: 0.017294758930802345\n",
      "iteration 1765, dc_loss: 0.017837390303611755, tv_loss: 0.017294539138674736\n",
      "iteration 1766, dc_loss: 0.017837289720773697, tv_loss: 0.017294390127062798\n",
      "iteration 1767, dc_loss: 0.01783721335232258, tv_loss: 0.01729404926300049\n",
      "iteration 1768, dc_loss: 0.017837123945355415, tv_loss: 0.017293784767389297\n",
      "iteration 1769, dc_loss: 0.017837055027484894, tv_loss: 0.017293503507971764\n",
      "iteration 1770, dc_loss: 0.01783698983490467, tv_loss: 0.0172931719571352\n",
      "iteration 1771, dc_loss: 0.01783694513142109, tv_loss: 0.01729285717010498\n",
      "iteration 1772, dc_loss: 0.017836904153227806, tv_loss: 0.017292672768235207\n",
      "iteration 1773, dc_loss: 0.017836855724453926, tv_loss: 0.017292380332946777\n",
      "iteration 1774, dc_loss: 0.01783677004277706, tv_loss: 0.017292046919465065\n",
      "iteration 1775, dc_loss: 0.01783665269613266, tv_loss: 0.017291825264692307\n",
      "iteration 1776, dc_loss: 0.017836537212133408, tv_loss: 0.017291715368628502\n",
      "iteration 1777, dc_loss: 0.017836442217230797, tv_loss: 0.017291298136115074\n",
      "iteration 1778, dc_loss: 0.01783636398613453, tv_loss: 0.017291009426116943\n",
      "iteration 1779, dc_loss: 0.017836257815361023, tv_loss: 0.017290841788053513\n",
      "iteration 1780, dc_loss: 0.017836153507232666, tv_loss: 0.01729067787528038\n",
      "iteration 1781, dc_loss: 0.017836060374975204, tv_loss: 0.017290456220507622\n",
      "iteration 1782, dc_loss: 0.017835984006524086, tv_loss: 0.017290273681282997\n",
      "iteration 1783, dc_loss: 0.017835937440395355, tv_loss: 0.017289824783802032\n",
      "iteration 1784, dc_loss: 0.01783590391278267, tv_loss: 0.017289485782384872\n",
      "iteration 1785, dc_loss: 0.017835844308137894, tv_loss: 0.017289279028773308\n",
      "iteration 1786, dc_loss: 0.01783575862646103, tv_loss: 0.017289089038968086\n",
      "iteration 1787, dc_loss: 0.017835693433880806, tv_loss: 0.017288710922002792\n",
      "iteration 1788, dc_loss: 0.017835630103945732, tv_loss: 0.017288364470005035\n",
      "iteration 1789, dc_loss: 0.017835581675171852, tv_loss: 0.0172880869358778\n",
      "iteration 1790, dc_loss: 0.017835522070527077, tv_loss: 0.017287852242588997\n",
      "iteration 1791, dc_loss: 0.01783538982272148, tv_loss: 0.017287680879235268\n",
      "iteration 1792, dc_loss: 0.017835240811109543, tv_loss: 0.017287572845816612\n",
      "iteration 1793, dc_loss: 0.017835121601819992, tv_loss: 0.01728718914091587\n",
      "iteration 1794, dc_loss: 0.017835024744272232, tv_loss: 0.017286989837884903\n",
      "iteration 1795, dc_loss: 0.017834926024079323, tv_loss: 0.017286987975239754\n",
      "iteration 1796, dc_loss: 0.017834797501564026, tv_loss: 0.017286736518144608\n",
      "iteration 1797, dc_loss: 0.01783469319343567, tv_loss: 0.017286503687500954\n",
      "iteration 1798, dc_loss: 0.01783463917672634, tv_loss: 0.017286159098148346\n",
      "iteration 1799, dc_loss: 0.017834587022662163, tv_loss: 0.017285874113440514\n",
      "iteration 1800, dc_loss: 0.017834536731243134, tv_loss: 0.017285514622926712\n",
      "iteration 1801, dc_loss: 0.017834525555372238, tv_loss: 0.01728517934679985\n",
      "iteration 1802, dc_loss: 0.01783451996743679, tv_loss: 0.017284821718931198\n",
      "iteration 1803, dc_loss: 0.017834490165114403, tv_loss: 0.017284324392676353\n",
      "iteration 1804, dc_loss: 0.017834458500146866, tv_loss: 0.017284173518419266\n",
      "iteration 1805, dc_loss: 0.01783437468111515, tv_loss: 0.01728391833603382\n",
      "iteration 1806, dc_loss: 0.017834246158599854, tv_loss: 0.017283674329519272\n",
      "iteration 1807, dc_loss: 0.017834078520536423, tv_loss: 0.017283542081713676\n",
      "iteration 1808, dc_loss: 0.01783394068479538, tv_loss: 0.01728319562971592\n",
      "iteration 1809, dc_loss: 0.017833849415183067, tv_loss: 0.017282936722040176\n",
      "iteration 1810, dc_loss: 0.017833780497312546, tv_loss: 0.017282845452427864\n",
      "iteration 1811, dc_loss: 0.017833685502409935, tv_loss: 0.017282526940107346\n",
      "iteration 1812, dc_loss: 0.017833631485700607, tv_loss: 0.01728219911456108\n",
      "iteration 1813, dc_loss: 0.01783360168337822, tv_loss: 0.017281875014305115\n",
      "iteration 1814, dc_loss: 0.01783357560634613, tv_loss: 0.01728157326579094\n",
      "iteration 1815, dc_loss: 0.0178335290402174, tv_loss: 0.01728127710521221\n",
      "iteration 1816, dc_loss: 0.017833419144153595, tv_loss: 0.017281079664826393\n",
      "iteration 1817, dc_loss: 0.017833292484283447, tv_loss: 0.0172809436917305\n",
      "iteration 1818, dc_loss: 0.01783318817615509, tv_loss: 0.01728072017431259\n",
      "iteration 1819, dc_loss: 0.01783309504389763, tv_loss: 0.01728060282766819\n",
      "iteration 1820, dc_loss: 0.017833013087511063, tv_loss: 0.017280345782637596\n",
      "iteration 1821, dc_loss: 0.01783292554318905, tv_loss: 0.01728018932044506\n",
      "iteration 1822, dc_loss: 0.01783285103738308, tv_loss: 0.017279943451285362\n",
      "iteration 1823, dc_loss: 0.01783278025686741, tv_loss: 0.01727965660393238\n",
      "iteration 1824, dc_loss: 0.01783273182809353, tv_loss: 0.017279338091611862\n",
      "iteration 1825, dc_loss: 0.01783268339931965, tv_loss: 0.017278891056776047\n",
      "iteration 1826, dc_loss: 0.01783260703086853, tv_loss: 0.01727873831987381\n",
      "iteration 1827, dc_loss: 0.017832497134804726, tv_loss: 0.017278624698519707\n",
      "iteration 1828, dc_loss: 0.017832383513450623, tv_loss: 0.017278436571359634\n",
      "iteration 1829, dc_loss: 0.017832310870289803, tv_loss: 0.017278145998716354\n",
      "iteration 1830, dc_loss: 0.017832273617386818, tv_loss: 0.01727779023349285\n",
      "iteration 1831, dc_loss: 0.017832234501838684, tv_loss: 0.017277568578720093\n",
      "iteration 1832, dc_loss: 0.017832156270742416, tv_loss: 0.01727721095085144\n",
      "iteration 1833, dc_loss: 0.017832042649388313, tv_loss: 0.01727708987891674\n",
      "iteration 1834, dc_loss: 0.01783190295100212, tv_loss: 0.01727692410349846\n",
      "iteration 1835, dc_loss: 0.017831798642873764, tv_loss: 0.017276503145694733\n",
      "iteration 1836, dc_loss: 0.01783171109855175, tv_loss: 0.017276352271437645\n",
      "iteration 1837, dc_loss: 0.01783166266977787, tv_loss: 0.017276093363761902\n",
      "iteration 1838, dc_loss: 0.017831603065133095, tv_loss: 0.017275778576731682\n",
      "iteration 1839, dc_loss: 0.017831552773714066, tv_loss: 0.017275547608733177\n",
      "iteration 1840, dc_loss: 0.017831532284617424, tv_loss: 0.01727508194744587\n",
      "iteration 1841, dc_loss: 0.017831481993198395, tv_loss: 0.01727491430938244\n",
      "iteration 1842, dc_loss: 0.017831437289714813, tv_loss: 0.0172746479511261\n",
      "iteration 1843, dc_loss: 0.017831390723586082, tv_loss: 0.01727423258125782\n",
      "iteration 1844, dc_loss: 0.017831331118941307, tv_loss: 0.01727398671209812\n",
      "iteration 1845, dc_loss: 0.017831282690167427, tv_loss: 0.017273779958486557\n",
      "iteration 1846, dc_loss: 0.017831208184361458, tv_loss: 0.017273569479584694\n",
      "iteration 1847, dc_loss: 0.017831098288297653, tv_loss: 0.017273418605327606\n",
      "iteration 1848, dc_loss: 0.01783095858991146, tv_loss: 0.0172731876373291\n",
      "iteration 1849, dc_loss: 0.017830781638622284, tv_loss: 0.017273036763072014\n",
      "iteration 1850, dc_loss: 0.01783064380288124, tv_loss: 0.017272839322686195\n",
      "iteration 1851, dc_loss: 0.017830558121204376, tv_loss: 0.017272574827075005\n",
      "iteration 1852, dc_loss: 0.017830505967140198, tv_loss: 0.017272358760237694\n",
      "iteration 1853, dc_loss: 0.017830463126301765, tv_loss: 0.017272042110562325\n",
      "iteration 1854, dc_loss: 0.017830397933721542, tv_loss: 0.017271699383854866\n",
      "iteration 1855, dc_loss: 0.01783032901585102, tv_loss: 0.017271338030695915\n",
      "iteration 1856, dc_loss: 0.017830297350883484, tv_loss: 0.017271073535084724\n",
      "iteration 1857, dc_loss: 0.017830243334174156, tv_loss: 0.017270969226956367\n",
      "iteration 1858, dc_loss: 0.017830172553658485, tv_loss: 0.017270565032958984\n",
      "iteration 1859, dc_loss: 0.017830131575465202, tv_loss: 0.01727038063108921\n",
      "iteration 1860, dc_loss: 0.01783013343811035, tv_loss: 0.017270203679800034\n",
      "iteration 1861, dc_loss: 0.01783008687198162, tv_loss: 0.017269937321543694\n",
      "iteration 1862, dc_loss: 0.017829978838562965, tv_loss: 0.017269445583224297\n",
      "iteration 1863, dc_loss: 0.017829863354563713, tv_loss: 0.017269274219870567\n",
      "iteration 1864, dc_loss: 0.01782977022230625, tv_loss: 0.017269287258386612\n",
      "iteration 1865, dc_loss: 0.017829662188887596, tv_loss: 0.017269128933548927\n",
      "iteration 1866, dc_loss: 0.017829537391662598, tv_loss: 0.017268840223550797\n",
      "iteration 1867, dc_loss: 0.017829475924372673, tv_loss: 0.01726859249174595\n",
      "iteration 1868, dc_loss: 0.01782943122088909, tv_loss: 0.017268357798457146\n",
      "iteration 1869, dc_loss: 0.017829393967986107, tv_loss: 0.017268044874072075\n",
      "iteration 1870, dc_loss: 0.01782933436334133, tv_loss: 0.01726778782904148\n",
      "iteration 1871, dc_loss: 0.01782923750579357, tv_loss: 0.017267659306526184\n",
      "iteration 1872, dc_loss: 0.017829157412052155, tv_loss: 0.017267389222979546\n",
      "iteration 1873, dc_loss: 0.017829084768891335, tv_loss: 0.01726706512272358\n",
      "iteration 1874, dc_loss: 0.017829004675149918, tv_loss: 0.017266906797885895\n",
      "iteration 1875, dc_loss: 0.01782895252108574, tv_loss: 0.017266806215047836\n",
      "iteration 1876, dc_loss: 0.017828891053795815, tv_loss: 0.017266424372792244\n",
      "iteration 1877, dc_loss: 0.017828814685344696, tv_loss: 0.01726614497601986\n",
      "iteration 1878, dc_loss: 0.01782877929508686, tv_loss: 0.017265858128666878\n",
      "iteration 1879, dc_loss: 0.017828745767474174, tv_loss: 0.017265647649765015\n",
      "iteration 1880, dc_loss: 0.01782868430018425, tv_loss: 0.01726541295647621\n",
      "iteration 1881, dc_loss: 0.01782859116792679, tv_loss: 0.0172653179615736\n",
      "iteration 1882, dc_loss: 0.01782849244773388, tv_loss: 0.01726497709751129\n",
      "iteration 1883, dc_loss: 0.017828376963734627, tv_loss: 0.017264682799577713\n",
      "iteration 1884, dc_loss: 0.017828281968832016, tv_loss: 0.017264649271965027\n",
      "iteration 1885, dc_loss: 0.01782817766070366, tv_loss: 0.01726454310119152\n",
      "iteration 1886, dc_loss: 0.01782809942960739, tv_loss: 0.017264237627387047\n",
      "iteration 1887, dc_loss: 0.017828090116381645, tv_loss: 0.01726381480693817\n",
      "iteration 1888, dc_loss: 0.017828091979026794, tv_loss: 0.01726359874010086\n",
      "iteration 1889, dc_loss: 0.01782805472612381, tv_loss: 0.017263412475585938\n",
      "iteration 1890, dc_loss: 0.01782798022031784, tv_loss: 0.017263147979974747\n",
      "iteration 1891, dc_loss: 0.01782791130244732, tv_loss: 0.017262820154428482\n",
      "iteration 1892, dc_loss: 0.01782780885696411, tv_loss: 0.01726270094513893\n",
      "iteration 1893, dc_loss: 0.017827698960900307, tv_loss: 0.017262594774365425\n",
      "iteration 1894, dc_loss: 0.01782759092748165, tv_loss: 0.01726228930056095\n",
      "iteration 1895, dc_loss: 0.017827481031417847, tv_loss: 0.01726212352514267\n",
      "iteration 1896, dc_loss: 0.017827384173870087, tv_loss: 0.017261842265725136\n",
      "iteration 1897, dc_loss: 0.017827332019805908, tv_loss: 0.017261458560824394\n",
      "iteration 1898, dc_loss: 0.017827292904257774, tv_loss: 0.017261188477277756\n",
      "iteration 1899, dc_loss: 0.01782727800309658, tv_loss: 0.017260951921343803\n",
      "iteration 1900, dc_loss: 0.017827246338129044, tv_loss: 0.017260733991861343\n",
      "iteration 1901, dc_loss: 0.017827194184064865, tv_loss: 0.017260486260056496\n",
      "iteration 1902, dc_loss: 0.0178271122276783, tv_loss: 0.0172603540122509\n",
      "iteration 1903, dc_loss: 0.017827026546001434, tv_loss: 0.017260035499930382\n",
      "iteration 1904, dc_loss: 0.017826922237873077, tv_loss: 0.017259744927287102\n",
      "iteration 1905, dc_loss: 0.017826853320002556, tv_loss: 0.01725955493748188\n",
      "iteration 1906, dc_loss: 0.017826758325099945, tv_loss: 0.017259497195482254\n",
      "iteration 1907, dc_loss: 0.017826685681939125, tv_loss: 0.0172592680901289\n",
      "iteration 1908, dc_loss: 0.017826613038778305, tv_loss: 0.01725894957780838\n",
      "iteration 1909, dc_loss: 0.017826547846198082, tv_loss: 0.017258737236261368\n",
      "iteration 1910, dc_loss: 0.017826512455940247, tv_loss: 0.017258331179618835\n",
      "iteration 1911, dc_loss: 0.01782645471394062, tv_loss: 0.017258064821362495\n",
      "iteration 1912, dc_loss: 0.017826393246650696, tv_loss: 0.017257822677493095\n",
      "iteration 1913, dc_loss: 0.017826354131102562, tv_loss: 0.017257576808333397\n",
      "iteration 1914, dc_loss: 0.01782626286149025, tv_loss: 0.017257506027817726\n",
      "iteration 1915, dc_loss: 0.01782616786658764, tv_loss: 0.01725734770298004\n",
      "iteration 1916, dc_loss: 0.017826084047555923, tv_loss: 0.017256969586014748\n",
      "iteration 1917, dc_loss: 0.017826026305556297, tv_loss: 0.017256833612918854\n",
      "iteration 1918, dc_loss: 0.017825933173298836, tv_loss: 0.017256660386919975\n",
      "iteration 1919, dc_loss: 0.017825869843363762, tv_loss: 0.017256369814276695\n",
      "iteration 1920, dc_loss: 0.017825817689299583, tv_loss: 0.017256062477827072\n",
      "iteration 1921, dc_loss: 0.017825795337557793, tv_loss: 0.01725597493350506\n",
      "iteration 1922, dc_loss: 0.01782575249671936, tv_loss: 0.017255689948797226\n",
      "iteration 1923, dc_loss: 0.01782572828233242, tv_loss: 0.017255233600735664\n",
      "iteration 1924, dc_loss: 0.017825691029429436, tv_loss: 0.01725512184202671\n",
      "iteration 1925, dc_loss: 0.01782563142478466, tv_loss: 0.017254769802093506\n",
      "iteration 1926, dc_loss: 0.0178255345672369, tv_loss: 0.017254535108804703\n",
      "iteration 1927, dc_loss: 0.017825432121753693, tv_loss: 0.01725449413061142\n",
      "iteration 1928, dc_loss: 0.017825331538915634, tv_loss: 0.017254313454031944\n",
      "iteration 1929, dc_loss: 0.017825236544013023, tv_loss: 0.017253896221518517\n",
      "iteration 1930, dc_loss: 0.01782514899969101, tv_loss: 0.017253773286938667\n",
      "iteration 1931, dc_loss: 0.017825070768594742, tv_loss: 0.017253654077649117\n",
      "iteration 1932, dc_loss: 0.017824996262788773, tv_loss: 0.01725340634584427\n",
      "iteration 1933, dc_loss: 0.017824910581111908, tv_loss: 0.017253093421459198\n",
      "iteration 1934, dc_loss: 0.0178248081356287, tv_loss: 0.017252974212169647\n",
      "iteration 1935, dc_loss: 0.01782473362982273, tv_loss: 0.017252806574106216\n",
      "iteration 1936, dc_loss: 0.017824718728661537, tv_loss: 0.017252475023269653\n",
      "iteration 1937, dc_loss: 0.017824728041887283, tv_loss: 0.01725221984088421\n",
      "iteration 1938, dc_loss: 0.01782471127808094, tv_loss: 0.017252027988433838\n",
      "iteration 1939, dc_loss: 0.017824655398726463, tv_loss: 0.017251744866371155\n",
      "iteration 1940, dc_loss: 0.017824606969952583, tv_loss: 0.017251448705792427\n",
      "iteration 1941, dc_loss: 0.01782454177737236, tv_loss: 0.017251264303922653\n",
      "iteration 1942, dc_loss: 0.01782446913421154, tv_loss: 0.017250988632440567\n",
      "iteration 1943, dc_loss: 0.017824357375502586, tv_loss: 0.017250733450055122\n",
      "iteration 1944, dc_loss: 0.01782427355647087, tv_loss: 0.01725052110850811\n",
      "iteration 1945, dc_loss: 0.017824171110987663, tv_loss: 0.017250461503863335\n",
      "iteration 1946, dc_loss: 0.017824063077569008, tv_loss: 0.017250245437026024\n",
      "iteration 1947, dc_loss: 0.017823992297053337, tv_loss: 0.017249953001737595\n",
      "iteration 1948, dc_loss: 0.01782393269240856, tv_loss: 0.01724972389638424\n",
      "iteration 1949, dc_loss: 0.017823878675699234, tv_loss: 0.01724948175251484\n",
      "iteration 1950, dc_loss: 0.017823852598667145, tv_loss: 0.017249293625354767\n",
      "iteration 1951, dc_loss: 0.017823824658989906, tv_loss: 0.017249057069420815\n",
      "iteration 1952, dc_loss: 0.01782379485666752, tv_loss: 0.0172487273812294\n",
      "iteration 1953, dc_loss: 0.01782378926873207, tv_loss: 0.017248528078198433\n",
      "iteration 1954, dc_loss: 0.017823781818151474, tv_loss: 0.017248235642910004\n",
      "iteration 1955, dc_loss: 0.017823753878474236, tv_loss: 0.01724785938858986\n",
      "iteration 1956, dc_loss: 0.01782366819679737, tv_loss: 0.017247604206204414\n",
      "iteration 1957, dc_loss: 0.01782354526221752, tv_loss: 0.017247548326849937\n",
      "iteration 1958, dc_loss: 0.017823418602347374, tv_loss: 0.017247391864657402\n",
      "iteration 1959, dc_loss: 0.01782328449189663, tv_loss: 0.017247242853045464\n",
      "iteration 1960, dc_loss: 0.017823180183768272, tv_loss: 0.01724722422659397\n",
      "iteration 1961, dc_loss: 0.0178231131285429, tv_loss: 0.017247015610337257\n",
      "iteration 1962, dc_loss: 0.017823077738285065, tv_loss: 0.01724676974117756\n",
      "iteration 1963, dc_loss: 0.017823057249188423, tv_loss: 0.017246436327695847\n",
      "iteration 1964, dc_loss: 0.017823006957769394, tv_loss: 0.01724611036479473\n",
      "iteration 1965, dc_loss: 0.017822910100221634, tv_loss: 0.017245832830667496\n",
      "iteration 1966, dc_loss: 0.01782282069325447, tv_loss: 0.017245719209313393\n",
      "iteration 1967, dc_loss: 0.017822762951254845, tv_loss: 0.017245514318346977\n",
      "iteration 1968, dc_loss: 0.017822744324803352, tv_loss: 0.017245184630155563\n",
      "iteration 1969, dc_loss: 0.01782272569835186, tv_loss: 0.017245061695575714\n",
      "iteration 1970, dc_loss: 0.01782265491783619, tv_loss: 0.017244795337319374\n",
      "iteration 1971, dc_loss: 0.017822567373514175, tv_loss: 0.01724451221525669\n",
      "iteration 1972, dc_loss: 0.01782245934009552, tv_loss: 0.017244279384613037\n",
      "iteration 1973, dc_loss: 0.017822375521063805, tv_loss: 0.0172440018504858\n",
      "iteration 1974, dc_loss: 0.01782231591641903, tv_loss: 0.017243770882487297\n",
      "iteration 1975, dc_loss: 0.017822248861193657, tv_loss: 0.01724364049732685\n",
      "iteration 1976, dc_loss: 0.01782219111919403, tv_loss: 0.0172434002161026\n",
      "iteration 1977, dc_loss: 0.017822153866291046, tv_loss: 0.01724318414926529\n",
      "iteration 1978, dc_loss: 0.01782212406396866, tv_loss: 0.017243029549717903\n",
      "iteration 1979, dc_loss: 0.01782207377254963, tv_loss: 0.01724260114133358\n",
      "iteration 1980, dc_loss: 0.017822032794356346, tv_loss: 0.01724233850836754\n",
      "iteration 1981, dc_loss: 0.017821988090872765, tv_loss: 0.01724216155707836\n",
      "iteration 1982, dc_loss: 0.017821935936808586, tv_loss: 0.01724194549024105\n",
      "iteration 1983, dc_loss: 0.017821872606873512, tv_loss: 0.017241690307855606\n",
      "iteration 1984, dc_loss: 0.017821773886680603, tv_loss: 0.01724143698811531\n",
      "iteration 1985, dc_loss: 0.017821693792939186, tv_loss: 0.017241306602954865\n",
      "iteration 1986, dc_loss: 0.01782163977622986, tv_loss: 0.01724117062985897\n",
      "iteration 1987, dc_loss: 0.017821598798036575, tv_loss: 0.017240973189473152\n",
      "iteration 1988, dc_loss: 0.017821522429585457, tv_loss: 0.01724078133702278\n",
      "iteration 1989, dc_loss: 0.017821451649069786, tv_loss: 0.01724052056670189\n",
      "iteration 1990, dc_loss: 0.017821410670876503, tv_loss: 0.01724025048315525\n",
      "iteration 1991, dc_loss: 0.01782139390707016, tv_loss: 0.017239892855286598\n",
      "iteration 1992, dc_loss: 0.017821358516812325, tv_loss: 0.017239786684513092\n",
      "iteration 1993, dc_loss: 0.017821285873651505, tv_loss: 0.01723967120051384\n",
      "iteration 1994, dc_loss: 0.017821190878748894, tv_loss: 0.017239391803741455\n",
      "iteration 1995, dc_loss: 0.017821138724684715, tv_loss: 0.017239239066839218\n",
      "iteration 1996, dc_loss: 0.017821084707975388, tv_loss: 0.017239149659872055\n",
      "iteration 1997, dc_loss: 0.017821060493588448, tv_loss: 0.01723884977400303\n",
      "iteration 1998, dc_loss: 0.017821025103330612, tv_loss: 0.01723848469555378\n",
      "iteration 1999, dc_loss: 0.01782091334462166, tv_loss: 0.01723838970065117\n",
      "iteration 2000, dc_loss: 0.017820751294493675, tv_loss: 0.017238257452845573\n",
      "iteration 2001, dc_loss: 0.017820628359913826, tv_loss: 0.017238233238458633\n",
      "iteration 2002, dc_loss: 0.01782054826617241, tv_loss: 0.017237866297364235\n",
      "iteration 2003, dc_loss: 0.017820490524172783, tv_loss: 0.017237462103366852\n",
      "iteration 2004, dc_loss: 0.017820483073592186, tv_loss: 0.0172372255474329\n",
      "iteration 2005, dc_loss: 0.017820479348301888, tv_loss: 0.01723715476691723\n",
      "iteration 2006, dc_loss: 0.01782049983739853, tv_loss: 0.017236726358532906\n",
      "iteration 2007, dc_loss: 0.01782047189772129, tv_loss: 0.01723651774227619\n",
      "iteration 2008, dc_loss: 0.017820395529270172, tv_loss: 0.017236320301890373\n",
      "iteration 2009, dc_loss: 0.017820283770561218, tv_loss: 0.01723623275756836\n",
      "iteration 2010, dc_loss: 0.017820151522755623, tv_loss: 0.01723603904247284\n",
      "iteration 2011, dc_loss: 0.017820047214627266, tv_loss: 0.01723588816821575\n",
      "iteration 2012, dc_loss: 0.017819972708821297, tv_loss: 0.01723555102944374\n",
      "iteration 2013, dc_loss: 0.017819968983530998, tv_loss: 0.017235543578863144\n",
      "iteration 2014, dc_loss: 0.017819974571466446, tv_loss: 0.017235279083251953\n",
      "iteration 2015, dc_loss: 0.017819907516241074, tv_loss: 0.01723489910364151\n",
      "iteration 2016, dc_loss: 0.017819778993725777, tv_loss: 0.01723487861454487\n",
      "iteration 2017, dc_loss: 0.01781969517469406, tv_loss: 0.017234738916158676\n",
      "iteration 2018, dc_loss: 0.01781965047121048, tv_loss: 0.017234375700354576\n",
      "iteration 2019, dc_loss: 0.017819633707404137, tv_loss: 0.01723424531519413\n",
      "iteration 2020, dc_loss: 0.0178196020424366, tv_loss: 0.017233986407518387\n",
      "iteration 2021, dc_loss: 0.017819557338953018, tv_loss: 0.017233695834875107\n",
      "iteration 2022, dc_loss: 0.01781953126192093, tv_loss: 0.01723342388868332\n",
      "iteration 2023, dc_loss: 0.017819460481405258, tv_loss: 0.01723329909145832\n",
      "iteration 2024, dc_loss: 0.017819341272115707, tv_loss: 0.017233330756425858\n",
      "iteration 2025, dc_loss: 0.017819225788116455, tv_loss: 0.01723317988216877\n",
      "iteration 2026, dc_loss: 0.017819156870245934, tv_loss: 0.017232779413461685\n",
      "iteration 2027, dc_loss: 0.0178191177546978, tv_loss: 0.017232436686754227\n",
      "iteration 2028, dc_loss: 0.017819087952375412, tv_loss: 0.01723245531320572\n",
      "iteration 2029, dc_loss: 0.017819035798311234, tv_loss: 0.017232203856110573\n",
      "iteration 2030, dc_loss: 0.017818978056311607, tv_loss: 0.017231786623597145\n",
      "iteration 2031, dc_loss: 0.01781892403960228, tv_loss: 0.017231641337275505\n",
      "iteration 2032, dc_loss: 0.0178188756108284, tv_loss: 0.017231520265340805\n",
      "iteration 2033, dc_loss: 0.017818789929151535, tv_loss: 0.017231132835149765\n",
      "iteration 2034, dc_loss: 0.017818724736571312, tv_loss: 0.01723099872469902\n",
      "iteration 2035, dc_loss: 0.017818652093410492, tv_loss: 0.01723082736134529\n",
      "iteration 2036, dc_loss: 0.01781858317553997, tv_loss: 0.017230508849024773\n",
      "iteration 2037, dc_loss: 0.017818571999669075, tv_loss: 0.017230292782187462\n",
      "iteration 2038, dc_loss: 0.017818545922636986, tv_loss: 0.017230000346899033\n",
      "iteration 2039, dc_loss: 0.017818493768572807, tv_loss: 0.017229784280061722\n",
      "iteration 2040, dc_loss: 0.017818456515669823, tv_loss: 0.0172295942902565\n",
      "iteration 2041, dc_loss: 0.01781841553747654, tv_loss: 0.01722952164709568\n",
      "iteration 2042, dc_loss: 0.01781836338341236, tv_loss: 0.01722920872271061\n",
      "iteration 2043, dc_loss: 0.017818275839090347, tv_loss: 0.017228996381163597\n",
      "iteration 2044, dc_loss: 0.01781817339360714, tv_loss: 0.017228851094841957\n",
      "iteration 2045, dc_loss: 0.017818061634898186, tv_loss: 0.01722879335284233\n",
      "iteration 2046, dc_loss: 0.01781798154115677, tv_loss: 0.01722860522568226\n",
      "iteration 2047, dc_loss: 0.017817912623286247, tv_loss: 0.017228292301297188\n",
      "iteration 2048, dc_loss: 0.01781785488128662, tv_loss: 0.01722818985581398\n",
      "iteration 2049, dc_loss: 0.01781783625483513, tv_loss: 0.017227884382009506\n",
      "iteration 2050, dc_loss: 0.017817839980125427, tv_loss: 0.017227644100785255\n",
      "iteration 2051, dc_loss: 0.017817838117480278, tv_loss: 0.017227355390787125\n",
      "iteration 2052, dc_loss: 0.01781775988638401, tv_loss: 0.017226988449692726\n",
      "iteration 2053, dc_loss: 0.017817610874772072, tv_loss: 0.01722673885524273\n",
      "iteration 2054, dc_loss: 0.017817499116063118, tv_loss: 0.0172268096357584\n",
      "iteration 2055, dc_loss: 0.017817441374063492, tv_loss: 0.017226656898856163\n",
      "iteration 2056, dc_loss: 0.01781742088496685, tv_loss: 0.017226219177246094\n",
      "iteration 2057, dc_loss: 0.01781739853322506, tv_loss: 0.01722581870853901\n",
      "iteration 2058, dc_loss: 0.017817404121160507, tv_loss: 0.01722567342221737\n",
      "iteration 2059, dc_loss: 0.017817379906773567, tv_loss: 0.017225677147507668\n",
      "iteration 2060, dc_loss: 0.017817329615354538, tv_loss: 0.017225345596671104\n",
      "iteration 2061, dc_loss: 0.01781727559864521, tv_loss: 0.01722530461847782\n",
      "iteration 2062, dc_loss: 0.017817169427871704, tv_loss: 0.017225004732608795\n",
      "iteration 2063, dc_loss: 0.017817066982388496, tv_loss: 0.01722477562725544\n",
      "iteration 2064, dc_loss: 0.017816975712776184, tv_loss: 0.017224736511707306\n",
      "iteration 2065, dc_loss: 0.017816908657550812, tv_loss: 0.017224600538611412\n",
      "iteration 2066, dc_loss: 0.017816852778196335, tv_loss: 0.017224323004484177\n",
      "iteration 2067, dc_loss: 0.017816834151744843, tv_loss: 0.017224054783582687\n",
      "iteration 2068, dc_loss: 0.0178168173879385, tv_loss: 0.01722385548055172\n",
      "iteration 2069, dc_loss: 0.017816787585616112, tv_loss: 0.01722368225455284\n",
      "iteration 2070, dc_loss: 0.017816733568906784, tv_loss: 0.017223378643393517\n",
      "iteration 2071, dc_loss: 0.01781669445335865, tv_loss: 0.01722332276403904\n",
      "iteration 2072, dc_loss: 0.017816614359617233, tv_loss: 0.017223136499524117\n",
      "iteration 2073, dc_loss: 0.017816508188843727, tv_loss: 0.017222849652171135\n",
      "iteration 2074, dc_loss: 0.017816470935940742, tv_loss: 0.01722259446978569\n",
      "iteration 2075, dc_loss: 0.0178164541721344, tv_loss: 0.017222456634044647\n",
      "iteration 2076, dc_loss: 0.017816465348005295, tv_loss: 0.0172222089022398\n",
      "iteration 2077, dc_loss: 0.017816411331295967, tv_loss: 0.017221806570887566\n",
      "iteration 2078, dc_loss: 0.017816318199038506, tv_loss: 0.017221635207533836\n",
      "iteration 2079, dc_loss: 0.017816198989748955, tv_loss: 0.017221689224243164\n",
      "iteration 2080, dc_loss: 0.017816048115491867, tv_loss: 0.017221592366695404\n",
      "iteration 2081, dc_loss: 0.01781596429646015, tv_loss: 0.017221475020051003\n",
      "iteration 2082, dc_loss: 0.017815912142395973, tv_loss: 0.01722121052443981\n",
      "iteration 2083, dc_loss: 0.01781589724123478, tv_loss: 0.01722104661166668\n",
      "iteration 2084, dc_loss: 0.017815889790654182, tv_loss: 0.017220845445990562\n",
      "iteration 2085, dc_loss: 0.01781584322452545, tv_loss: 0.01722072996199131\n",
      "iteration 2086, dc_loss: 0.01781577058136463, tv_loss: 0.01722041517496109\n",
      "iteration 2087, dc_loss: 0.017815731465816498, tv_loss: 0.017220057547092438\n",
      "iteration 2088, dc_loss: 0.017815686762332916, tv_loss: 0.017219962552189827\n",
      "iteration 2089, dc_loss: 0.017815621569752693, tv_loss: 0.01721980608999729\n",
      "iteration 2090, dc_loss: 0.017815548926591873, tv_loss: 0.017219381406903267\n",
      "iteration 2091, dc_loss: 0.017815470695495605, tv_loss: 0.01721939817070961\n",
      "iteration 2092, dc_loss: 0.017815416678786278, tv_loss: 0.017219144850969315\n",
      "iteration 2093, dc_loss: 0.017815375700592995, tv_loss: 0.017218979075551033\n",
      "iteration 2094, dc_loss: 0.017815329134464264, tv_loss: 0.017218785360455513\n",
      "iteration 2095, dc_loss: 0.01781529374420643, tv_loss: 0.017218520864844322\n",
      "iteration 2096, dc_loss: 0.0178152434527874, tv_loss: 0.01721828430891037\n",
      "iteration 2097, dc_loss: 0.017815198749303818, tv_loss: 0.01721801422536373\n",
      "iteration 2098, dc_loss: 0.017815176397562027, tv_loss: 0.017217805609107018\n",
      "iteration 2099, dc_loss: 0.01781512051820755, tv_loss: 0.01721765659749508\n",
      "iteration 2100, dc_loss: 0.017815062776207924, tv_loss: 0.01721743308007717\n",
      "iteration 2101, dc_loss: 0.017814991995692253, tv_loss: 0.017217256128787994\n",
      "iteration 2102, dc_loss: 0.01781490072607994, tv_loss: 0.017216991633176804\n",
      "iteration 2103, dc_loss: 0.01781482622027397, tv_loss: 0.017217013984918594\n",
      "iteration 2104, dc_loss: 0.01781475730240345, tv_loss: 0.01721678487956524\n",
      "iteration 2105, dc_loss: 0.01781471259891987, tv_loss: 0.017216457054018974\n",
      "iteration 2106, dc_loss: 0.017814673483371735, tv_loss: 0.017216239124536514\n",
      "iteration 2107, dc_loss: 0.017814626917243004, tv_loss: 0.01721617579460144\n",
      "iteration 2108, dc_loss: 0.017814544960856438, tv_loss: 0.017216144129633904\n",
      "iteration 2109, dc_loss: 0.01781449466943741, tv_loss: 0.017215875908732414\n",
      "iteration 2110, dc_loss: 0.017814449965953827, tv_loss: 0.017215454950928688\n",
      "iteration 2111, dc_loss: 0.017814427614212036, tv_loss: 0.017215408384799957\n",
      "iteration 2112, dc_loss: 0.017814399674534798, tv_loss: 0.017215140163898468\n",
      "iteration 2113, dc_loss: 0.017814386636018753, tv_loss: 0.017214864492416382\n",
      "iteration 2114, dc_loss: 0.01781432516872883, tv_loss: 0.017214620485901833\n",
      "iteration 2115, dc_loss: 0.01781422458589077, tv_loss: 0.017214510589838028\n",
      "iteration 2116, dc_loss: 0.01781415566802025, tv_loss: 0.017214462161064148\n",
      "iteration 2117, dc_loss: 0.017814084887504578, tv_loss: 0.017214324325323105\n",
      "iteration 2118, dc_loss: 0.017813988029956818, tv_loss: 0.01721402443945408\n",
      "iteration 2119, dc_loss: 0.017813928425312042, tv_loss: 0.01721382327377796\n",
      "iteration 2120, dc_loss: 0.01781388372182846, tv_loss: 0.017213713377714157\n",
      "iteration 2121, dc_loss: 0.01781383715569973, tv_loss: 0.01721353456377983\n",
      "iteration 2122, dc_loss: 0.017813803628087044, tv_loss: 0.01721327193081379\n",
      "iteration 2123, dc_loss: 0.017813758924603462, tv_loss: 0.017212999984622\n",
      "iteration 2124, dc_loss: 0.017813686281442642, tv_loss: 0.017212925478816032\n",
      "iteration 2125, dc_loss: 0.01781364530324936, tv_loss: 0.01721280999481678\n",
      "iteration 2126, dc_loss: 0.01781361736357212, tv_loss: 0.017212480306625366\n",
      "iteration 2127, dc_loss: 0.01781361550092697, tv_loss: 0.017212459817528725\n",
      "iteration 2128, dc_loss: 0.017813581973314285, tv_loss: 0.017212120816111565\n",
      "iteration 2129, dc_loss: 0.01781356707215309, tv_loss: 0.017211822792887688\n",
      "iteration 2130, dc_loss: 0.017813486978411674, tv_loss: 0.017211798578500748\n",
      "iteration 2131, dc_loss: 0.01781340129673481, tv_loss: 0.017211604863405228\n",
      "iteration 2132, dc_loss: 0.017813347280025482, tv_loss: 0.01721131056547165\n",
      "iteration 2133, dc_loss: 0.017813261598348618, tv_loss: 0.01721106469631195\n",
      "iteration 2134, dc_loss: 0.017813168466091156, tv_loss: 0.017211003229022026\n",
      "iteration 2135, dc_loss: 0.017813092097640038, tv_loss: 0.0172110628336668\n",
      "iteration 2136, dc_loss: 0.01781303621828556, tv_loss: 0.01721063069999218\n",
      "iteration 2137, dc_loss: 0.017813002690672874, tv_loss: 0.017210500314831734\n",
      "iteration 2138, dc_loss: 0.017812956124544144, tv_loss: 0.017210297286510468\n",
      "iteration 2139, dc_loss: 0.01781289465725422, tv_loss: 0.01721014268696308\n",
      "iteration 2140, dc_loss: 0.0178128220140934, tv_loss: 0.017210014164447784\n",
      "iteration 2141, dc_loss: 0.01781274937093258, tv_loss: 0.01720982789993286\n",
      "iteration 2142, dc_loss: 0.017812684178352356, tv_loss: 0.017209656536579132\n",
      "iteration 2143, dc_loss: 0.017812613397836685, tv_loss: 0.01720949448645115\n",
      "iteration 2144, dc_loss: 0.017812564969062805, tv_loss: 0.017209229990839958\n",
      "iteration 2145, dc_loss: 0.01781255193054676, tv_loss: 0.01720898412168026\n",
      "iteration 2146, dc_loss: 0.01781250536441803, tv_loss: 0.0172088835388422\n",
      "iteration 2147, dc_loss: 0.017812486737966537, tv_loss: 0.017208609730005264\n",
      "iteration 2148, dc_loss: 0.01781250722706318, tv_loss: 0.017208442091941833\n",
      "iteration 2149, dc_loss: 0.017812514677643776, tv_loss: 0.017208145931363106\n",
      "iteration 2150, dc_loss: 0.017812469974160194, tv_loss: 0.017207825556397438\n",
      "iteration 2151, dc_loss: 0.017812389880418777, tv_loss: 0.01720774918794632\n",
      "iteration 2152, dc_loss: 0.01781226508319378, tv_loss: 0.01720760017633438\n",
      "iteration 2153, dc_loss: 0.017812147736549377, tv_loss: 0.017207378521561623\n",
      "iteration 2154, dc_loss: 0.017812039703130722, tv_loss: 0.017207344993948936\n",
      "iteration 2155, dc_loss: 0.017811981961131096, tv_loss: 0.017207233235239983\n",
      "iteration 2156, dc_loss: 0.017811939120292664, tv_loss: 0.017206944525241852\n",
      "iteration 2157, dc_loss: 0.01781190000474453, tv_loss: 0.017206786200404167\n",
      "iteration 2158, dc_loss: 0.017811866477131844, tv_loss: 0.017206596210598946\n",
      "iteration 2159, dc_loss: 0.017811868339776993, tv_loss: 0.017206501215696335\n",
      "iteration 2160, dc_loss: 0.017811903730034828, tv_loss: 0.017206190153956413\n",
      "iteration 2161, dc_loss: 0.01781190000474453, tv_loss: 0.017206020653247833\n",
      "iteration 2162, dc_loss: 0.0178118497133255, tv_loss: 0.017205825075507164\n",
      "iteration 2163, dc_loss: 0.01781177707016468, tv_loss: 0.017205514013767242\n",
      "iteration 2164, dc_loss: 0.017811685800552368, tv_loss: 0.0172054935246706\n",
      "iteration 2165, dc_loss: 0.017811566591262817, tv_loss: 0.01720537804067135\n",
      "iteration 2166, dc_loss: 0.017811458557844162, tv_loss: 0.01720515266060829\n",
      "iteration 2167, dc_loss: 0.017811397090554237, tv_loss: 0.01720501482486725\n",
      "iteration 2168, dc_loss: 0.017811385914683342, tv_loss: 0.01720474474132061\n",
      "iteration 2169, dc_loss: 0.017811382189393044, tv_loss: 0.01720472238957882\n",
      "iteration 2170, dc_loss: 0.017811309546232224, tv_loss: 0.01720454730093479\n",
      "iteration 2171, dc_loss: 0.017811235040426254, tv_loss: 0.017204010859131813\n",
      "iteration 2172, dc_loss: 0.01781119965016842, tv_loss: 0.017203863710165024\n",
      "iteration 2173, dc_loss: 0.017811164259910583, tv_loss: 0.01720382459461689\n",
      "iteration 2174, dc_loss: 0.01781112514436245, tv_loss: 0.017203645780682564\n",
      "iteration 2175, dc_loss: 0.017811043187975883, tv_loss: 0.017203515395522118\n",
      "iteration 2176, dc_loss: 0.017810922116041183, tv_loss: 0.01720344088971615\n",
      "iteration 2177, dc_loss: 0.017810840159654617, tv_loss: 0.017203371971845627\n",
      "iteration 2178, dc_loss: 0.017810791730880737, tv_loss: 0.017203189432621002\n",
      "iteration 2179, dc_loss: 0.01781076192855835, tv_loss: 0.017202863469719887\n",
      "iteration 2180, dc_loss: 0.01781071349978447, tv_loss: 0.0172027125954628\n",
      "iteration 2181, dc_loss: 0.017810683697462082, tv_loss: 0.017202628776431084\n",
      "iteration 2182, dc_loss: 0.017810670658946037, tv_loss: 0.017202291637659073\n",
      "iteration 2183, dc_loss: 0.01781063713133335, tv_loss: 0.017201896756887436\n",
      "iteration 2184, dc_loss: 0.01781059429049492, tv_loss: 0.017201725393533707\n",
      "iteration 2185, dc_loss: 0.017810555174946785, tv_loss: 0.0172016229480505\n",
      "iteration 2186, dc_loss: 0.0178105216473341, tv_loss: 0.017201414331793785\n",
      "iteration 2187, dc_loss: 0.017810460180044174, tv_loss: 0.017201321199536324\n",
      "iteration 2188, dc_loss: 0.017810380086302757, tv_loss: 0.017201270908117294\n",
      "iteration 2189, dc_loss: 0.017810314893722534, tv_loss: 0.017200974747538567\n",
      "iteration 2190, dc_loss: 0.01781027764081955, tv_loss: 0.017200864851474762\n",
      "iteration 2191, dc_loss: 0.01781025156378746, tv_loss: 0.01720065250992775\n",
      "iteration 2192, dc_loss: 0.017810210585594177, tv_loss: 0.0172004085034132\n",
      "iteration 2193, dc_loss: 0.017810145393013954, tv_loss: 0.017200268805027008\n",
      "iteration 2194, dc_loss: 0.017810096964240074, tv_loss: 0.017200136557221413\n",
      "iteration 2195, dc_loss: 0.01781000941991806, tv_loss: 0.017199890688061714\n",
      "iteration 2196, dc_loss: 0.017809918150305748, tv_loss: 0.017199689522385597\n",
      "iteration 2197, dc_loss: 0.017809884622693062, tv_loss: 0.01719953678548336\n",
      "iteration 2198, dc_loss: 0.017809847369790077, tv_loss: 0.017199497669935226\n",
      "iteration 2199, dc_loss: 0.017809797078371048, tv_loss: 0.017199259251356125\n",
      "iteration 2200, dc_loss: 0.017809756100177765, tv_loss: 0.017198950052261353\n",
      "iteration 2201, dc_loss: 0.017809752374887466, tv_loss: 0.017198899760842323\n",
      "iteration 2202, dc_loss: 0.017809756100177765, tv_loss: 0.017198573797941208\n",
      "iteration 2203, dc_loss: 0.017809728160500526, tv_loss: 0.017198387533426285\n",
      "iteration 2204, dc_loss: 0.01780964620411396, tv_loss: 0.017198292538523674\n",
      "iteration 2205, dc_loss: 0.017809560522437096, tv_loss: 0.01719800941646099\n",
      "iteration 2206, dc_loss: 0.017809493467211723, tv_loss: 0.01719782128930092\n",
      "iteration 2207, dc_loss: 0.017809446901082993, tv_loss: 0.017197728157043457\n",
      "iteration 2208, dc_loss: 0.017809411510825157, tv_loss: 0.01719757914543152\n",
      "iteration 2209, dc_loss: 0.017809391021728516, tv_loss: 0.017197497189044952\n",
      "iteration 2210, dc_loss: 0.017809346318244934, tv_loss: 0.01719708926975727\n",
      "iteration 2211, dc_loss: 0.017809292301535606, tv_loss: 0.01719709485769272\n",
      "iteration 2212, dc_loss: 0.017809201031923294, tv_loss: 0.017197007313370705\n",
      "iteration 2213, dc_loss: 0.017809156328439713, tv_loss: 0.017196709290146828\n",
      "iteration 2214, dc_loss: 0.017809078097343445, tv_loss: 0.017196590080857277\n",
      "iteration 2215, dc_loss: 0.017808957025408745, tv_loss: 0.017196454107761383\n",
      "iteration 2216, dc_loss: 0.017808837816119194, tv_loss: 0.01719638518989086\n",
      "iteration 2217, dc_loss: 0.017808755859732628, tv_loss: 0.017196351662278175\n",
      "iteration 2218, dc_loss: 0.01780872419476509, tv_loss: 0.017196014523506165\n",
      "iteration 2219, dc_loss: 0.01780872792005539, tv_loss: 0.017195727676153183\n",
      "iteration 2220, dc_loss: 0.01780874654650688, tv_loss: 0.01719578169286251\n",
      "iteration 2221, dc_loss: 0.017808768898248672, tv_loss: 0.017195379361510277\n",
      "iteration 2222, dc_loss: 0.017808783799409866, tv_loss: 0.017195116728544235\n",
      "iteration 2223, dc_loss: 0.017808763310313225, tv_loss: 0.01719486154615879\n",
      "iteration 2224, dc_loss: 0.017808707430958748, tv_loss: 0.017194803804159164\n",
      "iteration 2225, dc_loss: 0.01780860126018524, tv_loss: 0.017194604501128197\n",
      "iteration 2226, dc_loss: 0.017808474600315094, tv_loss: 0.017194580286741257\n",
      "iteration 2227, dc_loss: 0.01780838891863823, tv_loss: 0.017194267362356186\n",
      "iteration 2228, dc_loss: 0.01780831441283226, tv_loss: 0.017194218933582306\n",
      "iteration 2229, dc_loss: 0.017808279022574425, tv_loss: 0.017194099724292755\n",
      "iteration 2230, dc_loss: 0.017808273434638977, tv_loss: 0.017193781211972237\n",
      "iteration 2231, dc_loss: 0.01780826412141323, tv_loss: 0.017193496227264404\n",
      "iteration 2232, dc_loss: 0.017808232456445694, tv_loss: 0.017193503677845\n",
      "iteration 2233, dc_loss: 0.017808182165026665, tv_loss: 0.017193257808685303\n",
      "iteration 2234, dc_loss: 0.017808087170124054, tv_loss: 0.017193270847201347\n",
      "iteration 2235, dc_loss: 0.01780799590051174, tv_loss: 0.017193013802170753\n",
      "iteration 2236, dc_loss: 0.01780795492231846, tv_loss: 0.017192896455526352\n",
      "iteration 2237, dc_loss: 0.017807934433221817, tv_loss: 0.017192766070365906\n",
      "iteration 2238, dc_loss: 0.017807887867093086, tv_loss: 0.017192434519529343\n",
      "iteration 2239, dc_loss: 0.0178078506141901, tv_loss: 0.01719236560165882\n",
      "iteration 2240, dc_loss: 0.01780783012509346, tv_loss: 0.017192238941788673\n",
      "iteration 2241, dc_loss: 0.01780780963599682, tv_loss: 0.017192009836435318\n",
      "iteration 2242, dc_loss: 0.017807789146900177, tv_loss: 0.017191722989082336\n",
      "iteration 2243, dc_loss: 0.017807770520448685, tv_loss: 0.017191559076309204\n",
      "iteration 2244, dc_loss: 0.01780768297612667, tv_loss: 0.017191369086503983\n",
      "iteration 2245, dc_loss: 0.01780756749212742, tv_loss: 0.017191313207149506\n",
      "iteration 2246, dc_loss: 0.017807450145483017, tv_loss: 0.017191164195537567\n",
      "iteration 2247, dc_loss: 0.017807360738515854, tv_loss: 0.017190957441926003\n",
      "iteration 2248, dc_loss: 0.017807334661483765, tv_loss: 0.01719084195792675\n",
      "iteration 2249, dc_loss: 0.017807330936193466, tv_loss: 0.017190586775541306\n",
      "iteration 2250, dc_loss: 0.017807362601161003, tv_loss: 0.01719019189476967\n",
      "iteration 2251, dc_loss: 0.017807388678193092, tv_loss: 0.01719001680612564\n",
      "iteration 2252, dc_loss: 0.0178073663264513, tv_loss: 0.017189871519804\n",
      "iteration 2253, dc_loss: 0.017807289958000183, tv_loss: 0.017189744859933853\n",
      "iteration 2254, dc_loss: 0.017807194963097572, tv_loss: 0.017189616337418556\n",
      "iteration 2255, dc_loss: 0.01780710741877556, tv_loss: 0.017189478501677513\n",
      "iteration 2256, dc_loss: 0.01780700869858265, tv_loss: 0.017189418897032738\n",
      "iteration 2257, dc_loss: 0.017806900665163994, tv_loss: 0.017189178615808487\n",
      "iteration 2258, dc_loss: 0.017806846648454666, tv_loss: 0.017189158126711845\n",
      "iteration 2259, dc_loss: 0.01780681125819683, tv_loss: 0.017188994213938713\n",
      "iteration 2260, dc_loss: 0.017806775867938995, tv_loss: 0.017188645899295807\n",
      "iteration 2261, dc_loss: 0.01780676282942295, tv_loss: 0.01718861050903797\n",
      "iteration 2262, dc_loss: 0.01780673675239086, tv_loss: 0.017188509926199913\n",
      "iteration 2263, dc_loss: 0.01780671626329422, tv_loss: 0.01718820258975029\n",
      "iteration 2264, dc_loss: 0.017806677147746086, tv_loss: 0.017188096418976784\n",
      "iteration 2265, dc_loss: 0.01780662126839161, tv_loss: 0.01718803308904171\n",
      "iteration 2266, dc_loss: 0.01780657283961773, tv_loss: 0.017187802121043205\n",
      "iteration 2267, dc_loss: 0.017806511372327805, tv_loss: 0.01718764193356037\n",
      "iteration 2268, dc_loss: 0.017806435003876686, tv_loss: 0.017187535762786865\n",
      "iteration 2269, dc_loss: 0.01780637353658676, tv_loss: 0.017187384888529778\n",
      "iteration 2270, dc_loss: 0.01780630461871624, tv_loss: 0.01718730479478836\n",
      "iteration 2271, dc_loss: 0.01780623011291027, tv_loss: 0.01718718558549881\n",
      "iteration 2272, dc_loss: 0.017806190997362137, tv_loss: 0.017186982557177544\n",
      "iteration 2273, dc_loss: 0.017806190997362137, tv_loss: 0.017186623066663742\n",
      "iteration 2274, dc_loss: 0.017806215211749077, tv_loss: 0.01718648336827755\n",
      "iteration 2275, dc_loss: 0.01780620403587818, tv_loss: 0.017186325043439865\n",
      "iteration 2276, dc_loss: 0.017806151881814003, tv_loss: 0.017186090350151062\n",
      "iteration 2277, dc_loss: 0.017806099727749825, tv_loss: 0.01718572899699211\n",
      "iteration 2278, dc_loss: 0.01780601590871811, tv_loss: 0.017185773700475693\n",
      "iteration 2279, dc_loss: 0.01780589669942856, tv_loss: 0.01718578115105629\n",
      "iteration 2280, dc_loss: 0.01780579797923565, tv_loss: 0.017185717821121216\n",
      "iteration 2281, dc_loss: 0.01780572719871998, tv_loss: 0.01718555949628353\n",
      "iteration 2282, dc_loss: 0.017805686220526695, tv_loss: 0.017185380682349205\n",
      "iteration 2283, dc_loss: 0.017805656418204308, tv_loss: 0.017185218632221222\n",
      "iteration 2284, dc_loss: 0.01780563034117222, tv_loss: 0.017185097560286522\n",
      "iteration 2285, dc_loss: 0.017805615440011024, tv_loss: 0.017184946686029434\n",
      "iteration 2286, dc_loss: 0.017805593088269234, tv_loss: 0.0171846654266119\n",
      "iteration 2287, dc_loss: 0.0178055502474308, tv_loss: 0.017184486612677574\n",
      "iteration 2288, dc_loss: 0.017805473878979683, tv_loss: 0.01718415878713131\n",
      "iteration 2289, dc_loss: 0.0178054366260767, tv_loss: 0.017184004187583923\n",
      "iteration 2290, dc_loss: 0.017805397510528564, tv_loss: 0.017184052616357803\n",
      "iteration 2291, dc_loss: 0.017805367708206177, tv_loss: 0.017183860763907433\n",
      "iteration 2292, dc_loss: 0.01780529133975506, tv_loss: 0.01718357391655445\n",
      "iteration 2293, dc_loss: 0.017805224284529686, tv_loss: 0.01718335971236229\n",
      "iteration 2294, dc_loss: 0.01780521497130394, tv_loss: 0.017183450981974602\n",
      "iteration 2295, dc_loss: 0.017805190756917, tv_loss: 0.01718311570584774\n",
      "iteration 2296, dc_loss: 0.01780511811375618, tv_loss: 0.017183033749461174\n",
      "iteration 2297, dc_loss: 0.017805034294724464, tv_loss: 0.017182890325784683\n",
      "iteration 2298, dc_loss: 0.017804957926273346, tv_loss: 0.01718260906636715\n",
      "iteration 2299, dc_loss: 0.017804883420467377, tv_loss: 0.01718258112668991\n",
      "iteration 2300, dc_loss: 0.017804834991693497, tv_loss: 0.017182450741529465\n",
      "iteration 2301, dc_loss: 0.01780477911233902, tv_loss: 0.01718217320740223\n",
      "iteration 2302, dc_loss: 0.017804749310016632, tv_loss: 0.017182039096951485\n",
      "iteration 2303, dc_loss: 0.017804687842726707, tv_loss: 0.017182065173983574\n",
      "iteration 2304, dc_loss: 0.017804650589823723, tv_loss: 0.01718175783753395\n",
      "iteration 2305, dc_loss: 0.017804613336920738, tv_loss: 0.01718161068856716\n",
      "iteration 2306, dc_loss: 0.017804574221372604, tv_loss: 0.01718151569366455\n",
      "iteration 2307, dc_loss: 0.017804542556405067, tv_loss: 0.017181264236569405\n",
      "iteration 2308, dc_loss: 0.017804481089115143, tv_loss: 0.017181072384119034\n",
      "iteration 2309, dc_loss: 0.01780444011092186, tv_loss: 0.017181074246764183\n",
      "iteration 2310, dc_loss: 0.01780439354479313, tv_loss: 0.01718078926205635\n",
      "iteration 2311, dc_loss: 0.01780436374247074, tv_loss: 0.01718069426715374\n",
      "iteration 2312, dc_loss: 0.017804358154535294, tv_loss: 0.017180515453219414\n",
      "iteration 2313, dc_loss: 0.01780434139072895, tv_loss: 0.017180247232317924\n",
      "iteration 2314, dc_loss: 0.017804326489567757, tv_loss: 0.017179986461997032\n",
      "iteration 2315, dc_loss: 0.01780429482460022, tv_loss: 0.017179884016513824\n",
      "iteration 2316, dc_loss: 0.017804255709052086, tv_loss: 0.01717977412045002\n",
      "iteration 2317, dc_loss: 0.017804201692342758, tv_loss: 0.017179662361741066\n",
      "iteration 2318, dc_loss: 0.017804116010665894, tv_loss: 0.017179515212774277\n",
      "iteration 2319, dc_loss: 0.01780402474105358, tv_loss: 0.017179228365421295\n",
      "iteration 2320, dc_loss: 0.017803935334086418, tv_loss: 0.01717921905219555\n",
      "iteration 2321, dc_loss: 0.017803864553570747, tv_loss: 0.017179202288389206\n",
      "iteration 2322, dc_loss: 0.017803819850087166, tv_loss: 0.017178917303681374\n",
      "iteration 2323, dc_loss: 0.017803790047764778, tv_loss: 0.017178701236844063\n",
      "iteration 2324, dc_loss: 0.017803754657506943, tv_loss: 0.017178623005747795\n",
      "iteration 2325, dc_loss: 0.01780369132757187, tv_loss: 0.017178459092974663\n",
      "iteration 2326, dc_loss: 0.017803659662604332, tv_loss: 0.017178406938910484\n",
      "iteration 2327, dc_loss: 0.017803650349378586, tv_loss: 0.017178215086460114\n",
      "iteration 2328, dc_loss: 0.01780361868441105, tv_loss: 0.017178058624267578\n",
      "iteration 2329, dc_loss: 0.017803549766540527, tv_loss: 0.017177918925881386\n",
      "iteration 2330, dc_loss: 0.01780346967279911, tv_loss: 0.017177795991301537\n",
      "iteration 2331, dc_loss: 0.017803406342864037, tv_loss: 0.01717766933143139\n",
      "iteration 2332, dc_loss: 0.017803417518734932, tv_loss: 0.017177553847432137\n",
      "iteration 2333, dc_loss: 0.017803432419896126, tv_loss: 0.017177334055304527\n",
      "iteration 2334, dc_loss: 0.017803404480218887, tv_loss: 0.017177121713757515\n",
      "iteration 2335, dc_loss: 0.017803316935896873, tv_loss: 0.0171769168227911\n",
      "iteration 2336, dc_loss: 0.017803218215703964, tv_loss: 0.017176693305373192\n",
      "iteration 2337, dc_loss: 0.017803151160478592, tv_loss: 0.017176592722535133\n",
      "iteration 2338, dc_loss: 0.01780310459434986, tv_loss: 0.017176533117890358\n",
      "iteration 2339, dc_loss: 0.017803039401769638, tv_loss: 0.017176443710923195\n",
      "iteration 2340, dc_loss: 0.017802979797124863, tv_loss: 0.017176220193505287\n",
      "iteration 2341, dc_loss: 0.017802925780415535, tv_loss: 0.01717599853873253\n",
      "iteration 2342, dc_loss: 0.017802879214286804, tv_loss: 0.017175791785120964\n",
      "iteration 2343, dc_loss: 0.017802869901061058, tv_loss: 0.017175815999507904\n",
      "iteration 2344, dc_loss: 0.017802856862545013, tv_loss: 0.017175499349832535\n",
      "iteration 2345, dc_loss: 0.017802832648158073, tv_loss: 0.017175260931253433\n",
      "iteration 2346, dc_loss: 0.017802778631448746, tv_loss: 0.017175359651446342\n",
      "iteration 2347, dc_loss: 0.017802737653255463, tv_loss: 0.0171752218157053\n",
      "iteration 2348, dc_loss: 0.01780269853770733, tv_loss: 0.017174914479255676\n",
      "iteration 2349, dc_loss: 0.017802678048610687, tv_loss: 0.0171747375279665\n",
      "iteration 2350, dc_loss: 0.017802666872739792, tv_loss: 0.017174692824482918\n",
      "iteration 2351, dc_loss: 0.01780259795486927, tv_loss: 0.017174577340483665\n",
      "iteration 2352, dc_loss: 0.01780252531170845, tv_loss: 0.017174292355775833\n",
      "iteration 2353, dc_loss: 0.017802467569708824, tv_loss: 0.017174214124679565\n",
      "iteration 2354, dc_loss: 0.017802443355321884, tv_loss: 0.01717424765229225\n",
      "iteration 2355, dc_loss: 0.017802435904741287, tv_loss: 0.01717405766248703\n",
      "iteration 2356, dc_loss: 0.017802443355321884, tv_loss: 0.01717371866106987\n",
      "iteration 2357, dc_loss: 0.01780240796506405, tv_loss: 0.017173556610941887\n",
      "iteration 2358, dc_loss: 0.017802365124225616, tv_loss: 0.01717337965965271\n",
      "iteration 2359, dc_loss: 0.017802312970161438, tv_loss: 0.017173247411847115\n",
      "iteration 2360, dc_loss: 0.017802255228161812, tv_loss: 0.01717325858771801\n",
      "iteration 2361, dc_loss: 0.01780219003558159, tv_loss: 0.017173007130622864\n",
      "iteration 2362, dc_loss: 0.017802121117711067, tv_loss: 0.017172832041978836\n",
      "iteration 2363, dc_loss: 0.017802061513066292, tv_loss: 0.01717279851436615\n",
      "iteration 2364, dc_loss: 0.017802001908421516, tv_loss: 0.017172569409012794\n",
      "iteration 2365, dc_loss: 0.01780194230377674, tv_loss: 0.017172282561659813\n",
      "iteration 2366, dc_loss: 0.017801865935325623, tv_loss: 0.01717214658856392\n",
      "iteration 2367, dc_loss: 0.017801836133003235, tv_loss: 0.01717204600572586\n",
      "iteration 2368, dc_loss: 0.017801813781261444, tv_loss: 0.017171937972307205\n",
      "iteration 2369, dc_loss: 0.017801787704229355, tv_loss: 0.017171764746308327\n",
      "iteration 2370, dc_loss: 0.017801737412810326, tv_loss: 0.017171727493405342\n",
      "iteration 2371, dc_loss: 0.017801672220230103, tv_loss: 0.017171617597341537\n",
      "iteration 2372, dc_loss: 0.017801614478230476, tv_loss: 0.017171572893857956\n",
      "iteration 2373, dc_loss: 0.017801521345973015, tv_loss: 0.017171338200569153\n",
      "iteration 2374, dc_loss: 0.01780143938958645, tv_loss: 0.017171258106827736\n",
      "iteration 2375, dc_loss: 0.017801398411393166, tv_loss: 0.017171485349535942\n",
      "iteration 2376, dc_loss: 0.01780138537287712, tv_loss: 0.017171142622828484\n",
      "iteration 2377, dc_loss: 0.017801379784941673, tv_loss: 0.017170893028378487\n",
      "iteration 2378, dc_loss: 0.017801370471715927, tv_loss: 0.017170557752251625\n",
      "iteration 2379, dc_loss: 0.017801357433199883, tv_loss: 0.01717052236199379\n",
      "iteration 2380, dc_loss: 0.017801305279135704, tv_loss: 0.017170457169413567\n",
      "iteration 2381, dc_loss: 0.017801234498620033, tv_loss: 0.017170270904898643\n",
      "iteration 2382, dc_loss: 0.017801184207201004, tv_loss: 0.017170105129480362\n",
      "iteration 2383, dc_loss: 0.01780114509165287, tv_loss: 0.017169920727610588\n",
      "iteration 2384, dc_loss: 0.01780111715197563, tv_loss: 0.017169896513223648\n",
      "iteration 2385, dc_loss: 0.017801102250814438, tv_loss: 0.01716967299580574\n",
      "iteration 2386, dc_loss: 0.017801089212298393, tv_loss: 0.017169488593935966\n",
      "iteration 2387, dc_loss: 0.017801042646169662, tv_loss: 0.017169520258903503\n",
      "iteration 2388, dc_loss: 0.017800968140363693, tv_loss: 0.017169365659356117\n",
      "iteration 2389, dc_loss: 0.01780090108513832, tv_loss: 0.017169171944260597\n",
      "iteration 2390, dc_loss: 0.017800837755203247, tv_loss: 0.017169054597616196\n",
      "iteration 2391, dc_loss: 0.01780080609023571, tv_loss: 0.017168907448649406\n",
      "iteration 2392, dc_loss: 0.01780075766146183, tv_loss: 0.017168838530778885\n",
      "iteration 2393, dc_loss: 0.01780070550739765, tv_loss: 0.0171685591340065\n",
      "iteration 2394, dc_loss: 0.01780065894126892, tv_loss: 0.017168264836072922\n",
      "iteration 2395, dc_loss: 0.01780061237514019, tv_loss: 0.017168240621685982\n",
      "iteration 2396, dc_loss: 0.017800573259592056, tv_loss: 0.017168307676911354\n",
      "iteration 2397, dc_loss: 0.017800532281398773, tv_loss: 0.017168138176202774\n",
      "iteration 2398, dc_loss: 0.017800496891140938, tv_loss: 0.017167912796139717\n",
      "iteration 2399, dc_loss: 0.01780046708881855, tv_loss: 0.017167823389172554\n",
      "iteration 2400, dc_loss: 0.01780044473707676, tv_loss: 0.017167625948786736\n",
      "iteration 2401, dc_loss: 0.017800407484173775, tv_loss: 0.01716739311814308\n",
      "iteration 2402, dc_loss: 0.017800364643335342, tv_loss: 0.017167232930660248\n",
      "iteration 2403, dc_loss: 0.017800360918045044, tv_loss: 0.017167100682854652\n",
      "iteration 2404, dc_loss: 0.01780034601688385, tv_loss: 0.017167001962661743\n",
      "iteration 2405, dc_loss: 0.01780029945075512, tv_loss: 0.017166711390018463\n",
      "iteration 2406, dc_loss: 0.017800254747271538, tv_loss: 0.0171663835644722\n",
      "iteration 2407, dc_loss: 0.017800187692046165, tv_loss: 0.017166459932923317\n",
      "iteration 2408, dc_loss: 0.017800098285079002, tv_loss: 0.017166484147310257\n",
      "iteration 2409, dc_loss: 0.017800021916627884, tv_loss: 0.01716621033847332\n",
      "iteration 2410, dc_loss: 0.017799964174628258, tv_loss: 0.017166057601571083\n",
      "iteration 2411, dc_loss: 0.01779990829527378, tv_loss: 0.017166106030344963\n",
      "iteration 2412, dc_loss: 0.017799856141209602, tv_loss: 0.017165912315249443\n",
      "iteration 2413, dc_loss: 0.01779983378946781, tv_loss: 0.017165735363960266\n",
      "iteration 2414, dc_loss: 0.01779979094862938, tv_loss: 0.017165565863251686\n",
      "iteration 2415, dc_loss: 0.01779979094862938, tv_loss: 0.017165416851639748\n",
      "iteration 2416, dc_loss: 0.017799783498048782, tv_loss: 0.017165392637252808\n",
      "iteration 2417, dc_loss: 0.017799757421016693, tv_loss: 0.01716509647667408\n",
      "iteration 2418, dc_loss: 0.017799705266952515, tv_loss: 0.017164750024676323\n",
      "iteration 2419, dc_loss: 0.01779964379966259, tv_loss: 0.01716485060751438\n",
      "iteration 2420, dc_loss: 0.017799604684114456, tv_loss: 0.017164794728159904\n",
      "iteration 2421, dc_loss: 0.01779954880475998, tv_loss: 0.017164720222353935\n",
      "iteration 2422, dc_loss: 0.017799457535147667, tv_loss: 0.01716456562280655\n",
      "iteration 2423, dc_loss: 0.017799388617277145, tv_loss: 0.0171643178910017\n",
      "iteration 2424, dc_loss: 0.017799343913793564, tv_loss: 0.017164265736937523\n",
      "iteration 2425, dc_loss: 0.017799314111471176, tv_loss: 0.017164068296551704\n",
      "iteration 2426, dc_loss: 0.017799286171793938, tv_loss: 0.01716393046081066\n",
      "iteration 2427, dc_loss: 0.01779928244650364, tv_loss: 0.017163705080747604\n",
      "iteration 2428, dc_loss: 0.01779928244650364, tv_loss: 0.017163651064038277\n",
      "iteration 2429, dc_loss: 0.017799263820052147, tv_loss: 0.017163507640361786\n",
      "iteration 2430, dc_loss: 0.017799226567149162, tv_loss: 0.017163274809718132\n",
      "iteration 2431, dc_loss: 0.017799120396375656, tv_loss: 0.017163196578621864\n",
      "iteration 2432, dc_loss: 0.017799029126763344, tv_loss: 0.017163176089525223\n",
      "iteration 2433, dc_loss: 0.017798947170376778, tv_loss: 0.017162946984171867\n",
      "iteration 2434, dc_loss: 0.01779891736805439, tv_loss: 0.01716289296746254\n",
      "iteration 2435, dc_loss: 0.017798911780118942, tv_loss: 0.017162790521979332\n",
      "iteration 2436, dc_loss: 0.017798960208892822, tv_loss: 0.0171626266092062\n",
      "iteration 2437, dc_loss: 0.01779898628592491, tv_loss: 0.017162319272756577\n",
      "iteration 2438, dc_loss: 0.01779894344508648, tv_loss: 0.017162198200821877\n",
      "iteration 2439, dc_loss: 0.017798887565732002, tv_loss: 0.01716209016740322\n",
      "iteration 2440, dc_loss: 0.017798813059926033, tv_loss: 0.017162151634693146\n",
      "iteration 2441, dc_loss: 0.017798682674765587, tv_loss: 0.017162000760436058\n",
      "iteration 2442, dc_loss: 0.01779858209192753, tv_loss: 0.017161890864372253\n",
      "iteration 2443, dc_loss: 0.01779852993786335, tv_loss: 0.017161812633275986\n",
      "iteration 2444, dc_loss: 0.017798500135540962, tv_loss: 0.01716161146759987\n",
      "iteration 2445, dc_loss: 0.01779848150908947, tv_loss: 0.017161481082439423\n",
      "iteration 2446, dc_loss: 0.017798462882637978, tv_loss: 0.017161471769213676\n",
      "iteration 2447, dc_loss: 0.01779842935502529, tv_loss: 0.017161307856440544\n",
      "iteration 2448, dc_loss: 0.01779835671186447, tv_loss: 0.017161162570118904\n",
      "iteration 2449, dc_loss: 0.017798278480768204, tv_loss: 0.01716098003089428\n",
      "iteration 2450, dc_loss: 0.017798224464058876, tv_loss: 0.0171609278768301\n",
      "iteration 2451, dc_loss: 0.017798203974962234, tv_loss: 0.01716083288192749\n",
      "iteration 2452, dc_loss: 0.017798198387026787, tv_loss: 0.017160650342702866\n",
      "iteration 2453, dc_loss: 0.01779817044734955, tv_loss: 0.01716051995754242\n",
      "iteration 2454, dc_loss: 0.01779814250767231, tv_loss: 0.01716035045683384\n",
      "iteration 2455, dc_loss: 0.01779807358980179, tv_loss: 0.017160149291157722\n",
      "iteration 2456, dc_loss: 0.01779799535870552, tv_loss: 0.017160098999738693\n",
      "iteration 2457, dc_loss: 0.01779795065522194, tv_loss: 0.0171600803732872\n",
      "iteration 2458, dc_loss: 0.0177979227155447, tv_loss: 0.017160026356577873\n",
      "iteration 2459, dc_loss: 0.017797933891415596, tv_loss: 0.01715974323451519\n",
      "iteration 2460, dc_loss: 0.017797956243157387, tv_loss: 0.01715943031013012\n",
      "iteration 2461, dc_loss: 0.017797937616705894, tv_loss: 0.017159322276711464\n",
      "iteration 2462, dc_loss: 0.01779790222644806, tv_loss: 0.017159095034003258\n",
      "iteration 2463, dc_loss: 0.01779789850115776, tv_loss: 0.017158927395939827\n",
      "iteration 2464, dc_loss: 0.01779790036380291, tv_loss: 0.01715887151658535\n",
      "iteration 2465, dc_loss: 0.01779785379767418, tv_loss: 0.017158830538392067\n",
      "iteration 2466, dc_loss: 0.01779775321483612, tv_loss: 0.01715848408639431\n",
      "iteration 2467, dc_loss: 0.017797665670514107, tv_loss: 0.01715843565762043\n",
      "iteration 2468, dc_loss: 0.017797576263546944, tv_loss: 0.017158538103103638\n",
      "iteration 2469, dc_loss: 0.017797501757740974, tv_loss: 0.017158320173621178\n",
      "iteration 2470, dc_loss: 0.017797453328967094, tv_loss: 0.017158187925815582\n",
      "iteration 2471, dc_loss: 0.01779741793870926, tv_loss: 0.017158132046461105\n",
      "iteration 2472, dc_loss: 0.01779739372432232, tv_loss: 0.01715792901813984\n",
      "iteration 2473, dc_loss: 0.01779731921851635, tv_loss: 0.0171577800065279\n",
      "iteration 2474, dc_loss: 0.017797250300645828, tv_loss: 0.01715773530304432\n",
      "iteration 2475, dc_loss: 0.017797211185097694, tv_loss: 0.017157571390271187\n",
      "iteration 2476, dc_loss: 0.017797186970710754, tv_loss: 0.01715737022459507\n",
      "iteration 2477, dc_loss: 0.017797153443098068, tv_loss: 0.017157280817627907\n",
      "iteration 2478, dc_loss: 0.017797116190195084, tv_loss: 0.017157141119241714\n",
      "iteration 2479, dc_loss: 0.017797108739614487, tv_loss: 0.01715718023478985\n",
      "iteration 2480, dc_loss: 0.017797088250517845, tv_loss: 0.017157025635242462\n",
      "iteration 2481, dc_loss: 0.017797056585550308, tv_loss: 0.017156844958662987\n",
      "iteration 2482, dc_loss: 0.017797011882066727, tv_loss: 0.017156729474663734\n",
      "iteration 2483, dc_loss: 0.017796985805034637, tv_loss: 0.017156649380922318\n",
      "iteration 2484, dc_loss: 0.01779690384864807, tv_loss: 0.017156481742858887\n",
      "iteration 2485, dc_loss: 0.017796862870454788, tv_loss: 0.017156405374407768\n",
      "iteration 2486, dc_loss: 0.017796875908970833, tv_loss: 0.0171563271433115\n",
      "iteration 2487, dc_loss: 0.017796911299228668, tv_loss: 0.017156057059764862\n",
      "iteration 2488, dc_loss: 0.01779690571129322, tv_loss: 0.0171558428555727\n",
      "iteration 2489, dc_loss: 0.017796874046325684, tv_loss: 0.0171557255089283\n",
      "iteration 2490, dc_loss: 0.01779683493077755, tv_loss: 0.017155470326542854\n",
      "iteration 2491, dc_loss: 0.017796792089939117, tv_loss: 0.017155464738607407\n",
      "iteration 2492, dc_loss: 0.017796708270907402, tv_loss: 0.017155349254608154\n",
      "iteration 2493, dc_loss: 0.017796596512198448, tv_loss: 0.017155442386865616\n",
      "iteration 2494, dc_loss: 0.017796510830521584, tv_loss: 0.01715530827641487\n",
      "iteration 2495, dc_loss: 0.0177964698523283, tv_loss: 0.017155207693576813\n",
      "iteration 2496, dc_loss: 0.017796456813812256, tv_loss: 0.017155040055513382\n",
      "iteration 2497, dc_loss: 0.017796460539102554, tv_loss: 0.017154855653643608\n",
      "iteration 2498, dc_loss: 0.017796479165554047, tv_loss: 0.01715465448796749\n",
      "iteration 2499, dc_loss: 0.017796477302908897, tv_loss: 0.017154449597001076\n",
      "iteration 2500, dc_loss: 0.017796430736780167, tv_loss: 0.017154356464743614\n",
      "iteration 2501, dc_loss: 0.017796343192458153, tv_loss: 0.017154304310679436\n",
      "iteration 2502, dc_loss: 0.017796220257878304, tv_loss: 0.017154289409518242\n",
      "iteration 2503, dc_loss: 0.017796127125620842, tv_loss: 0.017154110595583916\n",
      "iteration 2504, dc_loss: 0.017796071246266365, tv_loss: 0.017154129222035408\n",
      "iteration 2505, dc_loss: 0.017796039581298828, tv_loss: 0.017153987661004066\n",
      "iteration 2506, dc_loss: 0.01779603585600853, tv_loss: 0.017153872177004814\n",
      "iteration 2507, dc_loss: 0.017796026542782784, tv_loss: 0.0171536672860384\n",
      "iteration 2508, dc_loss: 0.017796017229557037, tv_loss: 0.01715351827442646\n",
      "iteration 2509, dc_loss: 0.017796000465750694, tv_loss: 0.017153339460492134\n",
      "iteration 2510, dc_loss: 0.017795970663428307, tv_loss: 0.017153192311525345\n",
      "iteration 2511, dc_loss: 0.01779593713581562, tv_loss: 0.017153238877654076\n",
      "iteration 2512, dc_loss: 0.01779589056968689, tv_loss: 0.017153067514300346\n",
      "iteration 2513, dc_loss: 0.017795825377106667, tv_loss: 0.017152806743979454\n",
      "iteration 2514, dc_loss: 0.017795780673623085, tv_loss: 0.01715278811752796\n",
      "iteration 2515, dc_loss: 0.017795739695429802, tv_loss: 0.01715274341404438\n",
      "iteration 2516, dc_loss: 0.01779571920633316, tv_loss: 0.017152583226561546\n",
      "iteration 2517, dc_loss: 0.017795685678720474, tv_loss: 0.017152365297079086\n",
      "iteration 2518, dc_loss: 0.017795663326978683, tv_loss: 0.01715228706598282\n",
      "iteration 2519, dc_loss: 0.017795639112591743, tv_loss: 0.017152251675724983\n",
      "iteration 2520, dc_loss: 0.017795614898204803, tv_loss: 0.01715211756527424\n",
      "iteration 2521, dc_loss: 0.01779557764530182, tv_loss: 0.01715187355875969\n",
      "iteration 2522, dc_loss: 0.01779554784297943, tv_loss: 0.01715168170630932\n",
      "iteration 2523, dc_loss: 0.017795488238334656, tv_loss: 0.017151713371276855\n",
      "iteration 2524, dc_loss: 0.017795400694012642, tv_loss: 0.017151592299342155\n",
      "iteration 2525, dc_loss: 0.01779531128704548, tv_loss: 0.017151568084955215\n",
      "iteration 2526, dc_loss: 0.017795272171497345, tv_loss: 0.017151588574051857\n",
      "iteration 2527, dc_loss: 0.017795240506529808, tv_loss: 0.01715145818889141\n",
      "iteration 2528, dc_loss: 0.017795180901885033, tv_loss: 0.01715129241347313\n",
      "iteration 2529, dc_loss: 0.01779516227543354, tv_loss: 0.01715121977031231\n",
      "iteration 2530, dc_loss: 0.017795180901885033, tv_loss: 0.01715095154941082\n",
      "iteration 2531, dc_loss: 0.017795180901885033, tv_loss: 0.017150862142443657\n",
      "iteration 2532, dc_loss: 0.017795180901885033, tv_loss: 0.01715078018605709\n",
      "iteration 2533, dc_loss: 0.01779511570930481, tv_loss: 0.0171505119651556\n",
      "iteration 2534, dc_loss: 0.017795108258724213, tv_loss: 0.01715034805238247\n",
      "iteration 2535, dc_loss: 0.01779511570930481, tv_loss: 0.017150292173027992\n",
      "iteration 2536, dc_loss: 0.017795095220208168, tv_loss: 0.017150219529867172\n",
      "iteration 2537, dc_loss: 0.017795052379369736, tv_loss: 0.0171499066054821\n",
      "iteration 2538, dc_loss: 0.017795009538531303, tv_loss: 0.01714976318180561\n",
      "iteration 2539, dc_loss: 0.01779494807124138, tv_loss: 0.017149899154901505\n",
      "iteration 2540, dc_loss: 0.017794858664274216, tv_loss: 0.017149558290839195\n",
      "iteration 2541, dc_loss: 0.017794795334339142, tv_loss: 0.01714952476322651\n",
      "iteration 2542, dc_loss: 0.017794737592339516, tv_loss: 0.017149543389678\n",
      "iteration 2543, dc_loss: 0.017794672399759293, tv_loss: 0.01714923046529293\n",
      "iteration 2544, dc_loss: 0.017794622108340263, tv_loss: 0.01714916154742241\n",
      "iteration 2545, dc_loss: 0.01779460348188877, tv_loss: 0.017149154096841812\n",
      "iteration 2546, dc_loss: 0.017794569954276085, tv_loss: 0.017148954793810844\n",
      "iteration 2547, dc_loss: 0.0177945364266634, tv_loss: 0.017148654907941818\n",
      "iteration 2548, dc_loss: 0.017794525250792503, tv_loss: 0.017148645594716072\n",
      "iteration 2549, dc_loss: 0.017794504761695862, tv_loss: 0.01714847795665264\n",
      "iteration 2550, dc_loss: 0.017794493585824966, tv_loss: 0.017148256301879883\n",
      "iteration 2551, dc_loss: 0.017794495448470116, tv_loss: 0.017148178070783615\n",
      "iteration 2552, dc_loss: 0.017794493585824966, tv_loss: 0.017148105427622795\n",
      "iteration 2553, dc_loss: 0.017794489860534668, tv_loss: 0.017147695645689964\n",
      "iteration 2554, dc_loss: 0.017794447019696236, tv_loss: 0.01714768446981907\n",
      "iteration 2555, dc_loss: 0.017794394865632057, tv_loss: 0.017147738486528397\n",
      "iteration 2556, dc_loss: 0.01779436133801937, tv_loss: 0.017147595062851906\n",
      "iteration 2557, dc_loss: 0.01779426820576191, tv_loss: 0.01714756339788437\n",
      "iteration 2558, dc_loss: 0.01779419369995594, tv_loss: 0.017147477716207504\n",
      "iteration 2559, dc_loss: 0.017794132232666016, tv_loss: 0.017147375270724297\n",
      "iteration 2560, dc_loss: 0.017794108018279076, tv_loss: 0.017147300764918327\n",
      "iteration 2561, dc_loss: 0.017794057726860046, tv_loss: 0.017147336155176163\n",
      "iteration 2562, dc_loss: 0.017794061452150345, tv_loss: 0.0171470046043396\n",
      "iteration 2563, dc_loss: 0.017794085666537285, tv_loss: 0.017146864905953407\n",
      "iteration 2564, dc_loss: 0.017794068902730942, tv_loss: 0.017146747559309006\n",
      "iteration 2565, dc_loss: 0.017794068902730942, tv_loss: 0.017146699130535126\n",
      "iteration 2566, dc_loss: 0.017794037237763405, tv_loss: 0.017146578058600426\n",
      "iteration 2567, dc_loss: 0.017793964594602585, tv_loss: 0.01714651845395565\n",
      "iteration 2568, dc_loss: 0.017793867737054825, tv_loss: 0.017146484926342964\n",
      "iteration 2569, dc_loss: 0.017793796956539154, tv_loss: 0.017146266996860504\n",
      "iteration 2570, dc_loss: 0.01779378205537796, tv_loss: 0.017146224156022072\n",
      "iteration 2571, dc_loss: 0.01779376156628132, tv_loss: 0.017146091908216476\n",
      "iteration 2572, dc_loss: 0.01779371313750744, tv_loss: 0.017146015539765358\n",
      "iteration 2573, dc_loss: 0.017793670296669006, tv_loss: 0.017145836725831032\n",
      "iteration 2574, dc_loss: 0.017793644219636917, tv_loss: 0.017145682126283646\n",
      "iteration 2575, dc_loss: 0.017793644219636917, tv_loss: 0.017145635560154915\n",
      "iteration 2576, dc_loss: 0.01779366098344326, tv_loss: 0.017145458608865738\n",
      "iteration 2577, dc_loss: 0.017793649807572365, tv_loss: 0.017145376652479172\n",
      "iteration 2578, dc_loss: 0.01779360882937908, tv_loss: 0.017145397141575813\n",
      "iteration 2579, dc_loss: 0.017793545499444008, tv_loss: 0.01714523695409298\n",
      "iteration 2580, dc_loss: 0.01779348775744438, tv_loss: 0.017145181074738503\n",
      "iteration 2581, dc_loss: 0.01779339276254177, tv_loss: 0.017145194113254547\n",
      "iteration 2582, dc_loss: 0.0177933257073164, tv_loss: 0.017145227640867233\n",
      "iteration 2583, dc_loss: 0.01779329590499401, tv_loss: 0.01714487001299858\n",
      "iteration 2584, dc_loss: 0.017793254926800728, tv_loss: 0.0171447042375803\n",
      "iteration 2585, dc_loss: 0.017793212085962296, tv_loss: 0.01714479736983776\n",
      "iteration 2586, dc_loss: 0.017793143168091774, tv_loss: 0.017144540324807167\n",
      "iteration 2587, dc_loss: 0.017793115228414536, tv_loss: 0.01714458130300045\n",
      "iteration 2588, dc_loss: 0.017793118953704834, tv_loss: 0.01714440993964672\n",
      "iteration 2589, dc_loss: 0.017793120816349983, tv_loss: 0.01714407466351986\n",
      "iteration 2590, dc_loss: 0.017793120816349983, tv_loss: 0.01714382879436016\n",
      "iteration 2591, dc_loss: 0.017793115228414536, tv_loss: 0.01714390330016613\n",
      "iteration 2592, dc_loss: 0.017793109640479088, tv_loss: 0.01714371144771576\n",
      "iteration 2593, dc_loss: 0.017793087288737297, tv_loss: 0.01714347116649151\n",
      "iteration 2594, dc_loss: 0.01779307797551155, tv_loss: 0.017143456265330315\n",
      "iteration 2595, dc_loss: 0.017793022096157074, tv_loss: 0.01714334823191166\n",
      "iteration 2596, dc_loss: 0.017792969942092896, tv_loss: 0.017143262550234795\n",
      "iteration 2597, dc_loss: 0.017792925238609314, tv_loss: 0.01714317686855793\n",
      "iteration 2598, dc_loss: 0.017792847007513046, tv_loss: 0.017143065109848976\n",
      "iteration 2599, dc_loss: 0.01779274269938469, tv_loss: 0.017143066972494125\n",
      "iteration 2600, dc_loss: 0.017792660742998123, tv_loss: 0.017142966389656067\n",
      "iteration 2601, dc_loss: 0.017792604863643646, tv_loss: 0.017142826691269875\n",
      "iteration 2602, dc_loss: 0.017792604863643646, tv_loss: 0.01714266650378704\n",
      "iteration 2603, dc_loss: 0.017792636528611183, tv_loss: 0.017142636701464653\n",
      "iteration 2604, dc_loss: 0.01779264584183693, tv_loss: 0.01714244857430458\n",
      "iteration 2605, dc_loss: 0.01779262162744999, tv_loss: 0.017142223194241524\n",
      "iteration 2606, dc_loss: 0.01779261790215969, tv_loss: 0.017142044380307198\n",
      "iteration 2607, dc_loss: 0.017792604863643646, tv_loss: 0.017142005264759064\n",
      "iteration 2608, dc_loss: 0.017792584374547005, tv_loss: 0.01714170165359974\n",
      "iteration 2609, dc_loss: 0.017792578786611557, tv_loss: 0.017141737043857574\n",
      "iteration 2610, dc_loss: 0.017792515456676483, tv_loss: 0.01714167930185795\n",
      "iteration 2611, dc_loss: 0.017792414873838425, tv_loss: 0.01714155077934265\n",
      "iteration 2612, dc_loss: 0.01779232919216156, tv_loss: 0.017141498625278473\n",
      "iteration 2613, dc_loss: 0.01779230125248432, tv_loss: 0.0171413142234087\n",
      "iteration 2614, dc_loss: 0.017792297527194023, tv_loss: 0.017141025513410568\n",
      "iteration 2615, dc_loss: 0.01779225841164589, tv_loss: 0.017141098156571388\n",
      "iteration 2616, dc_loss: 0.01779221184551716, tv_loss: 0.017141150310635567\n",
      "iteration 2617, dc_loss: 0.017792152240872383, tv_loss: 0.017141077667474747\n",
      "iteration 2618, dc_loss: 0.017792116850614548, tv_loss: 0.017140785232186317\n",
      "iteration 2619, dc_loss: 0.017792072147130966, tv_loss: 0.017140937969088554\n",
      "iteration 2620, dc_loss: 0.01779204048216343, tv_loss: 0.01714097522199154\n",
      "iteration 2621, dc_loss: 0.017792025581002235, tv_loss: 0.017140734940767288\n",
      "iteration 2622, dc_loss: 0.017792001366615295, tv_loss: 0.017140565440058708\n",
      "iteration 2623, dc_loss: 0.01779201440513134, tv_loss: 0.017140354961156845\n",
      "iteration 2624, dc_loss: 0.01779204048216343, tv_loss: 0.017140163108706474\n",
      "iteration 2625, dc_loss: 0.017792051658034325, tv_loss: 0.017140040174126625\n",
      "iteration 2626, dc_loss: 0.01779201813042164, tv_loss: 0.017139935865998268\n",
      "iteration 2627, dc_loss: 0.01779194362461567, tv_loss: 0.01713981106877327\n",
      "iteration 2628, dc_loss: 0.017791856080293655, tv_loss: 0.017139732837677002\n",
      "iteration 2629, dc_loss: 0.017791805788874626, tv_loss: 0.017139600589871407\n",
      "iteration 2630, dc_loss: 0.01779177412390709, tv_loss: 0.01713954098522663\n",
      "iteration 2631, dc_loss: 0.017791738733649254, tv_loss: 0.017139412462711334\n",
      "iteration 2632, dc_loss: 0.017791690304875374, tv_loss: 0.01713927648961544\n",
      "iteration 2633, dc_loss: 0.017791632562875748, tv_loss: 0.017139405012130737\n",
      "iteration 2634, dc_loss: 0.017791591584682465, tv_loss: 0.01713935099542141\n",
      "iteration 2635, dc_loss: 0.01779155060648918, tv_loss: 0.017138991504907608\n",
      "iteration 2636, dc_loss: 0.017791511490941048, tv_loss: 0.01713891513645649\n",
      "iteration 2637, dc_loss: 0.01779147982597351, tv_loss: 0.017138876020908356\n",
      "iteration 2638, dc_loss: 0.01779147982597351, tv_loss: 0.01713879406452179\n",
      "iteration 2639, dc_loss: 0.017791450023651123, tv_loss: 0.01713866926729679\n",
      "iteration 2640, dc_loss: 0.017791423946619034, tv_loss: 0.017138535156846046\n",
      "iteration 2641, dc_loss: 0.0177913848310709, tv_loss: 0.017138369381427765\n",
      "iteration 2642, dc_loss: 0.01779133640229702, tv_loss: 0.01713838055729866\n",
      "iteration 2643, dc_loss: 0.017791304737329483, tv_loss: 0.0171381663531065\n",
      "iteration 2644, dc_loss: 0.01779131032526493, tv_loss: 0.017138076946139336\n",
      "iteration 2645, dc_loss: 0.01779130846261978, tv_loss: 0.017137857154011726\n",
      "iteration 2646, dc_loss: 0.01779133640229702, tv_loss: 0.01713789626955986\n",
      "iteration 2647, dc_loss: 0.017791325226426125, tv_loss: 0.017137575894594193\n",
      "iteration 2648, dc_loss: 0.017791254445910454, tv_loss: 0.017137570306658745\n",
      "iteration 2649, dc_loss: 0.01779118739068508, tv_loss: 0.01713750511407852\n",
      "iteration 2650, dc_loss: 0.01779116503894329, tv_loss: 0.017137547954916954\n",
      "iteration 2651, dc_loss: 0.0177911389619112, tv_loss: 0.017137425020337105\n",
      "iteration 2652, dc_loss: 0.017791111022233963, tv_loss: 0.017137259244918823\n",
      "iteration 2653, dc_loss: 0.017791077494621277, tv_loss: 0.0171371977776289\n",
      "iteration 2654, dc_loss: 0.01779104210436344, tv_loss: 0.017137130722403526\n",
      "iteration 2655, dc_loss: 0.017790984362363815, tv_loss: 0.01713711768388748\n",
      "iteration 2656, dc_loss: 0.01779092289507389, tv_loss: 0.017136884853243828\n",
      "iteration 2657, dc_loss: 0.017790867015719414, tv_loss: 0.01713663898408413\n",
      "iteration 2658, dc_loss: 0.017790835350751877, tv_loss: 0.017136793583631516\n",
      "iteration 2659, dc_loss: 0.017790870741009712, tv_loss: 0.01713661104440689\n",
      "iteration 2660, dc_loss: 0.017790909856557846, tv_loss: 0.017136378213763237\n",
      "iteration 2661, dc_loss: 0.017790881916880608, tv_loss: 0.017136383801698685\n",
      "iteration 2662, dc_loss: 0.017790798097848892, tv_loss: 0.01713629998266697\n",
      "iteration 2663, dc_loss: 0.017790716141462326, tv_loss: 0.017136113718152046\n",
      "iteration 2664, dc_loss: 0.01779063418507576, tv_loss: 0.017136171460151672\n",
      "iteration 2665, dc_loss: 0.01779060997068882, tv_loss: 0.01713600382208824\n",
      "iteration 2666, dc_loss: 0.017790615558624268, tv_loss: 0.0171358659863472\n",
      "iteration 2667, dc_loss: 0.017790595069527626, tv_loss: 0.017135728150606155\n",
      "iteration 2668, dc_loss: 0.01779055781662464, tv_loss: 0.017135709524154663\n",
      "iteration 2669, dc_loss: 0.017790531739592552, tv_loss: 0.017135584726929665\n",
      "iteration 2670, dc_loss: 0.017790544778108597, tv_loss: 0.01713542267680168\n",
      "iteration 2671, dc_loss: 0.017790529876947403, tv_loss: 0.017135290428996086\n",
      "iteration 2672, dc_loss: 0.017790479585528374, tv_loss: 0.01713525503873825\n",
      "iteration 2673, dc_loss: 0.0177904162555933, tv_loss: 0.017135106027126312\n",
      "iteration 2674, dc_loss: 0.01779039204120636, tv_loss: 0.017135024070739746\n",
      "iteration 2675, dc_loss: 0.01779037155210972, tv_loss: 0.017135100439190865\n",
      "iteration 2676, dc_loss: 0.01779033988714218, tv_loss: 0.017134888097643852\n",
      "iteration 2677, dc_loss: 0.017790330573916435, tv_loss: 0.01713481731712818\n",
      "iteration 2678, dc_loss: 0.017790300771594048, tv_loss: 0.01713455654680729\n",
      "iteration 2679, dc_loss: 0.017790252342820168, tv_loss: 0.017134465277194977\n",
      "iteration 2680, dc_loss: 0.01779024302959442, tv_loss: 0.01713448576629162\n",
      "iteration 2681, dc_loss: 0.01779024489223957, tv_loss: 0.017134282737970352\n",
      "iteration 2682, dc_loss: 0.017790228128433228, tv_loss: 0.017134152352809906\n",
      "iteration 2683, dc_loss: 0.017790185287594795, tv_loss: 0.01713400147855282\n",
      "iteration 2684, dc_loss: 0.01779012754559517, tv_loss: 0.01713394746184349\n",
      "iteration 2685, dc_loss: 0.01779002696275711, tv_loss: 0.017133841291069984\n",
      "iteration 2686, dc_loss: 0.01778995245695114, tv_loss: 0.017133835703134537\n",
      "iteration 2687, dc_loss: 0.017789902165532112, tv_loss: 0.01713385432958603\n",
      "iteration 2688, dc_loss: 0.01778988726437092, tv_loss: 0.017133669927716255\n",
      "iteration 2689, dc_loss: 0.01778985559940338, tv_loss: 0.017133427783846855\n",
      "iteration 2690, dc_loss: 0.017789846286177635, tv_loss: 0.01713337004184723\n",
      "iteration 2691, dc_loss: 0.017789866775274277, tv_loss: 0.017133254557847977\n",
      "iteration 2692, dc_loss: 0.017789918929338455, tv_loss: 0.017133105546236038\n",
      "iteration 2693, dc_loss: 0.017789920791983604, tv_loss: 0.01713295467197895\n",
      "iteration 2694, dc_loss: 0.017789876088500023, tv_loss: 0.017132822424173355\n",
      "iteration 2695, dc_loss: 0.0177898071706295, tv_loss: 0.017132805660367012\n",
      "iteration 2696, dc_loss: 0.017789723351597786, tv_loss: 0.0171327106654644\n",
      "iteration 2697, dc_loss: 0.017789671197533607, tv_loss: 0.017132846638560295\n",
      "iteration 2698, dc_loss: 0.017789624631404877, tv_loss: 0.017132850363850594\n",
      "iteration 2699, dc_loss: 0.017789578065276146, tv_loss: 0.017132600769400597\n",
      "iteration 2700, dc_loss: 0.017789551988244057, tv_loss: 0.017132468521595\n",
      "iteration 2701, dc_loss: 0.017789505422115326, tv_loss: 0.017132354900240898\n",
      "iteration 2702, dc_loss: 0.01778949424624443, tv_loss: 0.017132384702563286\n",
      "iteration 2703, dc_loss: 0.01778946816921234, tv_loss: 0.017132379114627838\n",
      "iteration 2704, dc_loss: 0.017789488658308983, tv_loss: 0.01713213138282299\n",
      "iteration 2705, dc_loss: 0.017789488658308983, tv_loss: 0.01713200844824314\n",
      "iteration 2706, dc_loss: 0.017789486795663834, tv_loss: 0.017131974920630455\n",
      "iteration 2707, dc_loss: 0.017789451405405998, tv_loss: 0.01713177002966404\n",
      "iteration 2708, dc_loss: 0.017789384350180626, tv_loss: 0.0171317458152771\n",
      "iteration 2709, dc_loss: 0.017789341509342194, tv_loss: 0.01713167317211628\n",
      "iteration 2710, dc_loss: 0.01778930053114891, tv_loss: 0.017131371423602104\n",
      "iteration 2711, dc_loss: 0.017789283767342567, tv_loss: 0.017131472006440163\n",
      "iteration 2712, dc_loss: 0.017789261415600777, tv_loss: 0.017131567001342773\n",
      "iteration 2713, dc_loss: 0.017789220437407494, tv_loss: 0.017131216824054718\n",
      "iteration 2714, dc_loss: 0.017789192497730255, tv_loss: 0.017131047323346138\n",
      "iteration 2715, dc_loss: 0.017789125442504883, tv_loss: 0.017131131142377853\n",
      "iteration 2716, dc_loss: 0.01778903231024742, tv_loss: 0.01713128574192524\n",
      "iteration 2717, dc_loss: 0.01778900995850563, tv_loss: 0.01713109388947487\n",
      "iteration 2718, dc_loss: 0.017789024859666824, tv_loss: 0.017131000757217407\n",
      "iteration 2719, dc_loss: 0.01778901182115078, tv_loss: 0.017130853608250618\n",
      "iteration 2720, dc_loss: 0.01778898574411869, tv_loss: 0.01713068038225174\n",
      "iteration 2721, dc_loss: 0.017788978293538094, tv_loss: 0.017130615189671516\n",
      "iteration 2722, dc_loss: 0.017788982018828392, tv_loss: 0.01713051088154316\n",
      "iteration 2723, dc_loss: 0.017788980156183243, tv_loss: 0.017130382359027863\n",
      "iteration 2724, dc_loss: 0.017788954079151154, tv_loss: 0.01713031902909279\n",
      "iteration 2725, dc_loss: 0.017788922414183617, tv_loss: 0.017130259424448013\n",
      "iteration 2726, dc_loss: 0.01778886839747429, tv_loss: 0.01713009923696518\n",
      "iteration 2727, dc_loss: 0.01778881996870041, tv_loss: 0.017129996791481972\n",
      "iteration 2728, dc_loss: 0.017788788303732872, tv_loss: 0.017129968851804733\n",
      "iteration 2729, dc_loss: 0.017788749188184738, tv_loss: 0.017129814252257347\n",
      "iteration 2730, dc_loss: 0.01778874360024929, tv_loss: 0.017129776999354362\n",
      "iteration 2731, dc_loss: 0.017788749188184738, tv_loss: 0.0171295627951622\n",
      "iteration 2732, dc_loss: 0.017788732424378395, tv_loss: 0.01712951995432377\n",
      "iteration 2733, dc_loss: 0.017788700759410858, tv_loss: 0.01712936908006668\n",
      "iteration 2734, dc_loss: 0.017788685858249664, tv_loss: 0.017129385843873024\n",
      "iteration 2735, dc_loss: 0.017788641154766083, tv_loss: 0.01712922938168049\n",
      "iteration 2736, dc_loss: 0.017788613215088844, tv_loss: 0.01712902821600437\n",
      "iteration 2737, dc_loss: 0.017788568511605263, tv_loss: 0.01712903566658497\n",
      "iteration 2738, dc_loss: 0.017788514494895935, tv_loss: 0.017128903418779373\n",
      "iteration 2739, dc_loss: 0.017788438126444817, tv_loss: 0.017128895968198776\n",
      "iteration 2740, dc_loss: 0.0177883543074131, tv_loss: 0.01712890900671482\n",
      "iteration 2741, dc_loss: 0.017788322642445564, tv_loss: 0.017128828912973404\n",
      "iteration 2742, dc_loss: 0.017788318917155266, tv_loss: 0.01712876930832863\n",
      "iteration 2743, dc_loss: 0.017788315191864967, tv_loss: 0.01712867245078087\n",
      "iteration 2744, dc_loss: 0.017788318917155266, tv_loss: 0.017128366976976395\n",
      "iteration 2745, dc_loss: 0.017788348719477654, tv_loss: 0.01712850108742714\n",
      "iteration 2746, dc_loss: 0.017788337543606758, tv_loss: 0.01712821237742901\n",
      "iteration 2747, dc_loss: 0.017788302153348923, tv_loss: 0.017128081992268562\n",
      "iteration 2748, dc_loss: 0.017788264900445938, tv_loss: 0.01712808758020401\n",
      "iteration 2749, dc_loss: 0.017788225784897804, tv_loss: 0.01712799444794655\n",
      "iteration 2750, dc_loss: 0.01778818666934967, tv_loss: 0.017127826809883118\n",
      "iteration 2751, dc_loss: 0.017788181081414223, tv_loss: 0.01712770015001297\n",
      "iteration 2752, dc_loss: 0.017788181081414223, tv_loss: 0.017127545550465584\n",
      "iteration 2753, dc_loss: 0.017788168042898178, tv_loss: 0.017127396538853645\n",
      "iteration 2754, dc_loss: 0.01778813824057579, tv_loss: 0.017127372324466705\n",
      "iteration 2755, dc_loss: 0.017788106575608253, tv_loss: 0.01712728664278984\n",
      "iteration 2756, dc_loss: 0.017788074910640717, tv_loss: 0.017127273604273796\n",
      "iteration 2757, dc_loss: 0.017788024619221687, tv_loss: 0.017127182334661484\n",
      "iteration 2758, dc_loss: 0.01778799667954445, tv_loss: 0.017127150669693947\n",
      "iteration 2759, dc_loss: 0.017787965014576912, tv_loss: 0.01712702214717865\n",
      "iteration 2760, dc_loss: 0.017787957563996315, tv_loss: 0.017127010971307755\n",
      "iteration 2761, dc_loss: 0.01778792217373848, tv_loss: 0.01712685264647007\n",
      "iteration 2762, dc_loss: 0.01778784580528736, tv_loss: 0.01712675578892231\n",
      "iteration 2763, dc_loss: 0.017787771299481392, tv_loss: 0.01712675206363201\n",
      "iteration 2764, dc_loss: 0.01778770238161087, tv_loss: 0.01712685264647007\n",
      "iteration 2765, dc_loss: 0.01778767630457878, tv_loss: 0.01712663657963276\n",
      "iteration 2766, dc_loss: 0.01778767816722393, tv_loss: 0.017126603052020073\n",
      "iteration 2767, dc_loss: 0.017787687480449677, tv_loss: 0.01712636463344097\n",
      "iteration 2768, dc_loss: 0.017787734046578407, tv_loss: 0.01712612994015217\n",
      "iteration 2769, dc_loss: 0.017787767574191093, tv_loss: 0.017126116901636124\n",
      "iteration 2770, dc_loss: 0.017787782475352287, tv_loss: 0.017125992104411125\n",
      "iteration 2771, dc_loss: 0.0177877526730299, tv_loss: 0.017125902697443962\n",
      "iteration 2772, dc_loss: 0.01778767630457878, tv_loss: 0.01712578535079956\n",
      "iteration 2773, dc_loss: 0.017787590622901917, tv_loss: 0.01712583191692829\n",
      "iteration 2774, dc_loss: 0.01778748445212841, tv_loss: 0.01712564192712307\n",
      "iteration 2775, dc_loss: 0.017787383869290352, tv_loss: 0.017125707119703293\n",
      "iteration 2776, dc_loss: 0.017787359654903412, tv_loss: 0.01712561771273613\n",
      "iteration 2777, dc_loss: 0.017787382006645203, tv_loss: 0.01712554134428501\n",
      "iteration 2778, dc_loss: 0.017787398770451546, tv_loss: 0.017125388607382774\n",
      "iteration 2779, dc_loss: 0.017787441611289978, tv_loss: 0.017125209793448448\n",
      "iteration 2780, dc_loss: 0.01778746396303177, tv_loss: 0.017125112935900688\n",
      "iteration 2781, dc_loss: 0.017787465825676918, tv_loss: 0.01712496392428875\n",
      "iteration 2782, dc_loss: 0.01778741367161274, tv_loss: 0.017124753445386887\n",
      "iteration 2783, dc_loss: 0.017787311226129532, tv_loss: 0.017124749720096588\n",
      "iteration 2784, dc_loss: 0.01778722181916237, tv_loss: 0.017124908044934273\n",
      "iteration 2785, dc_loss: 0.017787139862775803, tv_loss: 0.017124898731708527\n",
      "iteration 2786, dc_loss: 0.017787078395485878, tv_loss: 0.01712479256093502\n",
      "iteration 2787, dc_loss: 0.017787037417292595, tv_loss: 0.017124589532613754\n",
      "iteration 2788, dc_loss: 0.017787013202905655, tv_loss: 0.017124490812420845\n",
      "iteration 2789, dc_loss: 0.017787039279937744, tv_loss: 0.0171244777739048\n",
      "iteration 2790, dc_loss: 0.017787078395485878, tv_loss: 0.01712435483932495\n",
      "iteration 2791, dc_loss: 0.017787082120776176, tv_loss: 0.01712404005229473\n",
      "iteration 2792, dc_loss: 0.017787093296647072, tv_loss: 0.017124034464359283\n",
      "iteration 2793, dc_loss: 0.01778702437877655, tv_loss: 0.01712404005229473\n",
      "iteration 2794, dc_loss: 0.017786933109164238, tv_loss: 0.01712397113442421\n",
      "iteration 2795, dc_loss: 0.017786841839551926, tv_loss: 0.017123747617006302\n",
      "iteration 2796, dc_loss: 0.017786802724003792, tv_loss: 0.017123887315392494\n",
      "iteration 2797, dc_loss: 0.017786800861358643, tv_loss: 0.01712385192513466\n",
      "iteration 2798, dc_loss: 0.01778685674071312, tv_loss: 0.01712358184158802\n",
      "iteration 2799, dc_loss: 0.01778690330684185, tv_loss: 0.017123306170105934\n",
      "iteration 2800, dc_loss: 0.017786893993616104, tv_loss: 0.017123457044363022\n",
      "iteration 2801, dc_loss: 0.017786823213100433, tv_loss: 0.01712336577475071\n",
      "iteration 2802, dc_loss: 0.01778675615787506, tv_loss: 0.01712324656546116\n",
      "iteration 2803, dc_loss: 0.01778668910264969, tv_loss: 0.017123190686106682\n",
      "iteration 2804, dc_loss: 0.017786670476198196, tv_loss: 0.017123250290751457\n",
      "iteration 2805, dc_loss: 0.017786672338843346, tv_loss: 0.017123093828558922\n",
      "iteration 2806, dc_loss: 0.017786679789423943, tv_loss: 0.01712282933294773\n",
      "iteration 2807, dc_loss: 0.017786651849746704, tv_loss: 0.01712266355752945\n",
      "iteration 2808, dc_loss: 0.01778661087155342, tv_loss: 0.017122782766819\n",
      "iteration 2809, dc_loss: 0.017786584794521332, tv_loss: 0.01712283492088318\n",
      "iteration 2810, dc_loss: 0.017786571756005287, tv_loss: 0.017122507095336914\n",
      "iteration 2811, dc_loss: 0.017786551266908646, tv_loss: 0.017122315242886543\n",
      "iteration 2812, dc_loss: 0.017786506563425064, tv_loss: 0.01712239906191826\n",
      "iteration 2813, dc_loss: 0.017786452546715736, tv_loss: 0.01712230034172535\n",
      "iteration 2814, dc_loss: 0.01778637431561947, tv_loss: 0.017122196033596992\n",
      "iteration 2815, dc_loss: 0.01778632029891014, tv_loss: 0.01712222956120968\n",
      "iteration 2816, dc_loss: 0.01778627373278141, tv_loss: 0.017122339457273483\n",
      "iteration 2817, dc_loss: 0.01778625324368477, tv_loss: 0.017122196033596992\n",
      "iteration 2818, dc_loss: 0.017786270007491112, tv_loss: 0.0171219352632761\n",
      "iteration 2819, dc_loss: 0.017786333337426186, tv_loss: 0.017121808603405952\n",
      "iteration 2820, dc_loss: 0.017786387354135513, tv_loss: 0.017121804878115654\n",
      "iteration 2821, dc_loss: 0.017786378040909767, tv_loss: 0.017121607437729836\n",
      "iteration 2822, dc_loss: 0.017786309123039246, tv_loss: 0.017121467739343643\n",
      "iteration 2823, dc_loss: 0.017786189913749695, tv_loss: 0.017121585085988045\n",
      "iteration 2824, dc_loss: 0.017786074429750443, tv_loss: 0.01712162047624588\n",
      "iteration 2825, dc_loss: 0.017785988748073578, tv_loss: 0.017121467739343643\n",
      "iteration 2826, dc_loss: 0.017785975709557533, tv_loss: 0.01712140627205372\n",
      "iteration 2827, dc_loss: 0.017786001786589622, tv_loss: 0.017121272161602974\n",
      "iteration 2828, dc_loss: 0.017786039039492607, tv_loss: 0.017121169716119766\n",
      "iteration 2829, dc_loss: 0.017786039039492607, tv_loss: 0.017121070995926857\n",
      "iteration 2830, dc_loss: 0.01778602786362171, tv_loss: 0.01712096855044365\n",
      "iteration 2831, dc_loss: 0.01778600923717022, tv_loss: 0.017120858654379845\n",
      "iteration 2832, dc_loss: 0.017785988748073578, tv_loss: 0.01712087169289589\n",
      "iteration 2833, dc_loss: 0.017785977572202682, tv_loss: 0.01712070405483246\n",
      "iteration 2834, dc_loss: 0.017785953357815742, tv_loss: 0.01712067238986492\n",
      "iteration 2835, dc_loss: 0.01778589002788067, tv_loss: 0.017120616510510445\n",
      "iteration 2836, dc_loss: 0.017785804346203804, tv_loss: 0.017120391130447388\n",
      "iteration 2837, dc_loss: 0.01778573729097843, tv_loss: 0.017120448872447014\n",
      "iteration 2838, dc_loss: 0.017785724252462387, tv_loss: 0.01712038181722164\n",
      "iteration 2839, dc_loss: 0.01778573729097843, tv_loss: 0.017120279371738434\n",
      "iteration 2840, dc_loss: 0.01778576150536537, tv_loss: 0.0171200018376112\n",
      "iteration 2841, dc_loss: 0.017785806208848953, tv_loss: 0.01711973361670971\n",
      "iteration 2842, dc_loss: 0.01778581365942955, tv_loss: 0.017119688913226128\n",
      "iteration 2843, dc_loss: 0.01778573729097843, tv_loss: 0.017119811847805977\n",
      "iteration 2844, dc_loss: 0.017785625532269478, tv_loss: 0.017119860276579857\n",
      "iteration 2845, dc_loss: 0.017785537987947464, tv_loss: 0.01711970753967762\n",
      "iteration 2846, dc_loss: 0.01778550259768963, tv_loss: 0.017119718715548515\n",
      "iteration 2847, dc_loss: 0.017785482108592987, tv_loss: 0.01711958646774292\n",
      "iteration 2848, dc_loss: 0.01778552122414112, tv_loss: 0.017119307070970535\n",
      "iteration 2849, dc_loss: 0.017785560339689255, tv_loss: 0.017119204625487328\n",
      "iteration 2850, dc_loss: 0.017785601317882538, tv_loss: 0.017119213938713074\n",
      "iteration 2851, dc_loss: 0.01778557524085045, tv_loss: 0.017119120806455612\n",
      "iteration 2852, dc_loss: 0.017785541713237762, tv_loss: 0.01711898483335972\n",
      "iteration 2853, dc_loss: 0.01778552122414112, tv_loss: 0.01711883395910263\n",
      "iteration 2854, dc_loss: 0.017785513773560524, tv_loss: 0.01711869426071644\n",
      "iteration 2855, dc_loss: 0.017785480245947838, tv_loss: 0.017118841409683228\n",
      "iteration 2856, dc_loss: 0.017785407602787018, tv_loss: 0.01711869053542614\n",
      "iteration 2857, dc_loss: 0.017785321921110153, tv_loss: 0.017118554562330246\n",
      "iteration 2858, dc_loss: 0.017785271629691124, tv_loss: 0.017118580639362335\n",
      "iteration 2859, dc_loss: 0.01778523437678814, tv_loss: 0.017118632793426514\n",
      "iteration 2860, dc_loss: 0.017785178497433662, tv_loss: 0.01711847260594368\n",
      "iteration 2861, dc_loss: 0.01778513751924038, tv_loss: 0.01711832545697689\n",
      "iteration 2862, dc_loss: 0.01778513751924038, tv_loss: 0.01711839810013771\n",
      "iteration 2863, dc_loss: 0.017785152420401573, tv_loss: 0.017118312418460846\n",
      "iteration 2864, dc_loss: 0.01778513751924038, tv_loss: 0.017118126153945923\n",
      "iteration 2865, dc_loss: 0.017785148695111275, tv_loss: 0.017117895185947418\n",
      "iteration 2866, dc_loss: 0.017785128206014633, tv_loss: 0.0171180572360754\n",
      "iteration 2867, dc_loss: 0.017785122618079185, tv_loss: 0.01711801066994667\n",
      "iteration 2868, dc_loss: 0.017785102128982544, tv_loss: 0.01711774617433548\n",
      "iteration 2869, dc_loss: 0.01778504066169262, tv_loss: 0.017117643728852272\n",
      "iteration 2870, dc_loss: 0.01778501085937023, tv_loss: 0.017117662355303764\n",
      "iteration 2871, dc_loss: 0.01778498850762844, tv_loss: 0.017117708921432495\n",
      "iteration 2872, dc_loss: 0.017784995958209038, tv_loss: 0.01711762137711048\n",
      "iteration 2873, dc_loss: 0.017784958705306053, tv_loss: 0.01711738109588623\n",
      "iteration 2874, dc_loss: 0.017784900963306427, tv_loss: 0.017117248848080635\n",
      "iteration 2875, dc_loss: 0.017784882336854935, tv_loss: 0.01711721532046795\n",
      "iteration 2876, dc_loss: 0.017784856259822845, tv_loss: 0.017117172479629517\n",
      "iteration 2877, dc_loss: 0.01778482273221016, tv_loss: 0.017117053270339966\n",
      "iteration 2878, dc_loss: 0.01778477616608143, tv_loss: 0.01711702160537243\n",
      "iteration 2879, dc_loss: 0.017784755676984787, tv_loss: 0.017116928473114967\n",
      "iteration 2880, dc_loss: 0.01778477430343628, tv_loss: 0.017116861417889595\n",
      "iteration 2881, dc_loss: 0.017784833908081055, tv_loss: 0.017116714268922806\n",
      "iteration 2882, dc_loss: 0.017784839496016502, tv_loss: 0.017116567119956017\n",
      "iteration 2883, dc_loss: 0.017784807831048965, tv_loss: 0.017116600647568703\n",
      "iteration 2884, dc_loss: 0.01778475195169449, tv_loss: 0.01711653172969818\n",
      "iteration 2885, dc_loss: 0.017784681171178818, tv_loss: 0.017116418108344078\n",
      "iteration 2886, dc_loss: 0.017784632742404938, tv_loss: 0.017116300761699677\n",
      "iteration 2887, dc_loss: 0.017784573137760162, tv_loss: 0.017116215080022812\n",
      "iteration 2888, dc_loss: 0.017784517258405685, tv_loss: 0.017116369679570198\n",
      "iteration 2889, dc_loss: 0.017784489318728447, tv_loss: 0.01711622253060341\n",
      "iteration 2890, dc_loss: 0.017784465104341507, tv_loss: 0.01711600087583065\n",
      "iteration 2891, dc_loss: 0.017784444615244865, tv_loss: 0.017115961760282516\n",
      "iteration 2892, dc_loss: 0.017784442752599716, tv_loss: 0.01711612194776535\n",
      "iteration 2893, dc_loss: 0.017784425988793373, tv_loss: 0.017115846276283264\n",
      "iteration 2894, dc_loss: 0.017784427851438522, tv_loss: 0.01711568608880043\n",
      "iteration 2895, dc_loss: 0.017784422263503075, tv_loss: 0.017115775495767593\n",
      "iteration 2896, dc_loss: 0.01778440922498703, tv_loss: 0.017115790396928787\n",
      "iteration 2897, dc_loss: 0.017784401774406433, tv_loss: 0.017115673050284386\n",
      "iteration 2898, dc_loss: 0.017784379422664642, tv_loss: 0.017115524038672447\n",
      "iteration 2899, dc_loss: 0.017784351482987404, tv_loss: 0.0171155147254467\n",
      "iteration 2900, dc_loss: 0.01778431050479412, tv_loss: 0.01711537130177021\n",
      "iteration 2901, dc_loss: 0.017784301191568375, tv_loss: 0.017115304246544838\n",
      "iteration 2902, dc_loss: 0.017784293740987778, tv_loss: 0.017115332186222076\n",
      "iteration 2903, dc_loss: 0.017784245312213898, tv_loss: 0.017115244641900063\n",
      "iteration 2904, dc_loss: 0.017784180119633675, tv_loss: 0.017115123569965363\n",
      "iteration 2905, dc_loss: 0.017784133553504944, tv_loss: 0.017115119844675064\n",
      "iteration 2906, dc_loss: 0.017784124240279198, tv_loss: 0.01711496151983738\n",
      "iteration 2907, dc_loss: 0.017784159630537033, tv_loss: 0.017114775255322456\n",
      "iteration 2908, dc_loss: 0.017784148454666138, tv_loss: 0.017114760354161263\n",
      "iteration 2909, dc_loss: 0.017784126102924347, tv_loss: 0.017114702612161636\n",
      "iteration 2910, dc_loss: 0.017784109339118004, tv_loss: 0.017114587128162384\n",
      "iteration 2911, dc_loss: 0.01778407022356987, tv_loss: 0.01711435429751873\n",
      "iteration 2912, dc_loss: 0.017784027382731438, tv_loss: 0.01711447909474373\n",
      "iteration 2913, dc_loss: 0.017784006893634796, tv_loss: 0.017114438116550446\n",
      "iteration 2914, dc_loss: 0.017783982679247856, tv_loss: 0.01711422950029373\n",
      "iteration 2915, dc_loss: 0.017783964052796364, tv_loss: 0.017114322632551193\n",
      "iteration 2916, dc_loss: 0.017783956602215767, tv_loss: 0.017114214599132538\n",
      "iteration 2917, dc_loss: 0.01778392679989338, tv_loss: 0.017114020884037018\n",
      "iteration 2918, dc_loss: 0.017783917486667633, tv_loss: 0.017114000394940376\n",
      "iteration 2919, dc_loss: 0.017783882096409798, tv_loss: 0.017114127054810524\n",
      "iteration 2920, dc_loss: 0.017783841118216515, tv_loss: 0.017113907262682915\n",
      "iteration 2921, dc_loss: 0.017783790826797485, tv_loss: 0.017113879323005676\n",
      "iteration 2922, dc_loss: 0.017783764749765396, tv_loss: 0.01711384579539299\n",
      "iteration 2923, dc_loss: 0.017783749848604202, tv_loss: 0.017113633453845978\n",
      "iteration 2924, dc_loss: 0.017783749848604202, tv_loss: 0.01711359992623329\n",
      "iteration 2925, dc_loss: 0.017783746123313904, tv_loss: 0.017113471403717995\n",
      "iteration 2926, dc_loss: 0.017783720046281815, tv_loss: 0.01711348071694374\n",
      "iteration 2927, dc_loss: 0.017783693969249725, tv_loss: 0.017113322392106056\n",
      "iteration 2928, dc_loss: 0.017783671617507935, tv_loss: 0.01711316965520382\n",
      "iteration 2929, dc_loss: 0.017783593386411667, tv_loss: 0.017113175243139267\n",
      "iteration 2930, dc_loss: 0.017783548682928085, tv_loss: 0.017113270238041878\n",
      "iteration 2931, dc_loss: 0.017783518880605698, tv_loss: 0.01711316779255867\n",
      "iteration 2932, dc_loss: 0.01778343692421913, tv_loss: 0.017113037407398224\n",
      "iteration 2933, dc_loss: 0.017783384770154953, tv_loss: 0.01711289957165718\n",
      "iteration 2934, dc_loss: 0.017783354967832565, tv_loss: 0.017112907022237778\n",
      "iteration 2935, dc_loss: 0.01778337173163891, tv_loss: 0.017112864181399345\n",
      "iteration 2936, dc_loss: 0.01778341270983219, tv_loss: 0.01711253449320793\n",
      "iteration 2937, dc_loss: 0.017783449962735176, tv_loss: 0.017112450674176216\n",
      "iteration 2938, dc_loss: 0.01778348535299301, tv_loss: 0.017112381756305695\n",
      "iteration 2939, dc_loss: 0.01778351329267025, tv_loss: 0.01711224764585495\n",
      "iteration 2940, dc_loss: 0.01778348907828331, tv_loss: 0.017112256959080696\n",
      "iteration 2941, dc_loss: 0.017783435061573982, tv_loss: 0.017111994326114655\n",
      "iteration 2942, dc_loss: 0.017783435061573982, tv_loss: 0.017111873254179955\n",
      "iteration 2943, dc_loss: 0.017783446237444878, tv_loss: 0.01711205206811428\n",
      "iteration 2944, dc_loss: 0.01778341829776764, tv_loss: 0.01711205393075943\n",
      "iteration 2945, dc_loss: 0.01778332330286503, tv_loss: 0.017111914232373238\n",
      "iteration 2946, dc_loss: 0.017783179879188538, tv_loss: 0.017111901193857193\n",
      "iteration 2947, dc_loss: 0.017783088609576225, tv_loss: 0.017112143337726593\n",
      "iteration 2948, dc_loss: 0.01778305694460869, tv_loss: 0.01711226999759674\n",
      "iteration 2949, dc_loss: 0.017783068120479584, tv_loss: 0.017111986875534058\n",
      "iteration 2950, dc_loss: 0.01778312958776951, tv_loss: 0.017111731693148613\n",
      "iteration 2951, dc_loss: 0.017783205956220627, tv_loss: 0.01711166277527809\n",
      "iteration 2952, dc_loss: 0.01778325065970421, tv_loss: 0.017111623659729958\n",
      "iteration 2953, dc_loss: 0.017783235758543015, tv_loss: 0.01711159758269787\n",
      "iteration 2954, dc_loss: 0.017783181741833687, tv_loss: 0.017111465334892273\n",
      "iteration 2955, dc_loss: 0.017783109098672867, tv_loss: 0.01711127907037735\n",
      "iteration 2956, dc_loss: 0.01778302527964115, tv_loss: 0.01711140386760235\n",
      "iteration 2957, dc_loss: 0.017782986164093018, tv_loss: 0.01711132936179638\n",
      "iteration 2958, dc_loss: 0.017782995477318764, tv_loss: 0.017111236229538918\n",
      "iteration 2959, dc_loss: 0.017782995477318764, tv_loss: 0.017111124470829964\n",
      "iteration 2960, dc_loss: 0.017782989889383316, tv_loss: 0.01711101643741131\n",
      "iteration 2961, dc_loss: 0.01778303273022175, tv_loss: 0.017111044377088547\n",
      "iteration 2962, dc_loss: 0.017783040180802345, tv_loss: 0.017110921442508698\n",
      "iteration 2963, dc_loss: 0.017782988026738167, tv_loss: 0.01711072213947773\n",
      "iteration 2964, dc_loss: 0.01778293214738369, tv_loss: 0.017110662534832954\n",
      "iteration 2965, dc_loss: 0.01778288185596466, tv_loss: 0.017110807821154594\n",
      "iteration 2966, dc_loss: 0.01778283342719078, tv_loss: 0.017110763117671013\n",
      "iteration 2967, dc_loss: 0.017782757058739662, tv_loss: 0.01711072213947773\n",
      "iteration 2968, dc_loss: 0.01778271608054638, tv_loss: 0.017110515385866165\n",
      "iteration 2969, dc_loss: 0.01778271794319153, tv_loss: 0.017110537737607956\n",
      "iteration 2970, dc_loss: 0.01778274029493332, tv_loss: 0.017110418528318405\n",
      "iteration 2971, dc_loss: 0.017782732844352722, tv_loss: 0.01711021549999714\n",
      "iteration 2972, dc_loss: 0.017782719805836678, tv_loss: 0.017110152170062065\n",
      "iteration 2973, dc_loss: 0.017782723531126976, tv_loss: 0.017110228538513184\n",
      "iteration 2974, dc_loss: 0.017782684415578842, tv_loss: 0.017110252752900124\n",
      "iteration 2975, dc_loss: 0.017782650887966156, tv_loss: 0.01711001992225647\n",
      "iteration 2976, dc_loss: 0.017782602459192276, tv_loss: 0.017110027372837067\n",
      "iteration 2977, dc_loss: 0.01778256893157959, tv_loss: 0.0171100664883852\n",
      "iteration 2978, dc_loss: 0.01778256706893444, tv_loss: 0.017109917476773262\n",
      "iteration 2979, dc_loss: 0.0177825428545475, tv_loss: 0.017109883949160576\n",
      "iteration 2980, dc_loss: 0.017782526090741158, tv_loss: 0.017109865322709084\n",
      "iteration 2981, dc_loss: 0.017782533541321754, tv_loss: 0.017109660431742668\n",
      "iteration 2982, dc_loss: 0.017782561480998993, tv_loss: 0.017109502106904984\n",
      "iteration 2983, dc_loss: 0.01778259500861168, tv_loss: 0.017109448090195656\n",
      "iteration 2984, dc_loss: 0.017782561480998993, tv_loss: 0.01710936799645424\n",
      "iteration 2985, dc_loss: 0.01778250001370907, tv_loss: 0.01710926927626133\n",
      "iteration 2986, dc_loss: 0.01778247021138668, tv_loss: 0.017109176144003868\n",
      "iteration 2987, dc_loss: 0.017782442271709442, tv_loss: 0.017109304666519165\n",
      "iteration 2988, dc_loss: 0.017782388255000114, tv_loss: 0.01710914447903633\n",
      "iteration 2989, dc_loss: 0.01778232678771019, tv_loss: 0.01710909605026245\n",
      "iteration 2990, dc_loss: 0.01778225600719452, tv_loss: 0.01710912585258484\n",
      "iteration 2991, dc_loss: 0.01778218150138855, tv_loss: 0.017109094187617302\n",
      "iteration 2992, dc_loss: 0.017782142385840416, tv_loss: 0.01710904948413372\n",
      "iteration 2993, dc_loss: 0.017782142385840416, tv_loss: 0.017108945176005363\n",
      "iteration 2994, dc_loss: 0.017782140523195267, tv_loss: 0.017109086737036705\n",
      "iteration 2995, dc_loss: 0.01778213120996952, tv_loss: 0.01710888184607029\n",
      "iteration 2996, dc_loss: 0.017782088369131088, tv_loss: 0.0171086173504591\n",
      "iteration 2997, dc_loss: 0.017782090231776237, tv_loss: 0.017108848318457603\n",
      "iteration 2998, dc_loss: 0.017782114446163177, tv_loss: 0.017108779400587082\n",
      "iteration 2999, dc_loss: 0.017782121896743774, tv_loss: 0.017108511179685593\n",
      "iteration 3000, dc_loss: 0.017782101407647133, tv_loss: 0.0171083752065897\n",
      "iteration 3001, dc_loss: 0.017782071605324745, tv_loss: 0.01710847020149231\n",
      "iteration 3002, dc_loss: 0.017782054841518402, tv_loss: 0.017108477652072906\n",
      "iteration 3003, dc_loss: 0.01778203248977661, tv_loss: 0.017108304426074028\n",
      "iteration 3004, dc_loss: 0.017781991511583328, tv_loss: 0.017108118161559105\n",
      "iteration 3005, dc_loss: 0.01778195984661579, tv_loss: 0.01710791513323784\n",
      "iteration 3006, dc_loss: 0.017781952396035194, tv_loss: 0.01710803061723709\n",
      "iteration 3007, dc_loss: 0.0177819412201643, tv_loss: 0.017108000814914703\n",
      "iteration 3008, dc_loss: 0.017781933769583702, tv_loss: 0.01710779219865799\n",
      "iteration 3009, dc_loss: 0.01778191328048706, tv_loss: 0.017107555642724037\n",
      "iteration 3010, dc_loss: 0.017781900241971016, tv_loss: 0.017107758671045303\n",
      "iteration 3011, dc_loss: 0.017781879752874374, tv_loss: 0.01710774563252926\n",
      "iteration 3012, dc_loss: 0.017781835049390793, tv_loss: 0.017107728868722916\n",
      "iteration 3013, dc_loss: 0.017781805247068405, tv_loss: 0.017107505351305008\n",
      "iteration 3014, dc_loss: 0.017781799659132957, tv_loss: 0.01710730604827404\n",
      "iteration 3015, dc_loss: 0.017781812697649002, tv_loss: 0.017107252031564713\n",
      "iteration 3016, dc_loss: 0.0177818201482296, tv_loss: 0.017107298597693443\n",
      "iteration 3017, dc_loss: 0.01778179220855236, tv_loss: 0.017107022926211357\n",
      "iteration 3018, dc_loss: 0.017781773582100868, tv_loss: 0.017107060179114342\n",
      "iteration 3019, dc_loss: 0.017781736329197884, tv_loss: 0.017107129096984863\n",
      "iteration 3020, dc_loss: 0.017781689763069153, tv_loss: 0.017106957733631134\n",
      "iteration 3021, dc_loss: 0.017781639471650124, tv_loss: 0.017107071354985237\n",
      "iteration 3022, dc_loss: 0.017781618982553482, tv_loss: 0.017107034102082253\n",
      "iteration 3023, dc_loss: 0.017781639471650124, tv_loss: 0.0171068012714386\n",
      "iteration 3024, dc_loss: 0.017781663686037064, tv_loss: 0.01710669696331024\n",
      "iteration 3025, dc_loss: 0.017781658098101616, tv_loss: 0.01710672304034233\n",
      "iteration 3026, dc_loss: 0.01778162643313408, tv_loss: 0.017106737941503525\n",
      "iteration 3027, dc_loss: 0.017781537026166916, tv_loss: 0.017106669023633003\n",
      "iteration 3028, dc_loss: 0.017781466245651245, tv_loss: 0.017106616869568825\n",
      "iteration 3029, dc_loss: 0.017781442031264305, tv_loss: 0.01710665412247181\n",
      "iteration 3030, dc_loss: 0.01778145506978035, tv_loss: 0.017106538638472557\n",
      "iteration 3031, dc_loss: 0.01778147742152214, tv_loss: 0.01710635982453823\n",
      "iteration 3032, dc_loss: 0.017781471833586693, tv_loss: 0.017106223851442337\n",
      "iteration 3033, dc_loss: 0.01778148114681244, tv_loss: 0.01710621826350689\n",
      "iteration 3034, dc_loss: 0.017781423404812813, tv_loss: 0.017105991020798683\n",
      "iteration 3035, dc_loss: 0.017781347036361694, tv_loss: 0.017106063663959503\n",
      "iteration 3036, dc_loss: 0.0177813321352005, tv_loss: 0.017106102779507637\n",
      "iteration 3037, dc_loss: 0.017781339585781097, tv_loss: 0.017105985432863235\n",
      "iteration 3038, dc_loss: 0.01778138242661953, tv_loss: 0.017105726525187492\n",
      "iteration 3039, dc_loss: 0.01778140477836132, tv_loss: 0.017105598002672195\n",
      "iteration 3040, dc_loss: 0.01778137870132923, tv_loss: 0.017105573788285255\n",
      "iteration 3041, dc_loss: 0.01778130792081356, tv_loss: 0.017105557024478912\n",
      "iteration 3042, dc_loss: 0.017781242728233337, tv_loss: 0.017105519771575928\n",
      "iteration 3043, dc_loss: 0.017781203612685204, tv_loss: 0.017105577513575554\n",
      "iteration 3044, dc_loss: 0.017781201750040054, tv_loss: 0.01710541732609272\n",
      "iteration 3045, dc_loss: 0.017781177535653114, tv_loss: 0.017105311155319214\n",
      "iteration 3046, dc_loss: 0.017781132832169533, tv_loss: 0.017105303704738617\n",
      "iteration 3047, dc_loss: 0.017781084403395653, tv_loss: 0.017105309292674065\n",
      "iteration 3048, dc_loss: 0.017781071364879608, tv_loss: 0.01710517331957817\n",
      "iteration 3049, dc_loss: 0.017781071364879608, tv_loss: 0.017105109989643097\n",
      "iteration 3050, dc_loss: 0.017781052738428116, tv_loss: 0.017105286940932274\n",
      "iteration 3051, dc_loss: 0.01778103969991207, tv_loss: 0.017105037346482277\n",
      "iteration 3052, dc_loss: 0.017781034111976624, tv_loss: 0.017104988917708397\n",
      "iteration 3053, dc_loss: 0.017781024798750877, tv_loss: 0.017104927450418472\n",
      "iteration 3054, dc_loss: 0.017781035974621773, tv_loss: 0.01710491068661213\n",
      "iteration 3055, dc_loss: 0.017781032249331474, tv_loss: 0.01710480824112892\n",
      "iteration 3056, dc_loss: 0.01778099685907364, tv_loss: 0.01710478775203228\n",
      "iteration 3057, dc_loss: 0.017780980095267296, tv_loss: 0.017104752361774445\n",
      "iteration 3058, dc_loss: 0.01778096705675125, tv_loss: 0.01710459589958191\n",
      "iteration 3059, dc_loss: 0.017780916765332222, tv_loss: 0.0171044934540987\n",
      "iteration 3060, dc_loss: 0.017780866473913193, tv_loss: 0.01710459031164646\n",
      "iteration 3061, dc_loss: 0.017780814319849014, tv_loss: 0.017104540020227432\n",
      "iteration 3062, dc_loss: 0.01778075285255909, tv_loss: 0.017104534432291985\n",
      "iteration 3063, dc_loss: 0.017780734226107597, tv_loss: 0.017104480415582657\n",
      "iteration 3064, dc_loss: 0.017780764028429985, tv_loss: 0.017104372382164\n",
      "iteration 3065, dc_loss: 0.01778079755604267, tv_loss: 0.017104296013712883\n",
      "iteration 3066, dc_loss: 0.017780829221010208, tv_loss: 0.017104148864746094\n",
      "iteration 3067, dc_loss: 0.017780808731913567, tv_loss: 0.01710405945777893\n",
      "iteration 3068, dc_loss: 0.017780747264623642, tv_loss: 0.017104053869843483\n",
      "iteration 3069, dc_loss: 0.01778070628643036, tv_loss: 0.017104042693972588\n",
      "iteration 3070, dc_loss: 0.01778067834675312, tv_loss: 0.017104201018810272\n",
      "iteration 3071, dc_loss: 0.017780672758817673, tv_loss: 0.01710398867726326\n",
      "iteration 3072, dc_loss: 0.01778067648410797, tv_loss: 0.01710360310971737\n",
      "iteration 3073, dc_loss: 0.017780667170882225, tv_loss: 0.017103690654039383\n",
      "iteration 3074, dc_loss: 0.01778067834675312, tv_loss: 0.017103644087910652\n",
      "iteration 3075, dc_loss: 0.01778067648410797, tv_loss: 0.017103547230362892\n",
      "iteration 3076, dc_loss: 0.017780635505914688, tv_loss: 0.01710350252687931\n",
      "iteration 3077, dc_loss: 0.017780570313334465, tv_loss: 0.017103401944041252\n",
      "iteration 3078, dc_loss: 0.017780546098947525, tv_loss: 0.01710331253707409\n",
      "iteration 3079, dc_loss: 0.01778053492307663, tv_loss: 0.017103498801589012\n",
      "iteration 3080, dc_loss: 0.017780505120754242, tv_loss: 0.01710338704288006\n",
      "iteration 3081, dc_loss: 0.017780495807528496, tv_loss: 0.017103150486946106\n",
      "iteration 3082, dc_loss: 0.017780479043722153, tv_loss: 0.017103295773267746\n",
      "iteration 3083, dc_loss: 0.01778045855462551, tv_loss: 0.01710330881178379\n",
      "iteration 3084, dc_loss: 0.01778043620288372, tv_loss: 0.017103232443332672\n",
      "iteration 3085, dc_loss: 0.017780382186174393, tv_loss: 0.017103103920817375\n",
      "iteration 3086, dc_loss: 0.017780352383852005, tv_loss: 0.017103128135204315\n",
      "iteration 3087, dc_loss: 0.017780335620045662, tv_loss: 0.017103036865592003\n",
      "iteration 3088, dc_loss: 0.017780276015400887, tv_loss: 0.017103109508752823\n",
      "iteration 3089, dc_loss: 0.0177802462130785, tv_loss: 0.017103053629398346\n",
      "iteration 3090, dc_loss: 0.0177802462130785, tv_loss: 0.017102954909205437\n",
      "iteration 3091, dc_loss: 0.01778028905391693, tv_loss: 0.017102815210819244\n",
      "iteration 3092, dc_loss: 0.017780354246497154, tv_loss: 0.017102740705013275\n",
      "iteration 3093, dc_loss: 0.017780382186174393, tv_loss: 0.017102602869272232\n",
      "iteration 3094, dc_loss: 0.01778033934533596, tv_loss: 0.017102599143981934\n",
      "iteration 3095, dc_loss: 0.017780255526304245, tv_loss: 0.01710275374352932\n",
      "iteration 3096, dc_loss: 0.017780190333724022, tv_loss: 0.017102641984820366\n",
      "iteration 3097, dc_loss: 0.017780154943466187, tv_loss: 0.017102450132369995\n",
      "iteration 3098, dc_loss: 0.017780132591724396, tv_loss: 0.017102520912885666\n",
      "iteration 3099, dc_loss: 0.01778009906411171, tv_loss: 0.01710246503353119\n",
      "iteration 3100, dc_loss: 0.017780080437660217, tv_loss: 0.017102425917983055\n",
      "iteration 3101, dc_loss: 0.017780063673853874, tv_loss: 0.01710236258804798\n",
      "iteration 3102, dc_loss: 0.017780102789402008, tv_loss: 0.017102211713790894\n",
      "iteration 3103, dc_loss: 0.01778014749288559, tv_loss: 0.017102142795920372\n",
      "iteration 3104, dc_loss: 0.017780141904950142, tv_loss: 0.017102165147662163\n",
      "iteration 3105, dc_loss: 0.017780132591724396, tv_loss: 0.01710200496017933\n",
      "iteration 3106, dc_loss: 0.01778012327849865, tv_loss: 0.017101801931858063\n",
      "iteration 3107, dc_loss: 0.01778007484972477, tv_loss: 0.017101814970374107\n",
      "iteration 3108, dc_loss: 0.01777999848127365, tv_loss: 0.017101828008890152\n",
      "iteration 3109, dc_loss: 0.017779899761080742, tv_loss: 0.017101716250181198\n",
      "iteration 3110, dc_loss: 0.017779821529984474, tv_loss: 0.017101891338825226\n",
      "iteration 3111, dc_loss: 0.017779797315597534, tv_loss: 0.017101755365729332\n",
      "iteration 3112, dc_loss: 0.017779827117919922, tv_loss: 0.017101546749472618\n",
      "iteration 3113, dc_loss: 0.017779886722564697, tv_loss: 0.01710153929889202\n",
      "iteration 3114, dc_loss: 0.01777990348637104, tv_loss: 0.01710144616663456\n",
      "iteration 3115, dc_loss: 0.017779869958758354, tv_loss: 0.017101457342505455\n",
      "iteration 3116, dc_loss: 0.017779825255274773, tv_loss: 0.01710152067244053\n",
      "iteration 3117, dc_loss: 0.017779747024178505, tv_loss: 0.017101401463150978\n",
      "iteration 3118, dc_loss: 0.01777971349656582, tv_loss: 0.01710130088031292\n",
      "iteration 3119, dc_loss: 0.017779720947146416, tv_loss: 0.017101308330893517\n",
      "iteration 3120, dc_loss: 0.017779773101210594, tv_loss: 0.01710106059908867\n",
      "iteration 3121, dc_loss: 0.01777980662882328, tv_loss: 0.0171008612960577\n",
      "iteration 3122, dc_loss: 0.017779819667339325, tv_loss: 0.01710103079676628\n",
      "iteration 3123, dc_loss: 0.01777981035411358, tv_loss: 0.01710103079676628\n",
      "iteration 3124, dc_loss: 0.017779814079403877, tv_loss: 0.017100578173995018\n",
      "iteration 3125, dc_loss: 0.01777980476617813, tv_loss: 0.017100675031542778\n",
      "iteration 3126, dc_loss: 0.01777978613972664, tv_loss: 0.017100585624575615\n",
      "iteration 3127, dc_loss: 0.017779754474759102, tv_loss: 0.017100559547543526\n",
      "iteration 3128, dc_loss: 0.01777968369424343, tv_loss: 0.017100688070058823\n",
      "iteration 3129, dc_loss: 0.01777959242463112, tv_loss: 0.017100617289543152\n",
      "iteration 3130, dc_loss: 0.017779473215341568, tv_loss: 0.017100563272833824\n",
      "iteration 3131, dc_loss: 0.017779408022761345, tv_loss: 0.017100676894187927\n",
      "iteration 3132, dc_loss: 0.01777937076985836, tv_loss: 0.01710062101483345\n",
      "iteration 3133, dc_loss: 0.01777937076985836, tv_loss: 0.017100410535931587\n",
      "iteration 3134, dc_loss: 0.017779413610696793, tv_loss: 0.017100507393479347\n",
      "iteration 3135, dc_loss: 0.017779460176825523, tv_loss: 0.017100337892770767\n",
      "iteration 3136, dc_loss: 0.017779501155018806, tv_loss: 0.01710028387606144\n",
      "iteration 3137, dc_loss: 0.017779530957341194, tv_loss: 0.01710011623799801\n",
      "iteration 3138, dc_loss: 0.017779506742954254, tv_loss: 0.017099998891353607\n",
      "iteration 3139, dc_loss: 0.01777949556708336, tv_loss: 0.017099911347031593\n",
      "iteration 3140, dc_loss: 0.01777946762740612, tv_loss: 0.017099939286708832\n",
      "iteration 3141, dc_loss: 0.017779450863599777, tv_loss: 0.017099915072321892\n",
      "iteration 3142, dc_loss: 0.017779426649212837, tv_loss: 0.017099827527999878\n",
      "iteration 3143, dc_loss: 0.017779357731342316, tv_loss: 0.017099691554903984\n",
      "iteration 3144, dc_loss: 0.017779307439923286, tv_loss: 0.017099598422646523\n",
      "iteration 3145, dc_loss: 0.017779303714632988, tv_loss: 0.017099834978580475\n",
      "iteration 3146, dc_loss: 0.017779307439923286, tv_loss: 0.017099654302001\n",
      "iteration 3147, dc_loss: 0.01777929998934269, tv_loss: 0.017099443823099136\n",
      "iteration 3148, dc_loss: 0.017779285088181496, tv_loss: 0.017099428921937943\n",
      "iteration 3149, dc_loss: 0.017779268324375153, tv_loss: 0.017099520191550255\n",
      "iteration 3150, dc_loss: 0.017779231071472168, tv_loss: 0.017099352553486824\n",
      "iteration 3151, dc_loss: 0.01777922734618187, tv_loss: 0.017099294811487198\n",
      "iteration 3152, dc_loss: 0.017779219895601273, tv_loss: 0.0170991700142622\n",
      "iteration 3153, dc_loss: 0.017779212445020676, tv_loss: 0.01709914393723011\n",
      "iteration 3154, dc_loss: 0.01777917519211769, tv_loss: 0.01709912158548832\n",
      "iteration 3155, dc_loss: 0.01777912676334381, tv_loss: 0.01709909364581108\n",
      "iteration 3156, dc_loss: 0.017779067158699036, tv_loss: 0.017099179327487946\n",
      "iteration 3157, dc_loss: 0.017779041081666946, tv_loss: 0.017099089920520782\n",
      "iteration 3158, dc_loss: 0.01777900941669941, tv_loss: 0.01709902286529541\n",
      "iteration 3159, dc_loss: 0.0177790317684412, tv_loss: 0.01709890365600586\n",
      "iteration 3160, dc_loss: 0.017779052257537842, tv_loss: 0.017098773270845413\n",
      "iteration 3161, dc_loss: 0.01777905598282814, tv_loss: 0.017098739743232727\n",
      "iteration 3162, dc_loss: 0.0177790354937315, tv_loss: 0.01709870435297489\n",
      "iteration 3163, dc_loss: 0.0177790317684412, tv_loss: 0.01709866337478161\n",
      "iteration 3164, dc_loss: 0.01777905412018299, tv_loss: 0.017098432406783104\n",
      "iteration 3165, dc_loss: 0.017779042944312096, tv_loss: 0.017098629847168922\n",
      "iteration 3166, dc_loss: 0.017778988927602768, tv_loss: 0.01709849014878273\n",
      "iteration 3167, dc_loss: 0.01777893863618374, tv_loss: 0.017098497599363327\n",
      "iteration 3168, dc_loss: 0.017778873443603516, tv_loss: 0.017098456621170044\n",
      "iteration 3169, dc_loss: 0.0177787896245718, tv_loss: 0.01709839515388012\n",
      "iteration 3170, dc_loss: 0.01777876354753971, tv_loss: 0.017098501324653625\n",
      "iteration 3171, dc_loss: 0.017778759822249413, tv_loss: 0.017098519951105118\n",
      "iteration 3172, dc_loss: 0.01777876913547516, tv_loss: 0.017098460346460342\n",
      "iteration 3173, dc_loss: 0.017778752371668816, tv_loss: 0.017098238691687584\n",
      "iteration 3174, dc_loss: 0.017778748646378517, tv_loss: 0.01709815301001072\n",
      "iteration 3175, dc_loss: 0.017778770998120308, tv_loss: 0.01709824427962303\n",
      "iteration 3176, dc_loss: 0.0177787933498621, tv_loss: 0.017098048701882362\n",
      "iteration 3177, dc_loss: 0.01777881197631359, tv_loss: 0.017097748816013336\n",
      "iteration 3178, dc_loss: 0.0177787896245718, tv_loss: 0.01709769293665886\n",
      "iteration 3179, dc_loss: 0.01777876541018486, tv_loss: 0.01709774136543274\n",
      "iteration 3180, dc_loss: 0.017778782173991203, tv_loss: 0.017097733914852142\n",
      "iteration 3181, dc_loss: 0.017778776586055756, tv_loss: 0.0170975960791111\n",
      "iteration 3182, dc_loss: 0.0177787933498621, tv_loss: 0.01709738001227379\n",
      "iteration 3183, dc_loss: 0.017778823152184486, tv_loss: 0.017097407951951027\n",
      "iteration 3184, dc_loss: 0.017778770998120308, tv_loss: 0.017097484320402145\n",
      "iteration 3185, dc_loss: 0.01777869462966919, tv_loss: 0.017097515985369682\n",
      "iteration 3186, dc_loss: 0.017778610810637474, tv_loss: 0.01709725148975849\n",
      "iteration 3187, dc_loss: 0.017778543755412102, tv_loss: 0.01709745079278946\n",
      "iteration 3188, dc_loss: 0.017778493463993073, tv_loss: 0.017097540199756622\n",
      "iteration 3189, dc_loss: 0.01777847670018673, tv_loss: 0.01709739863872528\n",
      "iteration 3190, dc_loss: 0.01777852699160576, tv_loss: 0.01709718070924282\n",
      "iteration 3191, dc_loss: 0.017778588458895683, tv_loss: 0.01709703356027603\n",
      "iteration 3192, dc_loss: 0.017778659239411354, tv_loss: 0.01709691435098648\n",
      "iteration 3193, dc_loss: 0.01777866668999195, tv_loss: 0.01709691807627678\n",
      "iteration 3194, dc_loss: 0.017778635025024414, tv_loss: 0.017096949741244316\n",
      "iteration 3195, dc_loss: 0.01777857169508934, tv_loss: 0.0170969869941473\n",
      "iteration 3196, dc_loss: 0.01777847856283188, tv_loss: 0.017096851021051407\n",
      "iteration 3197, dc_loss: 0.01777838170528412, tv_loss: 0.017096860334277153\n",
      "iteration 3198, dc_loss: 0.01777832582592964, tv_loss: 0.017096878960728645\n",
      "iteration 3199, dc_loss: 0.01777826063334942, tv_loss: 0.017096703872084618\n",
      "iteration 3200, dc_loss: 0.017778249457478523, tv_loss: 0.017096776515245438\n",
      "iteration 3201, dc_loss: 0.017778290435671806, tv_loss: 0.017096739262342453\n",
      "iteration 3202, dc_loss: 0.017778314650058746, tv_loss: 0.01709653250873089\n",
      "iteration 3203, dc_loss: 0.017778314650058746, tv_loss: 0.017096417024731636\n",
      "iteration 3204, dc_loss: 0.017778323963284492, tv_loss: 0.017096392810344696\n",
      "iteration 3205, dc_loss: 0.017778383567929268, tv_loss: 0.017096377909183502\n",
      "iteration 3206, dc_loss: 0.017778433859348297, tv_loss: 0.01709626242518425\n",
      "iteration 3207, dc_loss: 0.017778420820832253, tv_loss: 0.01709609106183052\n",
      "iteration 3208, dc_loss: 0.017778368666768074, tv_loss: 0.01709608919918537\n",
      "iteration 3209, dc_loss: 0.017778292298316956, tv_loss: 0.017096104100346565\n",
      "iteration 3210, dc_loss: 0.017778221517801285, tv_loss: 0.01709623634815216\n",
      "iteration 3211, dc_loss: 0.017778180539608, tv_loss: 0.01709611900150776\n",
      "iteration 3212, dc_loss: 0.01777816191315651, tv_loss: 0.017096085473895073\n",
      "iteration 3213, dc_loss: 0.017778174951672554, tv_loss: 0.017096025869250298\n",
      "iteration 3214, dc_loss: 0.017778202891349792, tv_loss: 0.017095880582928658\n",
      "iteration 3215, dc_loss: 0.017778202891349792, tv_loss: 0.01709584891796112\n",
      "iteration 3216, dc_loss: 0.017778191715478897, tv_loss: 0.017095787450671196\n",
      "iteration 3217, dc_loss: 0.017778145149350166, tv_loss: 0.017095748335123062\n",
      "iteration 3218, dc_loss: 0.01777808740735054, tv_loss: 0.017095891758799553\n",
      "iteration 3219, dc_loss: 0.01777806133031845, tv_loss: 0.017095787450671196\n",
      "iteration 3220, dc_loss: 0.0177780669182539, tv_loss: 0.01709570921957493\n",
      "iteration 3221, dc_loss: 0.017778033390641212, tv_loss: 0.017095554620027542\n",
      "iteration 3222, dc_loss: 0.01777801662683487, tv_loss: 0.017095541581511497\n",
      "iteration 3223, dc_loss: 0.01777801476418972, tv_loss: 0.017095619812607765\n",
      "iteration 3224, dc_loss: 0.01777801103889942, tv_loss: 0.01709553226828575\n",
      "iteration 3225, dc_loss: 0.01777801103889942, tv_loss: 0.01709536649286747\n",
      "iteration 3226, dc_loss: 0.017778001725673676, tv_loss: 0.017095424234867096\n",
      "iteration 3227, dc_loss: 0.017777955159544945, tv_loss: 0.017095424234867096\n",
      "iteration 3228, dc_loss: 0.017777925357222557, tv_loss: 0.017095288261771202\n",
      "iteration 3229, dc_loss: 0.017777927219867706, tv_loss: 0.017095157876610756\n",
      "iteration 3230, dc_loss: 0.0177779458463192, tv_loss: 0.017095092684030533\n",
      "iteration 3231, dc_loss: 0.01777792163193226, tv_loss: 0.01709510199725628\n",
      "iteration 3232, dc_loss: 0.017777908593416214, tv_loss: 0.017095085233449936\n",
      "iteration 3233, dc_loss: 0.01777788996696472, tv_loss: 0.017095157876610756\n",
      "iteration 3234, dc_loss: 0.01777787134051323, tv_loss: 0.017095116898417473\n",
      "iteration 3235, dc_loss: 0.017777811735868454, tv_loss: 0.01709519512951374\n",
      "iteration 3236, dc_loss: 0.017777753993868828, tv_loss: 0.017095189541578293\n",
      "iteration 3237, dc_loss: 0.017777733504772186, tv_loss: 0.017095200717449188\n",
      "iteration 3238, dc_loss: 0.01777772046625614, tv_loss: 0.017095109447836876\n",
      "iteration 3239, dc_loss: 0.017777742817997932, tv_loss: 0.017094969749450684\n",
      "iteration 3240, dc_loss: 0.017777778208255768, tv_loss: 0.01709495484828949\n",
      "iteration 3241, dc_loss: 0.017777785658836365, tv_loss: 0.017094796523451805\n",
      "iteration 3242, dc_loss: 0.01777774840593338, tv_loss: 0.017094751819968224\n",
      "iteration 3243, dc_loss: 0.017777705565094948, tv_loss: 0.017094824463129044\n",
      "iteration 3244, dc_loss: 0.017777660861611366, tv_loss: 0.017094871029257774\n",
      "iteration 3245, dc_loss: 0.017777634784579277, tv_loss: 0.01709470897912979\n",
      "iteration 3246, dc_loss: 0.017777640372514725, tv_loss: 0.017094604671001434\n",
      "iteration 3247, dc_loss: 0.017777660861611366, tv_loss: 0.017094695940613747\n",
      "iteration 3248, dc_loss: 0.017777686938643456, tv_loss: 0.017094578593969345\n",
      "iteration 3249, dc_loss: 0.01777769811451435, tv_loss: 0.017094440758228302\n",
      "iteration 3250, dc_loss: 0.01777767948806286, tv_loss: 0.017094293609261513\n",
      "iteration 3251, dc_loss: 0.01777762919664383, tv_loss: 0.017094286158680916\n",
      "iteration 3252, dc_loss: 0.017777565866708755, tv_loss: 0.017094429582357407\n",
      "iteration 3253, dc_loss: 0.01777748204767704, tv_loss: 0.01709432154893875\n",
      "iteration 3254, dc_loss: 0.017777470871806145, tv_loss: 0.01709417812526226\n",
      "iteration 3255, dc_loss: 0.0177774578332901, tv_loss: 0.017094314098358154\n",
      "iteration 3256, dc_loss: 0.017777474597096443, tv_loss: 0.017094161361455917\n",
      "iteration 3257, dc_loss: 0.017777487635612488, tv_loss: 0.017094166949391365\n",
      "iteration 3258, dc_loss: 0.01777750998735428, tv_loss: 0.017093976959586143\n",
      "iteration 3259, dc_loss: 0.01777753047645092, tv_loss: 0.017093665897846222\n",
      "iteration 3260, dc_loss: 0.017777511849999428, tv_loss: 0.017093820497393608\n",
      "iteration 3261, dc_loss: 0.017777470871806145, tv_loss: 0.017093883827328682\n",
      "iteration 3262, dc_loss: 0.017777418717741966, tv_loss: 0.017093785107135773\n",
      "iteration 3263, dc_loss: 0.017777379602193832, tv_loss: 0.017093781381845474\n",
      "iteration 3264, dc_loss: 0.017777375876903534, tv_loss: 0.017093710601329803\n",
      "iteration 3265, dc_loss: 0.017777392640709877, tv_loss: 0.017093664035201073\n",
      "iteration 3266, dc_loss: 0.017777414992451668, tv_loss: 0.017093582078814507\n",
      "iteration 3267, dc_loss: 0.01777738891541958, tv_loss: 0.017093509435653687\n",
      "iteration 3268, dc_loss: 0.017777346074581146, tv_loss: 0.017093636095523834\n",
      "iteration 3269, dc_loss: 0.01777731254696846, tv_loss: 0.01709350012242794\n",
      "iteration 3270, dc_loss: 0.01777726225554943, tv_loss: 0.017093364149332047\n",
      "iteration 3271, dc_loss: 0.01777721382677555, tv_loss: 0.017093433067202568\n",
      "iteration 3272, dc_loss: 0.01777716539800167, tv_loss: 0.01709338091313839\n",
      "iteration 3273, dc_loss: 0.017777103930711746, tv_loss: 0.01709326170384884\n",
      "iteration 3274, dc_loss: 0.017777102068066597, tv_loss: 0.01709325984120369\n",
      "iteration 3275, dc_loss: 0.017777150496840477, tv_loss: 0.01709325611591339\n",
      "iteration 3276, dc_loss: 0.017777221277356148, tv_loss: 0.017093049362301826\n",
      "iteration 3277, dc_loss: 0.017777245491743088, tv_loss: 0.017092766240239143\n",
      "iteration 3278, dc_loss: 0.017777280882000923, tv_loss: 0.017092689871788025\n",
      "iteration 3279, dc_loss: 0.017777280882000923, tv_loss: 0.01709260791540146\n",
      "iteration 3280, dc_loss: 0.01777723617851734, tv_loss: 0.017092591151595116\n",
      "iteration 3281, dc_loss: 0.01777714677155018, tv_loss: 0.017092691734433174\n",
      "iteration 3282, dc_loss: 0.017777040600776672, tv_loss: 0.01709267497062683\n",
      "iteration 3283, dc_loss: 0.017776990309357643, tv_loss: 0.017092790454626083\n",
      "iteration 3284, dc_loss: 0.017776992172002792, tv_loss: 0.017092593014240265\n",
      "iteration 3285, dc_loss: 0.01777702383697033, tv_loss: 0.017092706635594368\n",
      "iteration 3286, dc_loss: 0.017777010798454285, tv_loss: 0.017092682421207428\n",
      "iteration 3287, dc_loss: 0.017777010798454285, tv_loss: 0.017092587426304817\n",
      "iteration 3288, dc_loss: 0.01777702197432518, tv_loss: 0.017092525959014893\n",
      "iteration 3289, dc_loss: 0.017777040600776672, tv_loss: 0.017092471942305565\n",
      "iteration 3290, dc_loss: 0.01777702197432518, tv_loss: 0.01709243282675743\n",
      "iteration 3291, dc_loss: 0.017777012661099434, tv_loss: 0.017092427238821983\n",
      "iteration 3292, dc_loss: 0.017777012661099434, tv_loss: 0.017092308029532433\n",
      "iteration 3293, dc_loss: 0.017777010798454285, tv_loss: 0.017092224210500717\n",
      "iteration 3294, dc_loss: 0.01777699775993824, tv_loss: 0.017092090100049973\n",
      "iteration 3295, dc_loss: 0.0177769735455513, tv_loss: 0.017092151567339897\n",
      "iteration 3296, dc_loss: 0.017776895314455032, tv_loss: 0.017092389985919\n",
      "iteration 3297, dc_loss: 0.01777682825922966, tv_loss: 0.017092503607273102\n",
      "iteration 3298, dc_loss: 0.017776794731616974, tv_loss: 0.01709224097430706\n",
      "iteration 3299, dc_loss: 0.017776770517230034, tv_loss: 0.017092248424887657\n",
      "iteration 3300, dc_loss: 0.01777677796781063, tv_loss: 0.017092125490307808\n",
      "iteration 3301, dc_loss: 0.017776791006326675, tv_loss: 0.017092017456889153\n",
      "iteration 3302, dc_loss: 0.017776813358068466, tv_loss: 0.0170920267701149\n",
      "iteration 3303, dc_loss: 0.01777680777013302, tv_loss: 0.017091993242502213\n",
      "iteration 3304, dc_loss: 0.017776815220713615, tv_loss: 0.017091838642954826\n",
      "iteration 3305, dc_loss: 0.017776798456907272, tv_loss: 0.017091883346438408\n",
      "iteration 3306, dc_loss: 0.017776774242520332, tv_loss: 0.017091821879148483\n",
      "iteration 3307, dc_loss: 0.01777675189077854, tv_loss: 0.017091698944568634\n",
      "iteration 3308, dc_loss: 0.017776714637875557, tv_loss: 0.017091665416955948\n",
      "iteration 3309, dc_loss: 0.017776694148778915, tv_loss: 0.017091814428567886\n",
      "iteration 3310, dc_loss: 0.017776714637875557, tv_loss: 0.01709180511534214\n",
      "iteration 3311, dc_loss: 0.01777670532464981, tv_loss: 0.017091576009988785\n",
      "iteration 3312, dc_loss: 0.01777668669819832, tv_loss: 0.017091453075408936\n",
      "iteration 3313, dc_loss: 0.01777666062116623, tv_loss: 0.017091507092118263\n",
      "iteration 3314, dc_loss: 0.017776640132069588, tv_loss: 0.017091644927859306\n",
      "iteration 3315, dc_loss: 0.017776625230908394, tv_loss: 0.017091400921344757\n",
      "iteration 3316, dc_loss: 0.017776599153876305, tv_loss: 0.017091332003474236\n",
      "iteration 3317, dc_loss: 0.017776530236005783, tv_loss: 0.017091358080506325\n",
      "iteration 3318, dc_loss: 0.017776494845747948, tv_loss: 0.017091277986764908\n",
      "iteration 3319, dc_loss: 0.017776470631361008, tv_loss: 0.017091302201151848\n",
      "iteration 3320, dc_loss: 0.01777646876871586, tv_loss: 0.017091209068894386\n",
      "iteration 3321, dc_loss: 0.01777650974690914, tv_loss: 0.017090950161218643\n",
      "iteration 3322, dc_loss: 0.017776530236005783, tv_loss: 0.01709110103547573\n",
      "iteration 3323, dc_loss: 0.017776532098650932, tv_loss: 0.017090993002057076\n",
      "iteration 3324, dc_loss: 0.017776500433683395, tv_loss: 0.017090901732444763\n",
      "iteration 3325, dc_loss: 0.017776494845747948, tv_loss: 0.01709095761179924\n",
      "iteration 3326, dc_loss: 0.0177764855325222, tv_loss: 0.017090825363993645\n",
      "iteration 3327, dc_loss: 0.0177764855325222, tv_loss: 0.017090676352381706\n",
      "iteration 3328, dc_loss: 0.01777646690607071, tv_loss: 0.017090750858187675\n",
      "iteration 3329, dc_loss: 0.017776403576135635, tv_loss: 0.017090795561671257\n",
      "iteration 3330, dc_loss: 0.017776351422071457, tv_loss: 0.017090605571866035\n",
      "iteration 3331, dc_loss: 0.017776306718587875, tv_loss: 0.01709063909947872\n",
      "iteration 3332, dc_loss: 0.017776308581233025, tv_loss: 0.017090722918510437\n",
      "iteration 3333, dc_loss: 0.017776306718587875, tv_loss: 0.017090652137994766\n",
      "iteration 3334, dc_loss: 0.017776327207684517, tv_loss: 0.01709040254354477\n",
      "iteration 3335, dc_loss: 0.017776358872652054, tv_loss: 0.017090190201997757\n",
      "iteration 3336, dc_loss: 0.017776355147361755, tv_loss: 0.01709035038948059\n",
      "iteration 3337, dc_loss: 0.017776379361748695, tv_loss: 0.01709030754864216\n",
      "iteration 3338, dc_loss: 0.017776351422071457, tv_loss: 0.01709018275141716\n",
      "iteration 3339, dc_loss: 0.017776314169168472, tv_loss: 0.017090212553739548\n",
      "iteration 3340, dc_loss: 0.017776260152459145, tv_loss: 0.01709013618528843\n",
      "iteration 3341, dc_loss: 0.01777622662484646, tv_loss: 0.017090322449803352\n",
      "iteration 3342, dc_loss: 0.017776189371943474, tv_loss: 0.017090149223804474\n",
      "iteration 3343, dc_loss: 0.017776157706975937, tv_loss: 0.017090240493416786\n",
      "iteration 3344, dc_loss: 0.01777614653110504, tv_loss: 0.017090223729610443\n",
      "iteration 3345, dc_loss: 0.01777614839375019, tv_loss: 0.01709006540477276\n",
      "iteration 3346, dc_loss: 0.017776155844330788, tv_loss: 0.017089979723095894\n",
      "iteration 3347, dc_loss: 0.017776144668459892, tv_loss: 0.01709001325070858\n",
      "iteration 3348, dc_loss: 0.017776085063815117, tv_loss: 0.017089921981096268\n",
      "iteration 3349, dc_loss: 0.01777605339884758, tv_loss: 0.01708991825580597\n",
      "iteration 3350, dc_loss: 0.017776018008589745, tv_loss: 0.01708991453051567\n",
      "iteration 3351, dc_loss: 0.017775991931557655, tv_loss: 0.01708984188735485\n",
      "iteration 3352, dc_loss: 0.0177760012447834, tv_loss: 0.01708972454071045\n",
      "iteration 3353, dc_loss: 0.017776066437363625, tv_loss: 0.017089635133743286\n",
      "iteration 3354, dc_loss: 0.017776096239686012, tv_loss: 0.01708955131471157\n",
      "iteration 3355, dc_loss: 0.017776096239686012, tv_loss: 0.017089644446969032\n",
      "iteration 3356, dc_loss: 0.017776088789105415, tv_loss: 0.017089392989873886\n",
      "iteration 3357, dc_loss: 0.017776086926460266, tv_loss: 0.017089294269680977\n",
      "iteration 3358, dc_loss: 0.01777608133852482, tv_loss: 0.017089540138840675\n",
      "iteration 3359, dc_loss: 0.017776034772396088, tv_loss: 0.01708938367664814\n",
      "iteration 3360, dc_loss: 0.017775939777493477, tv_loss: 0.017089394852519035\n",
      "iteration 3361, dc_loss: 0.017775841057300568, tv_loss: 0.017089389264583588\n",
      "iteration 3362, dc_loss: 0.017775753512978554, tv_loss: 0.017089350149035454\n",
      "iteration 3363, dc_loss: 0.01777574233710766, tv_loss: 0.017089562490582466\n",
      "iteration 3364, dc_loss: 0.017775796353816986, tv_loss: 0.01708937995135784\n",
      "iteration 3365, dc_loss: 0.017775852233171463, tv_loss: 0.01708916202187538\n",
      "iteration 3366, dc_loss: 0.017775915563106537, tv_loss: 0.0170891173183918\n",
      "iteration 3367, dc_loss: 0.017775945365428925, tv_loss: 0.017089128494262695\n",
      "iteration 3368, dc_loss: 0.01777593605220318, tv_loss: 0.017089026048779488\n",
      "iteration 3369, dc_loss: 0.017775926738977432, tv_loss: 0.01708906702697277\n",
      "iteration 3370, dc_loss: 0.017775898799300194, tv_loss: 0.01708909124135971\n",
      "iteration 3371, dc_loss: 0.0177758801728487, tv_loss: 0.01708899438381195\n",
      "iteration 3372, dc_loss: 0.017775870859622955, tv_loss: 0.0170888751745224\n",
      "iteration 3373, dc_loss: 0.01777586154639721, tv_loss: 0.017088886350393295\n",
      "iteration 3374, dc_loss: 0.017775822430849075, tv_loss: 0.017088918015360832\n",
      "iteration 3375, dc_loss: 0.01777578331530094, tv_loss: 0.017088858410716057\n",
      "iteration 3376, dc_loss: 0.017775757238268852, tv_loss: 0.01708892360329628\n",
      "iteration 3377, dc_loss: 0.017775721848011017, tv_loss: 0.017088890075683594\n",
      "iteration 3378, dc_loss: 0.017775706946849823, tv_loss: 0.01708880439400673\n",
      "iteration 3379, dc_loss: 0.017775680869817734, tv_loss: 0.01708878017961979\n",
      "iteration 3380, dc_loss: 0.017775682732462883, tv_loss: 0.017088579013943672\n",
      "iteration 3381, dc_loss: 0.017775660380721092, tv_loss: 0.017088530585169792\n",
      "iteration 3382, dc_loss: 0.017775626853108406, tv_loss: 0.01708856038749218\n",
      "iteration 3383, dc_loss: 0.017775654792785645, tv_loss: 0.01708856038749218\n",
      "iteration 3384, dc_loss: 0.01777563989162445, tv_loss: 0.0170883871614933\n",
      "iteration 3385, dc_loss: 0.017775585874915123, tv_loss: 0.017088480293750763\n",
      "iteration 3386, dc_loss: 0.017775535583496094, tv_loss: 0.017088508233428\n",
      "iteration 3387, dc_loss: 0.017775511369109154, tv_loss: 0.01708834432065487\n",
      "iteration 3388, dc_loss: 0.017775526270270348, tv_loss: 0.01708829775452614\n",
      "iteration 3389, dc_loss: 0.017775556072592735, tv_loss: 0.01708816923201084\n",
      "iteration 3390, dc_loss: 0.01777559332549572, tv_loss: 0.01708819530904293\n",
      "iteration 3391, dc_loss: 0.01777559518814087, tv_loss: 0.017088068649172783\n",
      "iteration 3392, dc_loss: 0.017775585874915123, tv_loss: 0.017087971791625023\n",
      "iteration 3393, dc_loss: 0.017775580286979675, tv_loss: 0.01708795316517353\n",
      "iteration 3394, dc_loss: 0.01777554117143154, tv_loss: 0.017087914049625397\n",
      "iteration 3395, dc_loss: 0.017775509506464005, tv_loss: 0.017087919637560844\n",
      "iteration 3396, dc_loss: 0.017775485292077065, tv_loss: 0.017087914049625397\n",
      "iteration 3397, dc_loss: 0.017775440588593483, tv_loss: 0.0170879028737545\n",
      "iteration 3398, dc_loss: 0.0177754033356905, tv_loss: 0.0170879103243351\n",
      "iteration 3399, dc_loss: 0.017775394022464752, tv_loss: 0.017087943851947784\n",
      "iteration 3400, dc_loss: 0.017775438725948334, tv_loss: 0.017087817192077637\n",
      "iteration 3401, dc_loss: 0.017775464802980423, tv_loss: 0.017087750136852264\n",
      "iteration 3402, dc_loss: 0.017775485292077065, tv_loss: 0.017087509855628014\n",
      "iteration 3403, dc_loss: 0.017775481566786766, tv_loss: 0.01708759367465973\n",
      "iteration 3404, dc_loss: 0.017775455489754677, tv_loss: 0.017087586224079132\n",
      "iteration 3405, dc_loss: 0.017775390297174454, tv_loss: 0.01708754152059555\n",
      "iteration 3406, dc_loss: 0.017775310203433037, tv_loss: 0.0170875396579504\n",
      "iteration 3407, dc_loss: 0.01777525804936886, tv_loss: 0.017087651416659355\n",
      "iteration 3408, dc_loss: 0.017775237560272217, tv_loss: 0.01708764024078846\n",
      "iteration 3409, dc_loss: 0.017775265499949455, tv_loss: 0.017087511718273163\n",
      "iteration 3410, dc_loss: 0.01777530461549759, tv_loss: 0.01708751544356346\n",
      "iteration 3411, dc_loss: 0.01777532696723938, tv_loss: 0.017087364569306374\n",
      "iteration 3412, dc_loss: 0.017775338143110275, tv_loss: 0.017087101936340332\n",
      "iteration 3413, dc_loss: 0.017775369808077812, tv_loss: 0.017087245360016823\n",
      "iteration 3414, dc_loss: 0.01777537725865841, tv_loss: 0.01708703115582466\n",
      "iteration 3415, dc_loss: 0.01777535118162632, tv_loss: 0.017086921259760857\n",
      "iteration 3416, dc_loss: 0.01777532882988453, tv_loss: 0.017087001353502274\n",
      "iteration 3417, dc_loss: 0.0177752785384655, tv_loss: 0.017087172716856003\n",
      "iteration 3418, dc_loss: 0.017775190994143486, tv_loss: 0.017087064683437347\n",
      "iteration 3419, dc_loss: 0.017775116488337517, tv_loss: 0.017086971551179886\n",
      "iteration 3420, dc_loss: 0.017775051295757294, tv_loss: 0.0170871764421463\n",
      "iteration 3421, dc_loss: 0.017774997279047966, tv_loss: 0.01708712987601757\n",
      "iteration 3422, dc_loss: 0.017774976789951324, tv_loss: 0.01708703674376011\n",
      "iteration 3423, dc_loss: 0.01777498424053192, tv_loss: 0.017087047919631004\n",
      "iteration 3424, dc_loss: 0.017775021493434906, tv_loss: 0.017086969688534737\n",
      "iteration 3425, dc_loss: 0.01777508109807968, tv_loss: 0.017086943611502647\n",
      "iteration 3426, dc_loss: 0.01777513138949871, tv_loss: 0.017086736857891083\n",
      "iteration 3427, dc_loss: 0.017775138840079308, tv_loss: 0.01708662323653698\n",
      "iteration 3428, dc_loss: 0.017775142565369606, tv_loss: 0.01708659715950489\n",
      "iteration 3429, dc_loss: 0.017775172367691994, tv_loss: 0.01708662323653698\n",
      "iteration 3430, dc_loss: 0.01777513511478901, tv_loss: 0.017086539417505264\n",
      "iteration 3431, dc_loss: 0.017775079235434532, tv_loss: 0.017086578533053398\n",
      "iteration 3432, dc_loss: 0.017775041982531548, tv_loss: 0.017086710780858994\n",
      "iteration 3433, dc_loss: 0.017774999141693115, tv_loss: 0.017086641862988472\n",
      "iteration 3434, dc_loss: 0.017774956300854683, tv_loss: 0.01708650216460228\n",
      "iteration 3435, dc_loss: 0.01777491718530655, tv_loss: 0.017086682841181755\n",
      "iteration 3436, dc_loss: 0.017774898558855057, tv_loss: 0.01708667166531086\n",
      "iteration 3437, dc_loss: 0.01777487061917782, tv_loss: 0.017086558043956757\n",
      "iteration 3438, dc_loss: 0.01777487061917782, tv_loss: 0.017086509615182877\n",
      "iteration 3439, dc_loss: 0.01777486503124237, tv_loss: 0.01708664558827877\n",
      "iteration 3440, dc_loss: 0.01777484640479088, tv_loss: 0.017086589708924294\n",
      "iteration 3441, dc_loss: 0.017774827778339386, tv_loss: 0.017086634412407875\n",
      "iteration 3442, dc_loss: 0.017774811014533043, tv_loss: 0.017086509615182877\n",
      "iteration 3443, dc_loss: 0.01777481846511364, tv_loss: 0.017086438834667206\n",
      "iteration 3444, dc_loss: 0.017774825915694237, tv_loss: 0.017086487263441086\n",
      "iteration 3445, dc_loss: 0.017774807289242744, tv_loss: 0.017086533829569817\n",
      "iteration 3446, dc_loss: 0.01777477003633976, tv_loss: 0.017086433246731758\n",
      "iteration 3447, dc_loss: 0.017774729058146477, tv_loss: 0.01708628423511982\n",
      "iteration 3448, dc_loss: 0.017774712294340134, tv_loss: 0.017086267471313477\n",
      "iteration 3449, dc_loss: 0.017774689942598343, tv_loss: 0.017086300998926163\n",
      "iteration 3450, dc_loss: 0.017774689942598343, tv_loss: 0.01708635687828064\n",
      "iteration 3451, dc_loss: 0.0177746769040823, tv_loss: 0.017086172476410866\n",
      "iteration 3452, dc_loss: 0.017774689942598343, tv_loss: 0.01708599366247654\n",
      "iteration 3453, dc_loss: 0.017774729058146477, tv_loss: 0.017085937783122063\n",
      "iteration 3454, dc_loss: 0.01777474395930767, tv_loss: 0.017085973173379898\n",
      "iteration 3455, dc_loss: 0.01777472347021103, tv_loss: 0.017085907980799675\n",
      "iteration 3456, dc_loss: 0.017774680629372597, tv_loss: 0.017085907980799675\n",
      "iteration 3457, dc_loss: 0.017774635925889015, tv_loss: 0.017085961997509003\n",
      "iteration 3458, dc_loss: 0.01777462661266327, tv_loss: 0.017085913568735123\n",
      "iteration 3459, dc_loss: 0.01777462288737297, tv_loss: 0.01708589494228363\n",
      "iteration 3460, dc_loss: 0.017774619162082672, tv_loss: 0.01708574965596199\n",
      "iteration 3461, dc_loss: 0.01777464896440506, tv_loss: 0.017085622996091843\n",
      "iteration 3462, dc_loss: 0.017774665728211403, tv_loss: 0.017085492610931396\n",
      "iteration 3463, dc_loss: 0.017774678766727448, tv_loss: 0.017085520550608635\n",
      "iteration 3464, dc_loss: 0.017774613574147224, tv_loss: 0.017085613682866096\n",
      "iteration 3465, dc_loss: 0.01777452975511551, tv_loss: 0.017085498198866844\n",
      "iteration 3466, dc_loss: 0.017774473875761032, tv_loss: 0.017085550352931023\n",
      "iteration 3467, dc_loss: 0.017774442210793495, tv_loss: 0.01708552986383438\n",
      "iteration 3468, dc_loss: 0.01777445524930954, tv_loss: 0.017085449770092964\n",
      "iteration 3469, dc_loss: 0.01777450554072857, tv_loss: 0.017085272818803787\n",
      "iteration 3470, dc_loss: 0.017774535343050957, tv_loss: 0.017085188999772072\n",
      "iteration 3471, dc_loss: 0.017774589359760284, tv_loss: 0.017085086554288864\n",
      "iteration 3472, dc_loss: 0.017774630337953568, tv_loss: 0.017085064202547073\n",
      "iteration 3473, dc_loss: 0.017774617299437523, tv_loss: 0.017085010185837746\n",
      "iteration 3474, dc_loss: 0.017774581909179688, tv_loss: 0.01708490028977394\n",
      "iteration 3475, dc_loss: 0.017774537205696106, tv_loss: 0.01708499900996685\n",
      "iteration 3476, dc_loss: 0.017774486914277077, tv_loss: 0.017085108906030655\n",
      "iteration 3477, dc_loss: 0.017774440348148346, tv_loss: 0.017085019499063492\n",
      "iteration 3478, dc_loss: 0.017774391919374466, tv_loss: 0.017084911465644836\n",
      "iteration 3479, dc_loss: 0.017774302512407303, tv_loss: 0.017084917053580284\n",
      "iteration 3480, dc_loss: 0.01777423359453678, tv_loss: 0.01708502694964409\n",
      "iteration 3481, dc_loss: 0.017774205654859543, tv_loss: 0.017084946855902672\n",
      "iteration 3482, dc_loss: 0.017774224281311035, tv_loss: 0.017084892839193344\n",
      "iteration 3483, dc_loss: 0.017774268984794617, tv_loss: 0.017084747552871704\n",
      "iteration 3484, dc_loss: 0.01777431182563305, tv_loss: 0.01708463579416275\n",
      "iteration 3485, dc_loss: 0.01777435466647148, tv_loss: 0.017084432765841484\n",
      "iteration 3486, dc_loss: 0.01777438446879387, tv_loss: 0.017084622755646706\n",
      "iteration 3487, dc_loss: 0.01777435652911663, tv_loss: 0.017084667459130287\n",
      "iteration 3488, dc_loss: 0.01777428761124611, tv_loss: 0.017084579914808273\n",
      "iteration 3489, dc_loss: 0.017774241045117378, tv_loss: 0.017084665596485138\n",
      "iteration 3490, dc_loss: 0.017774218693375587, tv_loss: 0.01708468422293663\n",
      "iteration 3491, dc_loss: 0.017774200066924095, tv_loss: 0.01708458922803402\n",
      "iteration 3492, dc_loss: 0.017774170264601707, tv_loss: 0.01708465814590454\n",
      "iteration 3493, dc_loss: 0.01777416095137596, tv_loss: 0.017084652557969093\n",
      "iteration 3494, dc_loss: 0.017774181440472603, tv_loss: 0.017084462568163872\n",
      "iteration 3495, dc_loss: 0.017774179577827454, tv_loss: 0.01708434522151947\n",
      "iteration 3496, dc_loss: 0.017774175852537155, tv_loss: 0.017084430903196335\n",
      "iteration 3497, dc_loss: 0.017774183303117752, tv_loss: 0.017084432765841484\n",
      "iteration 3498, dc_loss: 0.0177741888910532, tv_loss: 0.01708424836397171\n",
      "iteration 3499, dc_loss: 0.017774157226085663, tv_loss: 0.01708408258855343\n",
      "iteration 3500, dc_loss: 0.01777411252260208, tv_loss: 0.017084142193198204\n",
      "iteration 3501, dc_loss: 0.017774047330021858, tv_loss: 0.017084253951907158\n",
      "iteration 3502, dc_loss: 0.01777399331331253, tv_loss: 0.017084214836359024\n",
      "iteration 3503, dc_loss: 0.01777396723628044, tv_loss: 0.017084073275327682\n",
      "iteration 3504, dc_loss: 0.017774006351828575, tv_loss: 0.017084144055843353\n",
      "iteration 3505, dc_loss: 0.017774084582924843, tv_loss: 0.017084045335650444\n",
      "iteration 3506, dc_loss: 0.017774127423763275, tv_loss: 0.017083927989006042\n",
      "iteration 3507, dc_loss: 0.017774153500795364, tv_loss: 0.017083728685975075\n",
      "iteration 3508, dc_loss: 0.017774127423763275, tv_loss: 0.01708388142287731\n",
      "iteration 3509, dc_loss: 0.017774075269699097, tv_loss: 0.017084043473005295\n",
      "iteration 3510, dc_loss: 0.017774052917957306, tv_loss: 0.017083903774619102\n",
      "iteration 3511, dc_loss: 0.017774052917957306, tv_loss: 0.017083745449781418\n",
      "iteration 3512, dc_loss: 0.017774047330021858, tv_loss: 0.017083704471588135\n",
      "iteration 3513, dc_loss: 0.017774023115634918, tv_loss: 0.01708371751010418\n",
      "iteration 3514, dc_loss: 0.017774028703570366, tv_loss: 0.017083793878555298\n",
      "iteration 3515, dc_loss: 0.017774026840925217, tv_loss: 0.017083732411265373\n",
      "iteration 3516, dc_loss: 0.01777399331331253, tv_loss: 0.017083650454878807\n",
      "iteration 3517, dc_loss: 0.01777394488453865, tv_loss: 0.017083628103137016\n",
      "iteration 3518, dc_loss: 0.01777389831840992, tv_loss: 0.017083561047911644\n",
      "iteration 3519, dc_loss: 0.017773892730474472, tv_loss: 0.01708359271287918\n",
      "iteration 3520, dc_loss: 0.01777391880750656, tv_loss: 0.01708349958062172\n",
      "iteration 3521, dc_loss: 0.017773957923054695, tv_loss: 0.017083384096622467\n",
      "iteration 3522, dc_loss: 0.017773959785699844, tv_loss: 0.017083367332816124\n",
      "iteration 3523, dc_loss: 0.017773929983377457, tv_loss: 0.017083335667848587\n",
      "iteration 3524, dc_loss: 0.01777389831840992, tv_loss: 0.01708330027759075\n",
      "iteration 3525, dc_loss: 0.017773881554603577, tv_loss: 0.017083456739783287\n",
      "iteration 3526, dc_loss: 0.017773840576410294, tv_loss: 0.017083441838622093\n",
      "iteration 3527, dc_loss: 0.017773816362023354, tv_loss: 0.017083242535591125\n",
      "iteration 3528, dc_loss: 0.01777380332350731, tv_loss: 0.017083100974559784\n",
      "iteration 3529, dc_loss: 0.01777377538383007, tv_loss: 0.01708322949707508\n",
      "iteration 3530, dc_loss: 0.017773769795894623, tv_loss: 0.01708313636481762\n",
      "iteration 3531, dc_loss: 0.017773760482668877, tv_loss: 0.017083149403333664\n",
      "iteration 3532, dc_loss: 0.017773712053894997, tv_loss: 0.017083140090107918\n",
      "iteration 3533, dc_loss: 0.01777367852628231, tv_loss: 0.01708296127617359\n",
      "iteration 3534, dc_loss: 0.017773689702153206, tv_loss: 0.01708300970494747\n",
      "iteration 3535, dc_loss: 0.017773721367120743, tv_loss: 0.017082957550883293\n",
      "iteration 3536, dc_loss: 0.01777377910912037, tv_loss: 0.017082760110497475\n",
      "iteration 3537, dc_loss: 0.017773866653442383, tv_loss: 0.017082717269659042\n",
      "iteration 3538, dc_loss: 0.01777394860982895, tv_loss: 0.01708248071372509\n",
      "iteration 3539, dc_loss: 0.017773976549506187, tv_loss: 0.0170823372900486\n",
      "iteration 3540, dc_loss: 0.01777394488453865, tv_loss: 0.017082447186112404\n",
      "iteration 3541, dc_loss: 0.01777384802699089, tv_loss: 0.017082447186112404\n",
      "iteration 3542, dc_loss: 0.017773713916540146, tv_loss: 0.017082579433918\n",
      "iteration 3543, dc_loss: 0.017773596569895744, tv_loss: 0.01708262227475643\n",
      "iteration 3544, dc_loss: 0.017773548141121864, tv_loss: 0.017082644626498222\n",
      "iteration 3545, dc_loss: 0.017773505300283432, tv_loss: 0.01708260178565979\n",
      "iteration 3546, dc_loss: 0.017773522064089775, tv_loss: 0.01708269491791725\n",
      "iteration 3547, dc_loss: 0.0177735835313797, tv_loss: 0.01708252541720867\n",
      "iteration 3548, dc_loss: 0.017773648723959923, tv_loss: 0.017082365229725838\n",
      "iteration 3549, dc_loss: 0.017773691564798355, tv_loss: 0.017082298174500465\n",
      "iteration 3550, dc_loss: 0.01777365617454052, tv_loss: 0.017082400619983673\n",
      "iteration 3551, dc_loss: 0.017773611471056938, tv_loss: 0.01708240434527397\n",
      "iteration 3552, dc_loss: 0.0177735835313797, tv_loss: 0.017082395032048225\n",
      "iteration 3553, dc_loss: 0.017773553729057312, tv_loss: 0.01708238758146763\n",
      "iteration 3554, dc_loss: 0.017773527652025223, tv_loss: 0.01708240993320942\n",
      "iteration 3555, dc_loss: 0.017773471772670746, tv_loss: 0.017082421109080315\n",
      "iteration 3556, dc_loss: 0.017773423343896866, tv_loss: 0.01708240434527397\n",
      "iteration 3557, dc_loss: 0.01777343451976776, tv_loss: 0.017082426697015762\n",
      "iteration 3558, dc_loss: 0.017773456871509552, tv_loss: 0.01708218827843666\n",
      "iteration 3559, dc_loss: 0.017773455008864403, tv_loss: 0.017082182690501213\n",
      "iteration 3560, dc_loss: 0.017773449420928955, tv_loss: 0.017082151025533676\n",
      "iteration 3561, dc_loss: 0.017773466184735298, tv_loss: 0.01708216778934002\n",
      "iteration 3562, dc_loss: 0.017773481085896492, tv_loss: 0.017082098871469498\n",
      "iteration 3563, dc_loss: 0.017773466184735298, tv_loss: 0.01708213798701763\n",
      "iteration 3564, dc_loss: 0.01777346059679985, tv_loss: 0.01708204671740532\n",
      "iteration 3565, dc_loss: 0.017773479223251343, tv_loss: 0.01708184741437435\n",
      "iteration 3566, dc_loss: 0.01777350902557373, tv_loss: 0.017081696540117264\n",
      "iteration 3567, dc_loss: 0.017773525789380074, tv_loss: 0.017081813886761665\n",
      "iteration 3568, dc_loss: 0.017773520201444626, tv_loss: 0.017081880941987038\n",
      "iteration 3569, dc_loss: 0.01777348294854164, tv_loss: 0.017081672325730324\n",
      "iteration 3570, dc_loss: 0.017773376777768135, tv_loss: 0.017081724479794502\n",
      "iteration 3571, dc_loss: 0.01777328923344612, tv_loss: 0.017081791535019875\n",
      "iteration 3572, dc_loss: 0.017773231491446495, tv_loss: 0.01708185113966465\n",
      "iteration 3573, dc_loss: 0.017773188650608063, tv_loss: 0.01708185113966465\n",
      "iteration 3574, dc_loss: 0.01777319610118866, tv_loss: 0.017081810161471367\n",
      "iteration 3575, dc_loss: 0.017773209139704704, tv_loss: 0.01708165556192398\n",
      "iteration 3576, dc_loss: 0.0177732203155756, tv_loss: 0.017081672325730324\n",
      "iteration 3577, dc_loss: 0.01777322217822075, tv_loss: 0.017081715166568756\n",
      "iteration 3578, dc_loss: 0.017773203551769257, tv_loss: 0.017081711441278458\n",
      "iteration 3579, dc_loss: 0.017773153260350227, tv_loss: 0.017081554979085922\n",
      "iteration 3580, dc_loss: 0.01777317002415657, tv_loss: 0.01708139106631279\n",
      "iteration 3581, dc_loss: 0.01777319796383381, tv_loss: 0.017081527039408684\n",
      "iteration 3582, dc_loss: 0.017773190513253212, tv_loss: 0.01708153262734413\n",
      "iteration 3583, dc_loss: 0.017773127183318138, tv_loss: 0.017081422731280327\n",
      "iteration 3584, dc_loss: 0.017773082479834557, tv_loss: 0.017081469297409058\n",
      "iteration 3585, dc_loss: 0.01777307689189911, tv_loss: 0.01708146370947361\n",
      "iteration 3586, dc_loss: 0.017773108556866646, tv_loss: 0.01708129420876503\n",
      "iteration 3587, dc_loss: 0.01777314953505993, tv_loss: 0.017081327736377716\n",
      "iteration 3588, dc_loss: 0.017773134633898735, tv_loss: 0.01708134077489376\n",
      "iteration 3589, dc_loss: 0.017773088067770004, tv_loss: 0.017081251367926598\n",
      "iteration 3590, dc_loss: 0.017773035913705826, tv_loss: 0.017081327736377716\n",
      "iteration 3591, dc_loss: 0.01777302846312523, tv_loss: 0.017081238329410553\n",
      "iteration 3592, dc_loss: 0.01777307689189911, tv_loss: 0.01708122529089451\n",
      "iteration 3593, dc_loss: 0.017773117870092392, tv_loss: 0.017081109806895256\n",
      "iteration 3594, dc_loss: 0.017773158848285675, tv_loss: 0.01708090677857399\n",
      "iteration 3595, dc_loss: 0.017773164436221123, tv_loss: 0.01708090677857399\n",
      "iteration 3596, dc_loss: 0.017773188650608063, tv_loss: 0.017080899327993393\n",
      "iteration 3597, dc_loss: 0.017773184925317764, tv_loss: 0.017080729827284813\n",
      "iteration 3598, dc_loss: 0.017773129045963287, tv_loss: 0.017080873250961304\n",
      "iteration 3599, dc_loss: 0.017773063853383064, tv_loss: 0.017080901190638542\n",
      "iteration 3600, dc_loss: 0.017772985622286797, tv_loss: 0.0170808844268322\n",
      "iteration 3601, dc_loss: 0.01777292974293232, tv_loss: 0.017080968245863914\n",
      "iteration 3602, dc_loss: 0.01777292601764202, tv_loss: 0.017080944031476974\n",
      "iteration 3603, dc_loss: 0.017772959545254707, tv_loss: 0.017080677673220634\n",
      "iteration 3604, dc_loss: 0.017772993072867393, tv_loss: 0.017080651596188545\n",
      "iteration 3605, dc_loss: 0.017773009836673737, tv_loss: 0.017080483958125114\n",
      "iteration 3606, dc_loss: 0.01777300052344799, tv_loss: 0.017080528661608696\n",
      "iteration 3607, dc_loss: 0.017773017287254333, tv_loss: 0.01708068884909153\n",
      "iteration 3608, dc_loss: 0.017773006111383438, tv_loss: 0.017080379649996758\n",
      "iteration 3609, dc_loss: 0.017772981896996498, tv_loss: 0.01708044297993183\n",
      "iteration 3610, dc_loss: 0.017772935330867767, tv_loss: 0.017080601304769516\n",
      "iteration 3611, dc_loss: 0.017772888764739037, tv_loss: 0.01708058826625347\n",
      "iteration 3612, dc_loss: 0.017772842198610306, tv_loss: 0.017080599442124367\n",
      "iteration 3613, dc_loss: 0.017772816121578217, tv_loss: 0.017080489546060562\n",
      "iteration 3614, dc_loss: 0.01777283102273941, tv_loss: 0.017080264165997505\n",
      "iteration 3615, dc_loss: 0.01777283102273941, tv_loss: 0.017080485820770264\n",
      "iteration 3616, dc_loss: 0.01777282916009426, tv_loss: 0.017080502584576607\n",
      "iteration 3617, dc_loss: 0.017772849649190903, tv_loss: 0.017080264165997505\n",
      "iteration 3618, dc_loss: 0.017772866412997246, tv_loss: 0.017080260440707207\n",
      "iteration 3619, dc_loss: 0.017772840335965157, tv_loss: 0.017080368474125862\n",
      "iteration 3620, dc_loss: 0.017772763967514038, tv_loss: 0.017080483958125114\n",
      "iteration 3621, dc_loss: 0.017772722989320755, tv_loss: 0.017080314457416534\n",
      "iteration 3622, dc_loss: 0.017772715538740158, tv_loss: 0.017080364748835564\n",
      "iteration 3623, dc_loss: 0.017772698774933815, tv_loss: 0.017080441117286682\n",
      "iteration 3624, dc_loss: 0.017772680148482323, tv_loss: 0.01708037406206131\n",
      "iteration 3625, dc_loss: 0.017772657796740532, tv_loss: 0.017080310732126236\n",
      "iteration 3626, dc_loss: 0.017772644758224487, tv_loss: 0.01708034798502922\n",
      "iteration 3627, dc_loss: 0.017772633582353592, tv_loss: 0.01708039455115795\n",
      "iteration 3628, dc_loss: 0.017772600054740906, tv_loss: 0.017080260440707207\n",
      "iteration 3629, dc_loss: 0.017772601917386055, tv_loss: 0.01708028092980385\n",
      "iteration 3630, dc_loss: 0.01777263917028904, tv_loss: 0.017080038785934448\n",
      "iteration 3631, dc_loss: 0.017772680148482323, tv_loss: 0.017080003395676613\n",
      "iteration 3632, dc_loss: 0.01777271181344986, tv_loss: 0.017079973593354225\n",
      "iteration 3633, dc_loss: 0.01777271367609501, tv_loss: 0.017079779878258705\n",
      "iteration 3634, dc_loss: 0.017772680148482323, tv_loss: 0.017079893499612808\n",
      "iteration 3635, dc_loss: 0.017772691324353218, tv_loss: 0.017079897224903107\n",
      "iteration 3636, dc_loss: 0.017772698774933815, tv_loss: 0.017079753801226616\n",
      "iteration 3637, dc_loss: 0.017772700637578964, tv_loss: 0.017079614102840424\n",
      "iteration 3638, dc_loss: 0.017772670835256577, tv_loss: 0.01707959547638893\n",
      "iteration 3639, dc_loss: 0.017772627994418144, tv_loss: 0.017079642042517662\n",
      "iteration 3640, dc_loss: 0.017772557213902473, tv_loss: 0.017079884186387062\n",
      "iteration 3641, dc_loss: 0.017772477120161057, tv_loss: 0.017079848796129227\n",
      "iteration 3642, dc_loss: 0.017772460356354713, tv_loss: 0.01707962155342102\n",
      "iteration 3643, dc_loss: 0.017772454768419266, tv_loss: 0.017079699784517288\n",
      "iteration 3644, dc_loss: 0.01777244359254837, tv_loss: 0.017079805955290794\n",
      "iteration 3645, dc_loss: 0.01777244545519352, tv_loss: 0.017079809680581093\n",
      "iteration 3646, dc_loss: 0.01777244359254837, tv_loss: 0.01707974635064602\n",
      "iteration 3647, dc_loss: 0.017772462218999863, tv_loss: 0.01707959547638893\n",
      "iteration 3648, dc_loss: 0.017772497609257698, tv_loss: 0.017079506069421768\n",
      "iteration 3649, dc_loss: 0.01777254045009613, tv_loss: 0.017079565674066544\n",
      "iteration 3650, dc_loss: 0.017772532999515533, tv_loss: 0.017079539597034454\n",
      "iteration 3651, dc_loss: 0.017772503197193146, tv_loss: 0.017079437151551247\n",
      "iteration 3652, dc_loss: 0.017772473394870758, tv_loss: 0.017079515382647514\n",
      "iteration 3653, dc_loss: 0.017772432416677475, tv_loss: 0.017079370096325874\n",
      "iteration 3654, dc_loss: 0.01777242310345173, tv_loss: 0.017079301178455353\n",
      "iteration 3655, dc_loss: 0.017772428691387177, tv_loss: 0.017079349607229233\n",
      "iteration 3656, dc_loss: 0.017772434279322624, tv_loss: 0.017079396173357964\n",
      "iteration 3657, dc_loss: 0.017772413790225983, tv_loss: 0.017079215496778488\n",
      "iteration 3658, dc_loss: 0.01777239888906479, tv_loss: 0.017079196870326996\n",
      "iteration 3659, dc_loss: 0.017772367224097252, tv_loss: 0.017079191282391548\n",
      "iteration 3660, dc_loss: 0.017772352322936058, tv_loss: 0.017079178243875504\n",
      "iteration 3661, dc_loss: 0.017772335559129715, tv_loss: 0.017079317942261696\n",
      "iteration 3662, dc_loss: 0.017772315070033073, tv_loss: 0.0170791894197464\n",
      "iteration 3663, dc_loss: 0.017772335559129715, tv_loss: 0.017079172655940056\n",
      "iteration 3664, dc_loss: 0.017772367224097252, tv_loss: 0.017079265788197517\n",
      "iteration 3665, dc_loss: 0.017772411927580833, tv_loss: 0.0170790683478117\n",
      "iteration 3666, dc_loss: 0.017772434279322624, tv_loss: 0.017078891396522522\n",
      "iteration 3667, dc_loss: 0.017772434279322624, tv_loss: 0.017078913748264313\n",
      "iteration 3668, dc_loss: 0.017772367224097252, tv_loss: 0.017079012468457222\n",
      "iteration 3669, dc_loss: 0.017772266641259193, tv_loss: 0.017079006880521774\n",
      "iteration 3670, dc_loss: 0.017772220075130463, tv_loss: 0.01707896590232849\n",
      "iteration 3671, dc_loss: 0.017772190272808075, tv_loss: 0.01707889512181282\n",
      "iteration 3672, dc_loss: 0.017772193998098373, tv_loss: 0.01707889884710312\n",
      "iteration 3673, dc_loss: 0.017772214487195015, tv_loss: 0.017078930512070656\n",
      "iteration 3674, dc_loss: 0.017772214487195015, tv_loss: 0.017078755423426628\n",
      "iteration 3675, dc_loss: 0.017772182822227478, tv_loss: 0.017078744247555733\n",
      "iteration 3676, dc_loss: 0.017772164195775986, tv_loss: 0.017078878358006477\n",
      "iteration 3677, dc_loss: 0.017772164195775986, tv_loss: 0.01707880198955536\n",
      "iteration 3678, dc_loss: 0.017772162333130836, tv_loss: 0.017078684642910957\n",
      "iteration 3679, dc_loss: 0.017772173509001732, tv_loss: 0.017078706994652748\n",
      "iteration 3680, dc_loss: 0.017772171646356583, tv_loss: 0.017078625038266182\n",
      "iteration 3681, dc_loss: 0.01777218095958233, tv_loss: 0.01707853563129902\n",
      "iteration 3682, dc_loss: 0.017772197723388672, tv_loss: 0.01707853563129902\n",
      "iteration 3683, dc_loss: 0.017772221937775612, tv_loss: 0.017078431323170662\n",
      "iteration 3684, dc_loss: 0.01777220144867897, tv_loss: 0.017078356817364693\n",
      "iteration 3685, dc_loss: 0.017772169783711433, tv_loss: 0.017078548669815063\n",
      "iteration 3686, dc_loss: 0.017772089689970016, tv_loss: 0.017078399658203125\n",
      "iteration 3687, dc_loss: 0.017772039398550987, tv_loss: 0.017078382894396782\n",
      "iteration 3688, dc_loss: 0.01777203194797039, tv_loss: 0.0170785840600729\n",
      "iteration 3689, dc_loss: 0.017772015184164047, tv_loss: 0.017078645527362823\n",
      "iteration 3690, dc_loss: 0.017772017046809196, tv_loss: 0.01707855612039566\n",
      "iteration 3691, dc_loss: 0.017772015184164047, tv_loss: 0.017078500241041183\n",
      "iteration 3692, dc_loss: 0.017772015184164047, tv_loss: 0.017078466713428497\n",
      "iteration 3693, dc_loss: 0.01777200773358345, tv_loss: 0.017078347504138947\n",
      "iteration 3694, dc_loss: 0.017772024497389793, tv_loss: 0.017078295350074768\n",
      "iteration 3695, dc_loss: 0.017772041261196136, tv_loss: 0.017078202217817307\n",
      "iteration 3696, dc_loss: 0.017772044986486435, tv_loss: 0.017078254371881485\n",
      "iteration 3697, dc_loss: 0.01777198724448681, tv_loss: 0.017078204080462456\n",
      "iteration 3698, dc_loss: 0.017771953716874123, tv_loss: 0.017078161239624023\n",
      "iteration 3699, dc_loss: 0.017771925777196884, tv_loss: 0.017078261822462082\n",
      "iteration 3700, dc_loss: 0.017771996557712555, tv_loss: 0.017078211531043053\n",
      "iteration 3701, dc_loss: 0.01777205988764763, tv_loss: 0.01707797311246395\n",
      "iteration 3702, dc_loss: 0.017772074788808823, tv_loss: 0.017077811062335968\n",
      "iteration 3703, dc_loss: 0.01777205616235733, tv_loss: 0.017077814787626266\n",
      "iteration 3704, dc_loss: 0.01777205429971218, tv_loss: 0.017077725380659103\n",
      "iteration 3705, dc_loss: 0.0177720095962286, tv_loss: 0.017078006640076637\n",
      "iteration 3706, dc_loss: 0.01777195930480957, tv_loss: 0.017077742144465446\n",
      "iteration 3707, dc_loss: 0.01777195930480957, tv_loss: 0.017077747732400894\n",
      "iteration 3708, dc_loss: 0.017771920189261436, tv_loss: 0.01707807555794716\n",
      "iteration 3709, dc_loss: 0.01777189038693905, tv_loss: 0.017077933996915817\n",
      "iteration 3710, dc_loss: 0.017771895974874496, tv_loss: 0.01707766205072403\n",
      "iteration 3711, dc_loss: 0.017771895974874496, tv_loss: 0.017077822238206863\n",
      "iteration 3712, dc_loss: 0.017771922051906586, tv_loss: 0.017077896744012833\n",
      "iteration 3713, dc_loss: 0.017771897837519646, tv_loss: 0.01707763597369194\n",
      "iteration 3714, dc_loss: 0.01777184009552002, tv_loss: 0.01707778312265873\n",
      "iteration 3715, dc_loss: 0.017771806567907333, tv_loss: 0.017077865079045296\n",
      "iteration 3716, dc_loss: 0.01777178980410099, tv_loss: 0.017077917233109474\n",
      "iteration 3717, dc_loss: 0.017771802842617035, tv_loss: 0.017077725380659103\n",
      "iteration 3718, dc_loss: 0.01777181774377823, tv_loss: 0.017077578231692314\n",
      "iteration 3719, dc_loss: 0.01777181774377823, tv_loss: 0.017077559605240822\n",
      "iteration 3720, dc_loss: 0.017771810293197632, tv_loss: 0.01707763969898224\n",
      "iteration 3721, dc_loss: 0.017771804705262184, tv_loss: 0.01707763411104679\n",
      "iteration 3722, dc_loss: 0.017771776765584946, tv_loss: 0.01707744598388672\n",
      "iteration 3723, dc_loss: 0.017771771177649498, tv_loss: 0.017077699303627014\n",
      "iteration 3724, dc_loss: 0.0177717674523592, tv_loss: 0.01707780361175537\n",
      "iteration 3725, dc_loss: 0.017771782353520393, tv_loss: 0.017077576369047165\n",
      "iteration 3726, dc_loss: 0.017771808430552483, tv_loss: 0.017077432945370674\n",
      "iteration 3727, dc_loss: 0.01777183823287487, tv_loss: 0.017077386379241943\n",
      "iteration 3728, dc_loss: 0.01777181774377823, tv_loss: 0.017077496275305748\n",
      "iteration 3729, dc_loss: 0.017771776765584946, tv_loss: 0.017077552154660225\n",
      "iteration 3730, dc_loss: 0.01777176558971405, tv_loss: 0.017077231779694557\n",
      "iteration 3731, dc_loss: 0.017771735787391663, tv_loss: 0.01707729697227478\n",
      "iteration 3732, dc_loss: 0.017771685495972633, tv_loss: 0.01707751676440239\n",
      "iteration 3733, dc_loss: 0.017771659418940544, tv_loss: 0.017077337950468063\n",
      "iteration 3734, dc_loss: 0.017771655693650246, tv_loss: 0.017077293246984482\n",
      "iteration 3735, dc_loss: 0.017771659418940544, tv_loss: 0.01707729883491993\n",
      "iteration 3736, dc_loss: 0.017771678045392036, tv_loss: 0.017077142372727394\n",
      "iteration 3737, dc_loss: 0.01777169480919838, tv_loss: 0.017077138647437096\n",
      "iteration 3738, dc_loss: 0.017771705985069275, tv_loss: 0.017077088356018066\n",
      "iteration 3739, dc_loss: 0.01777167245745659, tv_loss: 0.017077181488275528\n",
      "iteration 3740, dc_loss: 0.017771625891327858, tv_loss: 0.01707703061401844\n",
      "iteration 3741, dc_loss: 0.017771583050489426, tv_loss: 0.017077026888728142\n",
      "iteration 3742, dc_loss: 0.01777157373726368, tv_loss: 0.01707708276808262\n",
      "iteration 3743, dc_loss: 0.01777159422636032, tv_loss: 0.01707686297595501\n",
      "iteration 3744, dc_loss: 0.017771601676940918, tv_loss: 0.01707681082189083\n",
      "iteration 3745, dc_loss: 0.01777157559990883, tv_loss: 0.017076807096600533\n",
      "iteration 3746, dc_loss: 0.01777155138552189, tv_loss: 0.017076751217246056\n",
      "iteration 3747, dc_loss: 0.017771529033780098, tv_loss: 0.01707688719034195\n",
      "iteration 3748, dc_loss: 0.017771508544683456, tv_loss: 0.01707676611840725\n",
      "iteration 3749, dc_loss: 0.017771519720554352, tv_loss: 0.017076833173632622\n",
      "iteration 3750, dc_loss: 0.017771536484360695, tv_loss: 0.017076751217246056\n",
      "iteration 3751, dc_loss: 0.01777154952287674, tv_loss: 0.017076728865504265\n",
      "iteration 3752, dc_loss: 0.017771530896425247, tv_loss: 0.017076754942536354\n",
      "iteration 3753, dc_loss: 0.017771540209650993, tv_loss: 0.017076728865504265\n",
      "iteration 3754, dc_loss: 0.017771542072296143, tv_loss: 0.01707652397453785\n",
      "iteration 3755, dc_loss: 0.0177715215831995, tv_loss: 0.01707652024924755\n",
      "iteration 3756, dc_loss: 0.017771514132618904, tv_loss: 0.01707669347524643\n",
      "iteration 3757, dc_loss: 0.017771495506167412, tv_loss: 0.017076529562473297\n",
      "iteration 3758, dc_loss: 0.01777150109410286, tv_loss: 0.017076503485441208\n",
      "iteration 3759, dc_loss: 0.01777150109410286, tv_loss: 0.017076561227440834\n",
      "iteration 3760, dc_loss: 0.017771489918231964, tv_loss: 0.017076609656214714\n",
      "iteration 3761, dc_loss: 0.01777147315442562, tv_loss: 0.01707642525434494\n",
      "iteration 3762, dc_loss: 0.017771447077393532, tv_loss: 0.017076371237635612\n",
      "iteration 3763, dc_loss: 0.01777137629687786, tv_loss: 0.01707647740840912\n",
      "iteration 3764, dc_loss: 0.017771348357200623, tv_loss: 0.01707659289240837\n",
      "iteration 3765, dc_loss: 0.01777135208249092, tv_loss: 0.017076529562473297\n",
      "iteration 3766, dc_loss: 0.017771312966942787, tv_loss: 0.01707645319402218\n",
      "iteration 3767, dc_loss: 0.017771296203136444, tv_loss: 0.017076313495635986\n",
      "iteration 3768, dc_loss: 0.017771324142813683, tv_loss: 0.017076320946216583\n",
      "iteration 3769, dc_loss: 0.017771391198039055, tv_loss: 0.017076142132282257\n",
      "iteration 3770, dc_loss: 0.01777143031358719, tv_loss: 0.017076091840863228\n",
      "iteration 3771, dc_loss: 0.01777142845094204, tv_loss: 0.017075952142477036\n",
      "iteration 3772, dc_loss: 0.017771383747458458, tv_loss: 0.01707603596150875\n",
      "iteration 3773, dc_loss: 0.01777133159339428, tv_loss: 0.017076073214411736\n",
      "iteration 3774, dc_loss: 0.017771301791071892, tv_loss: 0.017076095566153526\n",
      "iteration 3775, dc_loss: 0.017771273851394653, tv_loss: 0.017076078802347183\n",
      "iteration 3776, dc_loss: 0.017771271988749504, tv_loss: 0.017076104879379272\n",
      "iteration 3777, dc_loss: 0.017771296203136444, tv_loss: 0.017076052725315094\n",
      "iteration 3778, dc_loss: 0.017771299928426743, tv_loss: 0.01707594096660614\n",
      "iteration 3779, dc_loss: 0.017771251499652863, tv_loss: 0.017076091840863228\n",
      "iteration 3780, dc_loss: 0.017771204933524132, tv_loss: 0.017076270654797554\n",
      "iteration 3781, dc_loss: 0.017771190032362938, tv_loss: 0.017075957730412483\n",
      "iteration 3782, dc_loss: 0.01777120679616928, tv_loss: 0.01707601547241211\n",
      "iteration 3783, dc_loss: 0.01777123473584652, tv_loss: 0.01707601547241211\n",
      "iteration 3784, dc_loss: 0.017771271988749504, tv_loss: 0.017075827345252037\n",
      "iteration 3785, dc_loss: 0.01777130737900734, tv_loss: 0.017075760290026665\n",
      "iteration 3786, dc_loss: 0.01777130365371704, tv_loss: 0.01707564853131771\n",
      "iteration 3787, dc_loss: 0.017771288752555847, tv_loss: 0.017075778916478157\n",
      "iteration 3788, dc_loss: 0.017771227285265923, tv_loss: 0.01707587204873562\n",
      "iteration 3789, dc_loss: 0.01777113415300846, tv_loss: 0.017075905576348305\n",
      "iteration 3790, dc_loss: 0.017771024256944656, tv_loss: 0.017076052725315094\n",
      "iteration 3791, dc_loss: 0.017771011218428612, tv_loss: 0.01707596331834793\n",
      "iteration 3792, dc_loss: 0.017771046608686447, tv_loss: 0.017075860872864723\n",
      "iteration 3793, dc_loss: 0.017771104350686073, tv_loss: 0.01707560569047928\n",
      "iteration 3794, dc_loss: 0.01777116395533085, tv_loss: 0.0170756783336401\n",
      "iteration 3795, dc_loss: 0.017771147191524506, tv_loss: 0.017075784504413605\n",
      "iteration 3796, dc_loss: 0.017771093174815178, tv_loss: 0.017075669020414352\n",
      "iteration 3797, dc_loss: 0.01777108572423458, tv_loss: 0.017075689509510994\n",
      "iteration 3798, dc_loss: 0.017771070823073387, tv_loss: 0.01707557961344719\n",
      "iteration 3799, dc_loss: 0.01777108944952488, tv_loss: 0.017075736075639725\n",
      "iteration 3800, dc_loss: 0.017771150916814804, tv_loss: 0.017075741663575172\n",
      "iteration 3801, dc_loss: 0.017771156504750252, tv_loss: 0.017075438052415848\n",
      "iteration 3802, dc_loss: 0.01777116395533085, tv_loss: 0.017075370997190475\n",
      "iteration 3803, dc_loss: 0.017771141603589058, tv_loss: 0.01707558147609234\n",
      "iteration 3804, dc_loss: 0.017771059647202492, tv_loss: 0.01707557961344719\n",
      "iteration 3805, dc_loss: 0.017771024256944656, tv_loss: 0.017075499519705772\n",
      "iteration 3806, dc_loss: 0.01777101494371891, tv_loss: 0.017075330018997192\n",
      "iteration 3807, dc_loss: 0.01777101494371891, tv_loss: 0.017075538635253906\n",
      "iteration 3808, dc_loss: 0.017771029844880104, tv_loss: 0.01707552745938301\n",
      "iteration 3809, dc_loss: 0.01777101494371891, tv_loss: 0.017075279727578163\n",
      "iteration 3810, dc_loss: 0.017771001905202866, tv_loss: 0.017075343057513237\n",
      "iteration 3811, dc_loss: 0.017770973965525627, tv_loss: 0.017075298354029655\n",
      "iteration 3812, dc_loss: 0.01777094043791294, tv_loss: 0.017075343057513237\n",
      "iteration 3813, dc_loss: 0.017770906910300255, tv_loss: 0.017075274139642715\n",
      "iteration 3814, dc_loss: 0.01777089573442936, tv_loss: 0.0170751865953207\n",
      "iteration 3815, dc_loss: 0.017770923674106598, tv_loss: 0.017075246199965477\n",
      "iteration 3816, dc_loss: 0.01777094230055809, tv_loss: 0.017075208947062492\n",
      "iteration 3817, dc_loss: 0.017770957201719284, tv_loss: 0.0170751865953207\n",
      "iteration 3818, dc_loss: 0.017770985141396523, tv_loss: 0.017075035721063614\n",
      "iteration 3819, dc_loss: 0.017770996317267418, tv_loss: 0.01707506738603115\n",
      "iteration 3820, dc_loss: 0.017770979553461075, tv_loss: 0.017074886709451675\n",
      "iteration 3821, dc_loss: 0.01777094416320324, tv_loss: 0.017074940726161003\n",
      "iteration 3822, dc_loss: 0.017770899459719658, tv_loss: 0.01707497239112854\n",
      "iteration 3823, dc_loss: 0.017770858481526375, tv_loss: 0.01707502454519272\n",
      "iteration 3824, dc_loss: 0.01777084544301033, tv_loss: 0.01707508973777294\n",
      "iteration 3825, dc_loss: 0.017770851030945778, tv_loss: 0.017075005918741226\n",
      "iteration 3826, dc_loss: 0.017770852893590927, tv_loss: 0.01707499846816063\n",
      "iteration 3827, dc_loss: 0.01777084916830063, tv_loss: 0.01707489788532257\n",
      "iteration 3828, dc_loss: 0.01777084730565548, tv_loss: 0.01707487739622593\n",
      "iteration 3829, dc_loss: 0.017770836129784584, tv_loss: 0.017075052484869957\n",
      "iteration 3830, dc_loss: 0.017770828679203987, tv_loss: 0.01707492023706436\n",
      "iteration 3831, dc_loss: 0.017770837992429733, tv_loss: 0.01707480475306511\n",
      "iteration 3832, dc_loss: 0.017770899459719658, tv_loss: 0.01707467809319496\n",
      "iteration 3833, dc_loss: 0.017770903185009956, tv_loss: 0.01707468554377556\n",
      "iteration 3834, dc_loss: 0.01777089200913906, tv_loss: 0.01707484945654869\n",
      "iteration 3835, dc_loss: 0.017770810052752495, tv_loss: 0.01707475259900093\n",
      "iteration 3836, dc_loss: 0.017770715057849884, tv_loss: 0.017074724659323692\n",
      "iteration 3837, dc_loss: 0.01777067594230175, tv_loss: 0.017074907198548317\n",
      "iteration 3838, dc_loss: 0.017770668491721153, tv_loss: 0.017074845731258392\n",
      "iteration 3839, dc_loss: 0.017770681530237198, tv_loss: 0.01707485131919384\n",
      "iteration 3840, dc_loss: 0.017770707607269287, tv_loss: 0.017074791714549065\n",
      "iteration 3841, dc_loss: 0.017770767211914062, tv_loss: 0.01707472838461399\n",
      "iteration 3842, dc_loss: 0.017770810052752495, tv_loss: 0.017074622213840485\n",
      "iteration 3843, dc_loss: 0.017770837992429733, tv_loss: 0.01707443594932556\n",
      "iteration 3844, dc_loss: 0.01777082495391369, tv_loss: 0.01707437075674534\n",
      "iteration 3845, dc_loss: 0.01777077279984951, tv_loss: 0.017074476927518845\n",
      "iteration 3846, dc_loss: 0.017770715057849884, tv_loss: 0.01707462966442108\n",
      "iteration 3847, dc_loss: 0.017770670354366302, tv_loss: 0.017074478790163994\n",
      "iteration 3848, dc_loss: 0.017770659178495407, tv_loss: 0.01707460545003414\n",
      "iteration 3849, dc_loss: 0.017770683392882347, tv_loss: 0.0170745886862278\n",
      "iteration 3850, dc_loss: 0.017770711332559586, tv_loss: 0.017074400559067726\n",
      "iteration 3851, dc_loss: 0.017770715057849884, tv_loss: 0.01707451231777668\n",
      "iteration 3852, dc_loss: 0.01777070201933384, tv_loss: 0.01707451231777668\n",
      "iteration 3853, dc_loss: 0.01777069829404354, tv_loss: 0.017074380069971085\n",
      "iteration 3854, dc_loss: 0.017770683392882347, tv_loss: 0.01707427203655243\n",
      "iteration 3855, dc_loss: 0.017770670354366302, tv_loss: 0.017074426636099815\n",
      "iteration 3856, dc_loss: 0.01777067221701145, tv_loss: 0.017074380069971085\n",
      "iteration 3857, dc_loss: 0.017770707607269287, tv_loss: 0.017074353992938995\n",
      "iteration 3858, dc_loss: 0.017770742997527122, tv_loss: 0.01707409881055355\n",
      "iteration 3859, dc_loss: 0.01777075231075287, tv_loss: 0.017074020579457283\n",
      "iteration 3860, dc_loss: 0.017770735546946526, tv_loss: 0.01707412116229534\n",
      "iteration 3861, dc_loss: 0.017770690843462944, tv_loss: 0.017074111849069595\n",
      "iteration 3862, dc_loss: 0.01777060329914093, tv_loss: 0.017074059695005417\n",
      "iteration 3863, dc_loss: 0.017770562320947647, tv_loss: 0.017074162140488625\n",
      "iteration 3864, dc_loss: 0.017770536243915558, tv_loss: 0.01707438938319683\n",
      "iteration 3865, dc_loss: 0.017770519480109215, tv_loss: 0.017074234783649445\n",
      "iteration 3866, dc_loss: 0.017770525068044662, tv_loss: 0.01707405410706997\n",
      "iteration 3867, dc_loss: 0.017770566046237946, tv_loss: 0.017074061557650566\n",
      "iteration 3868, dc_loss: 0.01777062378823757, tv_loss: 0.017073912546038628\n",
      "iteration 3869, dc_loss: 0.017770668491721153, tv_loss: 0.01707383058965206\n",
      "iteration 3870, dc_loss: 0.017770715057849884, tv_loss: 0.01707381010055542\n",
      "iteration 3871, dc_loss: 0.017770707607269287, tv_loss: 0.017073720693588257\n",
      "iteration 3872, dc_loss: 0.017770638689398766, tv_loss: 0.01707368716597557\n",
      "iteration 3873, dc_loss: 0.017770566046237946, tv_loss: 0.017073683440685272\n",
      "iteration 3874, dc_loss: 0.017770513892173767, tv_loss: 0.017073949798941612\n",
      "iteration 3875, dc_loss: 0.01777047850191593, tv_loss: 0.017073890194296837\n",
      "iteration 3876, dc_loss: 0.01777041330933571, tv_loss: 0.01707361452281475\n",
      "iteration 3877, dc_loss: 0.017770368605852127, tv_loss: 0.01707383617758751\n",
      "iteration 3878, dc_loss: 0.01777038536965847, tv_loss: 0.017073914408683777\n",
      "iteration 3879, dc_loss: 0.017770415171980858, tv_loss: 0.01707366481423378\n",
      "iteration 3880, dc_loss: 0.017770474776625633, tv_loss: 0.017073584720492363\n",
      "iteration 3881, dc_loss: 0.017770549282431602, tv_loss: 0.017073635011911392\n",
      "iteration 3882, dc_loss: 0.017770592123270035, tv_loss: 0.017073677852749825\n",
      "iteration 3883, dc_loss: 0.017770590260624886, tv_loss: 0.017073435708880424\n",
      "iteration 3884, dc_loss: 0.017770595848560333, tv_loss: 0.017073392868041992\n",
      "iteration 3885, dc_loss: 0.017770588397979736, tv_loss: 0.017073534429073334\n",
      "iteration 3886, dc_loss: 0.01777058094739914, tv_loss: 0.017073556780815125\n",
      "iteration 3887, dc_loss: 0.017770564183592796, tv_loss: 0.01707345061004162\n",
      "iteration 3888, dc_loss: 0.017770489677786827, tv_loss: 0.017073480412364006\n",
      "iteration 3889, dc_loss: 0.017770422622561455, tv_loss: 0.017073525115847588\n",
      "iteration 3890, dc_loss: 0.01777038350701332, tv_loss: 0.01707344315946102\n",
      "iteration 3891, dc_loss: 0.017770374193787575, tv_loss: 0.01707354187965393\n",
      "iteration 3892, dc_loss: 0.01777040772140026, tv_loss: 0.017073528841137886\n",
      "iteration 3893, dc_loss: 0.01777045615017414, tv_loss: 0.01707340218126774\n",
      "iteration 3894, dc_loss: 0.01777051016688347, tv_loss: 0.01707322709262371\n",
      "iteration 3895, dc_loss: 0.017770493403077126, tv_loss: 0.01707332953810692\n",
      "iteration 3896, dc_loss: 0.01777045428752899, tv_loss: 0.01707317866384983\n",
      "iteration 3897, dc_loss: 0.01777038350701332, tv_loss: 0.017073316499590874\n",
      "iteration 3898, dc_loss: 0.017770295962691307, tv_loss: 0.01707329787313938\n",
      "iteration 3899, dc_loss: 0.017770223319530487, tv_loss: 0.017073264345526695\n",
      "iteration 3900, dc_loss: 0.017770208418369293, tv_loss: 0.017073433846235275\n",
      "iteration 3901, dc_loss: 0.017770227044820786, tv_loss: 0.017073392868041992\n",
      "iteration 3902, dc_loss: 0.017770294100046158, tv_loss: 0.01707325503230095\n",
      "iteration 3903, dc_loss: 0.017770344391465187, tv_loss: 0.01707315817475319\n",
      "iteration 3904, dc_loss: 0.01777038909494877, tv_loss: 0.01707320474088192\n",
      "iteration 3905, dc_loss: 0.017770398408174515, tv_loss: 0.017073065042495728\n",
      "iteration 3906, dc_loss: 0.01777038350701332, tv_loss: 0.01707298867404461\n",
      "iteration 3907, dc_loss: 0.017770348116755486, tv_loss: 0.017073078081011772\n",
      "iteration 3908, dc_loss: 0.017770342528820038, tv_loss: 0.017073005437850952\n",
      "iteration 3909, dc_loss: 0.01777033507823944, tv_loss: 0.01707299053668976\n",
      "iteration 3910, dc_loss: 0.017770318314433098, tv_loss: 0.017072932794690132\n",
      "iteration 3911, dc_loss: 0.017770254984498024, tv_loss: 0.01707291603088379\n",
      "iteration 3912, dc_loss: 0.01777016744017601, tv_loss: 0.017073020339012146\n",
      "iteration 3913, dc_loss: 0.017770100384950638, tv_loss: 0.01707311160862446\n",
      "iteration 3914, dc_loss: 0.017770083621144295, tv_loss: 0.017073046416044235\n",
      "iteration 3915, dc_loss: 0.017770109698176384, tv_loss: 0.01707298308610916\n",
      "iteration 3916, dc_loss: 0.01777011901140213, tv_loss: 0.01707294024527073\n",
      "iteration 3917, dc_loss: 0.01777017116546631, tv_loss: 0.017072981223464012\n",
      "iteration 3918, dc_loss: 0.017770221456885338, tv_loss: 0.017072878777980804\n",
      "iteration 3919, dc_loss: 0.017770221456885338, tv_loss: 0.017072809860110283\n",
      "iteration 3920, dc_loss: 0.017770206555724144, tv_loss: 0.01707286573946476\n",
      "iteration 3921, dc_loss: 0.01777017116546631, tv_loss: 0.017072774469852448\n",
      "iteration 3922, dc_loss: 0.017770159989595413, tv_loss: 0.017072876915335655\n",
      "iteration 3923, dc_loss: 0.01777013950049877, tv_loss: 0.017072932794690132\n",
      "iteration 3924, dc_loss: 0.01777014695107937, tv_loss: 0.017072753980755806\n",
      "iteration 3925, dc_loss: 0.01777021214365959, tv_loss: 0.017072658985853195\n",
      "iteration 3926, dc_loss: 0.017770297825336456, tv_loss: 0.017072632908821106\n",
      "iteration 3927, dc_loss: 0.017770331352949142, tv_loss: 0.017072485759854317\n",
      "iteration 3928, dc_loss: 0.017770325765013695, tv_loss: 0.017072439193725586\n",
      "iteration 3929, dc_loss: 0.017770245671272278, tv_loss: 0.017072482034564018\n",
      "iteration 3930, dc_loss: 0.017770156264305115, tv_loss: 0.017072433605790138\n",
      "iteration 3931, dc_loss: 0.017770083621144295, tv_loss: 0.017072604969143867\n",
      "iteration 3932, dc_loss: 0.01777000166475773, tv_loss: 0.017072804272174835\n",
      "iteration 3933, dc_loss: 0.017769958823919296, tv_loss: 0.017072590067982674\n",
      "iteration 3934, dc_loss: 0.017769983038306236, tv_loss: 0.017072562128305435\n",
      "iteration 3935, dc_loss: 0.01776999793946743, tv_loss: 0.01707262173295021\n",
      "iteration 3936, dc_loss: 0.017769966274499893, tv_loss: 0.017072558403015137\n",
      "iteration 3937, dc_loss: 0.017769960686564445, tv_loss: 0.017072591930627823\n",
      "iteration 3938, dc_loss: 0.01776997186243534, tv_loss: 0.01707272417843342\n",
      "iteration 3939, dc_loss: 0.01776997558772564, tv_loss: 0.017072532325983047\n",
      "iteration 3940, dc_loss: 0.017769955098628998, tv_loss: 0.01707235723733902\n",
      "iteration 3941, dc_loss: 0.01776996999979019, tv_loss: 0.017072442919015884\n",
      "iteration 3942, dc_loss: 0.017769984900951385, tv_loss: 0.017072517424821854\n",
      "iteration 3943, dc_loss: 0.01776999421417713, tv_loss: 0.01707233302295208\n",
      "iteration 3944, dc_loss: 0.017770040780305862, tv_loss: 0.01707226224243641\n",
      "iteration 3945, dc_loss: 0.017770085483789444, tv_loss: 0.017072288319468498\n",
      "iteration 3946, dc_loss: 0.01777011528611183, tv_loss: 0.01707225851714611\n",
      "iteration 3947, dc_loss: 0.017770150676369667, tv_loss: 0.01707213930785656\n",
      "iteration 3948, dc_loss: 0.017770137637853622, tv_loss: 0.017072241753339767\n",
      "iteration 3949, dc_loss: 0.0177700724452734, tv_loss: 0.017072228714823723\n",
      "iteration 3950, dc_loss: 0.01777002029120922, tv_loss: 0.017072156071662903\n",
      "iteration 3951, dc_loss: 0.01776996999979019, tv_loss: 0.017072243615984917\n",
      "iteration 3952, dc_loss: 0.01776995323598385, tv_loss: 0.01707213930785656\n",
      "iteration 3953, dc_loss: 0.017769960686564445, tv_loss: 0.017072204500436783\n",
      "iteration 3954, dc_loss: 0.017769966274499893, tv_loss: 0.01707226037979126\n",
      "iteration 3955, dc_loss: 0.017769979313015938, tv_loss: 0.017072034999728203\n",
      "iteration 3956, dc_loss: 0.017769960686564445, tv_loss: 0.017072005197405815\n",
      "iteration 3957, dc_loss: 0.01776990108191967, tv_loss: 0.017072094604372978\n",
      "iteration 3958, dc_loss: 0.01776982843875885, tv_loss: 0.017072267830371857\n",
      "iteration 3959, dc_loss: 0.017769813537597656, tv_loss: 0.01707235909998417\n",
      "iteration 3960, dc_loss: 0.017769822850823402, tv_loss: 0.01707218401134014\n",
      "iteration 3961, dc_loss: 0.017769863829016685, tv_loss: 0.0170720387250185\n",
      "iteration 3962, dc_loss: 0.01776992529630661, tv_loss: 0.017072008922696114\n",
      "iteration 3963, dc_loss: 0.01776997186243534, tv_loss: 0.017072008922696114\n",
      "iteration 3964, dc_loss: 0.017770005390048027, tv_loss: 0.01707172580063343\n",
      "iteration 3965, dc_loss: 0.017770010977983475, tv_loss: 0.0170718003064394\n",
      "iteration 3966, dc_loss: 0.017770012840628624, tv_loss: 0.017071915790438652\n",
      "iteration 3967, dc_loss: 0.01776997745037079, tv_loss: 0.017071764916181564\n",
      "iteration 3968, dc_loss: 0.017769908532500267, tv_loss: 0.01707177795469761\n",
      "iteration 3969, dc_loss: 0.017769860103726387, tv_loss: 0.017071878537535667\n",
      "iteration 3970, dc_loss: 0.01776980422437191, tv_loss: 0.017071953043341637\n",
      "iteration 3971, dc_loss: 0.01776975207030773, tv_loss: 0.017072079703211784\n",
      "iteration 3972, dc_loss: 0.01776972971856594, tv_loss: 0.0170717965811491\n",
      "iteration 3973, dc_loss: 0.017769725993275642, tv_loss: 0.017071805894374847\n",
      "iteration 3974, dc_loss: 0.017769770696759224, tv_loss: 0.017071988433599472\n",
      "iteration 3975, dc_loss: 0.017769819125533104, tv_loss: 0.017071660608053207\n",
      "iteration 3976, dc_loss: 0.017769858241081238, tv_loss: 0.01707157865166664\n",
      "iteration 3977, dc_loss: 0.01776987873017788, tv_loss: 0.017071662470698357\n",
      "iteration 3978, dc_loss: 0.017769884318113327, tv_loss: 0.01707177795469761\n",
      "iteration 3979, dc_loss: 0.017769841477274895, tv_loss: 0.01707165688276291\n",
      "iteration 3980, dc_loss: 0.017769796773791313, tv_loss: 0.017071492969989777\n",
      "iteration 3981, dc_loss: 0.01776982843875885, tv_loss: 0.017071550711989403\n",
      "iteration 3982, dc_loss: 0.01776987314224243, tv_loss: 0.01707165502011776\n",
      "iteration 3983, dc_loss: 0.017769889906048775, tv_loss: 0.017071420326828957\n",
      "iteration 3984, dc_loss: 0.01776990108191967, tv_loss: 0.017071329057216644\n",
      "iteration 3985, dc_loss: 0.01776985637843609, tv_loss: 0.017071586102247238\n",
      "iteration 3986, dc_loss: 0.017769748345017433, tv_loss: 0.017071569338440895\n",
      "iteration 3987, dc_loss: 0.017769601196050644, tv_loss: 0.017071625217795372\n",
      "iteration 3988, dc_loss: 0.017769495025277138, tv_loss: 0.017071684822440147\n",
      "iteration 3989, dc_loss: 0.0177694670855999, tv_loss: 0.017071615904569626\n",
      "iteration 3990, dc_loss: 0.01776951178908348, tv_loss: 0.017071641981601715\n",
      "iteration 3991, dc_loss: 0.017769569531083107, tv_loss: 0.01707162894308567\n",
      "iteration 3992, dc_loss: 0.017769647762179375, tv_loss: 0.017071537673473358\n",
      "iteration 3993, dc_loss: 0.01776970736682415, tv_loss: 0.017071224749088287\n",
      "iteration 3994, dc_loss: 0.01776975207030773, tv_loss: 0.017071109265089035\n",
      "iteration 3995, dc_loss: 0.01776972971856594, tv_loss: 0.017071275040507317\n",
      "iteration 3996, dc_loss: 0.01776966266334057, tv_loss: 0.01707138866186142\n",
      "iteration 3997, dc_loss: 0.017769616097211838, tv_loss: 0.017071256414055824\n",
      "iteration 3998, dc_loss: 0.017769647762179375, tv_loss: 0.017071163281798363\n",
      "iteration 3999, dc_loss: 0.017769675701856613, tv_loss: 0.01707133837044239\n",
      "iteration 4000, dc_loss: 0.017769664525985718, tv_loss: 0.017071131616830826\n",
      "PSNR Value mt1: 34.17838255083465\n",
      "SSIM Value mt1: 0.7646187979246476\n",
      "tensor([[[6.0409e-04, 7.4484e-05]]]) tensor([[[1.2870, 1.2171]]])\n",
      "torch.Size([1, 1, 2]) torch.Size([1, 1, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| params.keys(): dict_keys(['grid'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "iteration 1, dc_loss: 2.99094820022583, tv_loss: 0.0\n",
      "iteration 2, dc_loss: 2.9311585426330566, tv_loss: 0.0002514319494366646\n",
      "iteration 3, dc_loss: 2.8724727630615234, tv_loss: 0.00049302022671327\n",
      "iteration 4, dc_loss: 2.8148691654205322, tv_loss: 0.0007322695455513895\n",
      "iteration 5, dc_loss: 2.7583281993865967, tv_loss: 0.0009708758443593979\n",
      "iteration 6, dc_loss: 2.7028329372406006, tv_loss: 0.0012089830124750733\n",
      "iteration 7, dc_loss: 2.648366689682007, tv_loss: 0.0014456139178946614\n",
      "iteration 8, dc_loss: 2.5949132442474365, tv_loss: 0.001682184636592865\n",
      "iteration 9, dc_loss: 2.5424578189849854, tv_loss: 0.0019143634708598256\n",
      "iteration 10, dc_loss: 2.49098539352417, tv_loss: 0.002145118312910199\n",
      "iteration 11, dc_loss: 2.440481662750244, tv_loss: 0.002372186630964279\n",
      "iteration 12, dc_loss: 2.390932321548462, tv_loss: 0.0025952947326004505\n",
      "iteration 13, dc_loss: 2.342323064804077, tv_loss: 0.0028159930370748043\n",
      "iteration 14, dc_loss: 2.294640302658081, tv_loss: 0.003032678971067071\n",
      "iteration 15, dc_loss: 2.247870445251465, tv_loss: 0.0032462948001921177\n",
      "iteration 16, dc_loss: 2.202000379562378, tv_loss: 0.0034558011684566736\n",
      "iteration 17, dc_loss: 2.1570160388946533, tv_loss: 0.003661800641566515\n",
      "iteration 18, dc_loss: 2.1129043102264404, tv_loss: 0.0038656964898109436\n",
      "iteration 19, dc_loss: 2.0696513652801514, tv_loss: 0.004066057503223419\n",
      "iteration 20, dc_loss: 2.0272440910339355, tv_loss: 0.00426314864307642\n",
      "iteration 21, dc_loss: 1.985668659210205, tv_loss: 0.004456875845789909\n",
      "iteration 22, dc_loss: 1.9449118375778198, tv_loss: 0.004646757617592812\n",
      "iteration 23, dc_loss: 1.90496027469635, tv_loss: 0.0048342421650886536\n",
      "iteration 24, dc_loss: 1.865800380706787, tv_loss: 0.005020014941692352\n",
      "iteration 25, dc_loss: 1.8274197578430176, tv_loss: 0.005202584899961948\n",
      "iteration 26, dc_loss: 1.7898043394088745, tv_loss: 0.005380013957619667\n",
      "iteration 27, dc_loss: 1.7529420852661133, tv_loss: 0.005556172225624323\n",
      "iteration 28, dc_loss: 1.716819405555725, tv_loss: 0.0057289982214570045\n",
      "iteration 29, dc_loss: 1.6814236640930176, tv_loss: 0.005898675881326199\n",
      "iteration 30, dc_loss: 1.646742343902588, tv_loss: 0.006064687389880419\n",
      "iteration 31, dc_loss: 1.6127625703811646, tv_loss: 0.006228995975106955\n",
      "iteration 32, dc_loss: 1.5794717073440552, tv_loss: 0.006390556693077087\n",
      "iteration 33, dc_loss: 1.5468573570251465, tv_loss: 0.006549356505274773\n",
      "iteration 34, dc_loss: 1.5149073600769043, tv_loss: 0.006704758852720261\n",
      "iteration 35, dc_loss: 1.4836091995239258, tv_loss: 0.006857602391391993\n",
      "iteration 36, dc_loss: 1.4529509544372559, tv_loss: 0.007009196560829878\n",
      "iteration 37, dc_loss: 1.422920823097229, tv_loss: 0.007158314809203148\n",
      "iteration 38, dc_loss: 1.393506646156311, tv_loss: 0.0073043727315962315\n",
      "iteration 39, dc_loss: 1.3646970987319946, tv_loss: 0.0074476208537817\n",
      "iteration 40, dc_loss: 1.3364803791046143, tv_loss: 0.007589138112962246\n",
      "iteration 41, dc_loss: 1.3088456392288208, tv_loss: 0.007728971540927887\n",
      "iteration 42, dc_loss: 1.2817809581756592, tv_loss: 0.007866502739489079\n",
      "iteration 43, dc_loss: 1.2552759647369385, tv_loss: 0.008002200163900852\n",
      "iteration 44, dc_loss: 1.2293193340301514, tv_loss: 0.008136644028127193\n",
      "iteration 45, dc_loss: 1.2039005756378174, tv_loss: 0.008268848992884159\n",
      "iteration 46, dc_loss: 1.1790088415145874, tv_loss: 0.008398933336138725\n",
      "iteration 47, dc_loss: 1.1546337604522705, tv_loss: 0.008526434190571308\n",
      "iteration 48, dc_loss: 1.1307651996612549, tv_loss: 0.008653555065393448\n",
      "iteration 49, dc_loss: 1.1073927879333496, tv_loss: 0.00877891480922699\n",
      "iteration 50, dc_loss: 1.0845067501068115, tv_loss: 0.008902381174266338\n",
      "iteration 51, dc_loss: 1.0620973110198975, tv_loss: 0.009023638442158699\n",
      "iteration 52, dc_loss: 1.0401545763015747, tv_loss: 0.00914230477064848\n",
      "iteration 53, dc_loss: 1.0186693668365479, tv_loss: 0.009261136874556541\n",
      "iteration 54, dc_loss: 0.9976322650909424, tv_loss: 0.009379739873111248\n",
      "iteration 55, dc_loss: 0.9770341515541077, tv_loss: 0.009496563114225864\n",
      "iteration 56, dc_loss: 0.9568662047386169, tv_loss: 0.009610035456717014\n",
      "iteration 57, dc_loss: 0.9371195435523987, tv_loss: 0.009722567163407803\n",
      "iteration 58, dc_loss: 0.9177854061126709, tv_loss: 0.009834402240812778\n",
      "iteration 59, dc_loss: 0.8988556265830994, tv_loss: 0.009944872930645943\n",
      "iteration 60, dc_loss: 0.880321741104126, tv_loss: 0.010053843259811401\n",
      "iteration 61, dc_loss: 0.8621756434440613, tv_loss: 0.010162166319787502\n",
      "iteration 62, dc_loss: 0.8444094061851501, tv_loss: 0.010268345475196838\n",
      "iteration 63, dc_loss: 0.827015221118927, tv_loss: 0.010372410528361797\n",
      "iteration 64, dc_loss: 0.8099852204322815, tv_loss: 0.010476780124008656\n",
      "iteration 65, dc_loss: 0.7933121919631958, tv_loss: 0.0105796093121171\n",
      "iteration 66, dc_loss: 0.7769884467124939, tv_loss: 0.010679174214601517\n",
      "iteration 67, dc_loss: 0.7610070705413818, tv_loss: 0.010779935866594315\n",
      "iteration 68, dc_loss: 0.745360791683197, tv_loss: 0.010879641398787498\n",
      "iteration 69, dc_loss: 0.7300427556037903, tv_loss: 0.010977066121995449\n",
      "iteration 70, dc_loss: 0.7150461673736572, tv_loss: 0.011071569286286831\n",
      "iteration 71, dc_loss: 0.7003641724586487, tv_loss: 0.01116815023124218\n",
      "iteration 72, dc_loss: 0.6859903931617737, tv_loss: 0.011262945830821991\n",
      "iteration 73, dc_loss: 0.671918511390686, tv_loss: 0.011355479247868061\n",
      "iteration 74, dc_loss: 0.65814208984375, tv_loss: 0.011446512304246426\n",
      "iteration 75, dc_loss: 0.6446550488471985, tv_loss: 0.011537713930010796\n",
      "iteration 76, dc_loss: 0.6314514875411987, tv_loss: 0.011628620326519012\n",
      "iteration 77, dc_loss: 0.6185253858566284, tv_loss: 0.011716634966433048\n",
      "iteration 78, dc_loss: 0.6058709621429443, tv_loss: 0.011803388595581055\n",
      "iteration 79, dc_loss: 0.593482494354248, tv_loss: 0.01188995223492384\n",
      "iteration 80, dc_loss: 0.5813546180725098, tv_loss: 0.011976290494203568\n",
      "iteration 81, dc_loss: 0.5694817900657654, tv_loss: 0.012061545625329018\n",
      "iteration 82, dc_loss: 0.5578587651252747, tv_loss: 0.012144980952143669\n",
      "iteration 83, dc_loss: 0.5464802384376526, tv_loss: 0.012226568534970284\n",
      "iteration 84, dc_loss: 0.5353412628173828, tv_loss: 0.012307983823120594\n",
      "iteration 85, dc_loss: 0.5244367122650146, tv_loss: 0.01239034254103899\n",
      "iteration 86, dc_loss: 0.5137616395950317, tv_loss: 0.012469781562685966\n",
      "iteration 87, dc_loss: 0.5033114552497864, tv_loss: 0.012548400089144707\n",
      "iteration 88, dc_loss: 0.49308130145072937, tv_loss: 0.012625668197870255\n",
      "iteration 89, dc_loss: 0.4830666780471802, tv_loss: 0.012701681815087795\n",
      "iteration 90, dc_loss: 0.47326305508613586, tv_loss: 0.012778829783201218\n",
      "iteration 91, dc_loss: 0.46366602182388306, tv_loss: 0.01285467203706503\n",
      "iteration 92, dc_loss: 0.4542711675167084, tv_loss: 0.012928205542266369\n",
      "iteration 93, dc_loss: 0.4450744688510895, tv_loss: 0.013000980019569397\n",
      "iteration 94, dc_loss: 0.436071515083313, tv_loss: 0.013072619214653969\n",
      "iteration 95, dc_loss: 0.4272584915161133, tv_loss: 0.013144733384251595\n",
      "iteration 96, dc_loss: 0.4186312258243561, tv_loss: 0.0132165951654315\n",
      "iteration 97, dc_loss: 0.41018596291542053, tv_loss: 0.01328614354133606\n",
      "iteration 98, dc_loss: 0.40191879868507385, tv_loss: 0.013353953137993813\n",
      "iteration 99, dc_loss: 0.39382612705230713, tv_loss: 0.013422747142612934\n",
      "iteration 100, dc_loss: 0.3859041631221771, tv_loss: 0.013490461744368076\n",
      "iteration 101, dc_loss: 0.3781493306159973, tv_loss: 0.013556533493101597\n",
      "iteration 102, dc_loss: 0.3705581724643707, tv_loss: 0.013620924204587936\n",
      "iteration 103, dc_loss: 0.36312732100486755, tv_loss: 0.013686646707355976\n",
      "iteration 104, dc_loss: 0.3558533489704132, tv_loss: 0.013751668855547905\n",
      "iteration 105, dc_loss: 0.34873291850090027, tv_loss: 0.013814110308885574\n",
      "iteration 106, dc_loss: 0.34176281094551086, tv_loss: 0.01387488842010498\n",
      "iteration 107, dc_loss: 0.3349398970603943, tv_loss: 0.013937451876699924\n",
      "iteration 108, dc_loss: 0.32826119661331177, tv_loss: 0.0139985466375947\n",
      "iteration 109, dc_loss: 0.3217235505580902, tv_loss: 0.014058688655495644\n",
      "iteration 110, dc_loss: 0.31532397866249084, tv_loss: 0.014116575010120869\n",
      "iteration 111, dc_loss: 0.30905964970588684, tv_loss: 0.014175314456224442\n",
      "iteration 112, dc_loss: 0.3029276430606842, tv_loss: 0.014233785681426525\n",
      "iteration 113, dc_loss: 0.29692521691322327, tv_loss: 0.014290152117609978\n",
      "iteration 114, dc_loss: 0.29104965925216675, tv_loss: 0.014345064759254456\n",
      "iteration 115, dc_loss: 0.28529831767082214, tv_loss: 0.014401466585695744\n",
      "iteration 116, dc_loss: 0.27966856956481934, tv_loss: 0.014457164332270622\n",
      "iteration 117, dc_loss: 0.2741577625274658, tv_loss: 0.014509973116219044\n",
      "iteration 118, dc_loss: 0.26876354217529297, tv_loss: 0.014561275020241737\n",
      "iteration 119, dc_loss: 0.26348331570625305, tv_loss: 0.01461662258952856\n",
      "iteration 120, dc_loss: 0.25831475853919983, tv_loss: 0.014669577591121197\n",
      "iteration 121, dc_loss: 0.2532554864883423, tv_loss: 0.014719244092702866\n",
      "iteration 122, dc_loss: 0.24830323457717896, tv_loss: 0.014769062399864197\n",
      "iteration 123, dc_loss: 0.2434556782245636, tv_loss: 0.014819946140050888\n",
      "iteration 124, dc_loss: 0.2387107014656067, tv_loss: 0.014869838953018188\n",
      "iteration 125, dc_loss: 0.23406606912612915, tv_loss: 0.014918148517608643\n",
      "iteration 126, dc_loss: 0.22951970994472504, tv_loss: 0.014966024085879326\n",
      "iteration 127, dc_loss: 0.22506947815418243, tv_loss: 0.015012703835964203\n",
      "iteration 128, dc_loss: 0.22071342170238495, tv_loss: 0.015060433186590672\n",
      "iteration 129, dc_loss: 0.216449573636055, tv_loss: 0.01510725449770689\n",
      "iteration 130, dc_loss: 0.212275892496109, tv_loss: 0.015153042040765285\n",
      "iteration 131, dc_loss: 0.20819057524204254, tv_loss: 0.015197664499282837\n",
      "iteration 132, dc_loss: 0.2041916698217392, tv_loss: 0.015240742824971676\n",
      "iteration 133, dc_loss: 0.2002773880958557, tv_loss: 0.015286164358258247\n",
      "iteration 134, dc_loss: 0.196446031332016, tv_loss: 0.01532946340739727\n",
      "iteration 135, dc_loss: 0.1926957219839096, tv_loss: 0.015370638109743595\n",
      "iteration 136, dc_loss: 0.18902482092380524, tv_loss: 0.015412528067827225\n",
      "iteration 137, dc_loss: 0.18543162941932678, tv_loss: 0.015454434789717197\n",
      "iteration 138, dc_loss: 0.18191447854042053, tv_loss: 0.015494607388973236\n",
      "iteration 139, dc_loss: 0.17847177386283875, tv_loss: 0.015535503625869751\n",
      "iteration 140, dc_loss: 0.17510196566581726, tv_loss: 0.015575529076159\n",
      "iteration 141, dc_loss: 0.1718035191297531, tv_loss: 0.01561480388045311\n",
      "iteration 142, dc_loss: 0.16857488453388214, tv_loss: 0.01565314643085003\n",
      "iteration 143, dc_loss: 0.16541467607021332, tv_loss: 0.015690675005316734\n",
      "iteration 144, dc_loss: 0.16232135891914368, tv_loss: 0.015727883204817772\n",
      "iteration 145, dc_loss: 0.1592935472726822, tv_loss: 0.01576550118625164\n",
      "iteration 146, dc_loss: 0.15632985532283783, tv_loss: 0.01580221764743328\n",
      "iteration 147, dc_loss: 0.15342892706394196, tv_loss: 0.01583772897720337\n",
      "iteration 148, dc_loss: 0.1505894511938095, tv_loss: 0.015874100849032402\n",
      "iteration 149, dc_loss: 0.14781010150909424, tv_loss: 0.0159086175262928\n",
      "iteration 150, dc_loss: 0.14508965611457825, tv_loss: 0.015942955389618874\n",
      "iteration 151, dc_loss: 0.14242683351039886, tv_loss: 0.01597733423113823\n",
      "iteration 152, dc_loss: 0.13982050120830536, tv_loss: 0.01601094752550125\n",
      "iteration 153, dc_loss: 0.13726936280727386, tv_loss: 0.01604430191218853\n",
      "iteration 154, dc_loss: 0.13477228581905365, tv_loss: 0.016076914966106415\n",
      "iteration 155, dc_loss: 0.13232818245887756, tv_loss: 0.01610945723950863\n",
      "iteration 156, dc_loss: 0.1299358308315277, tv_loss: 0.016141321510076523\n",
      "iteration 157, dc_loss: 0.1275942474603653, tv_loss: 0.016172491014003754\n",
      "iteration 158, dc_loss: 0.1253022849559784, tv_loss: 0.016203874722123146\n",
      "iteration 159, dc_loss: 0.12305890023708344, tv_loss: 0.016235193237662315\n",
      "iteration 160, dc_loss: 0.12086308747529984, tv_loss: 0.01626521162688732\n",
      "iteration 161, dc_loss: 0.11871383339166641, tv_loss: 0.016295170411467552\n",
      "iteration 162, dc_loss: 0.11661015450954437, tv_loss: 0.016324101015925407\n",
      "iteration 163, dc_loss: 0.11455107480287552, tv_loss: 0.01635289005935192\n",
      "iteration 164, dc_loss: 0.11253568530082703, tv_loss: 0.0163822453469038\n",
      "iteration 165, dc_loss: 0.11056306958198547, tv_loss: 0.016410043463110924\n",
      "iteration 166, dc_loss: 0.10863228887319565, tv_loss: 0.01643763668835163\n",
      "iteration 167, dc_loss: 0.10674243420362473, tv_loss: 0.016464805230498314\n",
      "iteration 168, dc_loss: 0.10489267855882645, tv_loss: 0.01649288460612297\n",
      "iteration 169, dc_loss: 0.10308215022087097, tv_loss: 0.016518931835889816\n",
      "iteration 170, dc_loss: 0.10131008923053741, tv_loss: 0.016544442623853683\n",
      "iteration 171, dc_loss: 0.09957566112279892, tv_loss: 0.016570424661040306\n",
      "iteration 172, dc_loss: 0.09787807613611221, tv_loss: 0.01659570448100567\n",
      "iteration 173, dc_loss: 0.09621651470661163, tv_loss: 0.016620969399809837\n",
      "iteration 174, dc_loss: 0.09459023177623749, tv_loss: 0.01664532534778118\n",
      "iteration 175, dc_loss: 0.09299846738576889, tv_loss: 0.016668913885951042\n",
      "iteration 176, dc_loss: 0.09144047647714615, tv_loss: 0.01669294387102127\n",
      "iteration 177, dc_loss: 0.08991555869579315, tv_loss: 0.01671707071363926\n",
      "iteration 178, dc_loss: 0.08842305839061737, tv_loss: 0.016739562153816223\n",
      "iteration 179, dc_loss: 0.08696228265762329, tv_loss: 0.016762442886829376\n",
      "iteration 180, dc_loss: 0.08553256839513779, tv_loss: 0.016785675659775734\n",
      "iteration 181, dc_loss: 0.08413321524858475, tv_loss: 0.01680799201130867\n",
      "iteration 182, dc_loss: 0.08276361227035522, tv_loss: 0.016829289495944977\n",
      "iteration 183, dc_loss: 0.08142312616109848, tv_loss: 0.01685037650167942\n",
      "iteration 184, dc_loss: 0.08011111617088318, tv_loss: 0.01687169261276722\n",
      "iteration 185, dc_loss: 0.07882700860500336, tv_loss: 0.016892824321985245\n",
      "iteration 186, dc_loss: 0.07757022976875305, tv_loss: 0.016913553699851036\n",
      "iteration 187, dc_loss: 0.07634016871452332, tv_loss: 0.016933748498558998\n",
      "iteration 188, dc_loss: 0.07513627409934998, tv_loss: 0.01695435494184494\n",
      "iteration 189, dc_loss: 0.07395800948143005, tv_loss: 0.01697346940636635\n",
      "iteration 190, dc_loss: 0.07280481606721878, tv_loss: 0.016992999240756035\n",
      "iteration 191, dc_loss: 0.07167613506317139, tv_loss: 0.017013220116496086\n",
      "iteration 192, dc_loss: 0.07057147473096848, tv_loss: 0.017032699659466743\n",
      "iteration 193, dc_loss: 0.06949032843112946, tv_loss: 0.01705082505941391\n",
      "iteration 194, dc_loss: 0.06843219697475433, tv_loss: 0.017068462446331978\n",
      "iteration 195, dc_loss: 0.0673966035246849, tv_loss: 0.017087852582335472\n",
      "iteration 196, dc_loss: 0.06638309359550476, tv_loss: 0.01710575632750988\n",
      "iteration 197, dc_loss: 0.0653911605477333, tv_loss: 0.017123205587267876\n",
      "iteration 198, dc_loss: 0.06442034989595413, tv_loss: 0.017140086740255356\n",
      "iteration 199, dc_loss: 0.06347024440765381, tv_loss: 0.017157383263111115\n",
      "iteration 200, dc_loss: 0.06254040449857712, tv_loss: 0.017174290493130684\n",
      "iteration 201, dc_loss: 0.06163036823272705, tv_loss: 0.017190465703606606\n",
      "iteration 202, dc_loss: 0.06073974817991257, tv_loss: 0.01720680296421051\n",
      "iteration 203, dc_loss: 0.059868089854717255, tv_loss: 0.017222540453076363\n",
      "iteration 204, dc_loss: 0.059015002101659775, tv_loss: 0.017239436507225037\n",
      "iteration 205, dc_loss: 0.05818009749054909, tv_loss: 0.017255358397960663\n",
      "iteration 206, dc_loss: 0.05736302584409714, tv_loss: 0.017270023003220558\n",
      "iteration 207, dc_loss: 0.056563399732112885, tv_loss: 0.017284376546740532\n",
      "iteration 208, dc_loss: 0.055780865252017975, tv_loss: 0.01729963719844818\n",
      "iteration 209, dc_loss: 0.05501503497362137, tv_loss: 0.017314137890934944\n",
      "iteration 210, dc_loss: 0.05426552891731262, tv_loss: 0.017327187582850456\n",
      "iteration 211, dc_loss: 0.05353200063109398, tv_loss: 0.017341436818242073\n",
      "iteration 212, dc_loss: 0.05281410738825798, tv_loss: 0.0173561479896307\n",
      "iteration 213, dc_loss: 0.052111558616161346, tv_loss: 0.017369447275996208\n",
      "iteration 214, dc_loss: 0.05142400041222572, tv_loss: 0.017381701618433\n",
      "iteration 215, dc_loss: 0.05075113847851753, tv_loss: 0.01739589124917984\n",
      "iteration 216, dc_loss: 0.050092663615942, tv_loss: 0.017410170286893845\n",
      "iteration 217, dc_loss: 0.049448274075984955, tv_loss: 0.017422355711460114\n",
      "iteration 218, dc_loss: 0.04881764575839043, tv_loss: 0.017433686181902885\n",
      "iteration 219, dc_loss: 0.04820047691464424, tv_loss: 0.017446571961045265\n",
      "iteration 220, dc_loss: 0.047596488147974014, tv_loss: 0.017459629103541374\n",
      "iteration 221, dc_loss: 0.047005411237478256, tv_loss: 0.017470987513661385\n",
      "iteration 222, dc_loss: 0.04642698913812637, tv_loss: 0.017482340335845947\n",
      "iteration 223, dc_loss: 0.04586093872785568, tv_loss: 0.017494380474090576\n",
      "iteration 224, dc_loss: 0.0453069768846035, tv_loss: 0.017506113275885582\n",
      "iteration 225, dc_loss: 0.04476488009095192, tv_loss: 0.017516791820526123\n",
      "iteration 226, dc_loss: 0.04423439875245094, tv_loss: 0.017527442425489426\n",
      "iteration 227, dc_loss: 0.043715257197618484, tv_loss: 0.017539015039801598\n",
      "iteration 228, dc_loss: 0.043207213282585144, tv_loss: 0.01754916086792946\n",
      "iteration 229, dc_loss: 0.04271002113819122, tv_loss: 0.01755993254482746\n",
      "iteration 230, dc_loss: 0.04222343862056732, tv_loss: 0.017570631578564644\n",
      "iteration 231, dc_loss: 0.04174724593758583, tv_loss: 0.017580805346369743\n",
      "iteration 232, dc_loss: 0.04128126800060272, tv_loss: 0.01759081520140171\n",
      "iteration 233, dc_loss: 0.040825255215168, tv_loss: 0.01760103367269039\n",
      "iteration 234, dc_loss: 0.04037902504205704, tv_loss: 0.017611024901270866\n",
      "iteration 235, dc_loss: 0.03994236886501312, tv_loss: 0.01762031950056553\n",
      "iteration 236, dc_loss: 0.039515070617198944, tv_loss: 0.017629804089665413\n",
      "iteration 237, dc_loss: 0.0390968918800354, tv_loss: 0.01763860508799553\n",
      "iteration 238, dc_loss: 0.03868767246603966, tv_loss: 0.01764838583767414\n",
      "iteration 239, dc_loss: 0.038287218660116196, tv_loss: 0.01765718124806881\n",
      "iteration 240, dc_loss: 0.03789535537362099, tv_loss: 0.01766582764685154\n",
      "iteration 241, dc_loss: 0.037511877715587616, tv_loss: 0.01767466776072979\n",
      "iteration 242, dc_loss: 0.037136610597372055, tv_loss: 0.017683427780866623\n",
      "iteration 243, dc_loss: 0.03676934540271759, tv_loss: 0.017691725865006447\n",
      "iteration 244, dc_loss: 0.03640994057059288, tv_loss: 0.017699193209409714\n",
      "iteration 245, dc_loss: 0.0360582172870636, tv_loss: 0.017707573249936104\n",
      "iteration 246, dc_loss: 0.03571402281522751, tv_loss: 0.017716139554977417\n",
      "iteration 247, dc_loss: 0.03537720441818237, tv_loss: 0.017722895368933678\n",
      "iteration 248, dc_loss: 0.03504759818315506, tv_loss: 0.017730869352817535\n",
      "iteration 249, dc_loss: 0.03472505882382393, tv_loss: 0.017739128321409225\n",
      "iteration 250, dc_loss: 0.034409429877996445, tv_loss: 0.017746446654200554\n",
      "iteration 251, dc_loss: 0.03410053625702858, tv_loss: 0.01775338314473629\n",
      "iteration 252, dc_loss: 0.0337982177734375, tv_loss: 0.01776045374572277\n",
      "iteration 253, dc_loss: 0.03350239247083664, tv_loss: 0.01776736043393612\n",
      "iteration 254, dc_loss: 0.03321291133761406, tv_loss: 0.017774416133761406\n",
      "iteration 255, dc_loss: 0.032929614186286926, tv_loss: 0.017781291157007217\n",
      "iteration 256, dc_loss: 0.0326523594558239, tv_loss: 0.017787307500839233\n",
      "iteration 257, dc_loss: 0.032381027936935425, tv_loss: 0.017794519662857056\n",
      "iteration 258, dc_loss: 0.032115500420331955, tv_loss: 0.017800888046622276\n",
      "iteration 259, dc_loss: 0.031855642795562744, tv_loss: 0.017807450145483017\n",
      "iteration 260, dc_loss: 0.03160133585333824, tv_loss: 0.01781296730041504\n",
      "iteration 261, dc_loss: 0.03135247528553009, tv_loss: 0.017818894237279892\n",
      "iteration 262, dc_loss: 0.031108912080526352, tv_loss: 0.017825188115239143\n",
      "iteration 263, dc_loss: 0.030870536342263222, tv_loss: 0.01783108152449131\n",
      "iteration 264, dc_loss: 0.030637232586741447, tv_loss: 0.017836352810263634\n",
      "iteration 265, dc_loss: 0.030408889055252075, tv_loss: 0.017841825261712074\n",
      "iteration 266, dc_loss: 0.030185438692569733, tv_loss: 0.017847860231995583\n",
      "iteration 267, dc_loss: 0.02996675670146942, tv_loss: 0.017853211611509323\n",
      "iteration 268, dc_loss: 0.029752718284726143, tv_loss: 0.01785752736032009\n",
      "iteration 269, dc_loss: 0.02954324334859848, tv_loss: 0.017862863838672638\n",
      "iteration 270, dc_loss: 0.029338227584958076, tv_loss: 0.01786835491657257\n",
      "iteration 271, dc_loss: 0.029137589037418365, tv_loss: 0.017873568460345268\n",
      "iteration 272, dc_loss: 0.028941268101334572, tv_loss: 0.017878057435154915\n",
      "iteration 273, dc_loss: 0.02874911017715931, tv_loss: 0.01788269728422165\n",
      "iteration 274, dc_loss: 0.028561020269989967, tv_loss: 0.01788729801774025\n",
      "iteration 275, dc_loss: 0.028376933187246323, tv_loss: 0.017892122268676758\n",
      "iteration 276, dc_loss: 0.028196750208735466, tv_loss: 0.017896687611937523\n",
      "iteration 277, dc_loss: 0.028020359575748444, tv_loss: 0.017901470884680748\n",
      "iteration 278, dc_loss: 0.02784770540893078, tv_loss: 0.01790526509284973\n",
      "iteration 279, dc_loss: 0.027678733691573143, tv_loss: 0.017909908667206764\n",
      "iteration 280, dc_loss: 0.027513349428772926, tv_loss: 0.01791432313621044\n",
      "iteration 281, dc_loss: 0.027351489290595055, tv_loss: 0.01791834458708763\n",
      "iteration 282, dc_loss: 0.02719302289187908, tv_loss: 0.017922233790159225\n",
      "iteration 283, dc_loss: 0.02703794091939926, tv_loss: 0.017925627529621124\n",
      "iteration 284, dc_loss: 0.02688615955412388, tv_loss: 0.01792963035404682\n",
      "iteration 285, dc_loss: 0.02673758566379547, tv_loss: 0.017934095114469528\n",
      "iteration 286, dc_loss: 0.026592127978801727, tv_loss: 0.01793757826089859\n",
      "iteration 287, dc_loss: 0.02644973434507847, tv_loss: 0.01794026419520378\n",
      "iteration 288, dc_loss: 0.026310313493013382, tv_loss: 0.01794428937137127\n",
      "iteration 289, dc_loss: 0.026173826307058334, tv_loss: 0.017948277294635773\n",
      "iteration 290, dc_loss: 0.026040224358439445, tv_loss: 0.01795193739235401\n",
      "iteration 291, dc_loss: 0.025909442454576492, tv_loss: 0.017954451963305473\n",
      "iteration 292, dc_loss: 0.025781404227018356, tv_loss: 0.017957372590899467\n",
      "iteration 293, dc_loss: 0.025656063109636307, tv_loss: 0.01796143315732479\n",
      "iteration 294, dc_loss: 0.025533366948366165, tv_loss: 0.017964765429496765\n",
      "iteration 295, dc_loss: 0.02541324310004711, tv_loss: 0.017967166379094124\n",
      "iteration 296, dc_loss: 0.025295626372098923, tv_loss: 0.017969587817788124\n",
      "iteration 297, dc_loss: 0.02518046833574772, tv_loss: 0.01797313429415226\n",
      "iteration 298, dc_loss: 0.025067731738090515, tv_loss: 0.01797596737742424\n",
      "iteration 299, dc_loss: 0.024957340210676193, tv_loss: 0.017978576943278313\n",
      "iteration 300, dc_loss: 0.02484925091266632, tv_loss: 0.017980672419071198\n",
      "iteration 301, dc_loss: 0.024743419140577316, tv_loss: 0.017983408644795418\n",
      "iteration 302, dc_loss: 0.02463979832828045, tv_loss: 0.017986396327614784\n",
      "iteration 303, dc_loss: 0.02453833818435669, tv_loss: 0.01798867993056774\n",
      "iteration 304, dc_loss: 0.02443896420300007, tv_loss: 0.017990555614233017\n",
      "iteration 305, dc_loss: 0.024341650307178497, tv_loss: 0.017993194982409477\n",
      "iteration 306, dc_loss: 0.024246349930763245, tv_loss: 0.01799600012600422\n",
      "iteration 307, dc_loss: 0.02415301837027073, tv_loss: 0.01799800992012024\n",
      "iteration 308, dc_loss: 0.024061623960733414, tv_loss: 0.017999647185206413\n",
      "iteration 309, dc_loss: 0.023972121998667717, tv_loss: 0.0180018562823534\n",
      "iteration 310, dc_loss: 0.0238844845443964, tv_loss: 0.018004484474658966\n",
      "iteration 311, dc_loss: 0.023798637092113495, tv_loss: 0.01800612546503544\n",
      "iteration 312, dc_loss: 0.02371455915272236, tv_loss: 0.018008003011345863\n",
      "iteration 313, dc_loss: 0.023632194846868515, tv_loss: 0.018009990453720093\n",
      "iteration 314, dc_loss: 0.023551523685455322, tv_loss: 0.018012268468737602\n",
      "iteration 315, dc_loss: 0.023472527042031288, tv_loss: 0.01801402121782303\n",
      "iteration 316, dc_loss: 0.02339513786137104, tv_loss: 0.018015502020716667\n",
      "iteration 317, dc_loss: 0.02331933006644249, tv_loss: 0.018017247319221497\n",
      "iteration 318, dc_loss: 0.023245079442858696, tv_loss: 0.01801879145205021\n",
      "iteration 319, dc_loss: 0.023172320798039436, tv_loss: 0.018020687624812126\n",
      "iteration 320, dc_loss: 0.02310103550553322, tv_loss: 0.01802198402583599\n",
      "iteration 321, dc_loss: 0.023031173273921013, tv_loss: 0.01802380196750164\n",
      "iteration 322, dc_loss: 0.02296270802617073, tv_loss: 0.018025673925876617\n",
      "iteration 323, dc_loss: 0.022895662114024162, tv_loss: 0.018026985228061676\n",
      "iteration 324, dc_loss: 0.022829987108707428, tv_loss: 0.018027938902378082\n",
      "iteration 325, dc_loss: 0.02276565693318844, tv_loss: 0.018029170110821724\n",
      "iteration 326, dc_loss: 0.022702613845467567, tv_loss: 0.018030859529972076\n",
      "iteration 327, dc_loss: 0.022640839219093323, tv_loss: 0.018032265827059746\n",
      "iteration 328, dc_loss: 0.022580282762646675, tv_loss: 0.01803305186331272\n",
      "iteration 329, dc_loss: 0.022520923987030983, tv_loss: 0.018034063279628754\n",
      "iteration 330, dc_loss: 0.02246273122727871, tv_loss: 0.01803574338555336\n",
      "iteration 331, dc_loss: 0.022405704483389854, tv_loss: 0.01803676411509514\n",
      "iteration 332, dc_loss: 0.022349799051880836, tv_loss: 0.018037576228380203\n",
      "iteration 333, dc_loss: 0.022295013070106506, tv_loss: 0.01803843304514885\n",
      "iteration 334, dc_loss: 0.022241299971938133, tv_loss: 0.018039749935269356\n",
      "iteration 335, dc_loss: 0.02218865044414997, tv_loss: 0.018041053786873817\n",
      "iteration 336, dc_loss: 0.022137043997645378, tv_loss: 0.01804148219525814\n",
      "iteration 337, dc_loss: 0.022086454555392265, tv_loss: 0.01804199256002903\n",
      "iteration 338, dc_loss: 0.022036883980035782, tv_loss: 0.018043210729956627\n",
      "iteration 339, dc_loss: 0.0219882819801569, tv_loss: 0.01804424449801445\n",
      "iteration 340, dc_loss: 0.021940622478723526, tv_loss: 0.01804453134536743\n",
      "iteration 341, dc_loss: 0.02189386449754238, tv_loss: 0.018044542521238327\n",
      "iteration 342, dc_loss: 0.02184802107512951, tv_loss: 0.01804560422897339\n",
      "iteration 343, dc_loss: 0.02180306985974312, tv_loss: 0.01804615557193756\n",
      "iteration 344, dc_loss: 0.021758979186415672, tv_loss: 0.018046794459223747\n",
      "iteration 345, dc_loss: 0.021715719252824783, tv_loss: 0.01804717816412449\n",
      "iteration 346, dc_loss: 0.0216732956469059, tv_loss: 0.018047846853733063\n",
      "iteration 347, dc_loss: 0.021631693467497826, tv_loss: 0.018048277124762535\n",
      "iteration 348, dc_loss: 0.02159089408814907, tv_loss: 0.018048632889986038\n",
      "iteration 349, dc_loss: 0.021550850942730904, tv_loss: 0.018048979341983795\n",
      "iteration 350, dc_loss: 0.021511591970920563, tv_loss: 0.018049735575914383\n",
      "iteration 351, dc_loss: 0.021473079919815063, tv_loss: 0.018050190061330795\n",
      "iteration 352, dc_loss: 0.021435311064124107, tv_loss: 0.018050028011202812\n",
      "iteration 353, dc_loss: 0.021398236975073814, tv_loss: 0.01805044710636139\n",
      "iteration 354, dc_loss: 0.021361825987696648, tv_loss: 0.01805105060338974\n",
      "iteration 355, dc_loss: 0.021326085552573204, tv_loss: 0.01805093325674534\n",
      "iteration 356, dc_loss: 0.021291039884090424, tv_loss: 0.01805117167532444\n",
      "iteration 357, dc_loss: 0.021256668493151665, tv_loss: 0.01805146038532257\n",
      "iteration 358, dc_loss: 0.021222922950983047, tv_loss: 0.018051519989967346\n",
      "iteration 359, dc_loss: 0.02118980512022972, tv_loss: 0.01805138774216175\n",
      "iteration 360, dc_loss: 0.021157311275601387, tv_loss: 0.018051456660032272\n",
      "iteration 361, dc_loss: 0.02112543024122715, tv_loss: 0.018051721155643463\n",
      "iteration 362, dc_loss: 0.021094102412462234, tv_loss: 0.018051601946353912\n",
      "iteration 363, dc_loss: 0.021063337102532387, tv_loss: 0.018051471561193466\n",
      "iteration 364, dc_loss: 0.02103312872350216, tv_loss: 0.018051445484161377\n",
      "iteration 365, dc_loss: 0.0210034791380167, tv_loss: 0.01805158518254757\n",
      "iteration 366, dc_loss: 0.02097434177994728, tv_loss: 0.018051614984869957\n",
      "iteration 367, dc_loss: 0.0209457129240036, tv_loss: 0.018051495775580406\n",
      "iteration 368, dc_loss: 0.020917600020766258, tv_loss: 0.018050970509648323\n",
      "iteration 369, dc_loss: 0.0208900049328804, tv_loss: 0.018051326274871826\n",
      "iteration 370, dc_loss: 0.020862922072410583, tv_loss: 0.01805131509900093\n",
      "iteration 371, dc_loss: 0.020836327224969864, tv_loss: 0.018050935119390488\n",
      "iteration 372, dc_loss: 0.020810173824429512, tv_loss: 0.018050843849778175\n",
      "iteration 373, dc_loss: 0.020784463733434677, tv_loss: 0.018050801008939743\n",
      "iteration 374, dc_loss: 0.020759189501404762, tv_loss: 0.01805068738758564\n",
      "iteration 375, dc_loss: 0.020734377205371857, tv_loss: 0.018049906939268112\n",
      "iteration 376, dc_loss: 0.02070998027920723, tv_loss: 0.018049804493784904\n",
      "iteration 377, dc_loss: 0.020685994997620583, tv_loss: 0.018049802631139755\n",
      "iteration 378, dc_loss: 0.020662425085902214, tv_loss: 0.018049689009785652\n",
      "iteration 379, dc_loss: 0.020639298483729362, tv_loss: 0.01804913394153118\n",
      "iteration 380, dc_loss: 0.020616574212908745, tv_loss: 0.018048442900180817\n",
      "iteration 381, dc_loss: 0.020594237372279167, tv_loss: 0.018048355355858803\n",
      "iteration 382, dc_loss: 0.02057228796184063, tv_loss: 0.018047863617539406\n",
      "iteration 383, dc_loss: 0.020550696179270744, tv_loss: 0.018047360703349113\n",
      "iteration 384, dc_loss: 0.020529456436634064, tv_loss: 0.01804693602025509\n",
      "iteration 385, dc_loss: 0.02050856128334999, tv_loss: 0.018046284094452858\n",
      "iteration 386, dc_loss: 0.020487988367676735, tv_loss: 0.018045704811811447\n",
      "iteration 387, dc_loss: 0.02046775631606579, tv_loss: 0.018045298755168915\n",
      "iteration 388, dc_loss: 0.020447861403226852, tv_loss: 0.01804528757929802\n",
      "iteration 389, dc_loss: 0.020428301766514778, tv_loss: 0.01804475300014019\n",
      "iteration 390, dc_loss: 0.02040906436741352, tv_loss: 0.01804416999220848\n",
      "iteration 391, dc_loss: 0.020390141755342484, tv_loss: 0.018043475225567818\n",
      "iteration 392, dc_loss: 0.02037154696881771, tv_loss: 0.018042657524347305\n",
      "iteration 393, dc_loss: 0.020353268831968307, tv_loss: 0.01804191991686821\n",
      "iteration 394, dc_loss: 0.020335283130407333, tv_loss: 0.01804129034280777\n",
      "iteration 395, dc_loss: 0.02031758986413479, tv_loss: 0.018040578812360764\n",
      "iteration 396, dc_loss: 0.020300159230828285, tv_loss: 0.018039608374238014\n",
      "iteration 397, dc_loss: 0.020282981917262077, tv_loss: 0.018039075657725334\n",
      "iteration 398, dc_loss: 0.02026607282459736, tv_loss: 0.018038544803857803\n",
      "iteration 399, dc_loss: 0.020249424502253532, tv_loss: 0.01803777925670147\n",
      "iteration 400, dc_loss: 0.020233051851391792, tv_loss: 0.01803681254386902\n",
      "iteration 401, dc_loss: 0.020216915756464005, tv_loss: 0.018036160618066788\n",
      "iteration 402, dc_loss: 0.02020101435482502, tv_loss: 0.018035277724266052\n",
      "iteration 403, dc_loss: 0.020185377448797226, tv_loss: 0.01803453080356121\n",
      "iteration 404, dc_loss: 0.02016996406018734, tv_loss: 0.018033819273114204\n",
      "iteration 405, dc_loss: 0.020154796540737152, tv_loss: 0.018033083528280258\n",
      "iteration 406, dc_loss: 0.020139874890446663, tv_loss: 0.01803247258067131\n",
      "iteration 407, dc_loss: 0.020125197246670723, tv_loss: 0.0180315300822258\n",
      "iteration 408, dc_loss: 0.02011074312031269, tv_loss: 0.018030434846878052\n",
      "iteration 409, dc_loss: 0.02009649947285652, tv_loss: 0.018029585480690002\n",
      "iteration 410, dc_loss: 0.020082445815205574, tv_loss: 0.01802905648946762\n",
      "iteration 411, dc_loss: 0.020068567246198654, tv_loss: 0.01802821084856987\n",
      "iteration 412, dc_loss: 0.0200548954308033, tv_loss: 0.018027186393737793\n",
      "iteration 413, dc_loss: 0.020041435956954956, tv_loss: 0.018026096746325493\n",
      "iteration 414, dc_loss: 0.02002818137407303, tv_loss: 0.01802525483071804\n",
      "iteration 415, dc_loss: 0.020015092566609383, tv_loss: 0.018024494871497154\n",
      "iteration 416, dc_loss: 0.020002171397209167, tv_loss: 0.01802380196750164\n",
      "iteration 417, dc_loss: 0.01998944580554962, tv_loss: 0.018022865056991577\n",
      "iteration 418, dc_loss: 0.019976947456598282, tv_loss: 0.018021948635578156\n",
      "iteration 419, dc_loss: 0.01996462605893612, tv_loss: 0.018020974472165108\n",
      "iteration 420, dc_loss: 0.019952481612563133, tv_loss: 0.018020087853074074\n",
      "iteration 421, dc_loss: 0.01994050294160843, tv_loss: 0.018019063398241997\n",
      "iteration 422, dc_loss: 0.019928667694330215, tv_loss: 0.018018247559666634\n",
      "iteration 423, dc_loss: 0.01991700939834118, tv_loss: 0.018017183989286423\n",
      "iteration 424, dc_loss: 0.019905520603060722, tv_loss: 0.01801631599664688\n",
      "iteration 425, dc_loss: 0.019894186407327652, tv_loss: 0.01801523193717003\n",
      "iteration 426, dc_loss: 0.019882971420884132, tv_loss: 0.018014222383499146\n",
      "iteration 427, dc_loss: 0.019871892407536507, tv_loss: 0.018013132736086845\n",
      "iteration 428, dc_loss: 0.01986098103225231, tv_loss: 0.0180122759193182\n",
      "iteration 429, dc_loss: 0.019850213080644608, tv_loss: 0.018011268228292465\n",
      "iteration 430, dc_loss: 0.019839614629745483, tv_loss: 0.018009953200817108\n",
      "iteration 431, dc_loss: 0.019829148426651955, tv_loss: 0.018008997663855553\n",
      "iteration 432, dc_loss: 0.01981879025697708, tv_loss: 0.01800822839140892\n",
      "iteration 433, dc_loss: 0.019808558747172356, tv_loss: 0.018006693571805954\n",
      "iteration 434, dc_loss: 0.019798453897237778, tv_loss: 0.018005577847361565\n",
      "iteration 435, dc_loss: 0.01978851482272148, tv_loss: 0.01800471916794777\n",
      "iteration 436, dc_loss: 0.019778693094849586, tv_loss: 0.01800377294421196\n",
      "iteration 437, dc_loss: 0.019768984988331795, tv_loss: 0.018002653494477272\n",
      "iteration 438, dc_loss: 0.01975940354168415, tv_loss: 0.018001137301325798\n",
      "iteration 439, dc_loss: 0.019749987870454788, tv_loss: 0.017999911680817604\n",
      "iteration 440, dc_loss: 0.019740674644708633, tv_loss: 0.01799916848540306\n",
      "iteration 441, dc_loss: 0.019731435924768448, tv_loss: 0.017998021095991135\n",
      "iteration 442, dc_loss: 0.019722307100892067, tv_loss: 0.017996402457356453\n",
      "iteration 443, dc_loss: 0.019713306799530983, tv_loss: 0.017995357513427734\n",
      "iteration 444, dc_loss: 0.019704438745975494, tv_loss: 0.01799454353749752\n",
      "iteration 445, dc_loss: 0.019695661962032318, tv_loss: 0.017993010580539703\n",
      "iteration 446, dc_loss: 0.0196869857609272, tv_loss: 0.017991606146097183\n",
      "iteration 447, dc_loss: 0.01967841386795044, tv_loss: 0.017990587279200554\n",
      "iteration 448, dc_loss: 0.019669929519295692, tv_loss: 0.017989447340369225\n",
      "iteration 449, dc_loss: 0.0196615532040596, tv_loss: 0.017988353967666626\n",
      "iteration 450, dc_loss: 0.019653301686048508, tv_loss: 0.017987288534641266\n",
      "iteration 451, dc_loss: 0.019645145162940025, tv_loss: 0.01798591949045658\n",
      "iteration 452, dc_loss: 0.01963704638183117, tv_loss: 0.017984598875045776\n",
      "iteration 453, dc_loss: 0.019629046320915222, tv_loss: 0.01798354648053646\n",
      "iteration 454, dc_loss: 0.019621144980192184, tv_loss: 0.017982538789510727\n",
      "iteration 455, dc_loss: 0.01961333490908146, tv_loss: 0.01798091270029545\n",
      "iteration 456, dc_loss: 0.019605619832873344, tv_loss: 0.017979828640818596\n",
      "iteration 457, dc_loss: 0.019597971811890602, tv_loss: 0.017978470772504807\n",
      "iteration 458, dc_loss: 0.019590413197875023, tv_loss: 0.017977530136704445\n",
      "iteration 459, dc_loss: 0.019582925364375114, tv_loss: 0.017976155504584312\n",
      "iteration 460, dc_loss: 0.01957554556429386, tv_loss: 0.017974665388464928\n",
      "iteration 461, dc_loss: 0.019568243995308876, tv_loss: 0.017973540350794792\n",
      "iteration 462, dc_loss: 0.01956101879477501, tv_loss: 0.017972394824028015\n",
      "iteration 463, dc_loss: 0.01955387368798256, tv_loss: 0.017970863729715347\n",
      "iteration 464, dc_loss: 0.019546817988157272, tv_loss: 0.01796942763030529\n",
      "iteration 465, dc_loss: 0.01953984424471855, tv_loss: 0.01796850375831127\n",
      "iteration 466, dc_loss: 0.0195329412817955, tv_loss: 0.017967047169804573\n",
      "iteration 467, dc_loss: 0.019526120275259018, tv_loss: 0.01796555332839489\n",
      "iteration 468, dc_loss: 0.019519345834851265, tv_loss: 0.017964303493499756\n",
      "iteration 469, dc_loss: 0.019512616097927094, tv_loss: 0.017963146790862083\n",
      "iteration 470, dc_loss: 0.019505983218550682, tv_loss: 0.017961746081709862\n",
      "iteration 471, dc_loss: 0.019499411806464195, tv_loss: 0.0179604459553957\n",
      "iteration 472, dc_loss: 0.019492898136377335, tv_loss: 0.01795959286391735\n",
      "iteration 473, dc_loss: 0.019486473873257637, tv_loss: 0.01795801892876625\n",
      "iteration 474, dc_loss: 0.019480135291814804, tv_loss: 0.017956577241420746\n",
      "iteration 475, dc_loss: 0.019473841413855553, tv_loss: 0.01795540750026703\n",
      "iteration 476, dc_loss: 0.01946759968996048, tv_loss: 0.017954301089048386\n",
      "iteration 477, dc_loss: 0.01946142502129078, tv_loss: 0.017952879890799522\n",
      "iteration 478, dc_loss: 0.019455304369330406, tv_loss: 0.017951346933841705\n",
      "iteration 479, dc_loss: 0.019449228420853615, tv_loss: 0.017949813976883888\n",
      "iteration 480, dc_loss: 0.019443193450570107, tv_loss: 0.017948929220438004\n",
      "iteration 481, dc_loss: 0.01943722926080227, tv_loss: 0.017947783693671227\n",
      "iteration 482, dc_loss: 0.019431360065937042, tv_loss: 0.0179461520165205\n",
      "iteration 483, dc_loss: 0.019425557926297188, tv_loss: 0.017944900318980217\n",
      "iteration 484, dc_loss: 0.01941981166601181, tv_loss: 0.0179437343031168\n",
      "iteration 485, dc_loss: 0.01941409334540367, tv_loss: 0.017942307516932487\n",
      "iteration 486, dc_loss: 0.019408395513892174, tv_loss: 0.017941055819392204\n",
      "iteration 487, dc_loss: 0.01940274052321911, tv_loss: 0.017939681187272072\n",
      "iteration 488, dc_loss: 0.019397174939513206, tv_loss: 0.01793835684657097\n",
      "iteration 489, dc_loss: 0.01939169131219387, tv_loss: 0.017936788499355316\n",
      "iteration 490, dc_loss: 0.01938628777861595, tv_loss: 0.01793557032942772\n",
      "iteration 491, dc_loss: 0.019380943849682808, tv_loss: 0.017934054136276245\n",
      "iteration 492, dc_loss: 0.0193756315857172, tv_loss: 0.017932601273059845\n",
      "iteration 493, dc_loss: 0.01937035471200943, tv_loss: 0.01793125830590725\n",
      "iteration 494, dc_loss: 0.01936511881649494, tv_loss: 0.017929809167981148\n",
      "iteration 495, dc_loss: 0.01935991458594799, tv_loss: 0.01792820356786251\n",
      "iteration 496, dc_loss: 0.019354727119207382, tv_loss: 0.017926964908838272\n",
      "iteration 497, dc_loss: 0.01934957131743431, tv_loss: 0.017925694584846497\n",
      "iteration 498, dc_loss: 0.019344499334692955, tv_loss: 0.017924338579177856\n",
      "iteration 499, dc_loss: 0.01933945342898369, tv_loss: 0.01792287267744541\n",
      "iteration 500, dc_loss: 0.0193344634026289, tv_loss: 0.017921509221196175\n",
      "iteration 501, dc_loss: 0.01932954043149948, tv_loss: 0.01792031340301037\n",
      "iteration 502, dc_loss: 0.019324632361531258, tv_loss: 0.017918657511472702\n",
      "iteration 503, dc_loss: 0.01931975781917572, tv_loss: 0.017917245626449585\n",
      "iteration 504, dc_loss: 0.019314952194690704, tv_loss: 0.017916103824973106\n",
      "iteration 505, dc_loss: 0.01931018941104412, tv_loss: 0.017914609983563423\n",
      "iteration 506, dc_loss: 0.019305476918816566, tv_loss: 0.017913077026605606\n",
      "iteration 507, dc_loss: 0.019300783053040504, tv_loss: 0.017911961302161217\n",
      "iteration 508, dc_loss: 0.01929612271487713, tv_loss: 0.0179106667637825\n",
      "iteration 509, dc_loss: 0.019291497766971588, tv_loss: 0.017909228801727295\n",
      "iteration 510, dc_loss: 0.019286949187517166, tv_loss: 0.017907746136188507\n",
      "iteration 511, dc_loss: 0.019282439723610878, tv_loss: 0.017906351014971733\n",
      "iteration 512, dc_loss: 0.019277947023510933, tv_loss: 0.017904965206980705\n",
      "iteration 513, dc_loss: 0.01927347667515278, tv_loss: 0.017903584986925125\n",
      "iteration 514, dc_loss: 0.019269047304987907, tv_loss: 0.017902079969644547\n",
      "iteration 515, dc_loss: 0.019264649599790573, tv_loss: 0.017900945618748665\n",
      "iteration 516, dc_loss: 0.019260261207818985, tv_loss: 0.017899388447403908\n",
      "iteration 517, dc_loss: 0.019255923107266426, tv_loss: 0.017897987738251686\n",
      "iteration 518, dc_loss: 0.019251624122262, tv_loss: 0.01789671555161476\n",
      "iteration 519, dc_loss: 0.01924736425280571, tv_loss: 0.01789533533155918\n",
      "iteration 520, dc_loss: 0.019243139773607254, tv_loss: 0.017894096672534943\n",
      "iteration 521, dc_loss: 0.019238945096731186, tv_loss: 0.017892684787511826\n",
      "iteration 522, dc_loss: 0.0192347913980484, tv_loss: 0.017891203984618187\n",
      "iteration 523, dc_loss: 0.0192306749522686, tv_loss: 0.017889635637402534\n",
      "iteration 524, dc_loss: 0.019226595759391785, tv_loss: 0.01788831129670143\n",
      "iteration 525, dc_loss: 0.01922256126999855, tv_loss: 0.017887117341160774\n",
      "iteration 526, dc_loss: 0.019218547269701958, tv_loss: 0.017885636538267136\n",
      "iteration 527, dc_loss: 0.019214551895856857, tv_loss: 0.017884185537695885\n",
      "iteration 528, dc_loss: 0.01921057142317295, tv_loss: 0.017882905900478363\n",
      "iteration 529, dc_loss: 0.019206615164875984, tv_loss: 0.017881475389003754\n",
      "iteration 530, dc_loss: 0.019202690571546555, tv_loss: 0.017880121245980263\n",
      "iteration 531, dc_loss: 0.019198806956410408, tv_loss: 0.01787864789366722\n",
      "iteration 532, dc_loss: 0.019194986671209335, tv_loss: 0.01787714846432209\n",
      "iteration 533, dc_loss: 0.019191186875104904, tv_loss: 0.017875703051686287\n",
      "iteration 534, dc_loss: 0.019187409430742264, tv_loss: 0.01787444017827511\n",
      "iteration 535, dc_loss: 0.019183635711669922, tv_loss: 0.017873210832476616\n",
      "iteration 536, dc_loss: 0.01917991228401661, tv_loss: 0.0178719162940979\n",
      "iteration 537, dc_loss: 0.019176214933395386, tv_loss: 0.01787060685455799\n",
      "iteration 538, dc_loss: 0.019172485917806625, tv_loss: 0.01786910742521286\n",
      "iteration 539, dc_loss: 0.01916876621544361, tv_loss: 0.01786777377128601\n",
      "iteration 540, dc_loss: 0.019165074452757835, tv_loss: 0.017866598442196846\n",
      "iteration 541, dc_loss: 0.01916145533323288, tv_loss: 0.01786521077156067\n",
      "iteration 542, dc_loss: 0.01915789395570755, tv_loss: 0.017863580957055092\n",
      "iteration 543, dc_loss: 0.019154366105794907, tv_loss: 0.0178624726831913\n",
      "iteration 544, dc_loss: 0.019150875508785248, tv_loss: 0.01786101795732975\n",
      "iteration 545, dc_loss: 0.01914736069738865, tv_loss: 0.01785941794514656\n",
      "iteration 546, dc_loss: 0.019143879413604736, tv_loss: 0.017858056351542473\n",
      "iteration 547, dc_loss: 0.019140424206852913, tv_loss: 0.017856869846582413\n",
      "iteration 548, dc_loss: 0.019136980175971985, tv_loss: 0.017855318263173103\n",
      "iteration 549, dc_loss: 0.019133541733026505, tv_loss: 0.01785404048860073\n",
      "iteration 550, dc_loss: 0.019130121916532516, tv_loss: 0.017852718010544777\n",
      "iteration 551, dc_loss: 0.019126754254102707, tv_loss: 0.01785118505358696\n",
      "iteration 552, dc_loss: 0.019123423844575882, tv_loss: 0.017849743366241455\n",
      "iteration 553, dc_loss: 0.019120099022984505, tv_loss: 0.017848709598183632\n",
      "iteration 554, dc_loss: 0.01911678910255432, tv_loss: 0.01784733682870865\n",
      "iteration 555, dc_loss: 0.019113503396511078, tv_loss: 0.017845910042524338\n",
      "iteration 556, dc_loss: 0.019110234454274178, tv_loss: 0.01784435659646988\n",
      "iteration 557, dc_loss: 0.019107000902295113, tv_loss: 0.017843155190348625\n",
      "iteration 558, dc_loss: 0.01910381019115448, tv_loss: 0.01784162037074566\n",
      "iteration 559, dc_loss: 0.019100641831755638, tv_loss: 0.01784031093120575\n",
      "iteration 560, dc_loss: 0.01909750886261463, tv_loss: 0.01783888228237629\n",
      "iteration 561, dc_loss: 0.019094381481409073, tv_loss: 0.017837345600128174\n",
      "iteration 562, dc_loss: 0.01909126341342926, tv_loss: 0.017836133018136024\n",
      "iteration 563, dc_loss: 0.019088156521320343, tv_loss: 0.017834899947047234\n",
      "iteration 564, dc_loss: 0.01908506453037262, tv_loss: 0.017833471298217773\n",
      "iteration 565, dc_loss: 0.01908198557794094, tv_loss: 0.017832091078162193\n",
      "iteration 566, dc_loss: 0.019078930839896202, tv_loss: 0.01783076673746109\n",
      "iteration 567, dc_loss: 0.019075922667980194, tv_loss: 0.01782951131463051\n",
      "iteration 568, dc_loss: 0.019072959199547768, tv_loss: 0.017827991396188736\n",
      "iteration 569, dc_loss: 0.019069984555244446, tv_loss: 0.017826590687036514\n",
      "iteration 570, dc_loss: 0.019066987559199333, tv_loss: 0.017825337126851082\n",
      "iteration 571, dc_loss: 0.019064005464315414, tv_loss: 0.017824247479438782\n",
      "iteration 572, dc_loss: 0.01906103827059269, tv_loss: 0.01782277598977089\n",
      "iteration 573, dc_loss: 0.01905813068151474, tv_loss: 0.01782136783003807\n",
      "iteration 574, dc_loss: 0.01905527152121067, tv_loss: 0.01782028190791607\n",
      "iteration 575, dc_loss: 0.019052444025874138, tv_loss: 0.01781902089715004\n",
      "iteration 576, dc_loss: 0.019049612805247307, tv_loss: 0.017817480489611626\n",
      "iteration 577, dc_loss: 0.019046787172555923, tv_loss: 0.017816273495554924\n",
      "iteration 578, dc_loss: 0.019043972715735435, tv_loss: 0.017815055325627327\n",
      "iteration 579, dc_loss: 0.01904115453362465, tv_loss: 0.01781361736357212\n",
      "iteration 580, dc_loss: 0.019038360565900803, tv_loss: 0.01781236194074154\n",
      "iteration 581, dc_loss: 0.0190355833619833, tv_loss: 0.017811177298426628\n",
      "iteration 582, dc_loss: 0.019032813608646393, tv_loss: 0.017809715121984482\n",
      "iteration 583, dc_loss: 0.01903008483350277, tv_loss: 0.017808448523283005\n",
      "iteration 584, dc_loss: 0.019027363508939743, tv_loss: 0.017807025462388992\n",
      "iteration 585, dc_loss: 0.019024647772312164, tv_loss: 0.017805559560656548\n",
      "iteration 586, dc_loss: 0.019021963700652122, tv_loss: 0.017804397270083427\n",
      "iteration 587, dc_loss: 0.01901932805776596, tv_loss: 0.017803417518734932\n",
      "iteration 588, dc_loss: 0.0190166924148798, tv_loss: 0.017802096903324127\n",
      "iteration 589, dc_loss: 0.019014041870832443, tv_loss: 0.01780078560113907\n",
      "iteration 590, dc_loss: 0.019011439755558968, tv_loss: 0.017799261957406998\n",
      "iteration 591, dc_loss: 0.019008878618478775, tv_loss: 0.017797941341996193\n",
      "iteration 592, dc_loss: 0.019006317481398582, tv_loss: 0.017796715721488\n",
      "iteration 593, dc_loss: 0.019003788009285927, tv_loss: 0.017795391380786896\n",
      "iteration 594, dc_loss: 0.019001254811882973, tv_loss: 0.017794132232666016\n",
      "iteration 595, dc_loss: 0.018998725339770317, tv_loss: 0.0177929550409317\n",
      "iteration 596, dc_loss: 0.01899617351591587, tv_loss: 0.01779155433177948\n",
      "iteration 597, dc_loss: 0.018993644043803215, tv_loss: 0.01779043860733509\n",
      "iteration 598, dc_loss: 0.018991146236658096, tv_loss: 0.017788974568247795\n",
      "iteration 599, dc_loss: 0.018988672643899918, tv_loss: 0.017787626013159752\n",
      "iteration 600, dc_loss: 0.018986204639077187, tv_loss: 0.017786523327231407\n",
      "iteration 601, dc_loss: 0.0189837496727705, tv_loss: 0.01778515800833702\n",
      "iteration 602, dc_loss: 0.018981315195560455, tv_loss: 0.017783865332603455\n",
      "iteration 603, dc_loss: 0.018978876993060112, tv_loss: 0.017782708629965782\n",
      "iteration 604, dc_loss: 0.018976453691720963, tv_loss: 0.017781320959329605\n",
      "iteration 605, dc_loss: 0.018974050879478455, tv_loss: 0.017779966816306114\n",
      "iteration 606, dc_loss: 0.018971676006913185, tv_loss: 0.017778784036636353\n",
      "iteration 607, dc_loss: 0.01896933652460575, tv_loss: 0.017777396366000175\n",
      "iteration 608, dc_loss: 0.018967019394040108, tv_loss: 0.017776183784008026\n",
      "iteration 609, dc_loss: 0.01896470971405506, tv_loss: 0.01777491346001625\n",
      "iteration 610, dc_loss: 0.018962416797876358, tv_loss: 0.017773449420928955\n",
      "iteration 611, dc_loss: 0.018960122019052505, tv_loss: 0.017772167921066284\n",
      "iteration 612, dc_loss: 0.01895781420171261, tv_loss: 0.017770906910300255\n",
      "iteration 613, dc_loss: 0.018955541774630547, tv_loss: 0.01776980049908161\n",
      "iteration 614, dc_loss: 0.01895328238606453, tv_loss: 0.01776844635605812\n",
      "iteration 615, dc_loss: 0.018951011821627617, tv_loss: 0.017767155542969704\n",
      "iteration 616, dc_loss: 0.018948767334222794, tv_loss: 0.017766131088137627\n",
      "iteration 617, dc_loss: 0.018946543335914612, tv_loss: 0.017764803022146225\n",
      "iteration 618, dc_loss: 0.018944330513477325, tv_loss: 0.017763568088412285\n",
      "iteration 619, dc_loss: 0.018942127004265785, tv_loss: 0.017762545496225357\n",
      "iteration 620, dc_loss: 0.018939929082989693, tv_loss: 0.01776127703487873\n",
      "iteration 621, dc_loss: 0.018937744200229645, tv_loss: 0.01775982789695263\n",
      "iteration 622, dc_loss: 0.018935563042759895, tv_loss: 0.017758743837475777\n",
      "iteration 623, dc_loss: 0.018933432176709175, tv_loss: 0.017757592722773552\n",
      "iteration 624, dc_loss: 0.01893134042620659, tv_loss: 0.017756225541234016\n",
      "iteration 625, dc_loss: 0.018929269164800644, tv_loss: 0.017754802480340004\n",
      "iteration 626, dc_loss: 0.01892717555165291, tv_loss: 0.017753468826413155\n",
      "iteration 627, dc_loss: 0.018925070762634277, tv_loss: 0.01775248534977436\n",
      "iteration 628, dc_loss: 0.018922971561551094, tv_loss: 0.017751233652234077\n",
      "iteration 629, dc_loss: 0.018920863047242165, tv_loss: 0.017749976366758347\n",
      "iteration 630, dc_loss: 0.01891876943409443, tv_loss: 0.017748938873410225\n",
      "iteration 631, dc_loss: 0.018916690722107887, tv_loss: 0.017747826874256134\n",
      "iteration 632, dc_loss: 0.018914656713604927, tv_loss: 0.017746560275554657\n",
      "iteration 633, dc_loss: 0.018912622705101967, tv_loss: 0.01774519309401512\n",
      "iteration 634, dc_loss: 0.018910624086856842, tv_loss: 0.01774408482015133\n",
      "iteration 635, dc_loss: 0.018908647820353508, tv_loss: 0.017742959782481194\n",
      "iteration 636, dc_loss: 0.018906649202108383, tv_loss: 0.017741648480296135\n",
      "iteration 637, dc_loss: 0.018904639407992363, tv_loss: 0.01774037629365921\n",
      "iteration 638, dc_loss: 0.018902642652392387, tv_loss: 0.01773909293115139\n",
      "iteration 639, dc_loss: 0.018900660797953606, tv_loss: 0.017738187685608864\n",
      "iteration 640, dc_loss: 0.018898697569966316, tv_loss: 0.01773711107671261\n",
      "iteration 641, dc_loss: 0.018896762281656265, tv_loss: 0.01773560233414173\n",
      "iteration 642, dc_loss: 0.01889483444392681, tv_loss: 0.017734263092279434\n",
      "iteration 643, dc_loss: 0.0188929233700037, tv_loss: 0.017733100801706314\n",
      "iteration 644, dc_loss: 0.018891002982854843, tv_loss: 0.01773213781416416\n",
      "iteration 645, dc_loss: 0.018889112398028374, tv_loss: 0.01773086003959179\n",
      "iteration 646, dc_loss: 0.01888720504939556, tv_loss: 0.017729412764310837\n",
      "iteration 647, dc_loss: 0.018885310739278793, tv_loss: 0.017728326842188835\n",
      "iteration 648, dc_loss: 0.018883422017097473, tv_loss: 0.01772727258503437\n",
      "iteration 649, dc_loss: 0.01888158731162548, tv_loss: 0.017725948244333267\n",
      "iteration 650, dc_loss: 0.018879765644669533, tv_loss: 0.017724789679050446\n",
      "iteration 651, dc_loss: 0.01887795701622963, tv_loss: 0.017723698168992996\n",
      "iteration 652, dc_loss: 0.018876155838370323, tv_loss: 0.01772231236100197\n",
      "iteration 653, dc_loss: 0.018874334171414375, tv_loss: 0.01772112026810646\n",
      "iteration 654, dc_loss: 0.0188724584877491, tv_loss: 0.017719978466629982\n",
      "iteration 655, dc_loss: 0.018870577216148376, tv_loss: 0.017718873918056488\n",
      "iteration 656, dc_loss: 0.018868716433644295, tv_loss: 0.01771761290729046\n",
      "iteration 657, dc_loss: 0.01886686310172081, tv_loss: 0.0177166685461998\n",
      "iteration 658, dc_loss: 0.018865041434764862, tv_loss: 0.01771552488207817\n",
      "iteration 659, dc_loss: 0.01886325515806675, tv_loss: 0.017714213579893112\n",
      "iteration 660, dc_loss: 0.01886153221130371, tv_loss: 0.01771312952041626\n",
      "iteration 661, dc_loss: 0.018859853968024254, tv_loss: 0.01771203987300396\n",
      "iteration 662, dc_loss: 0.018858186900615692, tv_loss: 0.017710600048303604\n",
      "iteration 663, dc_loss: 0.018856510519981384, tv_loss: 0.01770942099392414\n",
      "iteration 664, dc_loss: 0.01885479874908924, tv_loss: 0.017708292230963707\n",
      "iteration 665, dc_loss: 0.01885305531322956, tv_loss: 0.0177073422819376\n",
      "iteration 666, dc_loss: 0.01885133795440197, tv_loss: 0.01770610921084881\n",
      "iteration 667, dc_loss: 0.018849628046154976, tv_loss: 0.017704809084534645\n",
      "iteration 668, dc_loss: 0.018847934901714325, tv_loss: 0.017703715711832047\n",
      "iteration 669, dc_loss: 0.018846236169338226, tv_loss: 0.017702827230095863\n",
      "iteration 670, dc_loss: 0.01884453371167183, tv_loss: 0.017701687291264534\n",
      "iteration 671, dc_loss: 0.018842829391360283, tv_loss: 0.017700497061014175\n",
      "iteration 672, dc_loss: 0.018841149285435677, tv_loss: 0.017699213698506355\n",
      "iteration 673, dc_loss: 0.018839504569768906, tv_loss: 0.017698226496577263\n",
      "iteration 674, dc_loss: 0.01883787289261818, tv_loss: 0.01769714243710041\n",
      "iteration 675, dc_loss: 0.01883624866604805, tv_loss: 0.017695702612400055\n",
      "iteration 676, dc_loss: 0.018834691494703293, tv_loss: 0.017694562673568726\n",
      "iteration 677, dc_loss: 0.018833130598068237, tv_loss: 0.017693454399704933\n",
      "iteration 678, dc_loss: 0.018831543624401093, tv_loss: 0.01769237406551838\n",
      "iteration 679, dc_loss: 0.018829941749572754, tv_loss: 0.017691247165203094\n",
      "iteration 680, dc_loss: 0.018828347325325012, tv_loss: 0.017690198495984077\n",
      "iteration 681, dc_loss: 0.018826765939593315, tv_loss: 0.017688985913991928\n",
      "iteration 682, dc_loss: 0.01882518269121647, tv_loss: 0.017687827348709106\n",
      "iteration 683, dc_loss: 0.018823588266968727, tv_loss: 0.017686890438199043\n",
      "iteration 684, dc_loss: 0.01882200501859188, tv_loss: 0.0176859050989151\n",
      "iteration 685, dc_loss: 0.018820423632860184, tv_loss: 0.017684880644083023\n",
      "iteration 686, dc_loss: 0.018818853422999382, tv_loss: 0.01768379844725132\n",
      "iteration 687, dc_loss: 0.018817303702235222, tv_loss: 0.017682798206806183\n",
      "iteration 688, dc_loss: 0.01881575770676136, tv_loss: 0.017681697383522987\n",
      "iteration 689, dc_loss: 0.018814224749803543, tv_loss: 0.017680609598755836\n",
      "iteration 690, dc_loss: 0.018812714144587517, tv_loss: 0.017679642885923386\n",
      "iteration 691, dc_loss: 0.018811218440532684, tv_loss: 0.017678529024124146\n",
      "iteration 692, dc_loss: 0.018809743225574493, tv_loss: 0.017677241936326027\n",
      "iteration 693, dc_loss: 0.018808238208293915, tv_loss: 0.017676319926977158\n",
      "iteration 694, dc_loss: 0.01880672201514244, tv_loss: 0.017675278708338737\n",
      "iteration 695, dc_loss: 0.01880522258579731, tv_loss: 0.01767425611615181\n",
      "iteration 696, dc_loss: 0.018803782761096954, tv_loss: 0.017673073336482048\n",
      "iteration 697, dc_loss: 0.0188023429363966, tv_loss: 0.01767203025519848\n",
      "iteration 698, dc_loss: 0.01880088448524475, tv_loss: 0.01767091639339924\n",
      "iteration 699, dc_loss: 0.01879943534731865, tv_loss: 0.017669737339019775\n",
      "iteration 700, dc_loss: 0.018798014149069786, tv_loss: 0.017668742686510086\n",
      "iteration 701, dc_loss: 0.018796585500240326, tv_loss: 0.017667770385742188\n",
      "iteration 702, dc_loss: 0.01879514381289482, tv_loss: 0.017666494473814964\n",
      "iteration 703, dc_loss: 0.01879369281232357, tv_loss: 0.017665397375822067\n",
      "iteration 704, dc_loss: 0.018792279064655304, tv_loss: 0.017664436250925064\n",
      "iteration 705, dc_loss: 0.01879085786640644, tv_loss: 0.017663652077317238\n",
      "iteration 706, dc_loss: 0.018789440393447876, tv_loss: 0.017662469297647476\n",
      "iteration 707, dc_loss: 0.018788039684295654, tv_loss: 0.01766119711101055\n",
      "iteration 708, dc_loss: 0.01878667250275612, tv_loss: 0.017659978941082954\n",
      "iteration 709, dc_loss: 0.01878531463444233, tv_loss: 0.017659198492765427\n",
      "iteration 710, dc_loss: 0.01878393068909645, tv_loss: 0.017658105120062828\n",
      "iteration 711, dc_loss: 0.01878254860639572, tv_loss: 0.017656991258263588\n",
      "iteration 712, dc_loss: 0.01878119818866253, tv_loss: 0.017656058073043823\n",
      "iteration 713, dc_loss: 0.018779853358864784, tv_loss: 0.017654910683631897\n",
      "iteration 714, dc_loss: 0.018778471276164055, tv_loss: 0.017653856426477432\n",
      "iteration 715, dc_loss: 0.01877709850668907, tv_loss: 0.017653150483965874\n",
      "iteration 716, dc_loss: 0.018775751814246178, tv_loss: 0.017652131617069244\n",
      "iteration 717, dc_loss: 0.01877439022064209, tv_loss: 0.017651010304689407\n",
      "iteration 718, dc_loss: 0.018773019313812256, tv_loss: 0.017650051042437553\n",
      "iteration 719, dc_loss: 0.018771681934595108, tv_loss: 0.017649061977863312\n",
      "iteration 720, dc_loss: 0.01877039298415184, tv_loss: 0.01764794997870922\n",
      "iteration 721, dc_loss: 0.01876913011074066, tv_loss: 0.017646918073296547\n",
      "iteration 722, dc_loss: 0.018767880275845528, tv_loss: 0.017645901069045067\n",
      "iteration 723, dc_loss: 0.018766609951853752, tv_loss: 0.017644749954342842\n",
      "iteration 724, dc_loss: 0.018765319138765335, tv_loss: 0.01764380745589733\n",
      "iteration 725, dc_loss: 0.018764043226838112, tv_loss: 0.0176427960395813\n",
      "iteration 726, dc_loss: 0.018762750551104546, tv_loss: 0.01764175295829773\n",
      "iteration 727, dc_loss: 0.018761493265628815, tv_loss: 0.017640739679336548\n",
      "iteration 728, dc_loss: 0.018760215491056442, tv_loss: 0.017639759927988052\n",
      "iteration 729, dc_loss: 0.01875893399119377, tv_loss: 0.017638929188251495\n",
      "iteration 730, dc_loss: 0.018757637590169907, tv_loss: 0.017637843266129494\n",
      "iteration 731, dc_loss: 0.018756359815597534, tv_loss: 0.017637044191360474\n",
      "iteration 732, dc_loss: 0.0187551099807024, tv_loss: 0.017636053264141083\n",
      "iteration 733, dc_loss: 0.018753886222839355, tv_loss: 0.017635082826018333\n",
      "iteration 734, dc_loss: 0.018752673640847206, tv_loss: 0.01763402856886387\n",
      "iteration 735, dc_loss: 0.018751446157693863, tv_loss: 0.01763305999338627\n",
      "iteration 736, dc_loss: 0.01875024475157261, tv_loss: 0.017632057890295982\n",
      "iteration 737, dc_loss: 0.018749037757515907, tv_loss: 0.017631124705076218\n",
      "iteration 738, dc_loss: 0.018747858703136444, tv_loss: 0.01763000153005123\n",
      "iteration 739, dc_loss: 0.01874665729701519, tv_loss: 0.01762896217405796\n",
      "iteration 740, dc_loss: 0.018745485693216324, tv_loss: 0.01762799359858036\n",
      "iteration 741, dc_loss: 0.01874431222677231, tv_loss: 0.01762702874839306\n",
      "iteration 742, dc_loss: 0.018743125721812248, tv_loss: 0.017625944688916206\n",
      "iteration 743, dc_loss: 0.018741950392723083, tv_loss: 0.017625002190470695\n",
      "iteration 744, dc_loss: 0.01874077133834362, tv_loss: 0.017624130472540855\n",
      "iteration 745, dc_loss: 0.018739590421319008, tv_loss: 0.017623161897063255\n",
      "iteration 746, dc_loss: 0.018738385289907455, tv_loss: 0.01762225478887558\n",
      "iteration 747, dc_loss: 0.0187371913343668, tv_loss: 0.017621299251914024\n",
      "iteration 748, dc_loss: 0.01873600482940674, tv_loss: 0.01762036792933941\n",
      "iteration 749, dc_loss: 0.018734846264123917, tv_loss: 0.017619382590055466\n",
      "iteration 750, dc_loss: 0.018733710050582886, tv_loss: 0.017618447542190552\n",
      "iteration 751, dc_loss: 0.01873256452381611, tv_loss: 0.017617400735616684\n",
      "iteration 752, dc_loss: 0.018731405958533287, tv_loss: 0.017616769298911095\n",
      "iteration 753, dc_loss: 0.018730226904153824, tv_loss: 0.01761573925614357\n",
      "iteration 754, dc_loss: 0.018729103729128838, tv_loss: 0.01761467754840851\n",
      "iteration 755, dc_loss: 0.018727995455265045, tv_loss: 0.01761382631957531\n",
      "iteration 756, dc_loss: 0.018726913258433342, tv_loss: 0.017612822353839874\n",
      "iteration 757, dc_loss: 0.018725823611021042, tv_loss: 0.017611829563975334\n",
      "iteration 758, dc_loss: 0.01872473582625389, tv_loss: 0.017610915005207062\n",
      "iteration 759, dc_loss: 0.018723662942647934, tv_loss: 0.017609981819987297\n",
      "iteration 760, dc_loss: 0.01872255839407444, tv_loss: 0.01760919578373432\n",
      "iteration 761, dc_loss: 0.018721414729952812, tv_loss: 0.017608273774385452\n",
      "iteration 762, dc_loss: 0.018720285966992378, tv_loss: 0.01760728843510151\n",
      "iteration 763, dc_loss: 0.01871916465461254, tv_loss: 0.01760639250278473\n",
      "iteration 764, dc_loss: 0.018718065693974495, tv_loss: 0.017605450004339218\n",
      "iteration 765, dc_loss: 0.018717007711529732, tv_loss: 0.01760476641356945\n",
      "iteration 766, dc_loss: 0.01871594972908497, tv_loss: 0.01760382205247879\n",
      "iteration 767, dc_loss: 0.018714914098381996, tv_loss: 0.01760275289416313\n",
      "iteration 768, dc_loss: 0.01871389150619507, tv_loss: 0.017601830884814262\n",
      "iteration 769, dc_loss: 0.018712861463427544, tv_loss: 0.017600903287529945\n",
      "iteration 770, dc_loss: 0.01871180348098278, tv_loss: 0.01759990118443966\n",
      "iteration 771, dc_loss: 0.018710743635892868, tv_loss: 0.01759895123541355\n",
      "iteration 772, dc_loss: 0.018709715455770493, tv_loss: 0.017597977072000504\n",
      "iteration 773, dc_loss: 0.018708694726228714, tv_loss: 0.01759706623852253\n",
      "iteration 774, dc_loss: 0.018707692623138428, tv_loss: 0.017596090212464333\n",
      "iteration 775, dc_loss: 0.018706655129790306, tv_loss: 0.017595382407307625\n",
      "iteration 776, dc_loss: 0.01870560087263584, tv_loss: 0.017594413831830025\n",
      "iteration 777, dc_loss: 0.01870455965399742, tv_loss: 0.017593584954738617\n",
      "iteration 778, dc_loss: 0.01870354823768139, tv_loss: 0.017592763528227806\n",
      "iteration 779, dc_loss: 0.018702542409300804, tv_loss: 0.017591888085007668\n",
      "iteration 780, dc_loss: 0.018701551482081413, tv_loss: 0.01759105734527111\n",
      "iteration 781, dc_loss: 0.01870054379105568, tv_loss: 0.017590096220374107\n",
      "iteration 782, dc_loss: 0.01869952492415905, tv_loss: 0.017589272931218147\n",
      "iteration 783, dc_loss: 0.018698539584875107, tv_loss: 0.017588336020708084\n",
      "iteration 784, dc_loss: 0.018697544932365417, tv_loss: 0.01758740097284317\n",
      "iteration 785, dc_loss: 0.018696580082178116, tv_loss: 0.01758667267858982\n",
      "iteration 786, dc_loss: 0.018695609644055367, tv_loss: 0.017585767433047295\n",
      "iteration 787, dc_loss: 0.018694620579481125, tv_loss: 0.017584802582859993\n",
      "iteration 788, dc_loss: 0.018693607300519943, tv_loss: 0.01758410967886448\n",
      "iteration 789, dc_loss: 0.01869262009859085, tv_loss: 0.017583271488547325\n",
      "iteration 790, dc_loss: 0.01869160495698452, tv_loss: 0.017582368105649948\n",
      "iteration 791, dc_loss: 0.01869063824415207, tv_loss: 0.017581474035978317\n",
      "iteration 792, dc_loss: 0.01868971437215805, tv_loss: 0.017580658197402954\n",
      "iteration 793, dc_loss: 0.018688788637518883, tv_loss: 0.01757984049618244\n",
      "iteration 794, dc_loss: 0.01868785172700882, tv_loss: 0.017579076811671257\n",
      "iteration 795, dc_loss: 0.01868690364062786, tv_loss: 0.01757821813225746\n",
      "iteration 796, dc_loss: 0.018685942515730858, tv_loss: 0.0175772774964571\n",
      "iteration 797, dc_loss: 0.01868501491844654, tv_loss: 0.017576396465301514\n",
      "iteration 798, dc_loss: 0.01868407614529133, tv_loss: 0.017575662583112717\n",
      "iteration 799, dc_loss: 0.018683159723877907, tv_loss: 0.017574695870280266\n",
      "iteration 800, dc_loss: 0.01868225447833538, tv_loss: 0.017573824152350426\n",
      "iteration 801, dc_loss: 0.018681365996599197, tv_loss: 0.017572950571775436\n",
      "iteration 802, dc_loss: 0.018680455163121223, tv_loss: 0.017572149634361267\n",
      "iteration 803, dc_loss: 0.018679514527320862, tv_loss: 0.017571251839399338\n",
      "iteration 804, dc_loss: 0.0186785738915205, tv_loss: 0.01757032237946987\n",
      "iteration 805, dc_loss: 0.018677646294236183, tv_loss: 0.017569780349731445\n",
      "iteration 806, dc_loss: 0.018676739186048508, tv_loss: 0.01756902225315571\n",
      "iteration 807, dc_loss: 0.01867581531405449, tv_loss: 0.01756792888045311\n",
      "iteration 808, dc_loss: 0.01867491751909256, tv_loss: 0.01756705529987812\n",
      "iteration 809, dc_loss: 0.01867402344942093, tv_loss: 0.017566567286849022\n",
      "iteration 810, dc_loss: 0.0186731219291687, tv_loss: 0.01756560057401657\n",
      "iteration 811, dc_loss: 0.018672240898013115, tv_loss: 0.017564823850989342\n",
      "iteration 812, dc_loss: 0.018671348690986633, tv_loss: 0.01756407506763935\n",
      "iteration 813, dc_loss: 0.018670450896024704, tv_loss: 0.01756315864622593\n",
      "iteration 814, dc_loss: 0.01866956800222397, tv_loss: 0.0175622571259737\n",
      "iteration 815, dc_loss: 0.018668727949261665, tv_loss: 0.01756146363914013\n",
      "iteration 816, dc_loss: 0.018667876720428467, tv_loss: 0.01756078004837036\n",
      "iteration 817, dc_loss: 0.018666991963982582, tv_loss: 0.01755996234714985\n",
      "iteration 818, dc_loss: 0.01866612397134304, tv_loss: 0.017559058964252472\n",
      "iteration 819, dc_loss: 0.018665283918380737, tv_loss: 0.017558319494128227\n",
      "iteration 820, dc_loss: 0.01866445690393448, tv_loss: 0.01755749247968197\n",
      "iteration 821, dc_loss: 0.01866362988948822, tv_loss: 0.01755666919052601\n",
      "iteration 822, dc_loss: 0.018662801012396812, tv_loss: 0.017555732280015945\n",
      "iteration 823, dc_loss: 0.01866196282207966, tv_loss: 0.01755501888692379\n",
      "iteration 824, dc_loss: 0.01866108924150467, tv_loss: 0.01755422167479992\n",
      "iteration 825, dc_loss: 0.01866021752357483, tv_loss: 0.017553379759192467\n",
      "iteration 826, dc_loss: 0.018659362569451332, tv_loss: 0.017552729696035385\n",
      "iteration 827, dc_loss: 0.018658515065908432, tv_loss: 0.01755191944539547\n",
      "iteration 828, dc_loss: 0.018657684326171875, tv_loss: 0.01755109243094921\n",
      "iteration 829, dc_loss: 0.018656888976693153, tv_loss: 0.017550358548760414\n",
      "iteration 830, dc_loss: 0.018656114116311073, tv_loss: 0.01754945144057274\n",
      "iteration 831, dc_loss: 0.01865537278354168, tv_loss: 0.01754850521683693\n",
      "iteration 832, dc_loss: 0.018654607236385345, tv_loss: 0.017547663301229477\n",
      "iteration 833, dc_loss: 0.01865381933748722, tv_loss: 0.01754702813923359\n",
      "iteration 834, dc_loss: 0.018652988597750664, tv_loss: 0.0175462756305933\n",
      "iteration 835, dc_loss: 0.018652116879820824, tv_loss: 0.017545467242598534\n",
      "iteration 836, dc_loss: 0.018651271238923073, tv_loss: 0.017544977366924286\n",
      "iteration 837, dc_loss: 0.018650459125638008, tv_loss: 0.01754404604434967\n",
      "iteration 838, dc_loss: 0.01864965632557869, tv_loss: 0.017543422058224678\n",
      "iteration 839, dc_loss: 0.018648875877261162, tv_loss: 0.017542824149131775\n",
      "iteration 840, dc_loss: 0.01864810101687908, tv_loss: 0.017542116343975067\n",
      "iteration 841, dc_loss: 0.018647365272045135, tv_loss: 0.01754106394946575\n",
      "iteration 842, dc_loss: 0.018646588549017906, tv_loss: 0.01754033751785755\n",
      "iteration 843, dc_loss: 0.018645770847797394, tv_loss: 0.017539765685796738\n",
      "iteration 844, dc_loss: 0.01864496059715748, tv_loss: 0.017539048567414284\n",
      "iteration 845, dc_loss: 0.01864413544535637, tv_loss: 0.017538169398903847\n",
      "iteration 846, dc_loss: 0.01864330656826496, tv_loss: 0.01753740943968296\n",
      "iteration 847, dc_loss: 0.018642520532011986, tv_loss: 0.017536772415041924\n",
      "iteration 848, dc_loss: 0.01864175871014595, tv_loss: 0.017535962164402008\n",
      "iteration 849, dc_loss: 0.018641021102666855, tv_loss: 0.017535239458084106\n",
      "iteration 850, dc_loss: 0.01864027976989746, tv_loss: 0.01753431372344494\n",
      "iteration 851, dc_loss: 0.01863955333828926, tv_loss: 0.017533591017127037\n",
      "iteration 852, dc_loss: 0.018638823181390762, tv_loss: 0.01753286086022854\n",
      "iteration 853, dc_loss: 0.01863807439804077, tv_loss: 0.017532190307974815\n",
      "iteration 854, dc_loss: 0.018637312576174736, tv_loss: 0.017531342804431915\n",
      "iteration 855, dc_loss: 0.01863654889166355, tv_loss: 0.01753072999417782\n",
      "iteration 856, dc_loss: 0.018635746091604233, tv_loss: 0.017530152574181557\n",
      "iteration 857, dc_loss: 0.018634973093867302, tv_loss: 0.017529457807540894\n",
      "iteration 858, dc_loss: 0.018634231761097908, tv_loss: 0.017528647556900978\n",
      "iteration 859, dc_loss: 0.018633509054780006, tv_loss: 0.017527882009744644\n",
      "iteration 860, dc_loss: 0.018632816150784492, tv_loss: 0.01752716861665249\n",
      "iteration 861, dc_loss: 0.01863214187324047, tv_loss: 0.017526233568787575\n",
      "iteration 862, dc_loss: 0.01863146759569645, tv_loss: 0.017525454983115196\n",
      "iteration 863, dc_loss: 0.018630754202604294, tv_loss: 0.017524825409054756\n",
      "iteration 864, dc_loss: 0.018630003556609154, tv_loss: 0.017524266615509987\n",
      "iteration 865, dc_loss: 0.018629273399710655, tv_loss: 0.017523501068353653\n",
      "iteration 866, dc_loss: 0.01862856186926365, tv_loss: 0.01752258464694023\n",
      "iteration 867, dc_loss: 0.01862785592675209, tv_loss: 0.017521915957331657\n",
      "iteration 868, dc_loss: 0.018627163022756577, tv_loss: 0.017521485686302185\n",
      "iteration 869, dc_loss: 0.018626462668180466, tv_loss: 0.01752074435353279\n",
      "iteration 870, dc_loss: 0.018625730648636818, tv_loss: 0.01751997508108616\n",
      "iteration 871, dc_loss: 0.01862499676644802, tv_loss: 0.01751946471631527\n",
      "iteration 872, dc_loss: 0.01862429827451706, tv_loss: 0.017518633976578712\n",
      "iteration 873, dc_loss: 0.01862361654639244, tv_loss: 0.01751788519322872\n",
      "iteration 874, dc_loss: 0.018622927367687225, tv_loss: 0.01751723699271679\n",
      "iteration 875, dc_loss: 0.01862223818898201, tv_loss: 0.017516501247882843\n",
      "iteration 876, dc_loss: 0.01862156391143799, tv_loss: 0.017515767365694046\n",
      "iteration 877, dc_loss: 0.01862093061208725, tv_loss: 0.017515067011117935\n",
      "iteration 878, dc_loss: 0.018620284274220467, tv_loss: 0.017514389008283615\n",
      "iteration 879, dc_loss: 0.0186196006834507, tv_loss: 0.01751355454325676\n",
      "iteration 880, dc_loss: 0.0186188705265522, tv_loss: 0.017513064667582512\n",
      "iteration 881, dc_loss: 0.018618140369653702, tv_loss: 0.01751246117055416\n",
      "iteration 882, dc_loss: 0.018617430701851845, tv_loss: 0.017511576414108276\n",
      "iteration 883, dc_loss: 0.01861676201224327, tv_loss: 0.017511021345853806\n",
      "iteration 884, dc_loss: 0.018616093322634697, tv_loss: 0.017510540783405304\n",
      "iteration 885, dc_loss: 0.018615443259477615, tv_loss: 0.01750984974205494\n",
      "iteration 886, dc_loss: 0.01861479878425598, tv_loss: 0.017509037628769875\n",
      "iteration 887, dc_loss: 0.018614159896969795, tv_loss: 0.01750807836651802\n",
      "iteration 888, dc_loss: 0.018613528460264206, tv_loss: 0.017507683485746384\n",
      "iteration 889, dc_loss: 0.018612900748848915, tv_loss: 0.01750715635716915\n",
      "iteration 890, dc_loss: 0.018612269312143326, tv_loss: 0.017506103962659836\n",
      "iteration 891, dc_loss: 0.01861165650188923, tv_loss: 0.017505444586277008\n",
      "iteration 892, dc_loss: 0.018611019477248192, tv_loss: 0.017504800111055374\n",
      "iteration 893, dc_loss: 0.01861037313938141, tv_loss: 0.017504321411252022\n",
      "iteration 894, dc_loss: 0.018609696999192238, tv_loss: 0.017503703013062477\n",
      "iteration 895, dc_loss: 0.018609018996357918, tv_loss: 0.01750280149281025\n",
      "iteration 896, dc_loss: 0.018608342856168747, tv_loss: 0.01750227063894272\n",
      "iteration 897, dc_loss: 0.01860770769417286, tv_loss: 0.017501801252365112\n",
      "iteration 898, dc_loss: 0.018607109785079956, tv_loss: 0.017501071095466614\n",
      "iteration 899, dc_loss: 0.018606508150696754, tv_loss: 0.01750016398727894\n",
      "iteration 900, dc_loss: 0.01860589161515236, tv_loss: 0.01749957539141178\n",
      "iteration 901, dc_loss: 0.01860528253018856, tv_loss: 0.017498983070254326\n",
      "iteration 902, dc_loss: 0.018604641780257225, tv_loss: 0.01749824360013008\n",
      "iteration 903, dc_loss: 0.01860400103032589, tv_loss: 0.017497576773166656\n",
      "iteration 904, dc_loss: 0.018603352829813957, tv_loss: 0.017497073858976364\n",
      "iteration 905, dc_loss: 0.01860271394252777, tv_loss: 0.017496474087238312\n",
      "iteration 906, dc_loss: 0.01860211230814457, tv_loss: 0.017495647072792053\n",
      "iteration 907, dc_loss: 0.01860153302550316, tv_loss: 0.017495110630989075\n",
      "iteration 908, dc_loss: 0.01860092766582966, tv_loss: 0.017494678497314453\n",
      "iteration 909, dc_loss: 0.018600329756736755, tv_loss: 0.017493830993771553\n",
      "iteration 910, dc_loss: 0.018599744886159897, tv_loss: 0.017493193969130516\n",
      "iteration 911, dc_loss: 0.0185991358011961, tv_loss: 0.01749255135655403\n",
      "iteration 912, dc_loss: 0.018598532304167747, tv_loss: 0.017491837963461876\n",
      "iteration 913, dc_loss: 0.018597915768623352, tv_loss: 0.01749134436249733\n",
      "iteration 914, dc_loss: 0.018597304821014404, tv_loss: 0.01749071292579174\n",
      "iteration 915, dc_loss: 0.018596667796373367, tv_loss: 0.01748993806540966\n",
      "iteration 916, dc_loss: 0.018596023321151733, tv_loss: 0.017489466816186905\n",
      "iteration 917, dc_loss: 0.018595410510897636, tv_loss: 0.017489034682512283\n",
      "iteration 918, dc_loss: 0.018594827502965927, tv_loss: 0.0174882709980011\n",
      "iteration 919, dc_loss: 0.01859426312148571, tv_loss: 0.017487552016973495\n",
      "iteration 920, dc_loss: 0.01859372854232788, tv_loss: 0.017486946657299995\n",
      "iteration 921, dc_loss: 0.018593188375234604, tv_loss: 0.017486322671175003\n",
      "iteration 922, dc_loss: 0.018592648208141327, tv_loss: 0.017485585063695908\n",
      "iteration 923, dc_loss: 0.01859210804104805, tv_loss: 0.017484867945313454\n",
      "iteration 924, dc_loss: 0.018591567873954773, tv_loss: 0.017484374344348907\n",
      "iteration 925, dc_loss: 0.018591010943055153, tv_loss: 0.017483685165643692\n",
      "iteration 926, dc_loss: 0.01859043911099434, tv_loss: 0.017483171075582504\n",
      "iteration 927, dc_loss: 0.018589865416288376, tv_loss: 0.017482515424489975\n",
      "iteration 928, dc_loss: 0.018589293584227562, tv_loss: 0.0174818467348814\n",
      "iteration 929, dc_loss: 0.018588760867714882, tv_loss: 0.017481235787272453\n",
      "iteration 930, dc_loss: 0.01858821138739586, tv_loss: 0.017480619251728058\n",
      "iteration 931, dc_loss: 0.018587665632367134, tv_loss: 0.017480039969086647\n",
      "iteration 932, dc_loss: 0.018587106838822365, tv_loss: 0.017479294911026955\n",
      "iteration 933, dc_loss: 0.018586544319987297, tv_loss: 0.01747877523303032\n",
      "iteration 934, dc_loss: 0.01858595199882984, tv_loss: 0.01747823879122734\n",
      "iteration 935, dc_loss: 0.01858534850180149, tv_loss: 0.017477720975875854\n",
      "iteration 936, dc_loss: 0.01858477294445038, tv_loss: 0.017477113753557205\n",
      "iteration 937, dc_loss: 0.018584227189421654, tv_loss: 0.01747639663517475\n",
      "iteration 938, dc_loss: 0.01858375407755375, tv_loss: 0.017475727945566177\n",
      "iteration 939, dc_loss: 0.018583262339234352, tv_loss: 0.01747504249215126\n",
      "iteration 940, dc_loss: 0.01858273707330227, tv_loss: 0.017474506050348282\n",
      "iteration 941, dc_loss: 0.018582165241241455, tv_loss: 0.01747404970228672\n",
      "iteration 942, dc_loss: 0.018581604585051537, tv_loss: 0.01747339777648449\n",
      "iteration 943, dc_loss: 0.018581051379442215, tv_loss: 0.0174726489931345\n",
      "iteration 944, dc_loss: 0.01858050748705864, tv_loss: 0.017472509294748306\n",
      "iteration 945, dc_loss: 0.018579963594675064, tv_loss: 0.01747169904410839\n",
      "iteration 946, dc_loss: 0.01857943646609783, tv_loss: 0.017470913007855415\n",
      "iteration 947, dc_loss: 0.018578918650746346, tv_loss: 0.01747061125934124\n",
      "iteration 948, dc_loss: 0.018578438088297844, tv_loss: 0.017469951882958412\n",
      "iteration 949, dc_loss: 0.01857798546552658, tv_loss: 0.017469167709350586\n",
      "iteration 950, dc_loss: 0.018577512353658676, tv_loss: 0.01746865175664425\n",
      "iteration 951, dc_loss: 0.018577011302113533, tv_loss: 0.01746816746890545\n",
      "iteration 952, dc_loss: 0.018576497212052345, tv_loss: 0.017467480152845383\n",
      "iteration 953, dc_loss: 0.018575984984636307, tv_loss: 0.017466872930526733\n",
      "iteration 954, dc_loss: 0.018575429916381836, tv_loss: 0.017466416582465172\n",
      "iteration 955, dc_loss: 0.018574900925159454, tv_loss: 0.017466051504015923\n",
      "iteration 956, dc_loss: 0.018574422225356102, tv_loss: 0.01746544986963272\n",
      "iteration 957, dc_loss: 0.018573930487036705, tv_loss: 0.017464639618992805\n",
      "iteration 958, dc_loss: 0.0185734573751688, tv_loss: 0.01746384985744953\n",
      "iteration 959, dc_loss: 0.018572993576526642, tv_loss: 0.017463495954871178\n",
      "iteration 960, dc_loss: 0.018572501838207245, tv_loss: 0.017463063821196556\n",
      "iteration 961, dc_loss: 0.018571989610791206, tv_loss: 0.01746239699423313\n",
      "iteration 962, dc_loss: 0.01857149787247181, tv_loss: 0.01746182329952717\n",
      "iteration 963, dc_loss: 0.018571021035313606, tv_loss: 0.01746123842895031\n",
      "iteration 964, dc_loss: 0.018570533022284508, tv_loss: 0.017460621893405914\n",
      "iteration 965, dc_loss: 0.01857002079486847, tv_loss: 0.017460279166698456\n",
      "iteration 966, dc_loss: 0.018569527193903923, tv_loss: 0.017459658905863762\n",
      "iteration 967, dc_loss: 0.018569042906165123, tv_loss: 0.01745888963341713\n",
      "iteration 968, dc_loss: 0.018568556755781174, tv_loss: 0.017458248883485794\n",
      "iteration 969, dc_loss: 0.018568074330687523, tv_loss: 0.01745793968439102\n",
      "iteration 970, dc_loss: 0.01856759935617447, tv_loss: 0.017457500100135803\n",
      "iteration 971, dc_loss: 0.018567107617855072, tv_loss: 0.017456870526075363\n",
      "iteration 972, dc_loss: 0.01856660097837448, tv_loss: 0.01745619811117649\n",
      "iteration 973, dc_loss: 0.01856612041592598, tv_loss: 0.01745583675801754\n",
      "iteration 974, dc_loss: 0.018565654754638672, tv_loss: 0.01745530217885971\n",
      "iteration 975, dc_loss: 0.018565207719802856, tv_loss: 0.017454447224736214\n",
      "iteration 976, dc_loss: 0.018564751371741295, tv_loss: 0.0174537505954504\n",
      "iteration 977, dc_loss: 0.018564272671937943, tv_loss: 0.017453443259000778\n",
      "iteration 978, dc_loss: 0.01856379583477974, tv_loss: 0.0174529030919075\n",
      "iteration 979, dc_loss: 0.01856335625052452, tv_loss: 0.01745203323662281\n",
      "iteration 980, dc_loss: 0.01856292597949505, tv_loss: 0.017451409250497818\n",
      "iteration 981, dc_loss: 0.018562495708465576, tv_loss: 0.017450954765081406\n",
      "iteration 982, dc_loss: 0.01856205053627491, tv_loss: 0.017450502142310143\n",
      "iteration 983, dc_loss: 0.018561620265245438, tv_loss: 0.01744971238076687\n",
      "iteration 984, dc_loss: 0.018561186268925667, tv_loss: 0.01744927279651165\n",
      "iteration 985, dc_loss: 0.018560724332928658, tv_loss: 0.017448790371418\n",
      "iteration 986, dc_loss: 0.01856027916073799, tv_loss: 0.017448097467422485\n",
      "iteration 987, dc_loss: 0.01855981908738613, tv_loss: 0.017447618767619133\n",
      "iteration 988, dc_loss: 0.01855935901403427, tv_loss: 0.017447227612137794\n",
      "iteration 989, dc_loss: 0.018558895215392113, tv_loss: 0.01744655892252922\n",
      "iteration 990, dc_loss: 0.018558433279395103, tv_loss: 0.017445920035243034\n",
      "iteration 991, dc_loss: 0.018557963892817497, tv_loss: 0.017445432022213936\n",
      "iteration 992, dc_loss: 0.01855751872062683, tv_loss: 0.01744498685002327\n",
      "iteration 993, dc_loss: 0.018557105213403702, tv_loss: 0.017444606870412827\n",
      "iteration 994, dc_loss: 0.018556706607341766, tv_loss: 0.017443884164094925\n",
      "iteration 995, dc_loss: 0.018556270748376846, tv_loss: 0.017443204298615456\n",
      "iteration 996, dc_loss: 0.018555818125605583, tv_loss: 0.01744287833571434\n",
      "iteration 997, dc_loss: 0.018555352464318275, tv_loss: 0.01744239218533039\n",
      "iteration 998, dc_loss: 0.0185549259185791, tv_loss: 0.017441675066947937\n",
      "iteration 999, dc_loss: 0.018554478883743286, tv_loss: 0.017441311851143837\n",
      "iteration 1000, dc_loss: 0.01855405420064926, tv_loss: 0.017440855503082275\n",
      "iteration 1001, dc_loss: 0.018553655594587326, tv_loss: 0.017440343275666237\n",
      "iteration 1002, dc_loss: 0.018553225323557854, tv_loss: 0.017439769580960274\n",
      "iteration 1003, dc_loss: 0.018552755936980247, tv_loss: 0.017439279705286026\n",
      "iteration 1004, dc_loss: 0.01855229027569294, tv_loss: 0.017438769340515137\n",
      "iteration 1005, dc_loss: 0.018551841378211975, tv_loss: 0.017438242211937904\n",
      "iteration 1006, dc_loss: 0.018551407381892204, tv_loss: 0.017437784001231194\n",
      "iteration 1007, dc_loss: 0.01855098456144333, tv_loss: 0.017437130212783813\n",
      "iteration 1008, dc_loss: 0.01855059340596199, tv_loss: 0.01743670366704464\n",
      "iteration 1009, dc_loss: 0.018550243228673935, tv_loss: 0.017436031252145767\n",
      "iteration 1010, dc_loss: 0.018549880012869835, tv_loss: 0.01743549294769764\n",
      "iteration 1011, dc_loss: 0.018549473956227303, tv_loss: 0.017435163259506226\n",
      "iteration 1012, dc_loss: 0.018549060449004173, tv_loss: 0.01743459515273571\n",
      "iteration 1013, dc_loss: 0.018548633903265, tv_loss: 0.017433973029255867\n",
      "iteration 1014, dc_loss: 0.018548209220170975, tv_loss: 0.0174335315823555\n",
      "iteration 1015, dc_loss: 0.018547803163528442, tv_loss: 0.017433134838938713\n",
      "iteration 1016, dc_loss: 0.018547380343079567, tv_loss: 0.017432453110814095\n",
      "iteration 1017, dc_loss: 0.018546970561146736, tv_loss: 0.01743197627365589\n",
      "iteration 1018, dc_loss: 0.018546581268310547, tv_loss: 0.01743141934275627\n",
      "iteration 1019, dc_loss: 0.018546180799603462, tv_loss: 0.0174308679997921\n",
      "iteration 1020, dc_loss: 0.018545759841799736, tv_loss: 0.017430517822504044\n",
      "iteration 1021, dc_loss: 0.018545329570770264, tv_loss: 0.01742994599044323\n",
      "iteration 1022, dc_loss: 0.01854492910206318, tv_loss: 0.017429450526833534\n",
      "iteration 1023, dc_loss: 0.018544532358646393, tv_loss: 0.017429014667868614\n",
      "iteration 1024, dc_loss: 0.018544135615229607, tv_loss: 0.017428498715162277\n",
      "iteration 1025, dc_loss: 0.018543720245361328, tv_loss: 0.017427831888198853\n",
      "iteration 1026, dc_loss: 0.0185433067381382, tv_loss: 0.017427334561944008\n",
      "iteration 1027, dc_loss: 0.018542909994721413, tv_loss: 0.01742718182504177\n",
      "iteration 1028, dc_loss: 0.018542539328336716, tv_loss: 0.017426621168851852\n",
      "iteration 1029, dc_loss: 0.018542176112532616, tv_loss: 0.01742585562169552\n",
      "iteration 1030, dc_loss: 0.01854180172085762, tv_loss: 0.01742532290518284\n",
      "iteration 1031, dc_loss: 0.018541403114795685, tv_loss: 0.01742490939795971\n",
      "iteration 1032, dc_loss: 0.018541039898991585, tv_loss: 0.01742454804480076\n",
      "iteration 1033, dc_loss: 0.01854066364467144, tv_loss: 0.0174238421022892\n",
      "iteration 1034, dc_loss: 0.018540261313319206, tv_loss: 0.017423424869775772\n",
      "iteration 1035, dc_loss: 0.018539872020483017, tv_loss: 0.01742282323539257\n",
      "iteration 1036, dc_loss: 0.01853950507938862, tv_loss: 0.017422236502170563\n",
      "iteration 1037, dc_loss: 0.01853913813829422, tv_loss: 0.017421862110495567\n",
      "iteration 1038, dc_loss: 0.01853874884545803, tv_loss: 0.017421558499336243\n",
      "iteration 1039, dc_loss: 0.018538374453783035, tv_loss: 0.017420999705791473\n",
      "iteration 1040, dc_loss: 0.018538016825914383, tv_loss: 0.017420334741473198\n",
      "iteration 1041, dc_loss: 0.01853766106069088, tv_loss: 0.017419865354895592\n",
      "iteration 1042, dc_loss: 0.018537329509854317, tv_loss: 0.017419351264834404\n",
      "iteration 1043, dc_loss: 0.01853700913488865, tv_loss: 0.01741889864206314\n",
      "iteration 1044, dc_loss: 0.018536673858761787, tv_loss: 0.01741848886013031\n",
      "iteration 1045, dc_loss: 0.018536277115345, tv_loss: 0.017417985945940018\n",
      "iteration 1046, dc_loss: 0.01853586733341217, tv_loss: 0.017417477443814278\n",
      "iteration 1047, dc_loss: 0.018535440787672997, tv_loss: 0.01741722598671913\n",
      "iteration 1048, dc_loss: 0.018535025417804718, tv_loss: 0.01741662435233593\n",
      "iteration 1049, dc_loss: 0.018534665927290916, tv_loss: 0.01741637848317623\n",
      "iteration 1050, dc_loss: 0.01853431575000286, tv_loss: 0.017415909096598625\n",
      "iteration 1051, dc_loss: 0.018533974885940552, tv_loss: 0.017415208742022514\n",
      "iteration 1052, dc_loss: 0.018533645197749138, tv_loss: 0.01741480641067028\n",
      "iteration 1053, dc_loss: 0.018533332273364067, tv_loss: 0.017414258792996407\n",
      "iteration 1054, dc_loss: 0.01853298582136631, tv_loss: 0.017413781955838203\n",
      "iteration 1055, dc_loss: 0.01853259652853012, tv_loss: 0.017413519322872162\n",
      "iteration 1056, dc_loss: 0.018532197922468185, tv_loss: 0.017412908375263214\n",
      "iteration 1057, dc_loss: 0.01853182539343834, tv_loss: 0.01741236262023449\n",
      "iteration 1058, dc_loss: 0.01853148266673088, tv_loss: 0.01741200126707554\n",
      "iteration 1059, dc_loss: 0.018531126901507378, tv_loss: 0.017411768436431885\n",
      "iteration 1060, dc_loss: 0.018530769273638725, tv_loss: 0.017411179840564728\n",
      "iteration 1061, dc_loss: 0.018530411645770073, tv_loss: 0.01741061918437481\n",
      "iteration 1062, dc_loss: 0.018530089408159256, tv_loss: 0.01741010695695877\n",
      "iteration 1063, dc_loss: 0.018529748544096947, tv_loss: 0.017409788444638252\n",
      "iteration 1064, dc_loss: 0.018529394641518593, tv_loss: 0.0174094308167696\n",
      "iteration 1065, dc_loss: 0.01852901466190815, tv_loss: 0.01740899123251438\n",
      "iteration 1066, dc_loss: 0.018528681248426437, tv_loss: 0.01740841194987297\n",
      "iteration 1067, dc_loss: 0.018528353422880173, tv_loss: 0.017407817766070366\n",
      "iteration 1068, dc_loss: 0.018528008833527565, tv_loss: 0.01740751415491104\n",
      "iteration 1069, dc_loss: 0.018527645617723465, tv_loss: 0.017407096922397614\n",
      "iteration 1070, dc_loss: 0.01852727122604847, tv_loss: 0.017406566068530083\n",
      "iteration 1071, dc_loss: 0.018526911735534668, tv_loss: 0.017406079918146133\n",
      "iteration 1072, dc_loss: 0.018526563420891762, tv_loss: 0.017405381426215172\n",
      "iteration 1073, dc_loss: 0.018526244908571243, tv_loss: 0.017405016347765923\n",
      "iteration 1074, dc_loss: 0.01852595992386341, tv_loss: 0.0174047090113163\n",
      "iteration 1075, dc_loss: 0.01852564699947834, tv_loss: 0.017404017969965935\n",
      "iteration 1076, dc_loss: 0.01852533034980297, tv_loss: 0.017403556033968925\n",
      "iteration 1077, dc_loss: 0.01852499507367611, tv_loss: 0.017402980476617813\n",
      "iteration 1078, dc_loss: 0.018524659797549248, tv_loss: 0.01740257628262043\n",
      "iteration 1079, dc_loss: 0.01852431520819664, tv_loss: 0.017402298748493195\n",
      "iteration 1080, dc_loss: 0.018523987382650375, tv_loss: 0.01740172505378723\n",
      "iteration 1081, dc_loss: 0.018523652106523514, tv_loss: 0.017401108518242836\n",
      "iteration 1082, dc_loss: 0.01852330192923546, tv_loss: 0.017400847747921944\n",
      "iteration 1083, dc_loss: 0.0185229629278183, tv_loss: 0.017400521785020828\n",
      "iteration 1084, dc_loss: 0.018522607162594795, tv_loss: 0.017400063574314117\n",
      "iteration 1085, dc_loss: 0.018522262573242188, tv_loss: 0.0173995029181242\n",
      "iteration 1086, dc_loss: 0.018521931022405624, tv_loss: 0.017399145290255547\n",
      "iteration 1087, dc_loss: 0.018521612510085106, tv_loss: 0.017398778349161148\n",
      "iteration 1088, dc_loss: 0.01852130889892578, tv_loss: 0.017398269847035408\n",
      "iteration 1089, dc_loss: 0.018521029502153397, tv_loss: 0.01739785075187683\n",
      "iteration 1090, dc_loss: 0.018520738929510117, tv_loss: 0.017397282645106316\n",
      "iteration 1091, dc_loss: 0.01852041482925415, tv_loss: 0.017396820709109306\n",
      "iteration 1092, dc_loss: 0.018520066514611244, tv_loss: 0.017396586015820503\n",
      "iteration 1093, dc_loss: 0.018519755452871323, tv_loss: 0.017396096140146255\n",
      "iteration 1094, dc_loss: 0.018519463017582893, tv_loss: 0.017395606264472008\n",
      "iteration 1095, dc_loss: 0.01851917989552021, tv_loss: 0.017395172268152237\n",
      "iteration 1096, dc_loss: 0.018518896773457527, tv_loss: 0.017394600436091423\n",
      "iteration 1097, dc_loss: 0.018518591299653053, tv_loss: 0.017394214868545532\n",
      "iteration 1098, dc_loss: 0.01851828210055828, tv_loss: 0.017393741756677628\n",
      "iteration 1099, dc_loss: 0.01851794682443142, tv_loss: 0.01739315688610077\n",
      "iteration 1100, dc_loss: 0.018517594784498215, tv_loss: 0.017393028363585472\n",
      "iteration 1101, dc_loss: 0.01851729117333889, tv_loss: 0.01739255152642727\n",
      "iteration 1102, dc_loss: 0.01851699687540531, tv_loss: 0.0173919927328825\n",
      "iteration 1103, dc_loss: 0.018516715615987778, tv_loss: 0.01739157736301422\n",
      "iteration 1104, dc_loss: 0.018516425043344498, tv_loss: 0.01739116758108139\n",
      "iteration 1105, dc_loss: 0.01851613074541092, tv_loss: 0.017390573397278786\n",
      "iteration 1106, dc_loss: 0.018515821546316147, tv_loss: 0.017390349879860878\n",
      "iteration 1107, dc_loss: 0.01851549744606018, tv_loss: 0.017390215769410133\n",
      "iteration 1108, dc_loss: 0.01851518452167511, tv_loss: 0.017389589920639992\n",
      "iteration 1109, dc_loss: 0.018514852970838547, tv_loss: 0.017389241605997086\n",
      "iteration 1110, dc_loss: 0.018514491617679596, tv_loss: 0.017388762906193733\n",
      "iteration 1111, dc_loss: 0.018514147028326988, tv_loss: 0.01738833077251911\n",
      "iteration 1112, dc_loss: 0.01851380616426468, tv_loss: 0.017388178035616875\n",
      "iteration 1113, dc_loss: 0.018513476476073265, tv_loss: 0.017387760803103447\n",
      "iteration 1114, dc_loss: 0.018513204529881477, tv_loss: 0.017387334257364273\n",
      "iteration 1115, dc_loss: 0.018512925133109093, tv_loss: 0.01738673262298107\n",
      "iteration 1116, dc_loss: 0.0185126680880785, tv_loss: 0.01738622784614563\n",
      "iteration 1117, dc_loss: 0.018512412905693054, tv_loss: 0.017385903745889664\n",
      "iteration 1118, dc_loss: 0.018512142822146416, tv_loss: 0.017385324463248253\n",
      "iteration 1119, dc_loss: 0.018511883914470673, tv_loss: 0.017384840175509453\n",
      "iteration 1120, dc_loss: 0.0185115747153759, tv_loss: 0.017384523525834084\n",
      "iteration 1121, dc_loss: 0.018511293455958366, tv_loss: 0.01738414354622364\n",
      "iteration 1122, dc_loss: 0.018511004745960236, tv_loss: 0.017383497208356857\n",
      "iteration 1123, dc_loss: 0.018510697409510612, tv_loss: 0.017383258789777756\n",
      "iteration 1124, dc_loss: 0.018510401248931885, tv_loss: 0.017383087426424026\n",
      "iteration 1125, dc_loss: 0.018510086461901665, tv_loss: 0.017382539808750153\n",
      "iteration 1126, dc_loss: 0.01850978471338749, tv_loss: 0.017382020130753517\n",
      "iteration 1127, dc_loss: 0.018509527668356895, tv_loss: 0.017381701618433\n",
      "iteration 1128, dc_loss: 0.018509292975068092, tv_loss: 0.0173813346773386\n",
      "iteration 1129, dc_loss: 0.018509062007069588, tv_loss: 0.01738058775663376\n",
      "iteration 1130, dc_loss: 0.018508829176425934, tv_loss: 0.017380231991410255\n",
      "iteration 1131, dc_loss: 0.018508577719330788, tv_loss: 0.01737985387444496\n",
      "iteration 1132, dc_loss: 0.01850830391049385, tv_loss: 0.01737947016954422\n",
      "iteration 1133, dc_loss: 0.01850801520049572, tv_loss: 0.017379039898514748\n",
      "iteration 1134, dc_loss: 0.018507706001400948, tv_loss: 0.01737857237458229\n",
      "iteration 1135, dc_loss: 0.01850741170346737, tv_loss: 0.0173783078789711\n",
      "iteration 1136, dc_loss: 0.018507108092308044, tv_loss: 0.017378052696585655\n",
      "iteration 1137, dc_loss: 0.01850682869553566, tv_loss: 0.017377538606524467\n",
      "iteration 1138, dc_loss: 0.018506549298763275, tv_loss: 0.017376959323883057\n",
      "iteration 1139, dc_loss: 0.018506277352571487, tv_loss: 0.017376666888594627\n",
      "iteration 1140, dc_loss: 0.018505984917283058, tv_loss: 0.017376184463500977\n",
      "iteration 1141, dc_loss: 0.018505707383155823, tv_loss: 0.01737590879201889\n",
      "iteration 1142, dc_loss: 0.018505414947867393, tv_loss: 0.017375651746988297\n",
      "iteration 1143, dc_loss: 0.018505116924643517, tv_loss: 0.0173752810806036\n",
      "iteration 1144, dc_loss: 0.018504824489355087, tv_loss: 0.017374811694025993\n",
      "iteration 1145, dc_loss: 0.018504556268453598, tv_loss: 0.017374392598867416\n",
      "iteration 1146, dc_loss: 0.0185043103992939, tv_loss: 0.017374029383063316\n",
      "iteration 1147, dc_loss: 0.01850404590368271, tv_loss: 0.01737368106842041\n",
      "iteration 1148, dc_loss: 0.018503746017813683, tv_loss: 0.017373330891132355\n",
      "iteration 1149, dc_loss: 0.0185034591704607, tv_loss: 0.01737290807068348\n",
      "iteration 1150, dc_loss: 0.018503211438655853, tv_loss: 0.01737225241959095\n",
      "iteration 1151, dc_loss: 0.018502986058592796, tv_loss: 0.01737191714346409\n",
      "iteration 1152, dc_loss: 0.018502753227949142, tv_loss: 0.017371483147144318\n",
      "iteration 1153, dc_loss: 0.018502485007047653, tv_loss: 0.01737106963992119\n",
      "iteration 1154, dc_loss: 0.018502241000533104, tv_loss: 0.017370475456118584\n",
      "iteration 1155, dc_loss: 0.018501976504921913, tv_loss: 0.017370276153087616\n",
      "iteration 1156, dc_loss: 0.018501723185181618, tv_loss: 0.017369868233799934\n",
      "iteration 1157, dc_loss: 0.018501488491892815, tv_loss: 0.017369307577610016\n",
      "iteration 1158, dc_loss: 0.01850126124918461, tv_loss: 0.01736890897154808\n",
      "iteration 1159, dc_loss: 0.01850101538002491, tv_loss: 0.017368663102388382\n",
      "iteration 1160, dc_loss: 0.018500732257962227, tv_loss: 0.01736828126013279\n",
      "iteration 1161, dc_loss: 0.018500428646802902, tv_loss: 0.01736769638955593\n",
      "iteration 1162, dc_loss: 0.018500149250030518, tv_loss: 0.017367379739880562\n",
      "iteration 1163, dc_loss: 0.018499895930290222, tv_loss: 0.017367105931043625\n",
      "iteration 1164, dc_loss: 0.018499622121453285, tv_loss: 0.017366787418723106\n",
      "iteration 1165, dc_loss: 0.01849932223558426, tv_loss: 0.017366250976920128\n",
      "iteration 1166, dc_loss: 0.01849905215203762, tv_loss: 0.017365939915180206\n",
      "iteration 1167, dc_loss: 0.018498798832297325, tv_loss: 0.017365531995892525\n",
      "iteration 1168, dc_loss: 0.01849857158958912, tv_loss: 0.017365144565701485\n",
      "iteration 1169, dc_loss: 0.01849835179746151, tv_loss: 0.017364684492349625\n",
      "iteration 1170, dc_loss: 0.018498146906495094, tv_loss: 0.017364228144288063\n",
      "iteration 1171, dc_loss: 0.01849791221320629, tv_loss: 0.017363764345645905\n",
      "iteration 1172, dc_loss: 0.018497657030820847, tv_loss: 0.017363587394356728\n",
      "iteration 1173, dc_loss: 0.018497377634048462, tv_loss: 0.017363281920552254\n",
      "iteration 1174, dc_loss: 0.018497096374630928, tv_loss: 0.017362680286169052\n",
      "iteration 1175, dc_loss: 0.018496835604310036, tv_loss: 0.017362577840685844\n",
      "iteration 1176, dc_loss: 0.018496597185730934, tv_loss: 0.017362214624881744\n",
      "iteration 1177, dc_loss: 0.018496381118893623, tv_loss: 0.01736176200211048\n",
      "iteration 1178, dc_loss: 0.01849617436528206, tv_loss: 0.017361192032694817\n",
      "iteration 1179, dc_loss: 0.01849597878754139, tv_loss: 0.01736084185540676\n",
      "iteration 1180, dc_loss: 0.018495788797736168, tv_loss: 0.017360443249344826\n",
      "iteration 1181, dc_loss: 0.018495550379157066, tv_loss: 0.01736018992960453\n",
      "iteration 1182, dc_loss: 0.018495287746191025, tv_loss: 0.017359662801027298\n",
      "iteration 1183, dc_loss: 0.018494995310902596, tv_loss: 0.017359254881739616\n",
      "iteration 1184, dc_loss: 0.01849471777677536, tv_loss: 0.017359063029289246\n",
      "iteration 1185, dc_loss: 0.018494464457035065, tv_loss: 0.017358697950839996\n",
      "iteration 1186, dc_loss: 0.018494226038455963, tv_loss: 0.01735811121761799\n",
      "iteration 1187, dc_loss: 0.018493950366973877, tv_loss: 0.017357992008328438\n",
      "iteration 1188, dc_loss: 0.01849365420639515, tv_loss: 0.017357725650072098\n",
      "iteration 1189, dc_loss: 0.01849338784813881, tv_loss: 0.017357341945171356\n",
      "iteration 1190, dc_loss: 0.0184931643307209, tv_loss: 0.01735694147646427\n",
      "iteration 1191, dc_loss: 0.018492944538593292, tv_loss: 0.017356429249048233\n",
      "iteration 1192, dc_loss: 0.018492739647626877, tv_loss: 0.01735602132976055\n",
      "iteration 1193, dc_loss: 0.018492547795176506, tv_loss: 0.01735573634505272\n",
      "iteration 1194, dc_loss: 0.01849234476685524, tv_loss: 0.01735534705221653\n",
      "iteration 1195, dc_loss: 0.01849213056266308, tv_loss: 0.017354849725961685\n",
      "iteration 1196, dc_loss: 0.01849192939698696, tv_loss: 0.017354419454932213\n",
      "iteration 1197, dc_loss: 0.018491709604859352, tv_loss: 0.017353998497128487\n",
      "iteration 1198, dc_loss: 0.01849149540066719, tv_loss: 0.01735374704003334\n",
      "iteration 1199, dc_loss: 0.018491242080926895, tv_loss: 0.017353372648358345\n",
      "iteration 1200, dc_loss: 0.018490949645638466, tv_loss: 0.017352856695652008\n",
      "iteration 1201, dc_loss: 0.01849069446325302, tv_loss: 0.01735258288681507\n",
      "iteration 1202, dc_loss: 0.018490443006157875, tv_loss: 0.017352188006043434\n",
      "iteration 1203, dc_loss: 0.018490204587578773, tv_loss: 0.017351916059851646\n",
      "iteration 1204, dc_loss: 0.018490014597773552, tv_loss: 0.017351502552628517\n",
      "iteration 1205, dc_loss: 0.018489867448806763, tv_loss: 0.017350945621728897\n",
      "iteration 1206, dc_loss: 0.018489673733711243, tv_loss: 0.017350707203149796\n",
      "iteration 1207, dc_loss: 0.018489383161067963, tv_loss: 0.01735023595392704\n",
      "iteration 1208, dc_loss: 0.01848909817636013, tv_loss: 0.017349829897284508\n",
      "iteration 1209, dc_loss: 0.018488852307200432, tv_loss: 0.017349522560834885\n",
      "iteration 1210, dc_loss: 0.018488606438040733, tv_loss: 0.017349230125546455\n",
      "iteration 1211, dc_loss: 0.018488368019461632, tv_loss: 0.0173488799482584\n",
      "iteration 1212, dc_loss: 0.018488159403204918, tv_loss: 0.01734847202897072\n",
      "iteration 1213, dc_loss: 0.01848795637488365, tv_loss: 0.0173481572419405\n",
      "iteration 1214, dc_loss: 0.01848774217069149, tv_loss: 0.017347758635878563\n",
      "iteration 1215, dc_loss: 0.018487507477402687, tv_loss: 0.017347488552331924\n",
      "iteration 1216, dc_loss: 0.018487250432372093, tv_loss: 0.017347194254398346\n",
      "iteration 1217, dc_loss: 0.018487052991986275, tv_loss: 0.01734662801027298\n",
      "iteration 1218, dc_loss: 0.018486905843019485, tv_loss: 0.017346197739243507\n",
      "iteration 1219, dc_loss: 0.018486719578504562, tv_loss: 0.017345881089568138\n",
      "iteration 1220, dc_loss: 0.01848651096224785, tv_loss: 0.01734551228582859\n",
      "iteration 1221, dc_loss: 0.0184862669557333, tv_loss: 0.01734514720737934\n",
      "iteration 1222, dc_loss: 0.018486009910702705, tv_loss: 0.01734483242034912\n",
      "iteration 1223, dc_loss: 0.018485771492123604, tv_loss: 0.017344366759061813\n",
      "iteration 1224, dc_loss: 0.018485568463802338, tv_loss: 0.017343981191515923\n",
      "iteration 1225, dc_loss: 0.018485384061932564, tv_loss: 0.017343806102871895\n",
      "iteration 1226, dc_loss: 0.018485177308321, tv_loss: 0.017343370243906975\n",
      "iteration 1227, dc_loss: 0.018484991043806076, tv_loss: 0.017342889681458473\n",
      "iteration 1228, dc_loss: 0.018484797328710556, tv_loss: 0.017342593520879745\n",
      "iteration 1229, dc_loss: 0.018484588712453842, tv_loss: 0.01734214648604393\n",
      "iteration 1230, dc_loss: 0.018484385684132576, tv_loss: 0.017341891303658485\n",
      "iteration 1231, dc_loss: 0.01848418451845646, tv_loss: 0.017341408878564835\n",
      "iteration 1232, dc_loss: 0.018484000116586685, tv_loss: 0.017340974882245064\n",
      "iteration 1233, dc_loss: 0.01848377287387848, tv_loss: 0.017340729013085365\n",
      "iteration 1234, dc_loss: 0.018483545631170273, tv_loss: 0.017340445891022682\n",
      "iteration 1235, dc_loss: 0.018483290448784828, tv_loss: 0.017340227961540222\n",
      "iteration 1236, dc_loss: 0.018483048304915428, tv_loss: 0.01733987219631672\n",
      "iteration 1237, dc_loss: 0.018482834100723267, tv_loss: 0.01733933389186859\n",
      "iteration 1238, dc_loss: 0.01848265342414379, tv_loss: 0.017339028418064117\n",
      "iteration 1239, dc_loss: 0.018482474610209465, tv_loss: 0.017338823527097702\n",
      "iteration 1240, dc_loss: 0.018482306972146034, tv_loss: 0.017338525503873825\n",
      "iteration 1241, dc_loss: 0.018482143059372902, tv_loss: 0.01733805052936077\n",
      "iteration 1242, dc_loss: 0.018481938168406487, tv_loss: 0.017337577417492867\n",
      "iteration 1243, dc_loss: 0.01848169043660164, tv_loss: 0.017337433993816376\n",
      "iteration 1244, dc_loss: 0.01848146691918373, tv_loss: 0.017337249591946602\n",
      "iteration 1245, dc_loss: 0.018481219187378883, tv_loss: 0.017336780205368996\n",
      "iteration 1246, dc_loss: 0.01848101057112217, tv_loss: 0.017336370423436165\n",
      "iteration 1247, dc_loss: 0.018480798229575157, tv_loss: 0.017336221411824226\n",
      "iteration 1248, dc_loss: 0.018480584025382996, tv_loss: 0.017335884273052216\n",
      "iteration 1249, dc_loss: 0.018480395898222923, tv_loss: 0.01733533665537834\n",
      "iteration 1250, dc_loss: 0.018480170518159866, tv_loss: 0.017334960401058197\n",
      "iteration 1251, dc_loss: 0.018479961901903152, tv_loss: 0.017334716394543648\n",
      "iteration 1252, dc_loss: 0.018479768186807632, tv_loss: 0.017334388568997383\n",
      "iteration 1253, dc_loss: 0.018479572609066963, tv_loss: 0.01733417436480522\n",
      "iteration 1254, dc_loss: 0.018479393795132637, tv_loss: 0.017333701252937317\n",
      "iteration 1255, dc_loss: 0.018479222431778908, tv_loss: 0.01733321323990822\n",
      "iteration 1256, dc_loss: 0.01847904548048973, tv_loss: 0.0173328947275877\n",
      "iteration 1257, dc_loss: 0.01847882568836212, tv_loss: 0.01733250729739666\n",
      "iteration 1258, dc_loss: 0.018478592857718468, tv_loss: 0.017332207411527634\n",
      "iteration 1259, dc_loss: 0.018478361889719963, tv_loss: 0.017331931740045547\n",
      "iteration 1260, dc_loss: 0.018478134647011757, tv_loss: 0.017331570386886597\n",
      "iteration 1261, dc_loss: 0.01847792975604534, tv_loss: 0.017331311479210854\n",
      "iteration 1262, dc_loss: 0.01847771555185318, tv_loss: 0.01733097992837429\n",
      "iteration 1263, dc_loss: 0.018477506935596466, tv_loss: 0.01733061484992504\n",
      "iteration 1264, dc_loss: 0.018477359786629677, tv_loss: 0.017330210655927658\n",
      "iteration 1265, dc_loss: 0.018477212637662888, tv_loss: 0.017329974099993706\n",
      "iteration 1266, dc_loss: 0.0184770654886961, tv_loss: 0.017329562455415726\n",
      "iteration 1267, dc_loss: 0.018476901575922966, tv_loss: 0.01732906512916088\n",
      "iteration 1268, dc_loss: 0.018476707860827446, tv_loss: 0.017328692600131035\n",
      "iteration 1269, dc_loss: 0.018476493656635284, tv_loss: 0.017328478395938873\n",
      "iteration 1270, dc_loss: 0.018476275727152824, tv_loss: 0.01732832007110119\n",
      "iteration 1271, dc_loss: 0.018476027995347977, tv_loss: 0.017327729612588882\n",
      "iteration 1272, dc_loss: 0.018475811928510666, tv_loss: 0.01732744090259075\n",
      "iteration 1273, dc_loss: 0.01847558468580246, tv_loss: 0.01732725277543068\n",
      "iteration 1274, dc_loss: 0.01847536861896515, tv_loss: 0.01732706092298031\n",
      "iteration 1275, dc_loss: 0.018475161865353584, tv_loss: 0.01732662506401539\n",
      "iteration 1276, dc_loss: 0.0184750035405159, tv_loss: 0.01732627861201763\n",
      "iteration 1277, dc_loss: 0.01847488060593605, tv_loss: 0.017325786873698235\n",
      "iteration 1278, dc_loss: 0.018474750220775604, tv_loss: 0.01732541061937809\n",
      "iteration 1279, dc_loss: 0.018474619835615158, tv_loss: 0.017325013875961304\n",
      "iteration 1280, dc_loss: 0.018474455922842026, tv_loss: 0.01732451282441616\n",
      "iteration 1281, dc_loss: 0.018474295735359192, tv_loss: 0.017324326559901237\n",
      "iteration 1282, dc_loss: 0.018474090844392776, tv_loss: 0.017324207350611687\n",
      "iteration 1283, dc_loss: 0.018473902717232704, tv_loss: 0.017323676496744156\n",
      "iteration 1284, dc_loss: 0.018473710864782333, tv_loss: 0.017323464155197144\n",
      "iteration 1285, dc_loss: 0.018473481759428978, tv_loss: 0.01732328161597252\n",
      "iteration 1286, dc_loss: 0.018473220989108086, tv_loss: 0.01732294261455536\n",
      "iteration 1287, dc_loss: 0.018472945317626, tv_loss: 0.0173227209597826\n",
      "iteration 1288, dc_loss: 0.018472686409950256, tv_loss: 0.01732240803539753\n",
      "iteration 1289, dc_loss: 0.018472488969564438, tv_loss: 0.017322177067399025\n",
      "iteration 1290, dc_loss: 0.018472328782081604, tv_loss: 0.017321888357400894\n",
      "iteration 1291, dc_loss: 0.01847217045724392, tv_loss: 0.017321394756436348\n",
      "iteration 1292, dc_loss: 0.018472010269761086, tv_loss: 0.017320921644568443\n",
      "iteration 1293, dc_loss: 0.018471868708729744, tv_loss: 0.017320748418569565\n",
      "iteration 1294, dc_loss: 0.01847173273563385, tv_loss: 0.017320431768894196\n",
      "iteration 1295, dc_loss: 0.018471598625183105, tv_loss: 0.01731998845934868\n",
      "iteration 1296, dc_loss: 0.018471455201506615, tv_loss: 0.01731957122683525\n",
      "iteration 1297, dc_loss: 0.01847127266228199, tv_loss: 0.017319142818450928\n",
      "iteration 1298, dc_loss: 0.018471086397767067, tv_loss: 0.017319131642580032\n",
      "iteration 1299, dc_loss: 0.018470890820026398, tv_loss: 0.017318762838840485\n",
      "iteration 1300, dc_loss: 0.01847068965435028, tv_loss: 0.017318427562713623\n",
      "iteration 1301, dc_loss: 0.01847049966454506, tv_loss: 0.017318015918135643\n",
      "iteration 1302, dc_loss: 0.018470320850610733, tv_loss: 0.0173176396638155\n",
      "iteration 1303, dc_loss: 0.01847013458609581, tv_loss: 0.01731749065220356\n",
      "iteration 1304, dc_loss: 0.01846996136009693, tv_loss: 0.017317119985818863\n",
      "iteration 1305, dc_loss: 0.0184697974473238, tv_loss: 0.01731656864285469\n",
      "iteration 1306, dc_loss: 0.018469618633389473, tv_loss: 0.01731623336672783\n",
      "iteration 1307, dc_loss: 0.018469447270035744, tv_loss: 0.01731608435511589\n",
      "iteration 1308, dc_loss: 0.018469249829649925, tv_loss: 0.017315668985247612\n",
      "iteration 1309, dc_loss: 0.018469056114554405, tv_loss: 0.017315326258540154\n",
      "iteration 1310, dc_loss: 0.018468843773007393, tv_loss: 0.017315058037638664\n",
      "iteration 1311, dc_loss: 0.018468644469976425, tv_loss: 0.017314719036221504\n",
      "iteration 1312, dc_loss: 0.0184684619307518, tv_loss: 0.017314517870545387\n",
      "iteration 1313, dc_loss: 0.018468279391527176, tv_loss: 0.017314160242676735\n",
      "iteration 1314, dc_loss: 0.018468094989657402, tv_loss: 0.01731388457119465\n",
      "iteration 1315, dc_loss: 0.018467919901013374, tv_loss: 0.017313575372099876\n",
      "iteration 1316, dc_loss: 0.01846776157617569, tv_loss: 0.01731315813958645\n",
      "iteration 1317, dc_loss: 0.018467659130692482, tv_loss: 0.017312804237008095\n",
      "iteration 1318, dc_loss: 0.01846753992140293, tv_loss: 0.01731254532933235\n",
      "iteration 1319, dc_loss: 0.01846740022301674, tv_loss: 0.01731213368475437\n",
      "iteration 1320, dc_loss: 0.018467245623469353, tv_loss: 0.017311885952949524\n",
      "iteration 1321, dc_loss: 0.01846710592508316, tv_loss: 0.01731150411069393\n",
      "iteration 1322, dc_loss: 0.018466904759407043, tv_loss: 0.017311180010437965\n",
      "iteration 1323, dc_loss: 0.018466686829924583, tv_loss: 0.01731093041598797\n",
      "iteration 1324, dc_loss: 0.018466470763087273, tv_loss: 0.0173106137663126\n",
      "iteration 1325, dc_loss: 0.018466264009475708, tv_loss: 0.01731048710644245\n",
      "iteration 1326, dc_loss: 0.018466081470251083, tv_loss: 0.0173101257532835\n",
      "iteration 1327, dc_loss: 0.0184659194201231, tv_loss: 0.017309732735157013\n",
      "iteration 1328, dc_loss: 0.018465735018253326, tv_loss: 0.017309630289673805\n",
      "iteration 1329, dc_loss: 0.018465574830770493, tv_loss: 0.01730921119451523\n",
      "iteration 1330, dc_loss: 0.018465423956513405, tv_loss: 0.017308911308646202\n",
      "iteration 1331, dc_loss: 0.018465273082256317, tv_loss: 0.0173084307461977\n",
      "iteration 1332, dc_loss: 0.01846511848270893, tv_loss: 0.01730830781161785\n",
      "iteration 1333, dc_loss: 0.018464943394064903, tv_loss: 0.017308056354522705\n",
      "iteration 1334, dc_loss: 0.01846477761864662, tv_loss: 0.017307598143815994\n",
      "iteration 1335, dc_loss: 0.018464641645550728, tv_loss: 0.017307374626398087\n",
      "iteration 1336, dc_loss: 0.018464475870132446, tv_loss: 0.017306892201304436\n",
      "iteration 1337, dc_loss: 0.01846429519355297, tv_loss: 0.01730664260685444\n",
      "iteration 1338, dc_loss: 0.018464133143424988, tv_loss: 0.017306432127952576\n",
      "iteration 1339, dc_loss: 0.018463991582393646, tv_loss: 0.017306167632341385\n",
      "iteration 1340, dc_loss: 0.01846381649374962, tv_loss: 0.017305858433246613\n",
      "iteration 1341, dc_loss: 0.0184636227786541, tv_loss: 0.017305471003055573\n",
      "iteration 1342, dc_loss: 0.018463432788848877, tv_loss: 0.01730513572692871\n",
      "iteration 1343, dc_loss: 0.018463270738720894, tv_loss: 0.017304914072155952\n",
      "iteration 1344, dc_loss: 0.01846310682594776, tv_loss: 0.017304640263319016\n",
      "iteration 1345, dc_loss: 0.018462959676980972, tv_loss: 0.01730435900390148\n",
      "iteration 1346, dc_loss: 0.018462831154465675, tv_loss: 0.01730390079319477\n",
      "iteration 1347, dc_loss: 0.018462704494595528, tv_loss: 0.01730353757739067\n",
      "iteration 1348, dc_loss: 0.018462559208273888, tv_loss: 0.017303364351391792\n",
      "iteration 1349, dc_loss: 0.018462393432855606, tv_loss: 0.01730308122932911\n",
      "iteration 1350, dc_loss: 0.01846219226717949, tv_loss: 0.017302753403782845\n",
      "iteration 1351, dc_loss: 0.01846202090382576, tv_loss: 0.017302345484495163\n",
      "iteration 1352, dc_loss: 0.01846182346343994, tv_loss: 0.01730227842926979\n",
      "iteration 1353, dc_loss: 0.01846160925924778, tv_loss: 0.01730222813785076\n",
      "iteration 1354, dc_loss: 0.018461400642991066, tv_loss: 0.017301777377724648\n",
      "iteration 1355, dc_loss: 0.01846119947731495, tv_loss: 0.017301470041275024\n",
      "iteration 1356, dc_loss: 0.018461033701896667, tv_loss: 0.017301229760050774\n",
      "iteration 1357, dc_loss: 0.018460921943187714, tv_loss: 0.01730095036327839\n",
      "iteration 1358, dc_loss: 0.0184608343988657, tv_loss: 0.01730058714747429\n",
      "iteration 1359, dc_loss: 0.018460741266608238, tv_loss: 0.017300037667155266\n",
      "iteration 1360, dc_loss: 0.018460584804415703, tv_loss: 0.01729978807270527\n",
      "iteration 1361, dc_loss: 0.018460379913449287, tv_loss: 0.017299501225352287\n",
      "iteration 1362, dc_loss: 0.018460165709257126, tv_loss: 0.017299149185419083\n",
      "iteration 1363, dc_loss: 0.018459966406226158, tv_loss: 0.017299041152000427\n",
      "iteration 1364, dc_loss: 0.01845981739461422, tv_loss: 0.017298566177487373\n",
      "iteration 1365, dc_loss: 0.018459709361195564, tv_loss: 0.017298191785812378\n",
      "iteration 1366, dc_loss: 0.018459586426615715, tv_loss: 0.017297955229878426\n",
      "iteration 1367, dc_loss: 0.01845943182706833, tv_loss: 0.01729763299226761\n",
      "iteration 1368, dc_loss: 0.018459267914295197, tv_loss: 0.017297254875302315\n",
      "iteration 1369, dc_loss: 0.018459118902683258, tv_loss: 0.017297079786658287\n",
      "iteration 1370, dc_loss: 0.018458956852555275, tv_loss: 0.017296751961112022\n",
      "iteration 1371, dc_loss: 0.018458761274814606, tv_loss: 0.01729651167988777\n",
      "iteration 1372, dc_loss: 0.01845858059823513, tv_loss: 0.017296316102147102\n",
      "iteration 1373, dc_loss: 0.0184584129601717, tv_loss: 0.01729593239724636\n",
      "iteration 1374, dc_loss: 0.018458260223269463, tv_loss: 0.017295729368925095\n",
      "iteration 1375, dc_loss: 0.018458103761076927, tv_loss: 0.017295345664024353\n",
      "iteration 1376, dc_loss: 0.018457960337400436, tv_loss: 0.01729513332247734\n",
      "iteration 1377, dc_loss: 0.018457826226949692, tv_loss: 0.017294686287641525\n",
      "iteration 1378, dc_loss: 0.018457721918821335, tv_loss: 0.017294378951191902\n",
      "iteration 1379, dc_loss: 0.01845758967101574, tv_loss: 0.0172940194606781\n",
      "iteration 1380, dc_loss: 0.018457453697919846, tv_loss: 0.01729382947087288\n",
      "iteration 1381, dc_loss: 0.018457289785146713, tv_loss: 0.017293602228164673\n",
      "iteration 1382, dc_loss: 0.01845713146030903, tv_loss: 0.01729307323694229\n",
      "iteration 1383, dc_loss: 0.018456989899277687, tv_loss: 0.017292987555265427\n",
      "iteration 1384, dc_loss: 0.018456868827342987, tv_loss: 0.017292629927396774\n",
      "iteration 1385, dc_loss: 0.018456706777215004, tv_loss: 0.017292439937591553\n",
      "iteration 1386, dc_loss: 0.01845650188624859, tv_loss: 0.01729210466146469\n",
      "iteration 1387, dc_loss: 0.018456311896443367, tv_loss: 0.017291944473981857\n",
      "iteration 1388, dc_loss: 0.01845613308250904, tv_loss: 0.017291799187660217\n",
      "iteration 1389, dc_loss: 0.0184559915214777, tv_loss: 0.017291359603405\n",
      "iteration 1390, dc_loss: 0.01845586858689785, tv_loss: 0.017290914431214333\n",
      "iteration 1391, dc_loss: 0.018455745652318, tv_loss: 0.01729070581495762\n",
      "iteration 1392, dc_loss: 0.018455617129802704, tv_loss: 0.0172903910279274\n",
      "iteration 1393, dc_loss: 0.01845547929406166, tv_loss: 0.017290079966187477\n",
      "iteration 1394, dc_loss: 0.018455324694514275, tv_loss: 0.01728980429470539\n",
      "iteration 1395, dc_loss: 0.018455175682902336, tv_loss: 0.017289428040385246\n",
      "iteration 1396, dc_loss: 0.018455011770129204, tv_loss: 0.017289219424128532\n",
      "iteration 1397, dc_loss: 0.018454819917678833, tv_loss: 0.017289092764258385\n",
      "iteration 1398, dc_loss: 0.01845463179051876, tv_loss: 0.017288783565163612\n",
      "iteration 1399, dc_loss: 0.018454505130648613, tv_loss: 0.0172884538769722\n",
      "iteration 1400, dc_loss: 0.01845439523458481, tv_loss: 0.017287956550717354\n",
      "iteration 1401, dc_loss: 0.018454281613230705, tv_loss: 0.017287857830524445\n",
      "iteration 1402, dc_loss: 0.018454132601618767, tv_loss: 0.017287570983171463\n",
      "iteration 1403, dc_loss: 0.018453987315297127, tv_loss: 0.01728709228336811\n",
      "iteration 1404, dc_loss: 0.01845381036400795, tv_loss: 0.01728701964020729\n",
      "iteration 1405, dc_loss: 0.018453635275363922, tv_loss: 0.017286740243434906\n",
      "iteration 1406, dc_loss: 0.018453441560268402, tv_loss: 0.01728637143969536\n",
      "iteration 1407, dc_loss: 0.018453316763043404, tv_loss: 0.01728634722530842\n",
      "iteration 1408, dc_loss: 0.018453195691108704, tv_loss: 0.017285920679569244\n",
      "iteration 1409, dc_loss: 0.018453054130077362, tv_loss: 0.017285609617829323\n",
      "iteration 1410, dc_loss: 0.018452906981110573, tv_loss: 0.01728527806699276\n",
      "iteration 1411, dc_loss: 0.01845276728272438, tv_loss: 0.017285073176026344\n",
      "iteration 1412, dc_loss: 0.018452636897563934, tv_loss: 0.017284784466028214\n",
      "iteration 1413, dc_loss: 0.018452510237693787, tv_loss: 0.017284443601965904\n",
      "iteration 1414, dc_loss: 0.018452370539307594, tv_loss: 0.017284154891967773\n",
      "iteration 1415, dc_loss: 0.018452228978276253, tv_loss: 0.017283914610743523\n",
      "iteration 1416, dc_loss: 0.01845208927989006, tv_loss: 0.01728367805480957\n",
      "iteration 1417, dc_loss: 0.018451998010277748, tv_loss: 0.017283355817198753\n",
      "iteration 1418, dc_loss: 0.018451891839504242, tv_loss: 0.01728312112390995\n",
      "iteration 1419, dc_loss: 0.01845177449285984, tv_loss: 0.017282702028751373\n",
      "iteration 1420, dc_loss: 0.01845165342092514, tv_loss: 0.017282407730817795\n",
      "iteration 1421, dc_loss: 0.01845150999724865, tv_loss: 0.017282264307141304\n",
      "iteration 1422, dc_loss: 0.018451347947120667, tv_loss: 0.01728195697069168\n",
      "iteration 1423, dc_loss: 0.018451178446412086, tv_loss: 0.017281556501984596\n",
      "iteration 1424, dc_loss: 0.018451038748025894, tv_loss: 0.017281292006373405\n",
      "iteration 1425, dc_loss: 0.018450891599059105, tv_loss: 0.017281021922826767\n",
      "iteration 1426, dc_loss: 0.01845073327422142, tv_loss: 0.017280930653214455\n",
      "iteration 1427, dc_loss: 0.018450545147061348, tv_loss: 0.017280442640185356\n",
      "iteration 1428, dc_loss: 0.018450340256094933, tv_loss: 0.01728019490838051\n",
      "iteration 1429, dc_loss: 0.01845015212893486, tv_loss: 0.017280152067542076\n",
      "iteration 1430, dc_loss: 0.018449999392032623, tv_loss: 0.01727992855012417\n",
      "iteration 1431, dc_loss: 0.018449900671839714, tv_loss: 0.01727958396077156\n",
      "iteration 1432, dc_loss: 0.018449831753969193, tv_loss: 0.01727919653058052\n",
      "iteration 1433, dc_loss: 0.018449699506163597, tv_loss: 0.017278824001550674\n",
      "iteration 1434, dc_loss: 0.01844951882958412, tv_loss: 0.017278708517551422\n",
      "iteration 1435, dc_loss: 0.018449341878294945, tv_loss: 0.017278457060456276\n",
      "iteration 1436, dc_loss: 0.018449198454618454, tv_loss: 0.017278334125876427\n",
      "iteration 1437, dc_loss: 0.01844905875623226, tv_loss: 0.01727805659174919\n",
      "iteration 1438, dc_loss: 0.0184489618986845, tv_loss: 0.017277659848332405\n",
      "iteration 1439, dc_loss: 0.018448863178491592, tv_loss: 0.017277350649237633\n",
      "iteration 1440, dc_loss: 0.01844874583184719, tv_loss: 0.017277022823691368\n",
      "iteration 1441, dc_loss: 0.018448632210493088, tv_loss: 0.01727677509188652\n",
      "iteration 1442, dc_loss: 0.01844850927591324, tv_loss: 0.017276499420404434\n",
      "iteration 1443, dc_loss: 0.018448354676365852, tv_loss: 0.017276061698794365\n",
      "iteration 1444, dc_loss: 0.018448185175657272, tv_loss: 0.01727575808763504\n",
      "iteration 1445, dc_loss: 0.018448036164045334, tv_loss: 0.017275625839829445\n",
      "iteration 1446, dc_loss: 0.01844784803688526, tv_loss: 0.017275631427764893\n",
      "iteration 1447, dc_loss: 0.01844767853617668, tv_loss: 0.01727534458041191\n",
      "iteration 1448, dc_loss: 0.01844756305217743, tv_loss: 0.017274990677833557\n",
      "iteration 1449, dc_loss: 0.01844749227166176, tv_loss: 0.017274724319577217\n",
      "iteration 1450, dc_loss: 0.018447427079081535, tv_loss: 0.01727442257106304\n",
      "iteration 1451, dc_loss: 0.01844733953475952, tv_loss: 0.01727398857474327\n",
      "iteration 1452, dc_loss: 0.018447227776050568, tv_loss: 0.017273714765906334\n",
      "iteration 1453, dc_loss: 0.018447093665599823, tv_loss: 0.017273595556616783\n",
      "iteration 1454, dc_loss: 0.01844695210456848, tv_loss: 0.017273254692554474\n",
      "iteration 1455, dc_loss: 0.0184467900544405, tv_loss: 0.01727294735610485\n",
      "iteration 1456, dc_loss: 0.01844664290547371, tv_loss: 0.017272965982556343\n",
      "iteration 1457, dc_loss: 0.018446460366249084, tv_loss: 0.01727263070642948\n",
      "iteration 1458, dc_loss: 0.01844624988734722, tv_loss: 0.017272301018238068\n",
      "iteration 1459, dc_loss: 0.018446074798703194, tv_loss: 0.017272278666496277\n",
      "iteration 1460, dc_loss: 0.018445929512381554, tv_loss: 0.017271962016820908\n",
      "iteration 1461, dc_loss: 0.018445806577801704, tv_loss: 0.017271650955080986\n",
      "iteration 1462, dc_loss: 0.018445705994963646, tv_loss: 0.01727154664695263\n",
      "iteration 1463, dc_loss: 0.018445586785674095, tv_loss: 0.01727118529379368\n",
      "iteration 1464, dc_loss: 0.01844549924135208, tv_loss: 0.017270861193537712\n",
      "iteration 1465, dc_loss: 0.018445421010255814, tv_loss: 0.017270544543862343\n",
      "iteration 1466, dc_loss: 0.018445318564772606, tv_loss: 0.017270108684897423\n",
      "iteration 1467, dc_loss: 0.018445201218128204, tv_loss: 0.01726975478231907\n",
      "iteration 1468, dc_loss: 0.018445050343871117, tv_loss: 0.01726965606212616\n",
      "iteration 1469, dc_loss: 0.018444882705807686, tv_loss: 0.017269372940063477\n",
      "iteration 1470, dc_loss: 0.018444733694195747, tv_loss: 0.017269037663936615\n",
      "iteration 1471, dc_loss: 0.018444616347551346, tv_loss: 0.017268890514969826\n",
      "iteration 1472, dc_loss: 0.01844450645148754, tv_loss: 0.01726866513490677\n",
      "iteration 1473, dc_loss: 0.018444376066327095, tv_loss: 0.01726834662258625\n",
      "iteration 1474, dc_loss: 0.018444227054715157, tv_loss: 0.017268048599362373\n",
      "iteration 1475, dc_loss: 0.01844404637813568, tv_loss: 0.017267940565943718\n",
      "iteration 1476, dc_loss: 0.018443867564201355, tv_loss: 0.01726754568517208\n",
      "iteration 1477, dc_loss: 0.018443718552589417, tv_loss: 0.017267296090722084\n",
      "iteration 1478, dc_loss: 0.018443604931235313, tv_loss: 0.017267214134335518\n",
      "iteration 1479, dc_loss: 0.018443474546074867, tv_loss: 0.017267026007175446\n",
      "iteration 1480, dc_loss: 0.01844332553446293, tv_loss: 0.017266571521759033\n",
      "iteration 1481, dc_loss: 0.01844320446252823, tv_loss: 0.0172664113342762\n",
      "iteration 1482, dc_loss: 0.01844310574233532, tv_loss: 0.017266223207116127\n",
      "iteration 1483, dc_loss: 0.018442988395690918, tv_loss: 0.01726578362286091\n",
      "iteration 1484, dc_loss: 0.01844285987317562, tv_loss: 0.017265554517507553\n",
      "iteration 1485, dc_loss: 0.018442753702402115, tv_loss: 0.017265355214476585\n",
      "iteration 1486, dc_loss: 0.018442627042531967, tv_loss: 0.01726509816944599\n",
      "iteration 1487, dc_loss: 0.01844250224530697, tv_loss: 0.01726476475596428\n",
      "iteration 1488, dc_loss: 0.018442390486598015, tv_loss: 0.017264503985643387\n",
      "iteration 1489, dc_loss: 0.018442239612340927, tv_loss: 0.017264410853385925\n",
      "iteration 1490, dc_loss: 0.018442071974277496, tv_loss: 0.017264217138290405\n",
      "iteration 1491, dc_loss: 0.018441950902342796, tv_loss: 0.017263740301132202\n",
      "iteration 1492, dc_loss: 0.01844188943505287, tv_loss: 0.017263498157262802\n",
      "iteration 1493, dc_loss: 0.01844184473156929, tv_loss: 0.017263317480683327\n",
      "iteration 1494, dc_loss: 0.018441734835505486, tv_loss: 0.017262743785977364\n",
      "iteration 1495, dc_loss: 0.018441595137119293, tv_loss: 0.017262516543269157\n",
      "iteration 1496, dc_loss: 0.018441451713442802, tv_loss: 0.01726248674094677\n",
      "iteration 1497, dc_loss: 0.018441306427121162, tv_loss: 0.017262181267142296\n",
      "iteration 1498, dc_loss: 0.018441174179315567, tv_loss: 0.01726185716688633\n",
      "iteration 1499, dc_loss: 0.01844102144241333, tv_loss: 0.01726158894598484\n",
      "iteration 1500, dc_loss: 0.018440891057252884, tv_loss: 0.01726146601140499\n",
      "iteration 1501, dc_loss: 0.01844073459506035, tv_loss: 0.017261182889342308\n",
      "iteration 1502, dc_loss: 0.018440600484609604, tv_loss: 0.017260875552892685\n",
      "iteration 1503, dc_loss: 0.018440477550029755, tv_loss: 0.017260657623410225\n",
      "iteration 1504, dc_loss: 0.018440378829836845, tv_loss: 0.017260584980249405\n",
      "iteration 1505, dc_loss: 0.018440287560224533, tv_loss: 0.017260247841477394\n",
      "iteration 1506, dc_loss: 0.018440157175064087, tv_loss: 0.01725972816348076\n",
      "iteration 1507, dc_loss: 0.018440041691064835, tv_loss: 0.017259668558835983\n",
      "iteration 1508, dc_loss: 0.018439913168549538, tv_loss: 0.017259445041418076\n",
      "iteration 1509, dc_loss: 0.018439769744873047, tv_loss: 0.017259255051612854\n",
      "iteration 1510, dc_loss: 0.01843964122235775, tv_loss: 0.0172590222209692\n",
      "iteration 1511, dc_loss: 0.018439505249261856, tv_loss: 0.017258696258068085\n",
      "iteration 1512, dc_loss: 0.018439380452036858, tv_loss: 0.01725863106548786\n",
      "iteration 1513, dc_loss: 0.018439238891005516, tv_loss: 0.01725836470723152\n",
      "iteration 1514, dc_loss: 0.01843908615410328, tv_loss: 0.017258042469620705\n",
      "iteration 1515, dc_loss: 0.018438950181007385, tv_loss: 0.01725780963897705\n",
      "iteration 1516, dc_loss: 0.018438810482621193, tv_loss: 0.01725771836936474\n",
      "iteration 1517, dc_loss: 0.018438678234815598, tv_loss: 0.017257515341043472\n",
      "iteration 1518, dc_loss: 0.018438545987010002, tv_loss: 0.01725720800459385\n",
      "iteration 1519, dc_loss: 0.01843845658004284, tv_loss: 0.017257118597626686\n",
      "iteration 1520, dc_loss: 0.018438395112752914, tv_loss: 0.017256859689950943\n",
      "iteration 1521, dc_loss: 0.0184383112937212, tv_loss: 0.01725637912750244\n",
      "iteration 1522, dc_loss: 0.018438220024108887, tv_loss: 0.017256060615181923\n",
      "iteration 1523, dc_loss: 0.018438149243593216, tv_loss: 0.017255928367376328\n",
      "iteration 1524, dc_loss: 0.018438076600432396, tv_loss: 0.017255522310733795\n",
      "iteration 1525, dc_loss: 0.01843799650669098, tv_loss: 0.017255248501896858\n",
      "iteration 1526, dc_loss: 0.01843786984682083, tv_loss: 0.017254963517189026\n",
      "iteration 1527, dc_loss: 0.018437715247273445, tv_loss: 0.0172546599060297\n",
      "iteration 1528, dc_loss: 0.018437551334500313, tv_loss: 0.01725454069674015\n",
      "iteration 1529, dc_loss: 0.018437402322888374, tv_loss: 0.017254406586289406\n",
      "iteration 1530, dc_loss: 0.018437253311276436, tv_loss: 0.017254093661904335\n",
      "iteration 1531, dc_loss: 0.018437106162309647, tv_loss: 0.017253819853067398\n",
      "iteration 1532, dc_loss: 0.018436968326568604, tv_loss: 0.017253806814551353\n",
      "iteration 1533, dc_loss: 0.018436836078763008, tv_loss: 0.01725354604423046\n",
      "iteration 1534, dc_loss: 0.018436724320054054, tv_loss: 0.017253223806619644\n",
      "iteration 1535, dc_loss: 0.01843664050102234, tv_loss: 0.017252983525395393\n",
      "iteration 1536, dc_loss: 0.018436545506119728, tv_loss: 0.01725275069475174\n",
      "iteration 1537, dc_loss: 0.018436431884765625, tv_loss: 0.01725231669843197\n",
      "iteration 1538, dc_loss: 0.018436340615153313, tv_loss: 0.017252212390303612\n",
      "iteration 1539, dc_loss: 0.018436256796121597, tv_loss: 0.01725207455456257\n",
      "iteration 1540, dc_loss: 0.0184361282736063, tv_loss: 0.017251720651984215\n",
      "iteration 1541, dc_loss: 0.018435977399349213, tv_loss: 0.017251597717404366\n",
      "iteration 1542, dc_loss: 0.018435850739479065, tv_loss: 0.01725129969418049\n",
      "iteration 1543, dc_loss: 0.01843572035431862, tv_loss: 0.017251087352633476\n",
      "iteration 1544, dc_loss: 0.01843556948006153, tv_loss: 0.017250945791602135\n",
      "iteration 1545, dc_loss: 0.01843542791903019, tv_loss: 0.017250750213861465\n",
      "iteration 1546, dc_loss: 0.018435301259160042, tv_loss: 0.017250485718250275\n",
      "iteration 1547, dc_loss: 0.018435196951031685, tv_loss: 0.017250332981348038\n",
      "iteration 1548, dc_loss: 0.01843511126935482, tv_loss: 0.01725008711218834\n",
      "iteration 1549, dc_loss: 0.018434995785355568, tv_loss: 0.01724986918270588\n",
      "iteration 1550, dc_loss: 0.018434885889291763, tv_loss: 0.017249656841158867\n",
      "iteration 1551, dc_loss: 0.018434759229421616, tv_loss: 0.017249327152967453\n",
      "iteration 1552, dc_loss: 0.01843464933335781, tv_loss: 0.01724904216825962\n",
      "iteration 1553, dc_loss: 0.018434539437294006, tv_loss: 0.01724887266755104\n",
      "iteration 1554, dc_loss: 0.018434414640069008, tv_loss: 0.017248837277293205\n",
      "iteration 1555, dc_loss: 0.018434328958392143, tv_loss: 0.017248423770070076\n",
      "iteration 1556, dc_loss: 0.01843421906232834, tv_loss: 0.017247987911105156\n",
      "iteration 1557, dc_loss: 0.018434105440974236, tv_loss: 0.017247894778847694\n",
      "iteration 1558, dc_loss: 0.018433989956974983, tv_loss: 0.017247846350073814\n",
      "iteration 1559, dc_loss: 0.018433859571814537, tv_loss: 0.017247376963496208\n",
      "iteration 1560, dc_loss: 0.018433760851621628, tv_loss: 0.01724707894027233\n",
      "iteration 1561, dc_loss: 0.018433669582009315, tv_loss: 0.017247002571821213\n",
      "iteration 1562, dc_loss: 0.01843355968594551, tv_loss: 0.017246972769498825\n",
      "iteration 1563, dc_loss: 0.018433421850204468, tv_loss: 0.01724649965763092\n",
      "iteration 1564, dc_loss: 0.018433263525366783, tv_loss: 0.017246317118406296\n",
      "iteration 1565, dc_loss: 0.018433118239045143, tv_loss: 0.01724614016711712\n",
      "iteration 1566, dc_loss: 0.0184329841285944, tv_loss: 0.017245939001441002\n",
      "iteration 1567, dc_loss: 0.018432877957820892, tv_loss: 0.017245681956410408\n",
      "iteration 1568, dc_loss: 0.01843278482556343, tv_loss: 0.017245344817638397\n",
      "iteration 1569, dc_loss: 0.01843271031975746, tv_loss: 0.0172450952231884\n",
      "iteration 1570, dc_loss: 0.018432628363370895, tv_loss: 0.017244918271899223\n",
      "iteration 1571, dc_loss: 0.018432550132274628, tv_loss: 0.017244664952158928\n",
      "iteration 1572, dc_loss: 0.01843247003853321, tv_loss: 0.017244374379515648\n",
      "iteration 1573, dc_loss: 0.01843234710395336, tv_loss: 0.01724420301616192\n",
      "iteration 1574, dc_loss: 0.01843220740556717, tv_loss: 0.017244035378098488\n",
      "iteration 1575, dc_loss: 0.01843205653131008, tv_loss: 0.0172437634319067\n",
      "iteration 1576, dc_loss: 0.018431877717375755, tv_loss: 0.01724349707365036\n",
      "iteration 1577, dc_loss: 0.018431751057505608, tv_loss: 0.017243416979908943\n",
      "iteration 1578, dc_loss: 0.018431656062602997, tv_loss: 0.017243321985006332\n",
      "iteration 1579, dc_loss: 0.01843155547976494, tv_loss: 0.017242996022105217\n",
      "iteration 1580, dc_loss: 0.018431514501571655, tv_loss: 0.017242467030882835\n",
      "iteration 1581, dc_loss: 0.01843145675957203, tv_loss: 0.017242325469851494\n",
      "iteration 1582, dc_loss: 0.01843138225376606, tv_loss: 0.01724223978817463\n",
      "iteration 1583, dc_loss: 0.018431255593895912, tv_loss: 0.017241984605789185\n",
      "iteration 1584, dc_loss: 0.01843111217021942, tv_loss: 0.017241591587662697\n",
      "iteration 1585, dc_loss: 0.01843096688389778, tv_loss: 0.017241578549146652\n",
      "iteration 1586, dc_loss: 0.018430858850479126, tv_loss: 0.017241474241018295\n",
      "iteration 1587, dc_loss: 0.018430737778544426, tv_loss: 0.017241183668375015\n",
      "iteration 1588, dc_loss: 0.018430661410093307, tv_loss: 0.017240844666957855\n",
      "iteration 1589, dc_loss: 0.01843058317899704, tv_loss: 0.01724071055650711\n",
      "iteration 1590, dc_loss: 0.01843046024441719, tv_loss: 0.01724051497876644\n",
      "iteration 1591, dc_loss: 0.018430327996611595, tv_loss: 0.01724022626876831\n",
      "iteration 1592, dc_loss: 0.01843019761145115, tv_loss: 0.01724000833928585\n",
      "iteration 1593, dc_loss: 0.018430054187774658, tv_loss: 0.01723981276154518\n",
      "iteration 1594, dc_loss: 0.01842990145087242, tv_loss: 0.017239611595869064\n",
      "iteration 1595, dc_loss: 0.018429802730679512, tv_loss: 0.017239363864064217\n",
      "iteration 1596, dc_loss: 0.0184297114610672, tv_loss: 0.017239056527614594\n",
      "iteration 1597, dc_loss: 0.01842964068055153, tv_loss: 0.017238881438970566\n",
      "iteration 1598, dc_loss: 0.018429571762681007, tv_loss: 0.017238568514585495\n",
      "iteration 1599, dc_loss: 0.018429473042488098, tv_loss: 0.017238298431038857\n",
      "iteration 1600, dc_loss: 0.01842937432229519, tv_loss: 0.01723821833729744\n",
      "iteration 1601, dc_loss: 0.018429260700941086, tv_loss: 0.017237845808267593\n",
      "iteration 1602, dc_loss: 0.018429117277264595, tv_loss: 0.017237799242138863\n",
      "iteration 1603, dc_loss: 0.018428979441523552, tv_loss: 0.01723766140639782\n",
      "iteration 1604, dc_loss: 0.018428860232234, tv_loss: 0.017237316817045212\n",
      "iteration 1605, dc_loss: 0.01842874474823475, tv_loss: 0.017236998304724693\n",
      "iteration 1606, dc_loss: 0.018428627401590347, tv_loss: 0.017236905172467232\n",
      "iteration 1607, dc_loss: 0.018428539857268333, tv_loss: 0.017236780375242233\n",
      "iteration 1608, dc_loss: 0.018428442999720573, tv_loss: 0.01723645254969597\n",
      "iteration 1609, dc_loss: 0.018428342416882515, tv_loss: 0.017236270010471344\n",
      "iteration 1610, dc_loss: 0.018428249284625053, tv_loss: 0.01723600924015045\n",
      "iteration 1611, dc_loss: 0.018428143113851547, tv_loss: 0.01723582111299038\n",
      "iteration 1612, dc_loss: 0.018428020179271698, tv_loss: 0.01723564974963665\n",
      "iteration 1613, dc_loss: 0.0184278916567564, tv_loss: 0.017235348001122475\n",
      "iteration 1614, dc_loss: 0.018427778035402298, tv_loss: 0.01723511703312397\n",
      "iteration 1615, dc_loss: 0.018427735194563866, tv_loss: 0.01723501831293106\n",
      "iteration 1616, dc_loss: 0.01842770166695118, tv_loss: 0.01723482273519039\n",
      "iteration 1617, dc_loss: 0.018427621573209763, tv_loss: 0.017234425991773605\n",
      "iteration 1618, dc_loss: 0.018427463248372078, tv_loss: 0.01723412051796913\n",
      "iteration 1619, dc_loss: 0.018427357077598572, tv_loss: 0.017234133556485176\n",
      "iteration 1620, dc_loss: 0.018427258357405663, tv_loss: 0.017233962193131447\n",
      "iteration 1621, dc_loss: 0.018427152186632156, tv_loss: 0.017233677208423615\n",
      "iteration 1622, dc_loss: 0.018427075818181038, tv_loss: 0.0172333475202322\n",
      "iteration 1623, dc_loss: 0.018426990136504173, tv_loss: 0.017232974991202354\n",
      "iteration 1624, dc_loss: 0.018426895141601562, tv_loss: 0.01723294146358967\n",
      "iteration 1625, dc_loss: 0.01842677779495716, tv_loss: 0.01723266765475273\n",
      "iteration 1626, dc_loss: 0.01842666231095791, tv_loss: 0.017232276499271393\n",
      "iteration 1627, dc_loss: 0.018426520749926567, tv_loss: 0.01723225601017475\n",
      "iteration 1628, dc_loss: 0.018426354974508286, tv_loss: 0.01723213866353035\n",
      "iteration 1629, dc_loss: 0.018426204100251198, tv_loss: 0.017231900244951248\n",
      "iteration 1630, dc_loss: 0.018426060676574707, tv_loss: 0.017231611534953117\n",
      "iteration 1631, dc_loss: 0.0184259582310915, tv_loss: 0.017231596633791924\n",
      "iteration 1632, dc_loss: 0.018425896763801575, tv_loss: 0.017231345176696777\n",
      "iteration 1633, dc_loss: 0.01842581108212471, tv_loss: 0.01723107136785984\n",
      "iteration 1634, dc_loss: 0.018425745889544487, tv_loss: 0.0172306839376688\n",
      "iteration 1635, dc_loss: 0.018425654619932175, tv_loss: 0.017230458557605743\n",
      "iteration 1636, dc_loss: 0.018425555899739265, tv_loss: 0.017230434343218803\n",
      "iteration 1637, dc_loss: 0.018425414338707924, tv_loss: 0.017230253666639328\n",
      "iteration 1638, dc_loss: 0.018425261601805687, tv_loss: 0.017230067402124405\n",
      "iteration 1639, dc_loss: 0.018425175920128822, tv_loss: 0.017229922115802765\n",
      "iteration 1640, dc_loss: 0.018425114452838898, tv_loss: 0.017229681834578514\n",
      "iteration 1641, dc_loss: 0.01842505671083927, tv_loss: 0.017229309305548668\n",
      "iteration 1642, dc_loss: 0.018424976617097855, tv_loss: 0.01722893863916397\n",
      "iteration 1643, dc_loss: 0.01842491514980793, tv_loss: 0.01722900941967964\n",
      "iteration 1644, dc_loss: 0.01842484064400196, tv_loss: 0.017228780314326286\n",
      "iteration 1645, dc_loss: 0.018424738198518753, tv_loss: 0.01722840778529644\n",
      "iteration 1646, dc_loss: 0.01842459663748741, tv_loss: 0.017228342592716217\n",
      "iteration 1647, dc_loss: 0.01842445321381092, tv_loss: 0.017228199169039726\n",
      "iteration 1648, dc_loss: 0.018424317240715027, tv_loss: 0.01722814328968525\n",
      "iteration 1649, dc_loss: 0.01842416264116764, tv_loss: 0.017227869480848312\n",
      "iteration 1650, dc_loss: 0.01842406950891018, tv_loss: 0.017227482050657272\n",
      "iteration 1651, dc_loss: 0.018423987552523613, tv_loss: 0.017227301374077797\n",
      "iteration 1652, dc_loss: 0.01842390187084675, tv_loss: 0.017227226868271828\n",
      "iteration 1653, dc_loss: 0.018423819914460182, tv_loss: 0.01722688600420952\n",
      "iteration 1654, dc_loss: 0.01842372491955757, tv_loss: 0.01722676306962967\n",
      "iteration 1655, dc_loss: 0.018423615023493767, tv_loss: 0.017226485535502434\n",
      "iteration 1656, dc_loss: 0.018423542380332947, tv_loss: 0.017226288095116615\n",
      "iteration 1657, dc_loss: 0.018423423171043396, tv_loss: 0.017226088792085648\n",
      "iteration 1658, dc_loss: 0.01842328906059265, tv_loss: 0.017225859686732292\n",
      "iteration 1659, dc_loss: 0.018423162400722504, tv_loss: 0.01722567342221737\n",
      "iteration 1660, dc_loss: 0.0184230525046587, tv_loss: 0.01722564361989498\n",
      "iteration 1661, dc_loss: 0.018422963097691536, tv_loss: 0.017225373536348343\n",
      "iteration 1662, dc_loss: 0.01842290349304676, tv_loss: 0.017224987968802452\n",
      "iteration 1663, dc_loss: 0.01842283271253109, tv_loss: 0.0172248724848032\n",
      "iteration 1664, dc_loss: 0.018422767519950867, tv_loss: 0.017224770039319992\n",
      "iteration 1665, dc_loss: 0.01842266134917736, tv_loss: 0.017224490642547607\n",
      "iteration 1666, dc_loss: 0.018422508612275124, tv_loss: 0.017224332317709923\n",
      "iteration 1667, dc_loss: 0.018422378227114677, tv_loss: 0.017224179580807686\n",
      "iteration 1668, dc_loss: 0.018422257155179977, tv_loss: 0.017223883420228958\n",
      "iteration 1669, dc_loss: 0.01842215470969677, tv_loss: 0.01722404733300209\n",
      "iteration 1670, dc_loss: 0.018422070890665054, tv_loss: 0.017223739996552467\n",
      "iteration 1671, dc_loss: 0.01842198520898819, tv_loss: 0.0172232948243618\n",
      "iteration 1672, dc_loss: 0.018421949818730354, tv_loss: 0.017223164439201355\n",
      "iteration 1673, dc_loss: 0.018421918153762817, tv_loss: 0.017222771421074867\n",
      "iteration 1674, dc_loss: 0.01842186041176319, tv_loss: 0.01722238026559353\n",
      "iteration 1675, dc_loss: 0.018421782180666924, tv_loss: 0.017222430557012558\n",
      "iteration 1676, dc_loss: 0.01842164248228073, tv_loss: 0.017222262918949127\n",
      "iteration 1677, dc_loss: 0.01842147298157215, tv_loss: 0.017221985384821892\n",
      "iteration 1678, dc_loss: 0.018421322107315063, tv_loss: 0.017221901565790176\n",
      "iteration 1679, dc_loss: 0.018421189859509468, tv_loss: 0.01722167246043682\n",
      "iteration 1680, dc_loss: 0.018421068787574768, tv_loss: 0.017221560701727867\n",
      "iteration 1681, dc_loss: 0.018421005457639694, tv_loss: 0.017221501097083092\n",
      "iteration 1682, dc_loss: 0.01842094026505947, tv_loss: 0.017221109941601753\n",
      "iteration 1683, dc_loss: 0.018420880660414696, tv_loss: 0.01722090132534504\n",
      "iteration 1684, dc_loss: 0.01842079870402813, tv_loss: 0.017220748588442802\n",
      "iteration 1685, dc_loss: 0.01842072606086731, tv_loss: 0.017220627516508102\n",
      "iteration 1686, dc_loss: 0.018420612439513206, tv_loss: 0.01722026616334915\n",
      "iteration 1687, dc_loss: 0.018420476466417313, tv_loss: 0.017220042645931244\n",
      "iteration 1688, dc_loss: 0.018420349806547165, tv_loss: 0.0172200296074152\n",
      "iteration 1689, dc_loss: 0.018420223146677017, tv_loss: 0.01721995696425438\n",
      "iteration 1690, dc_loss: 0.018420105800032616, tv_loss: 0.01721964031457901\n",
      "iteration 1691, dc_loss: 0.018420035019516945, tv_loss: 0.01721935160458088\n",
      "iteration 1692, dc_loss: 0.018419943749904633, tv_loss: 0.017219282686710358\n",
      "iteration 1693, dc_loss: 0.018419872969388962, tv_loss: 0.01721886731684208\n",
      "iteration 1694, dc_loss: 0.01841980405151844, tv_loss: 0.01721884123980999\n",
      "iteration 1695, dc_loss: 0.018419725820422173, tv_loss: 0.017218705266714096\n",
      "iteration 1696, dc_loss: 0.018419645726680756, tv_loss: 0.017218461260199547\n",
      "iteration 1697, dc_loss: 0.018419558182358742, tv_loss: 0.017218343913555145\n",
      "iteration 1698, dc_loss: 0.018419478088617325, tv_loss: 0.01721828803420067\n",
      "iteration 1699, dc_loss: 0.01841936632990837, tv_loss: 0.017217928543686867\n",
      "iteration 1700, dc_loss: 0.018419239670038223, tv_loss: 0.01721777208149433\n",
      "iteration 1701, dc_loss: 0.01841910369694233, tv_loss: 0.017217623069882393\n",
      "iteration 1702, dc_loss: 0.01841897703707218, tv_loss: 0.017217500135302544\n",
      "iteration 1703, dc_loss: 0.018418824300169945, tv_loss: 0.017217319458723068\n",
      "iteration 1704, dc_loss: 0.018418721854686737, tv_loss: 0.017217068001627922\n",
      "iteration 1705, dc_loss: 0.01841866970062256, tv_loss: 0.017216859385371208\n",
      "iteration 1706, dc_loss: 0.018418628722429276, tv_loss: 0.017216725274920464\n",
      "iteration 1707, dc_loss: 0.01841861382126808, tv_loss: 0.017216457054018974\n",
      "iteration 1708, dc_loss: 0.018418554216623306, tv_loss: 0.017216240987181664\n",
      "iteration 1709, dc_loss: 0.018418453633785248, tv_loss: 0.0172160342335701\n",
      "iteration 1710, dc_loss: 0.018418334424495697, tv_loss: 0.017215924337506294\n",
      "iteration 1711, dc_loss: 0.018418237566947937, tv_loss: 0.017215434461832047\n",
      "iteration 1712, dc_loss: 0.018418144434690475, tv_loss: 0.017215318977832794\n",
      "iteration 1713, dc_loss: 0.018418073654174805, tv_loss: 0.017215318977832794\n",
      "iteration 1714, dc_loss: 0.018417956307530403, tv_loss: 0.017215130850672722\n",
      "iteration 1715, dc_loss: 0.018417812883853912, tv_loss: 0.01721472665667534\n",
      "iteration 1716, dc_loss: 0.018417667597532272, tv_loss: 0.017214607447385788\n",
      "iteration 1717, dc_loss: 0.018417567014694214, tv_loss: 0.017214396968483925\n",
      "iteration 1718, dc_loss: 0.018417492508888245, tv_loss: 0.01721428520977497\n",
      "iteration 1719, dc_loss: 0.018417425453662872, tv_loss: 0.01721402443945408\n",
      "iteration 1720, dc_loss: 0.018417339771986008, tv_loss: 0.017213759943842888\n",
      "iteration 1721, dc_loss: 0.018417229875922203, tv_loss: 0.017213590443134308\n",
      "iteration 1722, dc_loss: 0.018417123705148697, tv_loss: 0.017213422805070877\n",
      "iteration 1723, dc_loss: 0.018417052924633026, tv_loss: 0.01721309684216976\n",
      "iteration 1724, dc_loss: 0.018417004495859146, tv_loss: 0.017212901264429092\n",
      "iteration 1725, dc_loss: 0.018416929990053177, tv_loss: 0.017212767153978348\n",
      "iteration 1726, dc_loss: 0.01841682940721512, tv_loss: 0.01721268892288208\n",
      "iteration 1727, dc_loss: 0.01841667667031288, tv_loss: 0.017212411388754845\n",
      "iteration 1728, dc_loss: 0.01841653883457184, tv_loss: 0.017212342470884323\n",
      "iteration 1729, dc_loss: 0.018416419625282288, tv_loss: 0.017212307080626488\n",
      "iteration 1730, dc_loss: 0.018416356295347214, tv_loss: 0.017212077975273132\n",
      "iteration 1731, dc_loss: 0.018416283652186394, tv_loss: 0.017211709171533585\n",
      "iteration 1732, dc_loss: 0.018416209146380424, tv_loss: 0.017211560159921646\n",
      "iteration 1733, dc_loss: 0.018416108563542366, tv_loss: 0.017211420461535454\n",
      "iteration 1734, dc_loss: 0.01841600053012371, tv_loss: 0.01721113547682762\n",
      "iteration 1735, dc_loss: 0.018415888771414757, tv_loss: 0.017211003229022026\n",
      "iteration 1736, dc_loss: 0.018415790051221848, tv_loss: 0.01721101440489292\n",
      "iteration 1737, dc_loss: 0.018415719270706177, tv_loss: 0.01721055433154106\n",
      "iteration 1738, dc_loss: 0.018415646627545357, tv_loss: 0.017210382968187332\n",
      "iteration 1739, dc_loss: 0.018415603786706924, tv_loss: 0.01721026934683323\n",
      "iteration 1740, dc_loss: 0.018415542319417, tv_loss: 0.017210036516189575\n",
      "iteration 1741, dc_loss: 0.018415460363030434, tv_loss: 0.017209649085998535\n",
      "iteration 1742, dc_loss: 0.018415376543998718, tv_loss: 0.017209509387612343\n",
      "iteration 1743, dc_loss: 0.01841524802148342, tv_loss: 0.017209546640515327\n",
      "iteration 1744, dc_loss: 0.01841512881219387, tv_loss: 0.017209425568580627\n",
      "iteration 1745, dc_loss: 0.018415020778775215, tv_loss: 0.0172092504799366\n",
      "iteration 1746, dc_loss: 0.0184149369597435, tv_loss: 0.017208928242325783\n",
      "iteration 1747, dc_loss: 0.018414832651615143, tv_loss: 0.01720888912677765\n",
      "iteration 1748, dc_loss: 0.018414732068777084, tv_loss: 0.017208853736519814\n",
      "iteration 1749, dc_loss: 0.018414625898003578, tv_loss: 0.017208699136972427\n",
      "iteration 1750, dc_loss: 0.018414514139294624, tv_loss: 0.017208455130457878\n",
      "iteration 1751, dc_loss: 0.018414419144392014, tv_loss: 0.017208321020007133\n",
      "iteration 1752, dc_loss: 0.01841433346271515, tv_loss: 0.017208214849233627\n",
      "iteration 1753, dc_loss: 0.018414277583360672, tv_loss: 0.01720786653459072\n",
      "iteration 1754, dc_loss: 0.01841420866549015, tv_loss: 0.017207656055688858\n",
      "iteration 1755, dc_loss: 0.018414126709103584, tv_loss: 0.01720758154988289\n",
      "iteration 1756, dc_loss: 0.018414046615362167, tv_loss: 0.017207235097885132\n",
      "iteration 1757, dc_loss: 0.018413962796330452, tv_loss: 0.017206905409693718\n",
      "iteration 1758, dc_loss: 0.018413856625556946, tv_loss: 0.01720692403614521\n",
      "iteration 1759, dc_loss: 0.018413765355944633, tv_loss: 0.017206674441695213\n",
      "iteration 1760, dc_loss: 0.018413683399558067, tv_loss: 0.01720646396279335\n",
      "iteration 1761, dc_loss: 0.018413545563817024, tv_loss: 0.01720646768808365\n",
      "iteration 1762, dc_loss: 0.018413446843624115, tv_loss: 0.017206309363245964\n",
      "iteration 1763, dc_loss: 0.018413357436656952, tv_loss: 0.017206033691763878\n",
      "iteration 1764, dc_loss: 0.01841329038143158, tv_loss: 0.017205847427248955\n",
      "iteration 1765, dc_loss: 0.018413197249174118, tv_loss: 0.01720566861331463\n",
      "iteration 1766, dc_loss: 0.018413100391626358, tv_loss: 0.01720544882118702\n",
      "iteration 1767, dc_loss: 0.018412983044981956, tv_loss: 0.017205357551574707\n",
      "iteration 1768, dc_loss: 0.018412888050079346, tv_loss: 0.01720517687499523\n",
      "iteration 1769, dc_loss: 0.018412819132208824, tv_loss: 0.01720503531396389\n",
      "iteration 1770, dc_loss: 0.018412766978144646, tv_loss: 0.01720481365919113\n",
      "iteration 1771, dc_loss: 0.018412725999951363, tv_loss: 0.017204493284225464\n",
      "iteration 1772, dc_loss: 0.01841263845562935, tv_loss: 0.017204325646162033\n",
      "iteration 1773, dc_loss: 0.018412552773952484, tv_loss: 0.0172041654586792\n",
      "iteration 1774, dc_loss: 0.018412411212921143, tv_loss: 0.017204057425260544\n",
      "iteration 1775, dc_loss: 0.01841227151453495, tv_loss: 0.01720390096306801\n",
      "iteration 1776, dc_loss: 0.01841217651963234, tv_loss: 0.01720392517745495\n",
      "iteration 1777, dc_loss: 0.018412122502923012, tv_loss: 0.01720353029668331\n",
      "iteration 1778, dc_loss: 0.018412087112665176, tv_loss: 0.017203394323587418\n",
      "iteration 1779, dc_loss: 0.01841207593679428, tv_loss: 0.01720319874584675\n",
      "iteration 1780, dc_loss: 0.018411997705698013, tv_loss: 0.01720280572772026\n",
      "iteration 1781, dc_loss: 0.018411897122859955, tv_loss: 0.0172025877982378\n",
      "iteration 1782, dc_loss: 0.018411759287118912, tv_loss: 0.017202533781528473\n",
      "iteration 1783, dc_loss: 0.018411636352539062, tv_loss: 0.017202602699398994\n",
      "iteration 1784, dc_loss: 0.01841152086853981, tv_loss: 0.017202358692884445\n",
      "iteration 1785, dc_loss: 0.01841142773628235, tv_loss: 0.017202060669660568\n",
      "iteration 1786, dc_loss: 0.018411342054605484, tv_loss: 0.017201991751790047\n",
      "iteration 1787, dc_loss: 0.018411267548799515, tv_loss: 0.017201827839016914\n",
      "iteration 1788, dc_loss: 0.018411187455058098, tv_loss: 0.01720164343714714\n",
      "iteration 1789, dc_loss: 0.01841108873486519, tv_loss: 0.01720128394663334\n",
      "iteration 1790, dc_loss: 0.01841101236641407, tv_loss: 0.017201075330376625\n",
      "iteration 1791, dc_loss: 0.018410932272672653, tv_loss: 0.01720111072063446\n",
      "iteration 1792, dc_loss: 0.018410898745059967, tv_loss: 0.017201017588377\n",
      "iteration 1793, dc_loss: 0.01841086521744728, tv_loss: 0.017200812697410583\n",
      "iteration 1794, dc_loss: 0.018410800024867058, tv_loss: 0.017200404778122902\n",
      "iteration 1795, dc_loss: 0.01841074973344803, tv_loss: 0.01720009557902813\n",
      "iteration 1796, dc_loss: 0.018410667777061462, tv_loss: 0.017200030386447906\n",
      "iteration 1797, dc_loss: 0.018410537391901016, tv_loss: 0.01719994843006134\n",
      "iteration 1798, dc_loss: 0.01841038279235363, tv_loss: 0.017199935391545296\n",
      "iteration 1799, dc_loss: 0.01841023936867714, tv_loss: 0.017199860885739326\n",
      "iteration 1800, dc_loss: 0.018410103395581245, tv_loss: 0.017199628055095673\n",
      "iteration 1801, dc_loss: 0.018410012125968933, tv_loss: 0.0171994436532259\n",
      "iteration 1802, dc_loss: 0.018409956246614456, tv_loss: 0.017199331894516945\n",
      "iteration 1803, dc_loss: 0.018409915268421173, tv_loss: 0.017199138179421425\n",
      "iteration 1804, dc_loss: 0.018409883603453636, tv_loss: 0.017198897898197174\n",
      "iteration 1805, dc_loss: 0.018409837037324905, tv_loss: 0.017198603600263596\n",
      "iteration 1806, dc_loss: 0.018409784883260727, tv_loss: 0.017198609188199043\n",
      "iteration 1807, dc_loss: 0.018409673124551773, tv_loss: 0.017198486253619194\n",
      "iteration 1808, dc_loss: 0.018409548327326775, tv_loss: 0.017198268324136734\n",
      "iteration 1809, dc_loss: 0.018409421667456627, tv_loss: 0.017198145389556885\n",
      "iteration 1810, dc_loss: 0.018409335985779762, tv_loss: 0.01719813235104084\n",
      "iteration 1811, dc_loss: 0.018409254029393196, tv_loss: 0.01719796471297741\n",
      "iteration 1812, dc_loss: 0.018409188836812973, tv_loss: 0.017197681590914726\n",
      "iteration 1813, dc_loss: 0.01840912364423275, tv_loss: 0.017197487875819206\n",
      "iteration 1814, dc_loss: 0.01840907521545887, tv_loss: 0.017197411507368088\n",
      "iteration 1815, dc_loss: 0.018408987671136856, tv_loss: 0.01719723269343376\n",
      "iteration 1816, dc_loss: 0.0184088833630085, tv_loss: 0.017197201028466225\n",
      "iteration 1817, dc_loss: 0.01840876042842865, tv_loss: 0.01719708740711212\n",
      "iteration 1818, dc_loss: 0.018408631905913353, tv_loss: 0.017196785658597946\n",
      "iteration 1819, dc_loss: 0.01840849407017231, tv_loss: 0.017196668311953545\n",
      "iteration 1820, dc_loss: 0.01840837299823761, tv_loss: 0.017196500673890114\n",
      "iteration 1821, dc_loss: 0.018408278003335, tv_loss: 0.01719639264047146\n",
      "iteration 1822, dc_loss: 0.01840820722281933, tv_loss: 0.01719614863395691\n",
      "iteration 1823, dc_loss: 0.018408197909593582, tv_loss: 0.01719575561583042\n",
      "iteration 1824, dc_loss: 0.018408169969916344, tv_loss: 0.0171955619007349\n",
      "iteration 1825, dc_loss: 0.0184081569314003, tv_loss: 0.0171955619007349\n",
      "iteration 1826, dc_loss: 0.018408099189400673, tv_loss: 0.017195360735058784\n",
      "iteration 1827, dc_loss: 0.01840803772211075, tv_loss: 0.017195207998156548\n",
      "iteration 1828, dc_loss: 0.01840796321630478, tv_loss: 0.017194729298353195\n",
      "iteration 1829, dc_loss: 0.018407870084047318, tv_loss: 0.017194528132677078\n",
      "iteration 1830, dc_loss: 0.0184077937155962, tv_loss: 0.01719466783106327\n",
      "iteration 1831, dc_loss: 0.018407681956887245, tv_loss: 0.017194556072354317\n",
      "iteration 1832, dc_loss: 0.018407581374049187, tv_loss: 0.017194271087646484\n",
      "iteration 1833, dc_loss: 0.01840742491185665, tv_loss: 0.017194146290421486\n",
      "iteration 1834, dc_loss: 0.018407292664051056, tv_loss: 0.017194023355841637\n",
      "iteration 1835, dc_loss: 0.01840721070766449, tv_loss: 0.017194056883454323\n",
      "iteration 1836, dc_loss: 0.018407167866826057, tv_loss: 0.017194019630551338\n",
      "iteration 1837, dc_loss: 0.01840713433921337, tv_loss: 0.017193691805005074\n",
      "iteration 1838, dc_loss: 0.018407071009278297, tv_loss: 0.017193341627717018\n",
      "iteration 1839, dc_loss: 0.018407009541988373, tv_loss: 0.017193317413330078\n",
      "iteration 1840, dc_loss: 0.018406955525279045, tv_loss: 0.01719311624765396\n",
      "iteration 1841, dc_loss: 0.018406899645924568, tv_loss: 0.01719299517571926\n",
      "iteration 1842, dc_loss: 0.018406789749860764, tv_loss: 0.017192836850881577\n",
      "iteration 1843, dc_loss: 0.018406659364700317, tv_loss: 0.01719263195991516\n",
      "iteration 1844, dc_loss: 0.01840655878186226, tv_loss: 0.01719263195991516\n",
      "iteration 1845, dc_loss: 0.018406463786959648, tv_loss: 0.01719239167869091\n",
      "iteration 1846, dc_loss: 0.018406393006443977, tv_loss: 0.01719222404062748\n",
      "iteration 1847, dc_loss: 0.018406372517347336, tv_loss: 0.017192136496305466\n",
      "iteration 1848, dc_loss: 0.018406329676508904, tv_loss: 0.0171916913241148\n",
      "iteration 1849, dc_loss: 0.018406305462121964, tv_loss: 0.01719152182340622\n",
      "iteration 1850, dc_loss: 0.018406221643090248, tv_loss: 0.017191294580698013\n",
      "iteration 1851, dc_loss: 0.018406130373477936, tv_loss: 0.01719117909669876\n",
      "iteration 1852, dc_loss: 0.018406052142381668, tv_loss: 0.017191099002957344\n",
      "iteration 1853, dc_loss: 0.01840595155954361, tv_loss: 0.017190946266055107\n",
      "iteration 1854, dc_loss: 0.01840580254793167, tv_loss: 0.017190774902701378\n",
      "iteration 1855, dc_loss: 0.01840563304722309, tv_loss: 0.01719064824283123\n",
      "iteration 1856, dc_loss: 0.018405497074127197, tv_loss: 0.01719079539179802\n",
      "iteration 1857, dc_loss: 0.01840544119477272, tv_loss: 0.017190760001540184\n",
      "iteration 1858, dc_loss: 0.018405405804514885, tv_loss: 0.01719037815928459\n",
      "iteration 1859, dc_loss: 0.01840534806251526, tv_loss: 0.017190149053931236\n",
      "iteration 1860, dc_loss: 0.01840527169406414, tv_loss: 0.017189960926771164\n",
      "iteration 1861, dc_loss: 0.01840522699058056, tv_loss: 0.017189795151352882\n",
      "iteration 1862, dc_loss: 0.018405182287096977, tv_loss: 0.017189735546708107\n",
      "iteration 1863, dc_loss: 0.0184051301330328, tv_loss: 0.0171895120292902\n",
      "iteration 1864, dc_loss: 0.01840507984161377, tv_loss: 0.017189251258969307\n",
      "iteration 1865, dc_loss: 0.01840505190193653, tv_loss: 0.01718924008309841\n",
      "iteration 1866, dc_loss: 0.018404969945549965, tv_loss: 0.01718902587890625\n",
      "iteration 1867, dc_loss: 0.01840486377477646, tv_loss: 0.01718871109187603\n",
      "iteration 1868, dc_loss: 0.018404750153422356, tv_loss: 0.017188603058457375\n",
      "iteration 1869, dc_loss: 0.018404647707939148, tv_loss: 0.017188502475619316\n",
      "iteration 1870, dc_loss: 0.01840454712510109, tv_loss: 0.017188431695103645\n",
      "iteration 1871, dc_loss: 0.01840442232787609, tv_loss: 0.01718827895820141\n",
      "iteration 1872, dc_loss: 0.01840432547032833, tv_loss: 0.01718810200691223\n",
      "iteration 1873, dc_loss: 0.018404267728328705, tv_loss: 0.017187803983688354\n",
      "iteration 1874, dc_loss: 0.018404223024845123, tv_loss: 0.017187712714076042\n",
      "iteration 1875, dc_loss: 0.01840415596961975, tv_loss: 0.017187470570206642\n",
      "iteration 1876, dc_loss: 0.018404072150588036, tv_loss: 0.017187248915433884\n",
      "iteration 1877, dc_loss: 0.01840401627123356, tv_loss: 0.017187219113111496\n",
      "iteration 1878, dc_loss: 0.018403979018330574, tv_loss: 0.017187006771564484\n",
      "iteration 1879, dc_loss: 0.018403910100460052, tv_loss: 0.017186744138598442\n",
      "iteration 1880, dc_loss: 0.018403850495815277, tv_loss: 0.017186589539051056\n",
      "iteration 1881, dc_loss: 0.018403785303235054, tv_loss: 0.017186513170599937\n",
      "iteration 1882, dc_loss: 0.018403714522719383, tv_loss: 0.017186347395181656\n",
      "iteration 1883, dc_loss: 0.01840360462665558, tv_loss: 0.01718616485595703\n",
      "iteration 1884, dc_loss: 0.018403496593236923, tv_loss: 0.01718619279563427\n",
      "iteration 1885, dc_loss: 0.018403392285108566, tv_loss: 0.017186159268021584\n",
      "iteration 1886, dc_loss: 0.01840326189994812, tv_loss: 0.017185937613248825\n",
      "iteration 1887, dc_loss: 0.018403181806206703, tv_loss: 0.01718572899699211\n",
      "iteration 1888, dc_loss: 0.018403124064207077, tv_loss: 0.017185773700475693\n",
      "iteration 1889, dc_loss: 0.01840307004749775, tv_loss: 0.017185671254992485\n",
      "iteration 1890, dc_loss: 0.018403010442852974, tv_loss: 0.017185229808092117\n",
      "iteration 1891, dc_loss: 0.018402932211756706, tv_loss: 0.017184987664222717\n",
      "iteration 1892, dc_loss: 0.018402867019176483, tv_loss: 0.017185155302286148\n",
      "iteration 1893, dc_loss: 0.01840278133749962, tv_loss: 0.017184965312480927\n",
      "iteration 1894, dc_loss: 0.018402719870209694, tv_loss: 0.017184624448418617\n",
      "iteration 1895, dc_loss: 0.01840265654027462, tv_loss: 0.0171845480799675\n",
      "iteration 1896, dc_loss: 0.018402619287371635, tv_loss: 0.01718432642519474\n",
      "iteration 1897, dc_loss: 0.018402567133307457, tv_loss: 0.01718427799642086\n",
      "iteration 1898, dc_loss: 0.018402481451630592, tv_loss: 0.017184095457196236\n",
      "iteration 1899, dc_loss: 0.01840236783027649, tv_loss: 0.01718391664326191\n",
      "iteration 1900, dc_loss: 0.018402231857180595, tv_loss: 0.017184030264616013\n",
      "iteration 1901, dc_loss: 0.018402082845568657, tv_loss: 0.01718396507203579\n",
      "iteration 1902, dc_loss: 0.01840195804834366, tv_loss: 0.017183756455779076\n",
      "iteration 1903, dc_loss: 0.018401863053441048, tv_loss: 0.01718364842236042\n",
      "iteration 1904, dc_loss: 0.018401818349957466, tv_loss: 0.01718352735042572\n",
      "iteration 1905, dc_loss: 0.018401799723505974, tv_loss: 0.0171833299100399\n",
      "iteration 1906, dc_loss: 0.01840178295969963, tv_loss: 0.017183318734169006\n",
      "iteration 1907, dc_loss: 0.018401766195893288, tv_loss: 0.017182834446430206\n",
      "iteration 1908, dc_loss: 0.018401743844151497, tv_loss: 0.017182545736432076\n",
      "iteration 1909, dc_loss: 0.018401717767119408, tv_loss: 0.01718238927423954\n",
      "iteration 1910, dc_loss: 0.01840166375041008, tv_loss: 0.017182204872369766\n",
      "iteration 1911, dc_loss: 0.018401548266410828, tv_loss: 0.017182033509016037\n",
      "iteration 1912, dc_loss: 0.01840141788125038, tv_loss: 0.01718190498650074\n",
      "iteration 1913, dc_loss: 0.018401242792606354, tv_loss: 0.01718183234333992\n",
      "iteration 1914, dc_loss: 0.018401121720671654, tv_loss: 0.01718180812895298\n",
      "iteration 1915, dc_loss: 0.01840103790163994, tv_loss: 0.017181701958179474\n",
      "iteration 1916, dc_loss: 0.018400991335511208, tv_loss: 0.0171815212816\n",
      "iteration 1917, dc_loss: 0.018400948494672775, tv_loss: 0.017181238159537315\n",
      "iteration 1918, dc_loss: 0.018400898203253746, tv_loss: 0.017181115224957466\n",
      "iteration 1919, dc_loss: 0.018400823697447777, tv_loss: 0.017181072384119034\n",
      "iteration 1920, dc_loss: 0.01840076595544815, tv_loss: 0.017180949449539185\n",
      "iteration 1921, dc_loss: 0.018400730565190315, tv_loss: 0.017180882394313812\n",
      "iteration 1922, dc_loss: 0.01840071752667427, tv_loss: 0.017180578783154488\n",
      "iteration 1923, dc_loss: 0.01840071752667427, tv_loss: 0.017180247232317924\n",
      "iteration 1924, dc_loss: 0.018400689586997032, tv_loss: 0.01718008518218994\n",
      "iteration 1925, dc_loss: 0.018400607630610466, tv_loss: 0.01717996597290039\n",
      "iteration 1926, dc_loss: 0.018400508910417557, tv_loss: 0.0171799473464489\n",
      "iteration 1927, dc_loss: 0.018400412052869797, tv_loss: 0.01717974804341793\n",
      "iteration 1928, dc_loss: 0.018400294706225395, tv_loss: 0.01717960834503174\n",
      "iteration 1929, dc_loss: 0.01840020716190338, tv_loss: 0.01717955991625786\n",
      "iteration 1930, dc_loss: 0.01840013824403286, tv_loss: 0.017179332673549652\n",
      "iteration 1931, dc_loss: 0.01840006187558174, tv_loss: 0.01717904582619667\n",
      "iteration 1932, dc_loss: 0.018399974331259727, tv_loss: 0.01717904582619667\n",
      "iteration 1933, dc_loss: 0.01839984394609928, tv_loss: 0.01717907376587391\n",
      "iteration 1934, dc_loss: 0.018399683758616447, tv_loss: 0.017179032787680626\n",
      "iteration 1935, dc_loss: 0.018399571999907494, tv_loss: 0.017178921028971672\n",
      "iteration 1936, dc_loss: 0.018399501219391823, tv_loss: 0.017178576439619064\n",
      "iteration 1937, dc_loss: 0.018399415537714958, tv_loss: 0.01717834174633026\n",
      "iteration 1938, dc_loss: 0.01839936152100563, tv_loss: 0.017178408801555634\n",
      "iteration 1939, dc_loss: 0.01839933916926384, tv_loss: 0.017178364098072052\n",
      "iteration 1940, dc_loss: 0.018399318680167198, tv_loss: 0.017177920788526535\n",
      "iteration 1941, dc_loss: 0.0183993149548769, tv_loss: 0.017177708446979523\n",
      "iteration 1942, dc_loss: 0.018399301916360855, tv_loss: 0.017177654430270195\n",
      "iteration 1943, dc_loss: 0.018399249762296677, tv_loss: 0.01717733033001423\n",
      "iteration 1944, dc_loss: 0.0183991901576519, tv_loss: 0.017177270725369453\n",
      "iteration 1945, dc_loss: 0.01839911751449108, tv_loss: 0.01717728190124035\n",
      "iteration 1946, dc_loss: 0.018399033695459366, tv_loss: 0.017177119851112366\n",
      "iteration 1947, dc_loss: 0.018398962914943695, tv_loss: 0.01717672310769558\n",
      "iteration 1948, dc_loss: 0.01839890144765377, tv_loss: 0.017176715657114983\n",
      "iteration 1949, dc_loss: 0.01839878410100937, tv_loss: 0.017176682129502296\n",
      "iteration 1950, dc_loss: 0.018398627638816833, tv_loss: 0.01717665232717991\n",
      "iteration 1951, dc_loss: 0.018398482352495193, tv_loss: 0.017176564782857895\n",
      "iteration 1952, dc_loss: 0.018398374319076538, tv_loss: 0.017176594585180283\n",
      "iteration 1953, dc_loss: 0.018398292362689972, tv_loss: 0.017176248133182526\n",
      "iteration 1954, dc_loss: 0.01839824952185154, tv_loss: 0.01717621460556984\n",
      "iteration 1955, dc_loss: 0.018398217856884003, tv_loss: 0.01717604510486126\n",
      "iteration 1956, dc_loss: 0.018398214131593704, tv_loss: 0.017175843939185143\n",
      "iteration 1957, dc_loss: 0.018398186191916466, tv_loss: 0.017175674438476562\n",
      "iteration 1958, dc_loss: 0.018398137763142586, tv_loss: 0.01717563532292843\n",
      "iteration 1959, dc_loss: 0.018398061394691467, tv_loss: 0.017175430431962013\n",
      "iteration 1960, dc_loss: 0.018397977575659752, tv_loss: 0.01717538759112358\n",
      "iteration 1961, dc_loss: 0.018397875130176544, tv_loss: 0.017175326123833656\n",
      "iteration 1962, dc_loss: 0.018397780135273933, tv_loss: 0.017175082117319107\n",
      "iteration 1963, dc_loss: 0.018397707492113113, tv_loss: 0.01717500016093254\n",
      "iteration 1964, dc_loss: 0.018397651612758636, tv_loss: 0.017174912616610527\n",
      "iteration 1965, dc_loss: 0.018397584557533264, tv_loss: 0.01717459224164486\n",
      "iteration 1966, dc_loss: 0.018397528678178787, tv_loss: 0.017174506559967995\n",
      "iteration 1967, dc_loss: 0.018397437408566475, tv_loss: 0.017174437642097473\n",
      "iteration 1968, dc_loss: 0.018397348001599312, tv_loss: 0.0171743705868721\n",
      "iteration 1969, dc_loss: 0.018397251144051552, tv_loss: 0.017174314707517624\n",
      "iteration 1970, dc_loss: 0.01839718222618103, tv_loss: 0.01717413403093815\n",
      "iteration 1971, dc_loss: 0.018397118896245956, tv_loss: 0.0171738900244236\n",
      "iteration 1972, dc_loss: 0.018397103995084763, tv_loss: 0.017173798754811287\n",
      "iteration 1973, dc_loss: 0.01839708350598812, tv_loss: 0.017173631116747856\n",
      "iteration 1974, dc_loss: 0.01839701645076275, tv_loss: 0.017173374071717262\n",
      "iteration 1975, dc_loss: 0.01839694194495678, tv_loss: 0.017173033207654953\n",
      "iteration 1976, dc_loss: 0.018396873027086258, tv_loss: 0.017172876745462418\n",
      "iteration 1977, dc_loss: 0.018396802246570587, tv_loss: 0.01717294007539749\n",
      "iteration 1978, dc_loss: 0.01839674636721611, tv_loss: 0.01717269793152809\n",
      "iteration 1979, dc_loss: 0.018396705389022827, tv_loss: 0.01717236265540123\n",
      "iteration 1980, dc_loss: 0.018396640196442604, tv_loss: 0.017172273248434067\n",
      "iteration 1981, dc_loss: 0.018396558240056038, tv_loss: 0.017172295600175858\n",
      "iteration 1982, dc_loss: 0.01839643344283104, tv_loss: 0.017172109335660934\n",
      "iteration 1983, dc_loss: 0.01839636266231537, tv_loss: 0.017171945422887802\n",
      "iteration 1984, dc_loss: 0.01839631050825119, tv_loss: 0.017171723768115044\n",
      "iteration 1985, dc_loss: 0.01839624159038067, tv_loss: 0.017171630635857582\n",
      "iteration 1986, dc_loss: 0.018396148458123207, tv_loss: 0.017171578481793404\n",
      "iteration 1987, dc_loss: 0.018396032974123955, tv_loss: 0.017171572893857956\n",
      "iteration 1988, dc_loss: 0.018395937979221344, tv_loss: 0.017171327024698257\n",
      "iteration 1989, dc_loss: 0.018395869061350822, tv_loss: 0.017171228304505348\n",
      "iteration 1990, dc_loss: 0.018395818769931793, tv_loss: 0.017171155661344528\n",
      "iteration 1991, dc_loss: 0.018395794555544853, tv_loss: 0.017171097919344902\n",
      "iteration 1992, dc_loss: 0.018395783379673958, tv_loss: 0.017170807346701622\n",
      "iteration 1993, dc_loss: 0.018395759165287018, tv_loss: 0.017170587554574013\n",
      "iteration 1994, dc_loss: 0.01839575171470642, tv_loss: 0.017170602455735207\n",
      "iteration 1995, dc_loss: 0.018395716324448586, tv_loss: 0.017170405015349388\n",
      "iteration 1996, dc_loss: 0.018395619466900826, tv_loss: 0.017170168459415436\n",
      "iteration 1997, dc_loss: 0.01839551329612732, tv_loss: 0.01717010699212551\n",
      "iteration 1998, dc_loss: 0.018395371735095978, tv_loss: 0.01717015914618969\n",
      "iteration 1999, dc_loss: 0.018395252525806427, tv_loss: 0.01717008650302887\n",
      "iteration 2000, dc_loss: 0.01839515194296837, tv_loss: 0.01717011258006096\n",
      "iteration 2001, dc_loss: 0.018395084887742996, tv_loss: 0.017169835045933723\n",
      "iteration 2002, dc_loss: 0.018395017832517624, tv_loss: 0.017169609665870667\n",
      "iteration 2003, dc_loss: 0.0183949526399374, tv_loss: 0.017169637605547905\n",
      "iteration 2004, dc_loss: 0.01839490421116352, tv_loss: 0.017169496044516563\n",
      "iteration 2005, dc_loss: 0.018394844606518745, tv_loss: 0.017169246450066566\n",
      "iteration 2006, dc_loss: 0.018394753336906433, tv_loss: 0.017169075086712837\n",
      "iteration 2007, dc_loss: 0.018394671380519867, tv_loss: 0.017169127240777016\n",
      "iteration 2008, dc_loss: 0.018394621089100838, tv_loss: 0.017169147729873657\n",
      "iteration 2009, dc_loss: 0.018394572660326958, tv_loss: 0.017168916761875153\n",
      "iteration 2010, dc_loss: 0.018394500017166138, tv_loss: 0.017168676480650902\n",
      "iteration 2011, dc_loss: 0.018394462764263153, tv_loss: 0.017168667167425156\n",
      "iteration 2012, dc_loss: 0.018394429236650467, tv_loss: 0.01716833934187889\n",
      "iteration 2013, dc_loss: 0.01839437335729599, tv_loss: 0.017168205231428146\n",
      "iteration 2014, dc_loss: 0.01839432306587696, tv_loss: 0.0171681996434927\n",
      "iteration 2015, dc_loss: 0.018394263461232185, tv_loss: 0.017167912796139717\n",
      "iteration 2016, dc_loss: 0.01839417964220047, tv_loss: 0.017167765647172928\n",
      "iteration 2017, dc_loss: 0.018394092097878456, tv_loss: 0.01716776192188263\n",
      "iteration 2018, dc_loss: 0.018394004553556442, tv_loss: 0.01716753840446472\n",
      "iteration 2019, dc_loss: 0.01839394122362137, tv_loss: 0.01716744713485241\n",
      "iteration 2020, dc_loss: 0.018393924459815025, tv_loss: 0.01716729812324047\n",
      "iteration 2021, dc_loss: 0.018393883481621742, tv_loss: 0.017167089506983757\n",
      "iteration 2022, dc_loss: 0.018393831327557564, tv_loss: 0.01716693677008152\n",
      "iteration 2023, dc_loss: 0.01839376427233219, tv_loss: 0.017166784033179283\n",
      "iteration 2024, dc_loss: 0.018393710255622864, tv_loss: 0.017166564241051674\n",
      "iteration 2025, dc_loss: 0.01839364320039749, tv_loss: 0.01716644875705242\n",
      "iteration 2026, dc_loss: 0.018393587321043015, tv_loss: 0.01716657355427742\n",
      "iteration 2027, dc_loss: 0.018393520265817642, tv_loss: 0.017166350036859512\n",
      "iteration 2028, dc_loss: 0.018393438309431076, tv_loss: 0.017166104167699814\n",
      "iteration 2029, dc_loss: 0.01839335449039936, tv_loss: 0.01716608926653862\n",
      "iteration 2030, dc_loss: 0.0183932613581419, tv_loss: 0.01716608926653862\n",
      "iteration 2031, dc_loss: 0.01839316450059414, tv_loss: 0.0171660128980875\n",
      "iteration 2032, dc_loss: 0.018393049016594887, tv_loss: 0.01716606132686138\n",
      "iteration 2033, dc_loss: 0.01839299127459526, tv_loss: 0.017166052013635635\n",
      "iteration 2034, dc_loss: 0.018392935395240784, tv_loss: 0.017165711149573326\n",
      "iteration 2035, dc_loss: 0.0183928944170475, tv_loss: 0.01716551184654236\n",
      "iteration 2036, dc_loss: 0.01839287206530571, tv_loss: 0.017165331169962883\n",
      "iteration 2037, dc_loss: 0.018392853438854218, tv_loss: 0.017165254801511765\n",
      "iteration 2038, dc_loss: 0.018392818048596382, tv_loss: 0.01716506853699684\n",
      "iteration 2039, dc_loss: 0.01839275099337101, tv_loss: 0.01716502569615841\n",
      "iteration 2040, dc_loss: 0.018392687663435936, tv_loss: 0.017164913937449455\n",
      "iteration 2041, dc_loss: 0.01839260384440422, tv_loss: 0.017164787277579308\n",
      "iteration 2042, dc_loss: 0.01839255727827549, tv_loss: 0.017164528369903564\n",
      "iteration 2043, dc_loss: 0.018392503261566162, tv_loss: 0.017164582386612892\n",
      "iteration 2044, dc_loss: 0.018392430618405342, tv_loss: 0.017164478078484535\n",
      "iteration 2045, dc_loss: 0.01839233562350273, tv_loss: 0.017164330929517746\n",
      "iteration 2046, dc_loss: 0.018392270430922508, tv_loss: 0.01716425269842148\n",
      "iteration 2047, dc_loss: 0.018392223864793777, tv_loss: 0.01716410554945469\n",
      "iteration 2048, dc_loss: 0.018392158672213554, tv_loss: 0.0171638373285532\n",
      "iteration 2049, dc_loss: 0.018392087891697884, tv_loss: 0.0171638336032629\n",
      "iteration 2050, dc_loss: 0.01839199848473072, tv_loss: 0.017163734883069992\n",
      "iteration 2051, dc_loss: 0.018391937017440796, tv_loss: 0.017163483425974846\n",
      "iteration 2052, dc_loss: 0.01839185506105423, tv_loss: 0.01716350018978119\n",
      "iteration 2053, dc_loss: 0.018391769379377365, tv_loss: 0.017163408920168877\n",
      "iteration 2054, dc_loss: 0.0183916874229908, tv_loss: 0.017163101583719254\n",
      "iteration 2055, dc_loss: 0.01839159056544304, tv_loss: 0.01716303452849388\n",
      "iteration 2056, dc_loss: 0.018391519784927368, tv_loss: 0.01716308854520321\n",
      "iteration 2057, dc_loss: 0.01839148811995983, tv_loss: 0.01716293953359127\n",
      "iteration 2058, dc_loss: 0.018391478806734085, tv_loss: 0.01716262474656105\n",
      "iteration 2059, dc_loss: 0.01839149184525013, tv_loss: 0.01716245524585247\n",
      "iteration 2060, dc_loss: 0.01839148998260498, tv_loss: 0.017162365838885307\n",
      "iteration 2061, dc_loss: 0.018391454592347145, tv_loss: 0.017162185162305832\n",
      "iteration 2062, dc_loss: 0.01839137263596058, tv_loss: 0.01716199517250061\n",
      "iteration 2063, dc_loss: 0.01839124970138073, tv_loss: 0.017162078991532326\n",
      "iteration 2064, dc_loss: 0.018391123041510582, tv_loss: 0.017162032425403595\n",
      "iteration 2065, dc_loss: 0.01839103363454342, tv_loss: 0.017161894589662552\n",
      "iteration 2066, dc_loss: 0.018390964716672897, tv_loss: 0.017161577939987183\n",
      "iteration 2067, dc_loss: 0.018390929326415062, tv_loss: 0.017161473631858826\n",
      "iteration 2068, dc_loss: 0.018390871584415436, tv_loss: 0.017161516472697258\n",
      "iteration 2069, dc_loss: 0.018390856683254242, tv_loss: 0.01716131903231144\n",
      "iteration 2070, dc_loss: 0.018390841782093048, tv_loss: 0.01716107502579689\n",
      "iteration 2071, dc_loss: 0.018390804529190063, tv_loss: 0.017160987481474876\n",
      "iteration 2072, dc_loss: 0.01839073747396469, tv_loss: 0.017160899937152863\n",
      "iteration 2073, dc_loss: 0.01839064620435238, tv_loss: 0.017160842195153236\n",
      "iteration 2074, dc_loss: 0.018390536308288574, tv_loss: 0.017160624265670776\n",
      "iteration 2075, dc_loss: 0.018390409648418427, tv_loss: 0.017160646617412567\n",
      "iteration 2076, dc_loss: 0.018390290439128876, tv_loss: 0.01716061867773533\n",
      "iteration 2077, dc_loss: 0.018390243873000145, tv_loss: 0.01716039702296257\n",
      "iteration 2078, dc_loss: 0.018390249460935593, tv_loss: 0.017160171642899513\n",
      "iteration 2079, dc_loss: 0.018390247598290443, tv_loss: 0.017159994691610336\n",
      "iteration 2080, dc_loss: 0.018390215933322906, tv_loss: 0.01715998724102974\n",
      "iteration 2081, dc_loss: 0.018390148878097534, tv_loss: 0.017159830778837204\n",
      "iteration 2082, dc_loss: 0.018390068784356117, tv_loss: 0.017159651964902878\n",
      "iteration 2083, dc_loss: 0.018390007317066193, tv_loss: 0.017159439623355865\n",
      "iteration 2084, dc_loss: 0.018389958888292313, tv_loss: 0.017159517854452133\n",
      "iteration 2085, dc_loss: 0.018389901146292686, tv_loss: 0.01715945638716221\n",
      "iteration 2086, dc_loss: 0.01838981918990612, tv_loss: 0.017159372568130493\n",
      "iteration 2087, dc_loss: 0.018389729782938957, tv_loss: 0.01715930551290512\n",
      "iteration 2088, dc_loss: 0.018389632925391197, tv_loss: 0.017159223556518555\n",
      "iteration 2089, dc_loss: 0.01838955283164978, tv_loss: 0.01715911738574505\n",
      "iteration 2090, dc_loss: 0.018389498814940453, tv_loss: 0.017159009352326393\n",
      "iteration 2091, dc_loss: 0.01838945969939232, tv_loss: 0.017158884555101395\n",
      "iteration 2092, dc_loss: 0.018389422446489334, tv_loss: 0.01715865358710289\n",
      "iteration 2093, dc_loss: 0.018389390781521797, tv_loss: 0.017158495262265205\n",
      "iteration 2094, dc_loss: 0.018389323726296425, tv_loss: 0.017158495262265205\n",
      "iteration 2095, dc_loss: 0.018389273434877396, tv_loss: 0.01715833693742752\n",
      "iteration 2096, dc_loss: 0.018389202654361725, tv_loss: 0.01715806871652603\n",
      "iteration 2097, dc_loss: 0.018389131873846054, tv_loss: 0.017158083617687225\n",
      "iteration 2098, dc_loss: 0.01838904246687889, tv_loss: 0.017158078029751778\n",
      "iteration 2099, dc_loss: 0.01838897541165352, tv_loss: 0.017158035188913345\n",
      "iteration 2100, dc_loss: 0.01838889718055725, tv_loss: 0.017157817259430885\n",
      "iteration 2101, dc_loss: 0.018388813361525536, tv_loss: 0.017157673835754395\n",
      "iteration 2102, dc_loss: 0.01838875375688076, tv_loss: 0.01715761609375477\n",
      "iteration 2103, dc_loss: 0.018388714641332626, tv_loss: 0.017157550901174545\n",
      "iteration 2104, dc_loss: 0.018388669937849045, tv_loss: 0.01715741865336895\n",
      "iteration 2105, dc_loss: 0.01838860847055912, tv_loss: 0.01715732179582119\n",
      "iteration 2106, dc_loss: 0.01838856190443039, tv_loss: 0.017157047986984253\n",
      "iteration 2107, dc_loss: 0.018388532102108, tv_loss: 0.017156919464468956\n",
      "iteration 2108, dc_loss: 0.01838848367333412, tv_loss: 0.017156850546598434\n",
      "iteration 2109, dc_loss: 0.0183884184807539, tv_loss: 0.017156781628727913\n",
      "iteration 2110, dc_loss: 0.01838837005198002, tv_loss: 0.017156723886728287\n",
      "iteration 2111, dc_loss: 0.018388353288173676, tv_loss: 0.017156625166535378\n",
      "iteration 2112, dc_loss: 0.018388278782367706, tv_loss: 0.0171564519405365\n",
      "iteration 2113, dc_loss: 0.01838817447423935, tv_loss: 0.017156295478343964\n",
      "iteration 2114, dc_loss: 0.018388107419013977, tv_loss: 0.017156235873699188\n",
      "iteration 2115, dc_loss: 0.018388062715530396, tv_loss: 0.017156265676021576\n",
      "iteration 2116, dc_loss: 0.018387988209724426, tv_loss: 0.017156079411506653\n",
      "iteration 2117, dc_loss: 0.018387900665402412, tv_loss: 0.017156025394797325\n",
      "iteration 2118, dc_loss: 0.018387824296951294, tv_loss: 0.01715603657066822\n",
      "iteration 2119, dc_loss: 0.018387800082564354, tv_loss: 0.017155656591057777\n",
      "iteration 2120, dc_loss: 0.018387794494628906, tv_loss: 0.01715552806854248\n",
      "iteration 2121, dc_loss: 0.018387744203209877, tv_loss: 0.017155462875962257\n",
      "iteration 2122, dc_loss: 0.01838766038417816, tv_loss: 0.017155364155769348\n",
      "iteration 2123, dc_loss: 0.01838758960366249, tv_loss: 0.017155228182673454\n",
      "iteration 2124, dc_loss: 0.018387511372566223, tv_loss: 0.01715502142906189\n",
      "iteration 2125, dc_loss: 0.018387483432888985, tv_loss: 0.01715501956641674\n",
      "iteration 2126, dc_loss: 0.01838746853172779, tv_loss: 0.017154864966869354\n",
      "iteration 2127, dc_loss: 0.01838742196559906, tv_loss: 0.017154725268483162\n",
      "iteration 2128, dc_loss: 0.018387380987405777, tv_loss: 0.017154626548290253\n",
      "iteration 2129, dc_loss: 0.018387334421277046, tv_loss: 0.01715453900396824\n",
      "iteration 2130, dc_loss: 0.018387306481599808, tv_loss: 0.017154326662421227\n",
      "iteration 2131, dc_loss: 0.018387196585536003, tv_loss: 0.017154347151517868\n",
      "iteration 2132, dc_loss: 0.018387097865343094, tv_loss: 0.017154304310679436\n",
      "iteration 2133, dc_loss: 0.01838701404631138, tv_loss: 0.01715402491390705\n",
      "iteration 2134, dc_loss: 0.018386948853731155, tv_loss: 0.017154105007648468\n",
      "iteration 2135, dc_loss: 0.018386906012892723, tv_loss: 0.017154138535261154\n",
      "iteration 2136, dc_loss: 0.01838689297437668, tv_loss: 0.017153749242424965\n",
      "iteration 2137, dc_loss: 0.0183868370950222, tv_loss: 0.017153633758425713\n",
      "iteration 2138, dc_loss: 0.018386758863925934, tv_loss: 0.01715364307165146\n",
      "iteration 2139, dc_loss: 0.018386688083410263, tv_loss: 0.017153704538941383\n",
      "iteration 2140, dc_loss: 0.0183865986764431, tv_loss: 0.017153551802039146\n",
      "iteration 2141, dc_loss: 0.018386542797088623, tv_loss: 0.01715342327952385\n",
      "iteration 2142, dc_loss: 0.01838649995625019, tv_loss: 0.017153330147266388\n",
      "iteration 2143, dc_loss: 0.018386466428637505, tv_loss: 0.01715327613055706\n",
      "iteration 2144, dc_loss: 0.01838642731308937, tv_loss: 0.017153101041913033\n",
      "iteration 2145, dc_loss: 0.01838635839521885, tv_loss: 0.017152901738882065\n",
      "iteration 2146, dc_loss: 0.01838628761470318, tv_loss: 0.017152829095721245\n",
      "iteration 2147, dc_loss: 0.0183862391859293, tv_loss: 0.017152955755591393\n",
      "iteration 2148, dc_loss: 0.018386175855994225, tv_loss: 0.017152782529592514\n",
      "iteration 2149, dc_loss: 0.018386097624897957, tv_loss: 0.017152415588498116\n",
      "iteration 2150, dc_loss: 0.018386049196124077, tv_loss: 0.017152369022369385\n",
      "iteration 2151, dc_loss: 0.0183859970420599, tv_loss: 0.01715247333049774\n",
      "iteration 2152, dc_loss: 0.01838594116270542, tv_loss: 0.017152193933725357\n",
      "iteration 2153, dc_loss: 0.018385903909802437, tv_loss: 0.01715194247663021\n",
      "iteration 2154, dc_loss: 0.0183858722448349, tv_loss: 0.01715179719030857\n",
      "iteration 2155, dc_loss: 0.01838587038218975, tv_loss: 0.0171518437564373\n",
      "iteration 2156, dc_loss: 0.018385855481028557, tv_loss: 0.017151769250631332\n",
      "iteration 2157, dc_loss: 0.01838580332696438, tv_loss: 0.017151420935988426\n",
      "iteration 2158, dc_loss: 0.0183857548981905, tv_loss: 0.01715131849050522\n",
      "iteration 2159, dc_loss: 0.01838568039238453, tv_loss: 0.017151234671473503\n",
      "iteration 2160, dc_loss: 0.01838558353483677, tv_loss: 0.01715138554573059\n",
      "iteration 2161, dc_loss: 0.01838546060025692, tv_loss: 0.017151204869151115\n",
      "iteration 2162, dc_loss: 0.01838536560535431, tv_loss: 0.017151011154055595\n",
      "iteration 2163, dc_loss: 0.01838531345129013, tv_loss: 0.01715111918747425\n",
      "iteration 2164, dc_loss: 0.018385276198387146, tv_loss: 0.017151039093732834\n",
      "iteration 2165, dc_loss: 0.018385235220193863, tv_loss: 0.017150720581412315\n",
      "iteration 2166, dc_loss: 0.01838519424200058, tv_loss: 0.01715080812573433\n",
      "iteration 2167, dc_loss: 0.01838516816496849, tv_loss: 0.017150746658444405\n",
      "iteration 2168, dc_loss: 0.01838514767587185, tv_loss: 0.01715049333870411\n",
      "iteration 2169, dc_loss: 0.018385110422968864, tv_loss: 0.01715041510760784\n",
      "iteration 2170, dc_loss: 0.018385019153356552, tv_loss: 0.01715043932199478\n",
      "iteration 2171, dc_loss: 0.018384922295808792, tv_loss: 0.017150385305285454\n",
      "iteration 2172, dc_loss: 0.018384842202067375, tv_loss: 0.017150381579995155\n",
      "iteration 2173, dc_loss: 0.0183847825974226, tv_loss: 0.017150061205029488\n",
      "iteration 2174, dc_loss: 0.018384721130132675, tv_loss: 0.01714995875954628\n",
      "iteration 2175, dc_loss: 0.01838468760251999, tv_loss: 0.017149941995739937\n",
      "iteration 2176, dc_loss: 0.018384644761681557, tv_loss: 0.017149681225419044\n",
      "iteration 2177, dc_loss: 0.01838461123406887, tv_loss: 0.017149580642580986\n",
      "iteration 2178, dc_loss: 0.01838456280529499, tv_loss: 0.017149372026324272\n",
      "iteration 2179, dc_loss: 0.018384508788585663, tv_loss: 0.017149345949292183\n",
      "iteration 2180, dc_loss: 0.018384482711553574, tv_loss: 0.017149167135357857\n",
      "iteration 2181, dc_loss: 0.01838446967303753, tv_loss: 0.01714904233813286\n",
      "iteration 2182, dc_loss: 0.018384426832199097, tv_loss: 0.017149001359939575\n",
      "iteration 2183, dc_loss: 0.01838436909019947, tv_loss: 0.01714891567826271\n",
      "iteration 2184, dc_loss: 0.01838427409529686, tv_loss: 0.01714864745736122\n",
      "iteration 2185, dc_loss: 0.0183841735124588, tv_loss: 0.01714865118265152\n",
      "iteration 2186, dc_loss: 0.018384119495749474, tv_loss: 0.017148565500974655\n",
      "iteration 2187, dc_loss: 0.018384067341685295, tv_loss: 0.01714853011071682\n",
      "iteration 2188, dc_loss: 0.01838398352265358, tv_loss: 0.01714838668704033\n",
      "iteration 2189, dc_loss: 0.01838388480246067, tv_loss: 0.017148299142718315\n",
      "iteration 2190, dc_loss: 0.018383823335170746, tv_loss: 0.017148349434137344\n",
      "iteration 2191, dc_loss: 0.018383773043751717, tv_loss: 0.017148224636912346\n",
      "iteration 2192, dc_loss: 0.018383760005235672, tv_loss: 0.017148016020655632\n",
      "iteration 2193, dc_loss: 0.018383756279945374, tv_loss: 0.017147855833172798\n",
      "iteration 2194, dc_loss: 0.018383726477622986, tv_loss: 0.01714766025543213\n",
      "iteration 2195, dc_loss: 0.018383678048849106, tv_loss: 0.017147738486528397\n",
      "iteration 2196, dc_loss: 0.018383610993623734, tv_loss: 0.017147688195109367\n",
      "iteration 2197, dc_loss: 0.018383560702204704, tv_loss: 0.017147544771432877\n",
      "iteration 2198, dc_loss: 0.01838351972401142, tv_loss: 0.017147386446595192\n",
      "iteration 2199, dc_loss: 0.018383480608463287, tv_loss: 0.017147354781627655\n",
      "iteration 2200, dc_loss: 0.018383421003818512, tv_loss: 0.017147256061434746\n",
      "iteration 2201, dc_loss: 0.01838335581123829, tv_loss: 0.017147280275821686\n",
      "iteration 2202, dc_loss: 0.018383268266916275, tv_loss: 0.01714714616537094\n",
      "iteration 2203, dc_loss: 0.01838318258523941, tv_loss: 0.017146972939372063\n",
      "iteration 2204, dc_loss: 0.018383126705884933, tv_loss: 0.017146961763501167\n",
      "iteration 2205, dc_loss: 0.018383115530014038, tv_loss: 0.017146892845630646\n",
      "iteration 2206, dc_loss: 0.018383102491497993, tv_loss: 0.017146557569503784\n",
      "iteration 2207, dc_loss: 0.018383067101240158, tv_loss: 0.017146460711956024\n",
      "iteration 2208, dc_loss: 0.018382981419563293, tv_loss: 0.017146563157439232\n",
      "iteration 2209, dc_loss: 0.018382884562015533, tv_loss: 0.017146430909633636\n",
      "iteration 2210, dc_loss: 0.018382808193564415, tv_loss: 0.017146237194538116\n",
      "iteration 2211, dc_loss: 0.018382737413048744, tv_loss: 0.01714608445763588\n",
      "iteration 2212, dc_loss: 0.018382707610726357, tv_loss: 0.01714613474905491\n",
      "iteration 2213, dc_loss: 0.01838269829750061, tv_loss: 0.017145968973636627\n",
      "iteration 2214, dc_loss: 0.01838264800608158, tv_loss: 0.01714600995182991\n",
      "iteration 2215, dc_loss: 0.018382586538791656, tv_loss: 0.0171459149569273\n",
      "iteration 2216, dc_loss: 0.018382519483566284, tv_loss: 0.017145713791251183\n",
      "iteration 2217, dc_loss: 0.01838243380188942, tv_loss: 0.017145656049251556\n",
      "iteration 2218, dc_loss: 0.018382351845502853, tv_loss: 0.017145778983831406\n",
      "iteration 2219, dc_loss: 0.018382316455245018, tv_loss: 0.017145486548542976\n",
      "iteration 2220, dc_loss: 0.018382320180535316, tv_loss: 0.017145194113254547\n",
      "iteration 2221, dc_loss: 0.018382340669631958, tv_loss: 0.017145207151770592\n",
      "iteration 2222, dc_loss: 0.018382320180535316, tv_loss: 0.01714498922228813\n",
      "iteration 2223, dc_loss: 0.018382271751761436, tv_loss: 0.01714489609003067\n",
      "iteration 2224, dc_loss: 0.018382208421826363, tv_loss: 0.01714487187564373\n",
      "iteration 2225, dc_loss: 0.018382135778665543, tv_loss: 0.01714481972157955\n",
      "iteration 2226, dc_loss: 0.01838204450905323, tv_loss: 0.017144713550806046\n",
      "iteration 2227, dc_loss: 0.018381943926215172, tv_loss: 0.017144715413451195\n",
      "iteration 2228, dc_loss: 0.01838185079395771, tv_loss: 0.017144616693258286\n",
      "iteration 2229, dc_loss: 0.018381791189312935, tv_loss: 0.017144598066806793\n",
      "iteration 2230, dc_loss: 0.0183817520737648, tv_loss: 0.017144469544291496\n",
      "iteration 2231, dc_loss: 0.01838170923292637, tv_loss: 0.017144300043582916\n",
      "iteration 2232, dc_loss: 0.01838168129324913, tv_loss: 0.017144154757261276\n",
      "iteration 2233, dc_loss: 0.018381692469120026, tv_loss: 0.01714395545423031\n",
      "iteration 2234, dc_loss: 0.018381692469120026, tv_loss: 0.01714373752474785\n",
      "iteration 2235, dc_loss: 0.018381645902991295, tv_loss: 0.01714373379945755\n",
      "iteration 2236, dc_loss: 0.01838156208395958, tv_loss: 0.017143698409199715\n",
      "iteration 2237, dc_loss: 0.018381455913186073, tv_loss: 0.017143668606877327\n",
      "iteration 2238, dc_loss: 0.018381377682089806, tv_loss: 0.017143571749329567\n",
      "iteration 2239, dc_loss: 0.018381381407380104, tv_loss: 0.017143474891781807\n",
      "iteration 2240, dc_loss: 0.018381357192993164, tv_loss: 0.017143240198493004\n",
      "iteration 2241, dc_loss: 0.018381299450993538, tv_loss: 0.017143167555332184\n",
      "iteration 2242, dc_loss: 0.018381230533123016, tv_loss: 0.01714320108294487\n",
      "iteration 2243, dc_loss: 0.018381139263510704, tv_loss: 0.01714318059384823\n",
      "iteration 2244, dc_loss: 0.01838103123009205, tv_loss: 0.017143143340945244\n",
      "iteration 2245, dc_loss: 0.01838097721338272, tv_loss: 0.017143024131655693\n",
      "iteration 2246, dc_loss: 0.018380967900156975, tv_loss: 0.017142770811915398\n",
      "iteration 2247, dc_loss: 0.018380967900156975, tv_loss: 0.017142511904239655\n",
      "iteration 2248, dc_loss: 0.01838095113635063, tv_loss: 0.017142510041594505\n",
      "iteration 2249, dc_loss: 0.01838092878460884, tv_loss: 0.017142407596111298\n",
      "iteration 2250, dc_loss: 0.018380891531705856, tv_loss: 0.017142312601208687\n",
      "iteration 2251, dc_loss: 0.01838083565235138, tv_loss: 0.01714216358959675\n",
      "iteration 2252, dc_loss: 0.018380753695964813, tv_loss: 0.01714223437011242\n",
      "iteration 2253, dc_loss: 0.018380705267190933, tv_loss: 0.017142126336693764\n",
      "iteration 2254, dc_loss: 0.01838066428899765, tv_loss: 0.017141928896307945\n",
      "iteration 2255, dc_loss: 0.01838059537112713, tv_loss: 0.017141740769147873\n",
      "iteration 2256, dc_loss: 0.018380509689450264, tv_loss: 0.0171419158577919\n",
      "iteration 2257, dc_loss: 0.018380464985966682, tv_loss: 0.017141863703727722\n",
      "iteration 2258, dc_loss: 0.01838041841983795, tv_loss: 0.017141519114375114\n",
      "iteration 2259, dc_loss: 0.018380336463451385, tv_loss: 0.01714158058166504\n",
      "iteration 2260, dc_loss: 0.01838025078177452, tv_loss: 0.017141664400696754\n",
      "iteration 2261, dc_loss: 0.018380189314484596, tv_loss: 0.0171414352953434\n",
      "iteration 2262, dc_loss: 0.01838013343513012, tv_loss: 0.017141414806246758\n",
      "iteration 2263, dc_loss: 0.018380118533968925, tv_loss: 0.017141282558441162\n",
      "iteration 2264, dc_loss: 0.01838013157248497, tv_loss: 0.01714116334915161\n",
      "iteration 2265, dc_loss: 0.01838015578687191, tv_loss: 0.017140980809926987\n",
      "iteration 2266, dc_loss: 0.01838013343513012, tv_loss: 0.017140723764896393\n",
      "iteration 2267, dc_loss: 0.018380071967840195, tv_loss: 0.017140690237283707\n",
      "iteration 2268, dc_loss: 0.01838003471493721, tv_loss: 0.017140833660960197\n",
      "iteration 2269, dc_loss: 0.01837998814880848, tv_loss: 0.017140517011284828\n",
      "iteration 2270, dc_loss: 0.01837991364300251, tv_loss: 0.01714041829109192\n",
      "iteration 2271, dc_loss: 0.018379846587777138, tv_loss: 0.017140528187155724\n",
      "iteration 2272, dc_loss: 0.01837977208197117, tv_loss: 0.017140457406640053\n",
      "iteration 2273, dc_loss: 0.018379731103777885, tv_loss: 0.01714024879038334\n",
      "iteration 2274, dc_loss: 0.018379662185907364, tv_loss: 0.017140371724963188\n",
      "iteration 2275, dc_loss: 0.01837957464158535, tv_loss: 0.01714020036160946\n",
      "iteration 2276, dc_loss: 0.01837950199842453, tv_loss: 0.017139950767159462\n",
      "iteration 2277, dc_loss: 0.018379468470811844, tv_loss: 0.017139950767159462\n",
      "iteration 2278, dc_loss: 0.018379466608166695, tv_loss: 0.017140137031674385\n",
      "iteration 2279, dc_loss: 0.018379418179392815, tv_loss: 0.017139870673418045\n",
      "iteration 2280, dc_loss: 0.01837933249771595, tv_loss: 0.01713973470032215\n",
      "iteration 2281, dc_loss: 0.018379246816039085, tv_loss: 0.01713978685438633\n",
      "iteration 2282, dc_loss: 0.018379203975200653, tv_loss: 0.01713971234858036\n",
      "iteration 2283, dc_loss: 0.018379202112555504, tv_loss: 0.017139507457613945\n",
      "iteration 2284, dc_loss: 0.018379196524620056, tv_loss: 0.017139455303549767\n",
      "iteration 2285, dc_loss: 0.01837918534874916, tv_loss: 0.017139270901679993\n",
      "iteration 2286, dc_loss: 0.018379146233201027, tv_loss: 0.017139233648777008\n",
      "iteration 2287, dc_loss: 0.01837906427681446, tv_loss: 0.017139174044132233\n",
      "iteration 2288, dc_loss: 0.01837896928191185, tv_loss: 0.017139120027422905\n",
      "iteration 2289, dc_loss: 0.01837889850139618, tv_loss: 0.017139188945293427\n",
      "iteration 2290, dc_loss: 0.018378807231783867, tv_loss: 0.017138879746198654\n",
      "iteration 2291, dc_loss: 0.018378762528300285, tv_loss: 0.01713884435594082\n",
      "iteration 2292, dc_loss: 0.018378762528300285, tv_loss: 0.017138808965682983\n",
      "iteration 2293, dc_loss: 0.018378766253590584, tv_loss: 0.017138483002781868\n",
      "iteration 2294, dc_loss: 0.01837877184152603, tv_loss: 0.017138386145234108\n",
      "iteration 2295, dc_loss: 0.018378743901848793, tv_loss: 0.017138369381427765\n",
      "iteration 2296, dc_loss: 0.01837870106101036, tv_loss: 0.017138229683041573\n",
      "iteration 2297, dc_loss: 0.01837863214313984, tv_loss: 0.017138144001364708\n",
      "iteration 2298, dc_loss: 0.018378570675849915, tv_loss: 0.01713825948536396\n",
      "iteration 2299, dc_loss: 0.018378522247076035, tv_loss: 0.017138127237558365\n",
      "iteration 2300, dc_loss: 0.018378475680947304, tv_loss: 0.017138037830591202\n",
      "iteration 2301, dc_loss: 0.018378425389528275, tv_loss: 0.017137866467237473\n",
      "iteration 2302, dc_loss: 0.018378354609012604, tv_loss: 0.017137747257947922\n",
      "iteration 2303, dc_loss: 0.01837831921875477, tv_loss: 0.017137542366981506\n",
      "iteration 2304, dc_loss: 0.018378281965851784, tv_loss: 0.01713746041059494\n",
      "iteration 2305, dc_loss: 0.018378237262368202, tv_loss: 0.017137454822659492\n",
      "iteration 2306, dc_loss: 0.018378188833594322, tv_loss: 0.017137398943305016\n",
      "iteration 2307, dc_loss: 0.01837810128927231, tv_loss: 0.01713727042078972\n",
      "iteration 2308, dc_loss: 0.01837802305817604, tv_loss: 0.017137259244918823\n",
      "iteration 2309, dc_loss: 0.018377959728240967, tv_loss: 0.017137305811047554\n",
      "iteration 2310, dc_loss: 0.01837790012359619, tv_loss: 0.017137261107563972\n",
      "iteration 2311, dc_loss: 0.01837782748043537, tv_loss: 0.017137156799435616\n",
      "iteration 2312, dc_loss: 0.01837780885398388, tv_loss: 0.017137126997113228\n",
      "iteration 2313, dc_loss: 0.018377825617790222, tv_loss: 0.017137199640274048\n",
      "iteration 2314, dc_loss: 0.018377795815467834, tv_loss: 0.017136866226792336\n",
      "iteration 2315, dc_loss: 0.018377771601080894, tv_loss: 0.017136866226792336\n",
      "iteration 2316, dc_loss: 0.018377764150500298, tv_loss: 0.017136866226792336\n",
      "iteration 2317, dc_loss: 0.01837773062288761, tv_loss: 0.017136627808213234\n",
      "iteration 2318, dc_loss: 0.018377643078565598, tv_loss: 0.01713649556040764\n",
      "iteration 2319, dc_loss: 0.01837758719921112, tv_loss: 0.01713641732931137\n",
      "iteration 2320, dc_loss: 0.018377553671598434, tv_loss: 0.01713629998266697\n",
      "iteration 2321, dc_loss: 0.01837751269340515, tv_loss: 0.017136409878730774\n",
      "iteration 2322, dc_loss: 0.01837741956114769, tv_loss: 0.017136283218860626\n",
      "iteration 2323, dc_loss: 0.01837732456624508, tv_loss: 0.017136018723249435\n",
      "iteration 2324, dc_loss: 0.018377240747213364, tv_loss: 0.01713608019053936\n",
      "iteration 2325, dc_loss: 0.018377183005213737, tv_loss: 0.01713602989912033\n",
      "iteration 2326, dc_loss: 0.018377110362052917, tv_loss: 0.01713590882718563\n",
      "iteration 2327, dc_loss: 0.018377067521214485, tv_loss: 0.017135951668024063\n",
      "iteration 2328, dc_loss: 0.018377048894762993, tv_loss: 0.017135905101895332\n",
      "iteration 2329, dc_loss: 0.018377065658569336, tv_loss: 0.01713557355105877\n",
      "iteration 2330, dc_loss: 0.018377091735601425, tv_loss: 0.017135562375187874\n",
      "iteration 2331, dc_loss: 0.018377115949988365, tv_loss: 0.017135441303253174\n",
      "iteration 2332, dc_loss: 0.01837710663676262, tv_loss: 0.017135147005319595\n",
      "iteration 2333, dc_loss: 0.018377045169472694, tv_loss: 0.01713523641228676\n",
      "iteration 2334, dc_loss: 0.01837693713605404, tv_loss: 0.017135242000222206\n",
      "iteration 2335, dc_loss: 0.018376801162958145, tv_loss: 0.017135243862867355\n",
      "iteration 2336, dc_loss: 0.01837671734392643, tv_loss: 0.017135262489318848\n",
      "iteration 2337, dc_loss: 0.018376639112830162, tv_loss: 0.01713509112596512\n",
      "iteration 2338, dc_loss: 0.018376579508185387, tv_loss: 0.017134997993707657\n",
      "iteration 2339, dc_loss: 0.01837654411792755, tv_loss: 0.017135022208094597\n",
      "iteration 2340, dc_loss: 0.01837652176618576, tv_loss: 0.01713486574590206\n",
      "iteration 2341, dc_loss: 0.018376527354121208, tv_loss: 0.017134593799710274\n",
      "iteration 2342, dc_loss: 0.018376542255282402, tv_loss: 0.017134686931967735\n",
      "iteration 2343, dc_loss: 0.01837652549147606, tv_loss: 0.01713443733751774\n",
      "iteration 2344, dc_loss: 0.01837649568915367, tv_loss: 0.017134323716163635\n",
      "iteration 2345, dc_loss: 0.018376456573605537, tv_loss: 0.01713435724377632\n",
      "iteration 2346, dc_loss: 0.018376391381025314, tv_loss: 0.017134208232164383\n",
      "iteration 2347, dc_loss: 0.01837632805109024, tv_loss: 0.01713407412171364\n",
      "iteration 2348, dc_loss: 0.01837627775967121, tv_loss: 0.017134223133325577\n",
      "iteration 2349, dc_loss: 0.01837620697915554, tv_loss: 0.017134184017777443\n",
      "iteration 2350, dc_loss: 0.018376130610704422, tv_loss: 0.017133979126811028\n",
      "iteration 2351, dc_loss: 0.01837605983018875, tv_loss: 0.01713396981358528\n",
      "iteration 2352, dc_loss: 0.01837601698935032, tv_loss: 0.01713387295603752\n",
      "iteration 2353, dc_loss: 0.01837596669793129, tv_loss: 0.017133774235844612\n",
      "iteration 2354, dc_loss: 0.018375955522060394, tv_loss: 0.017133574932813644\n",
      "iteration 2355, dc_loss: 0.01837594248354435, tv_loss: 0.017133397981524467\n",
      "iteration 2356, dc_loss: 0.01837593875825405, tv_loss: 0.01713324896991253\n",
      "iteration 2357, dc_loss: 0.018375948071479797, tv_loss: 0.017133193090558052\n",
      "iteration 2358, dc_loss: 0.0183759406208992, tv_loss: 0.01713310182094574\n",
      "iteration 2359, dc_loss: 0.01837586611509323, tv_loss: 0.01713288389146328\n",
      "iteration 2360, dc_loss: 0.018375786021351814, tv_loss: 0.0171329565346241\n",
      "iteration 2361, dc_loss: 0.018375694751739502, tv_loss: 0.017132990062236786\n",
      "iteration 2362, dc_loss: 0.018375596031546593, tv_loss: 0.017133040353655815\n",
      "iteration 2363, dc_loss: 0.018375534564256668, tv_loss: 0.017132965847849846\n",
      "iteration 2364, dc_loss: 0.01837548054754734, tv_loss: 0.017132669687271118\n",
      "iteration 2365, dc_loss: 0.018375394865870476, tv_loss: 0.01713269017636776\n",
      "iteration 2366, dc_loss: 0.018375344574451447, tv_loss: 0.017132746055722237\n",
      "iteration 2367, dc_loss: 0.01837531477212906, tv_loss: 0.01713269017636776\n",
      "iteration 2368, dc_loss: 0.018375324085354805, tv_loss: 0.017132285982370377\n",
      "iteration 2369, dc_loss: 0.01837538555264473, tv_loss: 0.017132233828306198\n",
      "iteration 2370, dc_loss: 0.01837540790438652, tv_loss: 0.017132168635725975\n",
      "iteration 2371, dc_loss: 0.018375374376773834, tv_loss: 0.017132077366113663\n",
      "iteration 2372, dc_loss: 0.018375305458903313, tv_loss: 0.017132004722952843\n",
      "iteration 2373, dc_loss: 0.01837521232664585, tv_loss: 0.017131952568888664\n",
      "iteration 2374, dc_loss: 0.01837511919438839, tv_loss: 0.017132103443145752\n",
      "iteration 2375, dc_loss: 0.01837504655122757, tv_loss: 0.017131978645920753\n",
      "iteration 2376, dc_loss: 0.01837502233684063, tv_loss: 0.01713189296424389\n",
      "iteration 2377, dc_loss: 0.018374990671873093, tv_loss: 0.017131848260760307\n",
      "iteration 2378, dc_loss: 0.018374966457486153, tv_loss: 0.017131704837083817\n",
      "iteration 2379, dc_loss: 0.018374931067228317, tv_loss: 0.01713171973824501\n",
      "iteration 2380, dc_loss: 0.018374910578131676, tv_loss: 0.017131537199020386\n",
      "iteration 2381, dc_loss: 0.018374864012002945, tv_loss: 0.017131300643086433\n",
      "iteration 2382, dc_loss: 0.018374819308519363, tv_loss: 0.01713121309876442\n",
      "iteration 2383, dc_loss: 0.018374767154455185, tv_loss: 0.01713121309876442\n",
      "iteration 2384, dc_loss: 0.018374770879745483, tv_loss: 0.017131123691797256\n",
      "iteration 2385, dc_loss: 0.018374763429164886, tv_loss: 0.017131073400378227\n",
      "iteration 2386, dc_loss: 0.018374713137745857, tv_loss: 0.01713103987276554\n",
      "iteration 2387, dc_loss: 0.018374629318714142, tv_loss: 0.017130805179476738\n",
      "iteration 2388, dc_loss: 0.018374545499682426, tv_loss: 0.01713077537715435\n",
      "iteration 2389, dc_loss: 0.01837451383471489, tv_loss: 0.017130862921476364\n",
      "iteration 2390, dc_loss: 0.018374472856521606, tv_loss: 0.017130883410573006\n",
      "iteration 2391, dc_loss: 0.018374456092715263, tv_loss: 0.017130732536315918\n",
      "iteration 2392, dc_loss: 0.018374435603618622, tv_loss: 0.017130402848124504\n",
      "iteration 2393, dc_loss: 0.018374400213360786, tv_loss: 0.017130399122834206\n",
      "iteration 2394, dc_loss: 0.01837431825697422, tv_loss: 0.017130471765995026\n",
      "iteration 2395, dc_loss: 0.018374236300587654, tv_loss: 0.01713048480451107\n",
      "iteration 2396, dc_loss: 0.018374184146523476, tv_loss: 0.017130209133028984\n",
      "iteration 2397, dc_loss: 0.018374169245362282, tv_loss: 0.017130263149738312\n",
      "iteration 2398, dc_loss: 0.018374180421233177, tv_loss: 0.01713002286851406\n",
      "iteration 2399, dc_loss: 0.01837417297065258, tv_loss: 0.01713000237941742\n",
      "iteration 2400, dc_loss: 0.018374145030975342, tv_loss: 0.017129911109805107\n",
      "iteration 2401, dc_loss: 0.01837412267923355, tv_loss: 0.017129812389612198\n",
      "iteration 2402, dc_loss: 0.01837407611310482, tv_loss: 0.017129795625805855\n",
      "iteration 2403, dc_loss: 0.018374016508460045, tv_loss: 0.01712965779006481\n",
      "iteration 2404, dc_loss: 0.018373919650912285, tv_loss: 0.017129555344581604\n",
      "iteration 2405, dc_loss: 0.01837383210659027, tv_loss: 0.017129499465227127\n",
      "iteration 2406, dc_loss: 0.01837378367781639, tv_loss: 0.01712949387729168\n",
      "iteration 2407, dc_loss: 0.01837371662259102, tv_loss: 0.01712941937148571\n",
      "iteration 2408, dc_loss: 0.0183736402541399, tv_loss: 0.017129458487033844\n",
      "iteration 2409, dc_loss: 0.018373606726527214, tv_loss: 0.01712927222251892\n",
      "iteration 2410, dc_loss: 0.01837357133626938, tv_loss: 0.017129182815551758\n",
      "iteration 2411, dc_loss: 0.018373539671301842, tv_loss: 0.017129257321357727\n",
      "iteration 2412, dc_loss: 0.018373487517237663, tv_loss: 0.017129015177488327\n",
      "iteration 2413, dc_loss: 0.01837342418730259, tv_loss: 0.017128994688391685\n",
      "iteration 2414, dc_loss: 0.018373414874076843, tv_loss: 0.017128773033618927\n",
      "iteration 2415, dc_loss: 0.01837342604994774, tv_loss: 0.01712864451110363\n",
      "iteration 2416, dc_loss: 0.018373407423496246, tv_loss: 0.01712874323129654\n",
      "iteration 2417, dc_loss: 0.018373383209109306, tv_loss: 0.01712842658162117\n",
      "iteration 2418, dc_loss: 0.01837334968149662, tv_loss: 0.017128296196460724\n",
      "iteration 2419, dc_loss: 0.018373312428593636, tv_loss: 0.017128458246588707\n",
      "iteration 2420, dc_loss: 0.018373262137174606, tv_loss: 0.017128407955169678\n",
      "iteration 2421, dc_loss: 0.01837320066988468, tv_loss: 0.01712806336581707\n",
      "iteration 2422, dc_loss: 0.018373165279626846, tv_loss: 0.017127947881817818\n",
      "iteration 2423, dc_loss: 0.018373124301433563, tv_loss: 0.017128271982073784\n",
      "iteration 2424, dc_loss: 0.018373090773820877, tv_loss: 0.017128100618720055\n",
      "iteration 2425, dc_loss: 0.01837310753762722, tv_loss: 0.01712779700756073\n",
      "iteration 2426, dc_loss: 0.01837313361465931, tv_loss: 0.017127880826592445\n",
      "iteration 2427, dc_loss: 0.018373094499111176, tv_loss: 0.01712770015001297\n",
      "iteration 2428, dc_loss: 0.018372979015111923, tv_loss: 0.017127687111496925\n",
      "iteration 2429, dc_loss: 0.01837289333343506, tv_loss: 0.017127739265561104\n",
      "iteration 2430, dc_loss: 0.01837281510233879, tv_loss: 0.017127688974142075\n",
      "iteration 2431, dc_loss: 0.018372753635048866, tv_loss: 0.017127662897109985\n",
      "iteration 2432, dc_loss: 0.01837274059653282, tv_loss: 0.017127713188529015\n",
      "iteration 2433, dc_loss: 0.01837274804711342, tv_loss: 0.01712762750685215\n",
      "iteration 2434, dc_loss: 0.018372759222984314, tv_loss: 0.017127368599176407\n",
      "iteration 2435, dc_loss: 0.018372712656855583, tv_loss: 0.017127325758337975\n",
      "iteration 2436, dc_loss: 0.018372641876339912, tv_loss: 0.01712758094072342\n",
      "iteration 2437, dc_loss: 0.018372561782598495, tv_loss: 0.01712758094072342\n",
      "iteration 2438, dc_loss: 0.01837250217795372, tv_loss: 0.017127377912402153\n",
      "iteration 2439, dc_loss: 0.018372414633631706, tv_loss: 0.017127297818660736\n",
      "iteration 2440, dc_loss: 0.01837236061692238, tv_loss: 0.017127228900790215\n",
      "iteration 2441, dc_loss: 0.01837237924337387, tv_loss: 0.01712712086737156\n",
      "iteration 2442, dc_loss: 0.018372414633631706, tv_loss: 0.017126992344856262\n",
      "iteration 2443, dc_loss: 0.018372416496276855, tv_loss: 0.01712680235505104\n",
      "iteration 2444, dc_loss: 0.018372373655438423, tv_loss: 0.017126746475696564\n",
      "iteration 2445, dc_loss: 0.018372297286987305, tv_loss: 0.01712680049240589\n",
      "iteration 2446, dc_loss: 0.01837225630879402, tv_loss: 0.017126768827438354\n",
      "iteration 2447, dc_loss: 0.018372222781181335, tv_loss: 0.017126619815826416\n",
      "iteration 2448, dc_loss: 0.018372178077697754, tv_loss: 0.01712654158473015\n",
      "iteration 2449, dc_loss: 0.01837211661040783, tv_loss: 0.017126593738794327\n",
      "iteration 2450, dc_loss: 0.01837204582989216, tv_loss: 0.01712648570537567\n",
      "iteration 2451, dc_loss: 0.018372001126408577, tv_loss: 0.01712637208402157\n",
      "iteration 2452, dc_loss: 0.018371980637311935, tv_loss: 0.017126312479376793\n",
      "iteration 2453, dc_loss: 0.018371978774666786, tv_loss: 0.01712622493505478\n",
      "iteration 2454, dc_loss: 0.018371956422924995, tv_loss: 0.017126156017184258\n",
      "iteration 2455, dc_loss: 0.01837191730737686, tv_loss: 0.01712608151137829\n",
      "iteration 2456, dc_loss: 0.018371859565377235, tv_loss: 0.017126092687249184\n",
      "iteration 2457, dc_loss: 0.018371790647506714, tv_loss: 0.01712607592344284\n",
      "iteration 2458, dc_loss: 0.01837175339460373, tv_loss: 0.017125988379120827\n",
      "iteration 2459, dc_loss: 0.01837172359228134, tv_loss: 0.017125925049185753\n",
      "iteration 2460, dc_loss: 0.018371671438217163, tv_loss: 0.017125936225056648\n",
      "iteration 2461, dc_loss: 0.01837163232266903, tv_loss: 0.017125913873314857\n",
      "iteration 2462, dc_loss: 0.018371576443314552, tv_loss: 0.017125682905316353\n",
      "iteration 2463, dc_loss: 0.018371539190411568, tv_loss: 0.017125597223639488\n",
      "iteration 2464, dc_loss: 0.018371496349573135, tv_loss: 0.01712564378976822\n",
      "iteration 2465, dc_loss: 0.018371447920799255, tv_loss: 0.01712554693222046\n",
      "iteration 2466, dc_loss: 0.01837141439318657, tv_loss: 0.017125464975833893\n",
      "iteration 2467, dc_loss: 0.01837139017879963, tv_loss: 0.01712537184357643\n",
      "iteration 2468, dc_loss: 0.018371369689702988, tv_loss: 0.017125440761446953\n",
      "iteration 2469, dc_loss: 0.018371351063251495, tv_loss: 0.017125282436609268\n",
      "iteration 2470, dc_loss: 0.018371324986219406, tv_loss: 0.017125139012932777\n",
      "iteration 2471, dc_loss: 0.018371257930994034, tv_loss: 0.01712525635957718\n",
      "iteration 2472, dc_loss: 0.018371207639575005, tv_loss: 0.017125124111771584\n",
      "iteration 2473, dc_loss: 0.01837112382054329, tv_loss: 0.017125025391578674\n",
      "iteration 2474, dc_loss: 0.018371043726801872, tv_loss: 0.01712515577673912\n",
      "iteration 2475, dc_loss: 0.018371019512414932, tv_loss: 0.01712505705654621\n",
      "iteration 2476, dc_loss: 0.018371006473898888, tv_loss: 0.017124896869063377\n",
      "iteration 2477, dc_loss: 0.018370987847447395, tv_loss: 0.01712476648390293\n",
      "iteration 2478, dc_loss: 0.0183709729462862, tv_loss: 0.01712469756603241\n",
      "iteration 2479, dc_loss: 0.01837093010544777, tv_loss: 0.017124496400356293\n",
      "iteration 2480, dc_loss: 0.018370887264609337, tv_loss: 0.017124570906162262\n",
      "iteration 2481, dc_loss: 0.01837082952260971, tv_loss: 0.017124492675065994\n",
      "iteration 2482, dc_loss: 0.018370792269706726, tv_loss: 0.017124414443969727\n",
      "iteration 2483, dc_loss: 0.01837076246738434, tv_loss: 0.017124226316809654\n",
      "iteration 2484, dc_loss: 0.018370723351836205, tv_loss: 0.017124107107520103\n",
      "iteration 2485, dc_loss: 0.018370723351836205, tv_loss: 0.017124099656939507\n",
      "iteration 2486, dc_loss: 0.01837073266506195, tv_loss: 0.01712401770055294\n",
      "iteration 2487, dc_loss: 0.01837075501680374, tv_loss: 0.017123930156230927\n",
      "iteration 2488, dc_loss: 0.0183707345277071, tv_loss: 0.017123868688941002\n",
      "iteration 2489, dc_loss: 0.01837070658802986, tv_loss: 0.017123786732554436\n",
      "iteration 2490, dc_loss: 0.018370648846030235, tv_loss: 0.01712375320494175\n",
      "iteration 2491, dc_loss: 0.018370570614933968, tv_loss: 0.01712372899055481\n",
      "iteration 2492, dc_loss: 0.018370473757386208, tv_loss: 0.017123626545071602\n",
      "iteration 2493, dc_loss: 0.018370410427451134, tv_loss: 0.017123492434620857\n",
      "iteration 2494, dc_loss: 0.01837034896016121, tv_loss: 0.017123548313975334\n",
      "iteration 2495, dc_loss: 0.018370309844613075, tv_loss: 0.017123593017458916\n",
      "iteration 2496, dc_loss: 0.018370309844613075, tv_loss: 0.01712346263229847\n",
      "iteration 2497, dc_loss: 0.018370339646935463, tv_loss: 0.017123254016041756\n",
      "iteration 2498, dc_loss: 0.018370361998677254, tv_loss: 0.01712307706475258\n",
      "iteration 2499, dc_loss: 0.01837032474577427, tv_loss: 0.017123090103268623\n",
      "iteration 2500, dc_loss: 0.018370239064097404, tv_loss: 0.01712300069630146\n",
      "iteration 2501, dc_loss: 0.018370164558291435, tv_loss: 0.017122982069849968\n",
      "iteration 2502, dc_loss: 0.01837008260190487, tv_loss: 0.01712304912507534\n",
      "iteration 2503, dc_loss: 0.01837005279958248, tv_loss: 0.017122970893979073\n",
      "iteration 2504, dc_loss: 0.01837002858519554, tv_loss: 0.017122823745012283\n",
      "iteration 2505, dc_loss: 0.01837003231048584, tv_loss: 0.017122987657785416\n",
      "iteration 2506, dc_loss: 0.018370041623711586, tv_loss: 0.01712283119559288\n",
      "iteration 2507, dc_loss: 0.018369993194937706, tv_loss: 0.01712261326611042\n",
      "iteration 2508, dc_loss: 0.018369918689131737, tv_loss: 0.017122633755207062\n",
      "iteration 2509, dc_loss: 0.018369881436228752, tv_loss: 0.017122412100434303\n",
      "iteration 2510, dc_loss: 0.01836983859539032, tv_loss: 0.017122501507401466\n",
      "iteration 2511, dc_loss: 0.018369778990745544, tv_loss: 0.017122479155659676\n",
      "iteration 2512, dc_loss: 0.018369724974036217, tv_loss: 0.017122358083724976\n",
      "iteration 2513, dc_loss: 0.018369728699326515, tv_loss: 0.017122212797403336\n",
      "iteration 2514, dc_loss: 0.01836971938610077, tv_loss: 0.017122119665145874\n",
      "iteration 2515, dc_loss: 0.018369680270552635, tv_loss: 0.01712205819785595\n",
      "iteration 2516, dc_loss: 0.018369635567069054, tv_loss: 0.017122048884630203\n",
      "iteration 2517, dc_loss: 0.01836964301764965, tv_loss: 0.01712191104888916\n",
      "iteration 2518, dc_loss: 0.01836964301764965, tv_loss: 0.017121871933341026\n",
      "iteration 2519, dc_loss: 0.018369609490036964, tv_loss: 0.017121821641921997\n",
      "iteration 2520, dc_loss: 0.018369518220424652, tv_loss: 0.01712169125676155\n",
      "iteration 2521, dc_loss: 0.018369445577263832, tv_loss: 0.017121614888310432\n",
      "iteration 2522, dc_loss: 0.018369365483522415, tv_loss: 0.017121853306889534\n",
      "iteration 2523, dc_loss: 0.018369315192103386, tv_loss: 0.017121819779276848\n",
      "iteration 2524, dc_loss: 0.018369268625974655, tv_loss: 0.017121581360697746\n",
      "iteration 2525, dc_loss: 0.01836921088397503, tv_loss: 0.017121531069278717\n",
      "iteration 2526, dc_loss: 0.018369166180491447, tv_loss: 0.017121531069278717\n",
      "iteration 2527, dc_loss: 0.018369160592556, tv_loss: 0.017121408134698868\n",
      "iteration 2528, dc_loss: 0.018369201570749283, tv_loss: 0.017121436074376106\n",
      "iteration 2529, dc_loss: 0.018369216471910477, tv_loss: 0.017121220007538795\n",
      "iteration 2530, dc_loss: 0.018369195982813835, tv_loss: 0.017121033743023872\n",
      "iteration 2531, dc_loss: 0.018369153141975403, tv_loss: 0.0171210877597332\n",
      "iteration 2532, dc_loss: 0.018369069322943687, tv_loss: 0.017121143639087677\n",
      "iteration 2533, dc_loss: 0.018369002267718315, tv_loss: 0.0171209666877985\n",
      "iteration 2534, dc_loss: 0.01836894080042839, tv_loss: 0.0171210840344429\n",
      "iteration 2535, dc_loss: 0.018368905410170555, tv_loss: 0.01712103746831417\n",
      "iteration 2536, dc_loss: 0.0183689147233963, tv_loss: 0.01712089776992798\n",
      "iteration 2537, dc_loss: 0.018368903547525406, tv_loss: 0.017120886594057083\n",
      "iteration 2538, dc_loss: 0.0183689184486866, tv_loss: 0.017120881006121635\n",
      "iteration 2539, dc_loss: 0.018368903547525406, tv_loss: 0.017120644450187683\n",
      "iteration 2540, dc_loss: 0.018368838354945183, tv_loss: 0.01712065003812313\n",
      "iteration 2541, dc_loss: 0.018368754535913467, tv_loss: 0.017120786011219025\n",
      "iteration 2542, dc_loss: 0.018368693068623543, tv_loss: 0.017120731994509697\n",
      "iteration 2543, dc_loss: 0.018368670716881752, tv_loss: 0.017120536416769028\n",
      "iteration 2544, dc_loss: 0.018368642777204514, tv_loss: 0.017120471224188805\n",
      "iteration 2545, dc_loss: 0.018368614837527275, tv_loss: 0.017120422795414925\n",
      "iteration 2546, dc_loss: 0.018368573859333992, tv_loss: 0.01712028682231903\n",
      "iteration 2547, dc_loss: 0.018368549644947052, tv_loss: 0.01712021231651306\n",
      "iteration 2548, dc_loss: 0.018368544057011604, tv_loss: 0.01712026447057724\n",
      "iteration 2549, dc_loss: 0.018368499353528023, tv_loss: 0.017120148986577988\n",
      "iteration 2550, dc_loss: 0.018368441611528397, tv_loss: 0.01712006703019142\n",
      "iteration 2551, dc_loss: 0.01836840808391571, tv_loss: 0.017119955271482468\n",
      "iteration 2552, dc_loss: 0.018368419259786606, tv_loss: 0.01711990498006344\n",
      "iteration 2553, dc_loss: 0.018368422985076904, tv_loss: 0.017119811847805977\n",
      "iteration 2554, dc_loss: 0.018368395045399666, tv_loss: 0.01711983233690262\n",
      "iteration 2555, dc_loss: 0.018368376418948174, tv_loss: 0.01711973361670971\n",
      "iteration 2556, dc_loss: 0.01836833544075489, tv_loss: 0.01711967960000038\n",
      "iteration 2557, dc_loss: 0.01836826466023922, tv_loss: 0.01711970940232277\n",
      "iteration 2558, dc_loss: 0.0183681920170784, tv_loss: 0.01711970940232277\n",
      "iteration 2559, dc_loss: 0.01836816966533661, tv_loss: 0.017119640484452248\n",
      "iteration 2560, dc_loss: 0.018368132412433624, tv_loss: 0.017119482159614563\n",
      "iteration 2561, dc_loss: 0.01836809515953064, tv_loss: 0.017119428142905235\n",
      "iteration 2562, dc_loss: 0.018368059769272804, tv_loss: 0.017119357362389565\n",
      "iteration 2563, dc_loss: 0.018368013203144073, tv_loss: 0.017119383439421654\n",
      "iteration 2564, dc_loss: 0.018367964774370193, tv_loss: 0.017119215801358223\n",
      "iteration 2565, dc_loss: 0.018367934972047806, tv_loss: 0.01711917296051979\n",
      "iteration 2566, dc_loss: 0.018367931246757507, tv_loss: 0.01711922325193882\n",
      "iteration 2567, dc_loss: 0.01836790144443512, tv_loss: 0.017119150608778\n",
      "iteration 2568, dc_loss: 0.01836785301566124, tv_loss: 0.017119089141488075\n",
      "iteration 2569, dc_loss: 0.01836780458688736, tv_loss: 0.017118945717811584\n",
      "iteration 2570, dc_loss: 0.018367767333984375, tv_loss: 0.017118822783231735\n",
      "iteration 2571, dc_loss: 0.018367741256952286, tv_loss: 0.01711893267929554\n",
      "iteration 2572, dc_loss: 0.018367720767855644, tv_loss: 0.017118876799941063\n",
      "iteration 2573, dc_loss: 0.018367690965533257, tv_loss: 0.017118703573942184\n",
      "iteration 2574, dc_loss: 0.018367666751146317, tv_loss: 0.01711866445839405\n",
      "iteration 2575, dc_loss: 0.018367627635598183, tv_loss: 0.017118649557232857\n",
      "iteration 2576, dc_loss: 0.01836758852005005, tv_loss: 0.017118576914072037\n",
      "iteration 2577, dc_loss: 0.018367527052760124, tv_loss: 0.017118429765105247\n",
      "iteration 2578, dc_loss: 0.01836748793721199, tv_loss: 0.01711857132613659\n",
      "iteration 2579, dc_loss: 0.018367474898695946, tv_loss: 0.017118481919169426\n",
      "iteration 2580, dc_loss: 0.018367473036050797, tv_loss: 0.017118273302912712\n",
      "iteration 2581, dc_loss: 0.018367402255535126, tv_loss: 0.01711840182542801\n",
      "iteration 2582, dc_loss: 0.01836734265089035, tv_loss: 0.017118388786911964\n",
      "iteration 2583, dc_loss: 0.01836732029914856, tv_loss: 0.017118124291300774\n",
      "iteration 2584, dc_loss: 0.018367314711213112, tv_loss: 0.01711823046207428\n",
      "iteration 2585, dc_loss: 0.018367309123277664, tv_loss: 0.0171180609613657\n",
      "iteration 2586, dc_loss: 0.018367312848567963, tv_loss: 0.01711788773536682\n",
      "iteration 2587, dc_loss: 0.01836732029914856, tv_loss: 0.017117921262979507\n",
      "iteration 2588, dc_loss: 0.018367327749729156, tv_loss: 0.017117859795689583\n",
      "iteration 2589, dc_loss: 0.01836729794740677, tv_loss: 0.017117656767368317\n",
      "iteration 2590, dc_loss: 0.01836727373301983, tv_loss: 0.01711779646575451\n",
      "iteration 2591, dc_loss: 0.01836717315018177, tv_loss: 0.017117900773882866\n",
      "iteration 2592, dc_loss: 0.018367061391472816, tv_loss: 0.01711767353117466\n",
      "iteration 2593, dc_loss: 0.018366971984505653, tv_loss: 0.017117660492658615\n",
      "iteration 2594, dc_loss: 0.018366890028119087, tv_loss: 0.017117803916335106\n",
      "iteration 2595, dc_loss: 0.01836690865457058, tv_loss: 0.017117682844400406\n",
      "iteration 2596, dc_loss: 0.018366917967796326, tv_loss: 0.017117474228143692\n",
      "iteration 2597, dc_loss: 0.01836695522069931, tv_loss: 0.01711738109588623\n",
      "iteration 2598, dc_loss: 0.018366962671279907, tv_loss: 0.01711738109588623\n",
      "iteration 2599, dc_loss: 0.018366936594247818, tv_loss: 0.01711718924343586\n",
      "iteration 2600, dc_loss: 0.018366925418376923, tv_loss: 0.01711725816130638\n",
      "iteration 2601, dc_loss: 0.018366873264312744, tv_loss: 0.017117217183113098\n",
      "iteration 2602, dc_loss: 0.018366802483797073, tv_loss: 0.017117153853178024\n",
      "iteration 2603, dc_loss: 0.018366700038313866, tv_loss: 0.017117248848080635\n",
      "iteration 2604, dc_loss: 0.018366629257798195, tv_loss: 0.017117150127887726\n",
      "iteration 2605, dc_loss: 0.018366575241088867, tv_loss: 0.017117192968726158\n",
      "iteration 2606, dc_loss: 0.018366526812314987, tv_loss: 0.017117027193307877\n",
      "iteration 2607, dc_loss: 0.018366504460573196, tv_loss: 0.01711692102253437\n",
      "iteration 2608, dc_loss: 0.01836651936173439, tv_loss: 0.017116937786340714\n",
      "iteration 2609, dc_loss: 0.018366510048508644, tv_loss: 0.017116757109761238\n",
      "iteration 2610, dc_loss: 0.018366515636444092, tv_loss: 0.017116621136665344\n",
      "iteration 2611, dc_loss: 0.018366552889347076, tv_loss: 0.017116647213697433\n",
      "iteration 2612, dc_loss: 0.018366534262895584, tv_loss: 0.017116766422986984\n",
      "iteration 2613, dc_loss: 0.01836649514734745, tv_loss: 0.017116520553827286\n",
      "iteration 2614, dc_loss: 0.018366409465670586, tv_loss: 0.017116548493504524\n",
      "iteration 2615, dc_loss: 0.01836630515754223, tv_loss: 0.017116598784923553\n",
      "iteration 2616, dc_loss: 0.018366245552897453, tv_loss: 0.017116576433181763\n",
      "iteration 2617, dc_loss: 0.018366225063800812, tv_loss: 0.017116516828536987\n",
      "iteration 2618, dc_loss: 0.01836618222296238, tv_loss: 0.017116403207182884\n",
      "iteration 2619, dc_loss: 0.018366128206253052, tv_loss: 0.01711641438305378\n",
      "iteration 2620, dc_loss: 0.018366074189543724, tv_loss: 0.017116276547312737\n",
      "iteration 2621, dc_loss: 0.01836603693664074, tv_loss: 0.017116369679570198\n",
      "iteration 2622, dc_loss: 0.018366031348705292, tv_loss: 0.01711636781692505\n",
      "iteration 2623, dc_loss: 0.018366072326898575, tv_loss: 0.017116110771894455\n",
      "iteration 2624, dc_loss: 0.018366115167737007, tv_loss: 0.017115911468863487\n",
      "iteration 2625, dc_loss: 0.018366143107414246, tv_loss: 0.01711593009531498\n",
      "iteration 2626, dc_loss: 0.018366118893027306, tv_loss: 0.017115838825702667\n",
      "iteration 2627, dc_loss: 0.018366079777479172, tv_loss: 0.017115775495767593\n",
      "iteration 2628, dc_loss: 0.01836598850786686, tv_loss: 0.017115799710154533\n",
      "iteration 2629, dc_loss: 0.018365895375609398, tv_loss: 0.01711580529808998\n",
      "iteration 2630, dc_loss: 0.018365807831287384, tv_loss: 0.017115645110607147\n",
      "iteration 2631, dc_loss: 0.018365759402513504, tv_loss: 0.017115755006670952\n",
      "iteration 2632, dc_loss: 0.01836574263870716, tv_loss: 0.017115626484155655\n",
      "iteration 2633, dc_loss: 0.018365738913416862, tv_loss: 0.017115404829382896\n",
      "iteration 2634, dc_loss: 0.01836574450135231, tv_loss: 0.017115412279963493\n",
      "iteration 2635, dc_loss: 0.018365751951932907, tv_loss: 0.01711544208228588\n",
      "iteration 2636, dc_loss: 0.018365729600191116, tv_loss: 0.017115453258156776\n",
      "iteration 2637, dc_loss: 0.018365725874900818, tv_loss: 0.01711522974073887\n",
      "iteration 2638, dc_loss: 0.018365709111094475, tv_loss: 0.01711508445441723\n",
      "iteration 2639, dc_loss: 0.018365683034062386, tv_loss: 0.017115101218223572\n",
      "iteration 2640, dc_loss: 0.018365658819675446, tv_loss: 0.017115138471126556\n",
      "iteration 2641, dc_loss: 0.01836557500064373, tv_loss: 0.017115099355578423\n",
      "iteration 2642, dc_loss: 0.01836553029716015, tv_loss: 0.01711519993841648\n",
      "iteration 2643, dc_loss: 0.01836550235748291, tv_loss: 0.01711505465209484\n",
      "iteration 2644, dc_loss: 0.018365435302257538, tv_loss: 0.01711495965719223\n",
      "iteration 2645, dc_loss: 0.0183654073625803, tv_loss: 0.017114898189902306\n",
      "iteration 2646, dc_loss: 0.018365394324064255, tv_loss: 0.017114723101258278\n",
      "iteration 2647, dc_loss: 0.018365388736128807, tv_loss: 0.01711467280983925\n",
      "iteration 2648, dc_loss: 0.01836537756025791, tv_loss: 0.01711464859545231\n",
      "iteration 2649, dc_loss: 0.018365364521741867, tv_loss: 0.01711454428732395\n",
      "iteration 2650, dc_loss: 0.01836533471941948, tv_loss: 0.017114371061325073\n",
      "iteration 2651, dc_loss: 0.018365290015935898, tv_loss: 0.017114480957388878\n",
      "iteration 2652, dc_loss: 0.018365249037742615, tv_loss: 0.017114613205194473\n",
      "iteration 2653, dc_loss: 0.01836521178483963, tv_loss: 0.01711449772119522\n",
      "iteration 2654, dc_loss: 0.018365176394581795, tv_loss: 0.017114300280809402\n",
      "iteration 2655, dc_loss: 0.018365181982517242, tv_loss: 0.017114289104938507\n",
      "iteration 2656, dc_loss: 0.018365178257226944, tv_loss: 0.017114341259002686\n",
      "iteration 2657, dc_loss: 0.018365168944001198, tv_loss: 0.01711425930261612\n",
      "iteration 2658, dc_loss: 0.018365150317549706, tv_loss: 0.01711398921906948\n",
      "iteration 2659, dc_loss: 0.01836513541638851, tv_loss: 0.017114145681262016\n",
      "iteration 2660, dc_loss: 0.018365077674388885, tv_loss: 0.017114154994487762\n",
      "iteration 2661, dc_loss: 0.01836502179503441, tv_loss: 0.017114095389842987\n",
      "iteration 2662, dc_loss: 0.018364986404776573, tv_loss: 0.017113983631134033\n",
      "iteration 2663, dc_loss: 0.0183649230748415, tv_loss: 0.017113981768488884\n",
      "iteration 2664, dc_loss: 0.018364878371357918, tv_loss: 0.017113856971263885\n",
      "iteration 2665, dc_loss: 0.018364859744906425, tv_loss: 0.01711396500468254\n",
      "iteration 2666, dc_loss: 0.018364842981100082, tv_loss: 0.017113974317908287\n",
      "iteration 2667, dc_loss: 0.018364816904067993, tv_loss: 0.017113754525780678\n",
      "iteration 2668, dc_loss: 0.018364818766713142, tv_loss: 0.017113663256168365\n",
      "iteration 2669, dc_loss: 0.01836482807993889, tv_loss: 0.017113633453845978\n",
      "iteration 2670, dc_loss: 0.01836477406322956, tv_loss: 0.017113476991653442\n",
      "iteration 2671, dc_loss: 0.018364710733294487, tv_loss: 0.017113488167524338\n",
      "iteration 2672, dc_loss: 0.018364621326327324, tv_loss: 0.017113514244556427\n",
      "iteration 2673, dc_loss: 0.01836455799639225, tv_loss: 0.017113421112298965\n",
      "iteration 2674, dc_loss: 0.01836450956761837, tv_loss: 0.017113449051976204\n",
      "iteration 2675, dc_loss: 0.01836446300148964, tv_loss: 0.017113471403717995\n",
      "iteration 2676, dc_loss: 0.01836443692445755, tv_loss: 0.017113488167524338\n",
      "iteration 2677, dc_loss: 0.018364468589425087, tv_loss: 0.017113137990236282\n",
      "iteration 2678, dc_loss: 0.01836453378200531, tv_loss: 0.017113037407398224\n",
      "iteration 2679, dc_loss: 0.018364597111940384, tv_loss: 0.017112912610173225\n",
      "iteration 2680, dc_loss: 0.018364598974585533, tv_loss: 0.01711297780275345\n",
      "iteration 2681, dc_loss: 0.018364552408456802, tv_loss: 0.017113031819462776\n",
      "iteration 2682, dc_loss: 0.018364472314715385, tv_loss: 0.017112841829657555\n",
      "iteration 2683, dc_loss: 0.01836436241865158, tv_loss: 0.01711290143430233\n",
      "iteration 2684, dc_loss: 0.018364261835813522, tv_loss: 0.01711280830204487\n",
      "iteration 2685, dc_loss: 0.01836417242884636, tv_loss: 0.017112916335463524\n",
      "iteration 2686, dc_loss: 0.01836412027478218, tv_loss: 0.017112895846366882\n",
      "iteration 2687, dc_loss: 0.018364138901233673, tv_loss: 0.01711275987327099\n",
      "iteration 2688, dc_loss: 0.01836414635181427, tv_loss: 0.01711258292198181\n",
      "iteration 2689, dc_loss: 0.01836416684091091, tv_loss: 0.017112646251916885\n",
      "iteration 2690, dc_loss: 0.018364164978265762, tv_loss: 0.017112497240304947\n",
      "iteration 2691, dc_loss: 0.018364204093813896, tv_loss: 0.017112402245402336\n",
      "iteration 2692, dc_loss: 0.01836419850587845, tv_loss: 0.017112404108047485\n",
      "iteration 2693, dc_loss: 0.018364189192652702, tv_loss: 0.017112335190176964\n",
      "iteration 2694, dc_loss: 0.018364178016781807, tv_loss: 0.017112253233790398\n",
      "iteration 2695, dc_loss: 0.018364133313298225, tv_loss: 0.017112093046307564\n",
      "iteration 2696, dc_loss: 0.018364084884524345, tv_loss: 0.01711217872798443\n",
      "iteration 2697, dc_loss: 0.018364038318395615, tv_loss: 0.01711237244307995\n",
      "iteration 2698, dc_loss: 0.018363960087299347, tv_loss: 0.017112189903855324\n",
      "iteration 2699, dc_loss: 0.01836390793323517, tv_loss: 0.017112109810113907\n",
      "iteration 2700, dc_loss: 0.018363889306783676, tv_loss: 0.017112351953983307\n",
      "iteration 2701, dc_loss: 0.018363846465945244, tv_loss: 0.017112189903855324\n",
      "iteration 2702, dc_loss: 0.01836381107568741, tv_loss: 0.017111916095018387\n",
      "iteration 2703, dc_loss: 0.018363753333687782, tv_loss: 0.017112093046307564\n",
      "iteration 2704, dc_loss: 0.01836368255317211, tv_loss: 0.017112135887145996\n",
      "iteration 2705, dc_loss: 0.018363652750849724, tv_loss: 0.017111854627728462\n",
      "iteration 2706, dc_loss: 0.01836364157497883, tv_loss: 0.01711183972656727\n",
      "iteration 2707, dc_loss: 0.01836363784968853, tv_loss: 0.017111826688051224\n",
      "iteration 2708, dc_loss: 0.018363624811172485, tv_loss: 0.017111776396632195\n",
      "iteration 2709, dc_loss: 0.018363576382398605, tv_loss: 0.01711193472146988\n",
      "iteration 2710, dc_loss: 0.01836351677775383, tv_loss: 0.017111925408244133\n",
      "iteration 2711, dc_loss: 0.0183634702116251, tv_loss: 0.01711171492934227\n",
      "iteration 2712, dc_loss: 0.018363462761044502, tv_loss: 0.017111660912632942\n",
      "iteration 2713, dc_loss: 0.01836349256336689, tv_loss: 0.017111608758568764\n",
      "iteration 2714, dc_loss: 0.018363503739237785, tv_loss: 0.017111454159021378\n",
      "iteration 2715, dc_loss: 0.01836351677775383, tv_loss: 0.01711137220263481\n",
      "iteration 2716, dc_loss: 0.01836351305246353, tv_loss: 0.01711137220263481\n",
      "iteration 2717, dc_loss: 0.018363533541560173, tv_loss: 0.017111249268054962\n",
      "iteration 2718, dc_loss: 0.018363485112786293, tv_loss: 0.017111074179410934\n",
      "iteration 2719, dc_loss: 0.01836344040930271, tv_loss: 0.01711106300354004\n",
      "iteration 2720, dc_loss: 0.018363360315561295, tv_loss: 0.017111003398895264\n",
      "iteration 2721, dc_loss: 0.018363306298851967, tv_loss: 0.017111053690314293\n",
      "iteration 2722, dc_loss: 0.01836325041949749, tv_loss: 0.01711108349263668\n",
      "iteration 2723, dc_loss: 0.018363215029239655, tv_loss: 0.017111146822571754\n",
      "iteration 2724, dc_loss: 0.01836318150162697, tv_loss: 0.01711099222302437\n",
      "iteration 2725, dc_loss: 0.01836317777633667, tv_loss: 0.01711081527173519\n",
      "iteration 2726, dc_loss: 0.018363190814852715, tv_loss: 0.017110830172896385\n",
      "iteration 2727, dc_loss: 0.018363168463110924, tv_loss: 0.017111044377088547\n",
      "iteration 2728, dc_loss: 0.01836315728724003, tv_loss: 0.017110709100961685\n",
      "iteration 2729, dc_loss: 0.018363118171691895, tv_loss: 0.017110522836446762\n",
      "iteration 2730, dc_loss: 0.018363064154982567, tv_loss: 0.01711064577102661\n",
      "iteration 2731, dc_loss: 0.018363049253821373, tv_loss: 0.017110807821154594\n",
      "iteration 2732, dc_loss: 0.01836305484175682, tv_loss: 0.017110692337155342\n",
      "iteration 2733, dc_loss: 0.018363071605563164, tv_loss: 0.01711053028702736\n",
      "iteration 2734, dc_loss: 0.018363075330853462, tv_loss: 0.017110351473093033\n",
      "iteration 2735, dc_loss: 0.01836303435266018, tv_loss: 0.017110461369156837\n",
      "iteration 2736, dc_loss: 0.018362969160079956, tv_loss: 0.017110589891672134\n",
      "iteration 2737, dc_loss: 0.018362896516919136, tv_loss: 0.01711035706102848\n",
      "iteration 2738, dc_loss: 0.018362831324338913, tv_loss: 0.01711030676960945\n",
      "iteration 2739, dc_loss: 0.01836276426911354, tv_loss: 0.017110450193285942\n",
      "iteration 2740, dc_loss: 0.018362706527113914, tv_loss: 0.017110364511609077\n",
      "iteration 2741, dc_loss: 0.01836269162595272, tv_loss: 0.017110034823417664\n",
      "iteration 2742, dc_loss: 0.01836268976330757, tv_loss: 0.017109878361225128\n",
      "iteration 2743, dc_loss: 0.018362702801823616, tv_loss: 0.017110003158450127\n",
      "iteration 2744, dc_loss: 0.01836269721388817, tv_loss: 0.01711016707122326\n",
      "iteration 2745, dc_loss: 0.018362678587436676, tv_loss: 0.01711004599928856\n",
      "iteration 2746, dc_loss: 0.018362630158662796, tv_loss: 0.017109811305999756\n",
      "iteration 2747, dc_loss: 0.018362583592534065, tv_loss: 0.01710997149348259\n",
      "iteration 2748, dc_loss: 0.018362516537308693, tv_loss: 0.01711004227399826\n",
      "iteration 2749, dc_loss: 0.018362516537308693, tv_loss: 0.017109829932451248\n",
      "iteration 2750, dc_loss: 0.01836254447698593, tv_loss: 0.017109636217355728\n",
      "iteration 2751, dc_loss: 0.01836260035634041, tv_loss: 0.01710965670645237\n",
      "iteration 2752, dc_loss: 0.01836262084543705, tv_loss: 0.017109613865613937\n",
      "iteration 2753, dc_loss: 0.018362591043114662, tv_loss: 0.017109503969550133\n",
      "iteration 2754, dc_loss: 0.018362566828727722, tv_loss: 0.01710946299135685\n",
      "iteration 2755, dc_loss: 0.018362510949373245, tv_loss: 0.01710963249206543\n",
      "iteration 2756, dc_loss: 0.01836243085563183, tv_loss: 0.017109666019678116\n",
      "iteration 2757, dc_loss: 0.01836235821247101, tv_loss: 0.017109453678131104\n",
      "iteration 2758, dc_loss: 0.01836233027279377, tv_loss: 0.017109286040067673\n",
      "iteration 2759, dc_loss: 0.018362311646342278, tv_loss: 0.01710928976535797\n",
      "iteration 2760, dc_loss: 0.01836228370666504, tv_loss: 0.017109274864196777\n",
      "iteration 2761, dc_loss: 0.018362252041697502, tv_loss: 0.0171093437820673\n",
      "iteration 2762, dc_loss: 0.01836220733821392, tv_loss: 0.017109375447034836\n",
      "iteration 2763, dc_loss: 0.018362166360020638, tv_loss: 0.01710933819413185\n",
      "iteration 2764, dc_loss: 0.018362142145633698, tv_loss: 0.01710931584239006\n",
      "iteration 2765, dc_loss: 0.018362153321504593, tv_loss: 0.017109261825680733\n",
      "iteration 2766, dc_loss: 0.01836215704679489, tv_loss: 0.017109323292970657\n",
      "iteration 2767, dc_loss: 0.018362130969762802, tv_loss: 0.017109211534261703\n",
      "iteration 2768, dc_loss: 0.0183621384203434, tv_loss: 0.017108894884586334\n",
      "iteration 2769, dc_loss: 0.018362121656537056, tv_loss: 0.017109008505940437\n",
      "iteration 2770, dc_loss: 0.01836213655769825, tv_loss: 0.017109066247940063\n",
      "iteration 2771, dc_loss: 0.01836213655769825, tv_loss: 0.017108824104070663\n",
      "iteration 2772, dc_loss: 0.018362103030085564, tv_loss: 0.01710875891149044\n",
      "iteration 2773, dc_loss: 0.01836203970015049, tv_loss: 0.01710883341729641\n",
      "iteration 2774, dc_loss: 0.01836196891963482, tv_loss: 0.017108963802456856\n",
      "iteration 2775, dc_loss: 0.01836191490292549, tv_loss: 0.017108965665102005\n",
      "iteration 2776, dc_loss: 0.018361883237957954, tv_loss: 0.017108798027038574\n",
      "iteration 2777, dc_loss: 0.018361829221248627, tv_loss: 0.017108622938394547\n",
      "iteration 2778, dc_loss: 0.01836177334189415, tv_loss: 0.01710876263678074\n",
      "iteration 2779, dc_loss: 0.018361739814281464, tv_loss: 0.017108727246522903\n",
      "iteration 2780, dc_loss: 0.018361717462539673, tv_loss: 0.017108647152781487\n",
      "iteration 2781, dc_loss: 0.018361717462539673, tv_loss: 0.017108554020524025\n",
      "iteration 2782, dc_loss: 0.01836167648434639, tv_loss: 0.017108524218201637\n",
      "iteration 2783, dc_loss: 0.018361657857894897, tv_loss: 0.017108479514718056\n",
      "iteration 2784, dc_loss: 0.018361639231443405, tv_loss: 0.0171083752065897\n",
      "iteration 2785, dc_loss: 0.018361637368798256, tv_loss: 0.017108451575040817\n",
      "iteration 2786, dc_loss: 0.01836162619292736, tv_loss: 0.01710841804742813\n",
      "iteration 2787, dc_loss: 0.01836160011589527, tv_loss: 0.017108095809817314\n",
      "iteration 2788, dc_loss: 0.018361616879701614, tv_loss: 0.017108047381043434\n",
      "iteration 2789, dc_loss: 0.01836160570383072, tv_loss: 0.017108021304011345\n",
      "iteration 2790, dc_loss: 0.018361547961831093, tv_loss: 0.0171080082654953\n",
      "iteration 2791, dc_loss: 0.018361488357186317, tv_loss: 0.01710793934762478\n",
      "iteration 2792, dc_loss: 0.01836148276925087, tv_loss: 0.01710796169936657\n",
      "iteration 2793, dc_loss: 0.01836150325834751, tv_loss: 0.017107989639043808\n",
      "iteration 2794, dc_loss: 0.018361520022153854, tv_loss: 0.017107870429754257\n",
      "iteration 2795, dc_loss: 0.018361467868089676, tv_loss: 0.017107877880334854\n",
      "iteration 2796, dc_loss: 0.018361441791057587, tv_loss: 0.01710786484181881\n",
      "iteration 2797, dc_loss: 0.01836143247783184, tv_loss: 0.017107808962464333\n",
      "iteration 2798, dc_loss: 0.018361374735832214, tv_loss: 0.01710764691233635\n",
      "iteration 2799, dc_loss: 0.018361296504735947, tv_loss: 0.01710762269794941\n",
      "iteration 2800, dc_loss: 0.01836123876273632, tv_loss: 0.017107559368014336\n",
      "iteration 2801, dc_loss: 0.018361197784543037, tv_loss: 0.017107654362916946\n",
      "iteration 2802, dc_loss: 0.018361149355769157, tv_loss: 0.017107702791690826\n",
      "iteration 2803, dc_loss: 0.018361086025834084, tv_loss: 0.017107686027884483\n",
      "iteration 2804, dc_loss: 0.01836107298731804, tv_loss: 0.017107684165239334\n",
      "iteration 2805, dc_loss: 0.018361074849963188, tv_loss: 0.017107592895627022\n",
      "iteration 2806, dc_loss: 0.018361065536737442, tv_loss: 0.017107440158724785\n",
      "iteration 2807, dc_loss: 0.018361061811447144, tv_loss: 0.017107533290982246\n",
      "iteration 2808, dc_loss: 0.018361061811447144, tv_loss: 0.017107417806982994\n",
      "iteration 2809, dc_loss: 0.01836104318499565, tv_loss: 0.017107341438531876\n",
      "iteration 2810, dc_loss: 0.018361007794737816, tv_loss: 0.017107507213950157\n",
      "iteration 2811, dc_loss: 0.018360963091254234, tv_loss: 0.017107471823692322\n",
      "iteration 2812, dc_loss: 0.0183609239757061, tv_loss: 0.01710728369653225\n",
      "iteration 2813, dc_loss: 0.01836090348660946, tv_loss: 0.01710725575685501\n",
      "iteration 2814, dc_loss: 0.01836092583835125, tv_loss: 0.017107253894209862\n",
      "iteration 2815, dc_loss: 0.018360964953899384, tv_loss: 0.017107252031564713\n",
      "iteration 2816, dc_loss: 0.018361009657382965, tv_loss: 0.01710689440369606\n",
      "iteration 2817, dc_loss: 0.018361002206802368, tv_loss: 0.017106881365180016\n",
      "iteration 2818, dc_loss: 0.018360910937190056, tv_loss: 0.017106924206018448\n",
      "iteration 2819, dc_loss: 0.018360815942287445, tv_loss: 0.01710708625614643\n",
      "iteration 2820, dc_loss: 0.018360795453190804, tv_loss: 0.017106955870985985\n",
      "iteration 2821, dc_loss: 0.01836077682673931, tv_loss: 0.017106758430600166\n",
      "iteration 2822, dc_loss: 0.01836073398590088, tv_loss: 0.01710684411227703\n",
      "iteration 2823, dc_loss: 0.018360672518610954, tv_loss: 0.017107021063566208\n",
      "iteration 2824, dc_loss: 0.018360581248998642, tv_loss: 0.017106838524341583\n",
      "iteration 2825, dc_loss: 0.018360530957579613, tv_loss: 0.017106929793953896\n",
      "iteration 2826, dc_loss: 0.01836055889725685, tv_loss: 0.017106899991631508\n",
      "iteration 2827, dc_loss: 0.018360603600740433, tv_loss: 0.017106816172599792\n",
      "iteration 2828, dc_loss: 0.018360638990998268, tv_loss: 0.017106501385569572\n",
      "iteration 2829, dc_loss: 0.018360653892159462, tv_loss: 0.017106348648667336\n",
      "iteration 2830, dc_loss: 0.01836066134274006, tv_loss: 0.01710655726492405\n",
      "iteration 2831, dc_loss: 0.018360640853643417, tv_loss: 0.017106512561440468\n",
      "iteration 2832, dc_loss: 0.01836058683693409, tv_loss: 0.01710631512105465\n",
      "iteration 2833, dc_loss: 0.01836051046848297, tv_loss: 0.017106397077441216\n",
      "iteration 2834, dc_loss: 0.018360428512096405, tv_loss: 0.01710646227002144\n",
      "iteration 2835, dc_loss: 0.01836039498448372, tv_loss: 0.017106465995311737\n",
      "iteration 2836, dc_loss: 0.018360387533903122, tv_loss: 0.017106471583247185\n",
      "iteration 2837, dc_loss: 0.01836039498448372, tv_loss: 0.01710641197860241\n",
      "iteration 2838, dc_loss: 0.01836038939654827, tv_loss: 0.017106322571635246\n",
      "iteration 2839, dc_loss: 0.018360359594225883, tv_loss: 0.017106330022215843\n",
      "iteration 2840, dc_loss: 0.018360324203968048, tv_loss: 0.017106173560023308\n",
      "iteration 2841, dc_loss: 0.018360259011387825, tv_loss: 0.017106158658862114\n",
      "iteration 2842, dc_loss: 0.018360214307904243, tv_loss: 0.017106279730796814\n",
      "iteration 2843, dc_loss: 0.018360206857323647, tv_loss: 0.01710616424679756\n",
      "iteration 2844, dc_loss: 0.018360186368227005, tv_loss: 0.017106162384152412\n",
      "iteration 2845, dc_loss: 0.018360178917646408, tv_loss: 0.017105991020798683\n",
      "iteration 2846, dc_loss: 0.018360156565904617, tv_loss: 0.01710605062544346\n",
      "iteration 2847, dc_loss: 0.018360096961259842, tv_loss: 0.017106197774410248\n",
      "iteration 2848, dc_loss: 0.018360024318099022, tv_loss: 0.017106108367443085\n",
      "iteration 2849, dc_loss: 0.018359992653131485, tv_loss: 0.017106030136346817\n",
      "iteration 2850, dc_loss: 0.01835998333990574, tv_loss: 0.017106063663959503\n",
      "iteration 2851, dc_loss: 0.01836000382900238, tv_loss: 0.017106086015701294\n",
      "iteration 2852, dc_loss: 0.018360013142228127, tv_loss: 0.01710604317486286\n",
      "iteration 2853, dc_loss: 0.018360039219260216, tv_loss: 0.01710587553679943\n",
      "iteration 2854, dc_loss: 0.018360072746872902, tv_loss: 0.01710580475628376\n",
      "iteration 2855, dc_loss: 0.018360063433647156, tv_loss: 0.01710577681660652\n",
      "iteration 2856, dc_loss: 0.018360020592808723, tv_loss: 0.017105761915445328\n",
      "iteration 2857, dc_loss: 0.01835998333990574, tv_loss: 0.01710568368434906\n",
      "iteration 2858, dc_loss: 0.018359970301389694, tv_loss: 0.01710563339293003\n",
      "iteration 2859, dc_loss: 0.018359927460551262, tv_loss: 0.017105773091316223\n",
      "iteration 2860, dc_loss: 0.018359849229454994, tv_loss: 0.017105786129832268\n",
      "iteration 2861, dc_loss: 0.018359776586294174, tv_loss: 0.017105745151638985\n",
      "iteration 2862, dc_loss: 0.018359744921326637, tv_loss: 0.017105722799897194\n",
      "iteration 2863, dc_loss: 0.018359774723649025, tv_loss: 0.017105741426348686\n",
      "iteration 2864, dc_loss: 0.018359798938035965, tv_loss: 0.017105583101511\n",
      "iteration 2865, dc_loss: 0.01835978589951992, tv_loss: 0.017105553299188614\n",
      "iteration 2866, dc_loss: 0.018359791487455368, tv_loss: 0.01710541732609272\n",
      "iteration 2867, dc_loss: 0.01835978776216507, tv_loss: 0.01710536517202854\n",
      "iteration 2868, dc_loss: 0.018359770998358727, tv_loss: 0.01710541732609272\n",
      "iteration 2869, dc_loss: 0.01835971511900425, tv_loss: 0.01710524968802929\n",
      "iteration 2870, dc_loss: 0.018359649926424026, tv_loss: 0.01710513047873974\n",
      "iteration 2871, dc_loss: 0.0183595921844244, tv_loss: 0.017105339094996452\n",
      "iteration 2872, dc_loss: 0.01835956983268261, tv_loss: 0.0171053409576416\n",
      "iteration 2873, dc_loss: 0.018359560519456863, tv_loss: 0.017105145379900932\n",
      "iteration 2874, dc_loss: 0.01835957169532776, tv_loss: 0.01710513047873974\n",
      "iteration 2875, dc_loss: 0.018359573557972908, tv_loss: 0.017104940488934517\n",
      "iteration 2876, dc_loss: 0.018359588459134102, tv_loss: 0.01710483245551586\n",
      "iteration 2877, dc_loss: 0.018359588459134102, tv_loss: 0.017104968428611755\n",
      "iteration 2878, dc_loss: 0.018359582871198654, tv_loss: 0.0171049814671278\n",
      "iteration 2879, dc_loss: 0.018359558656811714, tv_loss: 0.01710466668009758\n",
      "iteration 2880, dc_loss: 0.01835951954126358, tv_loss: 0.017104819416999817\n",
      "iteration 2881, dc_loss: 0.018359459936618805, tv_loss: 0.017104914411902428\n",
      "iteration 2882, dc_loss: 0.018359411507844925, tv_loss: 0.017104841768741608\n",
      "iteration 2883, dc_loss: 0.018359383568167686, tv_loss: 0.017104702070355415\n",
      "iteration 2884, dc_loss: 0.01835937425494194, tv_loss: 0.017104879021644592\n",
      "iteration 2885, dc_loss: 0.018359366804361343, tv_loss: 0.01710485853254795\n",
      "iteration 2886, dc_loss: 0.018359310925006866, tv_loss: 0.01710464246571064\n",
      "iteration 2887, dc_loss: 0.018359240144491196, tv_loss: 0.01710469089448452\n",
      "iteration 2888, dc_loss: 0.018359186127781868, tv_loss: 0.01710488460958004\n",
      "iteration 2889, dc_loss: 0.018359147012233734, tv_loss: 0.017104795202612877\n",
      "iteration 2890, dc_loss: 0.018359141424298286, tv_loss: 0.0171047355979681\n",
      "iteration 2891, dc_loss: 0.018359189853072166, tv_loss: 0.017104655504226685\n",
      "iteration 2892, dc_loss: 0.018359219655394554, tv_loss: 0.01710444875061512\n",
      "iteration 2893, dc_loss: 0.018359240144491196, tv_loss: 0.017104467377066612\n",
      "iteration 2894, dc_loss: 0.018359219655394554, tv_loss: 0.017104439437389374\n",
      "iteration 2895, dc_loss: 0.01835918053984642, tv_loss: 0.01710432581603527\n",
      "iteration 2896, dc_loss: 0.018359161913394928, tv_loss: 0.017104294151067734\n",
      "iteration 2897, dc_loss: 0.01835913397371769, tv_loss: 0.01710435375571251\n",
      "iteration 2898, dc_loss: 0.018359098583459854, tv_loss: 0.017104335129261017\n",
      "iteration 2899, dc_loss: 0.018359078094363213, tv_loss: 0.017104223370552063\n",
      "iteration 2900, dc_loss: 0.01835905760526657, tv_loss: 0.01710421033203602\n",
      "iteration 2901, dc_loss: 0.018359022215008736, tv_loss: 0.017104312777519226\n",
      "iteration 2902, dc_loss: 0.018358981236815453, tv_loss: 0.017104262486100197\n",
      "iteration 2903, dc_loss: 0.01835894212126732, tv_loss: 0.017104072496294975\n",
      "iteration 2904, dc_loss: 0.01835893653333187, tv_loss: 0.01710405945777893\n",
      "iteration 2905, dc_loss: 0.01835894212126732, tv_loss: 0.017104050144553185\n",
      "iteration 2906, dc_loss: 0.018358932808041573, tv_loss: 0.01710408367216587\n",
      "iteration 2907, dc_loss: 0.018358910456299782, tv_loss: 0.017103975638747215\n",
      "iteration 2908, dc_loss: 0.018358876928687096, tv_loss: 0.017103897407650948\n",
      "iteration 2909, dc_loss: 0.01835881732404232, tv_loss: 0.01710399053990841\n",
      "iteration 2910, dc_loss: 0.018358752131462097, tv_loss: 0.0171040091663599\n",
      "iteration 2911, dc_loss: 0.018358729779720306, tv_loss: 0.01710399053990841\n",
      "iteration 2912, dc_loss: 0.01835871860384941, tv_loss: 0.01710386760532856\n",
      "iteration 2913, dc_loss: 0.018358726054430008, tv_loss: 0.017103904858231544\n",
      "iteration 2914, dc_loss: 0.018358731642365456, tv_loss: 0.01710386946797371\n",
      "iteration 2915, dc_loss: 0.01835872046649456, tv_loss: 0.0171037670224905\n",
      "iteration 2916, dc_loss: 0.018358681350946426, tv_loss: 0.017103713005781174\n",
      "iteration 2917, dc_loss: 0.018358679488301277, tv_loss: 0.01710374280810356\n",
      "iteration 2918, dc_loss: 0.018358726054430008, tv_loss: 0.017103547230362892\n",
      "iteration 2919, dc_loss: 0.01835877262055874, tv_loss: 0.017103364691138268\n",
      "iteration 2920, dc_loss: 0.018358757719397545, tv_loss: 0.017103323712944984\n",
      "iteration 2921, dc_loss: 0.018358703702688217, tv_loss: 0.017103277146816254\n",
      "iteration 2922, dc_loss: 0.018358640372753143, tv_loss: 0.01710328459739685\n",
      "iteration 2923, dc_loss: 0.018358595669269562, tv_loss: 0.01710348203778267\n",
      "iteration 2924, dc_loss: 0.01835857331752777, tv_loss: 0.017103273421525955\n",
      "iteration 2925, dc_loss: 0.01835855282843113, tv_loss: 0.01710313931107521\n",
      "iteration 2926, dc_loss: 0.018358541652560234, tv_loss: 0.017103226855397224\n",
      "iteration 2927, dc_loss: 0.018358508124947548, tv_loss: 0.017103176563978195\n",
      "iteration 2928, dc_loss: 0.01835847832262516, tv_loss: 0.017103135585784912\n",
      "iteration 2929, dc_loss: 0.018358442932367325, tv_loss: 0.01710311882197857\n",
      "iteration 2930, dc_loss: 0.01835842803120613, tv_loss: 0.01710304245352745\n",
      "iteration 2931, dc_loss: 0.018358387053012848, tv_loss: 0.0171029232442379\n",
      "iteration 2932, dc_loss: 0.01835830695927143, tv_loss: 0.017103105783462524\n",
      "iteration 2933, dc_loss: 0.018358241766691208, tv_loss: 0.017103053629398346\n",
      "iteration 2934, dc_loss: 0.01835823804140091, tv_loss: 0.017103083431720734\n",
      "iteration 2935, dc_loss: 0.018358241766691208, tv_loss: 0.01710306480526924\n",
      "iteration 2936, dc_loss: 0.018358290195465088, tv_loss: 0.017102917656302452\n",
      "iteration 2937, dc_loss: 0.018358349800109863, tv_loss: 0.01710275188088417\n",
      "iteration 2938, dc_loss: 0.018358396366238594, tv_loss: 0.01710275001823902\n",
      "iteration 2939, dc_loss: 0.018358368426561356, tv_loss: 0.017102813348174095\n",
      "iteration 2940, dc_loss: 0.018358318135142326, tv_loss: 0.01710275188088417\n",
      "iteration 2941, dc_loss: 0.018358221277594566, tv_loss: 0.017102759331464767\n",
      "iteration 2942, dc_loss: 0.018358144909143448, tv_loss: 0.017102692276239395\n",
      "iteration 2943, dc_loss: 0.018358077853918076, tv_loss: 0.017102906480431557\n",
      "iteration 2944, dc_loss: 0.01835804060101509, tv_loss: 0.017102861776947975\n",
      "iteration 2945, dc_loss: 0.01835808902978897, tv_loss: 0.017102934420108795\n",
      "iteration 2946, dc_loss: 0.018358122557401657, tv_loss: 0.01710265874862671\n",
      "iteration 2947, dc_loss: 0.018358156085014343, tv_loss: 0.017102627083659172\n",
      "iteration 2948, dc_loss: 0.01835813745856285, tv_loss: 0.017102539539337158\n",
      "iteration 2949, dc_loss: 0.01835808902978897, tv_loss: 0.017102710902690887\n",
      "iteration 2950, dc_loss: 0.018358036875724792, tv_loss: 0.017102720215916634\n",
      "iteration 2951, dc_loss: 0.01835799403488636, tv_loss: 0.017102522775530815\n",
      "iteration 2952, dc_loss: 0.01835799030959606, tv_loss: 0.017102351412177086\n",
      "iteration 2953, dc_loss: 0.018357979133725166, tv_loss: 0.017102504149079323\n",
      "iteration 2954, dc_loss: 0.01835794560611248, tv_loss: 0.01710250787436962\n",
      "iteration 2955, dc_loss: 0.01835792511701584, tv_loss: 0.017102370038628578\n",
      "iteration 2956, dc_loss: 0.018357936292886734, tv_loss: 0.017102358862757683\n",
      "iteration 2957, dc_loss: 0.01835797354578972, tv_loss: 0.01710229180753231\n",
      "iteration 2958, dc_loss: 0.018357956781983376, tv_loss: 0.017102215439081192\n",
      "iteration 2959, dc_loss: 0.01835789903998375, tv_loss: 0.017102079465985298\n",
      "iteration 2960, dc_loss: 0.018357854336500168, tv_loss: 0.017102032899856567\n",
      "iteration 2961, dc_loss: 0.01835782639682293, tv_loss: 0.017102325335144997\n",
      "iteration 2962, dc_loss: 0.01835780218243599, tv_loss: 0.01710222288966179\n",
      "iteration 2963, dc_loss: 0.01835780218243599, tv_loss: 0.017102016136050224\n",
      "iteration 2964, dc_loss: 0.018357817083597183, tv_loss: 0.017101995646953583\n",
      "iteration 2965, dc_loss: 0.018357783555984497, tv_loss: 0.017102070152759552\n",
      "iteration 2966, dc_loss: 0.018357770517468452, tv_loss: 0.017102040350437164\n",
      "iteration 2967, dc_loss: 0.018357770517468452, tv_loss: 0.017101867124438286\n",
      "iteration 2968, dc_loss: 0.018357785418629646, tv_loss: 0.017101788893342018\n",
      "iteration 2969, dc_loss: 0.01835782453417778, tv_loss: 0.01710175909101963\n",
      "iteration 2970, dc_loss: 0.01835780404508114, tv_loss: 0.017101749777793884\n",
      "iteration 2971, dc_loss: 0.01835774816572666, tv_loss: 0.017101628705859184\n",
      "iteration 2972, dc_loss: 0.018357684835791588, tv_loss: 0.01710161752998829\n",
      "iteration 2973, dc_loss: 0.018357623368501663, tv_loss: 0.01710176281630993\n",
      "iteration 2974, dc_loss: 0.018357569351792336, tv_loss: 0.017101876437664032\n",
      "iteration 2975, dc_loss: 0.0183575339615345, tv_loss: 0.017101721838116646\n",
      "iteration 2976, dc_loss: 0.018357528373599052, tv_loss: 0.017101602628827095\n",
      "iteration 2977, dc_loss: 0.018357541412115097, tv_loss: 0.017101626843214035\n",
      "iteration 2978, dc_loss: 0.018357548862695694, tv_loss: 0.017101505771279335\n",
      "iteration 2979, dc_loss: 0.018357539549469948, tv_loss: 0.017101414501667023\n",
      "iteration 2980, dc_loss: 0.018357547000050545, tv_loss: 0.017101416364312172\n",
      "iteration 2981, dc_loss: 0.018357541412115097, tv_loss: 0.017101461067795753\n",
      "iteration 2982, dc_loss: 0.01835753582417965, tv_loss: 0.017101434990763664\n",
      "iteration 2983, dc_loss: 0.01835750788450241, tv_loss: 0.01710130088031292\n",
      "iteration 2984, dc_loss: 0.018357479944825172, tv_loss: 0.017101354897022247\n",
      "iteration 2985, dc_loss: 0.01835743710398674, tv_loss: 0.01710146851837635\n",
      "iteration 2986, dc_loss: 0.018357383087277412, tv_loss: 0.01710130088031292\n",
      "iteration 2987, dc_loss: 0.01835736446082592, tv_loss: 0.017101384699344635\n",
      "iteration 2988, dc_loss: 0.01835734397172928, tv_loss: 0.017101364210247993\n",
      "iteration 2989, dc_loss: 0.018357329070568085, tv_loss: 0.01710118167102337\n",
      "iteration 2990, dc_loss: 0.01835733652114868, tv_loss: 0.01710110902786255\n",
      "iteration 2991, dc_loss: 0.018357345834374428, tv_loss: 0.017100971192121506\n",
      "iteration 2992, dc_loss: 0.018357355147600174, tv_loss: 0.017101017758250237\n",
      "iteration 2993, dc_loss: 0.018357351422309875, tv_loss: 0.017100874334573746\n",
      "iteration 2994, dc_loss: 0.018357353284955025, tv_loss: 0.017100609838962555\n",
      "iteration 2995, dc_loss: 0.01835733652114868, tv_loss: 0.017100650817155838\n",
      "iteration 2996, dc_loss: 0.018357323482632637, tv_loss: 0.017100827768445015\n",
      "iteration 2997, dc_loss: 0.018357275053858757, tv_loss: 0.01710074581205845\n",
      "iteration 2998, dc_loss: 0.018357187509536743, tv_loss: 0.017100684344768524\n",
      "iteration 2999, dc_loss: 0.018357127904891968, tv_loss: 0.017100809141993523\n",
      "iteration 3000, dc_loss: 0.018357085064053535, tv_loss: 0.01710083708167076\n",
      "iteration 3001, dc_loss: 0.01835707016289234, tv_loss: 0.017100859433412552\n",
      "iteration 3002, dc_loss: 0.01835707016289234, tv_loss: 0.017100881785154343\n",
      "iteration 3003, dc_loss: 0.01835705153644085, tv_loss: 0.017100779339671135\n",
      "iteration 3004, dc_loss: 0.018357016146183014, tv_loss: 0.017100851982831955\n",
      "iteration 3005, dc_loss: 0.01835699751973152, tv_loss: 0.017100797966122627\n",
      "iteration 3006, dc_loss: 0.01835699751973152, tv_loss: 0.017100710421800613\n",
      "iteration 3007, dc_loss: 0.01835700497031212, tv_loss: 0.017100738361477852\n",
      "iteration 3008, dc_loss: 0.018357008695602417, tv_loss: 0.017100760713219643\n",
      "iteration 3009, dc_loss: 0.01835700310766697, tv_loss: 0.01710069179534912\n",
      "iteration 3010, dc_loss: 0.018356963992118835, tv_loss: 0.0171007439494133\n",
      "iteration 3011, dc_loss: 0.018356937915086746, tv_loss: 0.017100678756833076\n",
      "iteration 3012, dc_loss: 0.018356909975409508, tv_loss: 0.017100678756833076\n",
      "iteration 3013, dc_loss: 0.01835687831044197, tv_loss: 0.017100708559155464\n",
      "iteration 3014, dc_loss: 0.018356891348958015, tv_loss: 0.01710057072341442\n",
      "iteration 3015, dc_loss: 0.018356928601861, tv_loss: 0.01710064336657524\n",
      "iteration 3016, dc_loss: 0.01835694909095764, tv_loss: 0.01710052601993084\n",
      "iteration 3017, dc_loss: 0.018356962129473686, tv_loss: 0.01710035651922226\n",
      "iteration 3018, dc_loss: 0.0183569248765707, tv_loss: 0.017100494354963303\n",
      "iteration 3019, dc_loss: 0.018356861546635628, tv_loss: 0.01710062101483345\n",
      "iteration 3020, dc_loss: 0.018356777727603912, tv_loss: 0.017100602388381958\n",
      "iteration 3021, dc_loss: 0.018356703221797943, tv_loss: 0.01710064336657524\n",
      "iteration 3022, dc_loss: 0.01835666596889496, tv_loss: 0.017100539058446884\n",
      "iteration 3023, dc_loss: 0.018356673419475555, tv_loss: 0.01710042916238308\n",
      "iteration 3024, dc_loss: 0.018356725573539734, tv_loss: 0.01710038259625435\n",
      "iteration 3025, dc_loss: 0.018356766551733017, tv_loss: 0.01710035465657711\n",
      "iteration 3026, dc_loss: 0.01835678145289421, tv_loss: 0.017100244760513306\n",
      "iteration 3027, dc_loss: 0.018356749787926674, tv_loss: 0.017100229859352112\n",
      "iteration 3028, dc_loss: 0.018356680870056152, tv_loss: 0.017100146040320396\n",
      "iteration 3029, dc_loss: 0.01835661008954048, tv_loss: 0.017100349068641663\n",
      "iteration 3030, dc_loss: 0.018356574699282646, tv_loss: 0.017100119963288307\n",
      "iteration 3031, dc_loss: 0.018356574699282646, tv_loss: 0.017100075259804726\n",
      "iteration 3032, dc_loss: 0.01835654303431511, tv_loss: 0.017100291326642036\n",
      "iteration 3033, dc_loss: 0.01835654489696026, tv_loss: 0.017100075259804726\n",
      "iteration 3034, dc_loss: 0.018356584012508392, tv_loss: 0.017099987715482712\n",
      "iteration 3035, dc_loss: 0.018356606364250183, tv_loss: 0.01710006408393383\n",
      "iteration 3036, dc_loss: 0.01835659146308899, tv_loss: 0.01710011251270771\n",
      "iteration 3037, dc_loss: 0.01835652068257332, tv_loss: 0.017099998891353607\n",
      "iteration 3038, dc_loss: 0.01835644245147705, tv_loss: 0.01709996536374092\n",
      "iteration 3039, dc_loss: 0.01835639774799347, tv_loss: 0.017100008204579353\n",
      "iteration 3040, dc_loss: 0.018356390297412872, tv_loss: 0.01710011251270771\n",
      "iteration 3041, dc_loss: 0.01835635118186474, tv_loss: 0.017100129276514053\n",
      "iteration 3042, dc_loss: 0.01835630275309086, tv_loss: 0.017100153490900993\n",
      "iteration 3043, dc_loss: 0.01835627853870392, tv_loss: 0.017099997028708458\n",
      "iteration 3044, dc_loss: 0.01835632510483265, tv_loss: 0.017099779099225998\n",
      "iteration 3045, dc_loss: 0.01835637353360653, tv_loss: 0.01709982380270958\n",
      "iteration 3046, dc_loss: 0.018356405198574066, tv_loss: 0.017099816352128983\n",
      "iteration 3047, dc_loss: 0.018356377258896828, tv_loss: 0.017099669203162193\n",
      "iteration 3048, dc_loss: 0.018356388434767723, tv_loss: 0.017099514603614807\n",
      "iteration 3049, dc_loss: 0.018356379121541977, tv_loss: 0.017099494114518166\n",
      "iteration 3050, dc_loss: 0.018356332555413246, tv_loss: 0.01709953509271145\n",
      "iteration 3051, dc_loss: 0.01835627108812332, tv_loss: 0.017099427059292793\n",
      "iteration 3052, dc_loss: 0.018356231972575188, tv_loss: 0.017099332064390182\n",
      "iteration 3053, dc_loss: 0.01835623010993004, tv_loss: 0.01709951087832451\n",
      "iteration 3054, dc_loss: 0.018356261774897575, tv_loss: 0.017099592834711075\n",
      "iteration 3055, dc_loss: 0.018356287851929665, tv_loss: 0.017099423334002495\n",
      "iteration 3056, dc_loss: 0.018356304615736008, tv_loss: 0.01709931157529354\n",
      "iteration 3057, dc_loss: 0.01835629530251026, tv_loss: 0.017099343240261078\n",
      "iteration 3058, dc_loss: 0.018356231972575188, tv_loss: 0.017099473625421524\n",
      "iteration 3059, dc_loss: 0.018356146290898323, tv_loss: 0.017099536955356598\n",
      "iteration 3060, dc_loss: 0.018356088548898697, tv_loss: 0.017099330201745033\n",
      "iteration 3061, dc_loss: 0.0183560810983181, tv_loss: 0.017099229618906975\n",
      "iteration 3062, dc_loss: 0.018356051295995712, tv_loss: 0.01709926500916481\n",
      "iteration 3063, dc_loss: 0.018356040120124817, tv_loss: 0.017099369317293167\n",
      "iteration 3064, dc_loss: 0.018356041982769966, tv_loss: 0.017099298536777496\n",
      "iteration 3065, dc_loss: 0.018356027081608772, tv_loss: 0.017098894342780113\n",
      "iteration 3066, dc_loss: 0.018356017768383026, tv_loss: 0.017099225893616676\n",
      "iteration 3067, dc_loss: 0.01835598424077034, tv_loss: 0.01709921844303608\n",
      "iteration 3068, dc_loss: 0.018355969339609146, tv_loss: 0.017099052667617798\n",
      "iteration 3069, dc_loss: 0.018355954438447952, tv_loss: 0.017098871991038322\n",
      "iteration 3070, dc_loss: 0.018355952575802803, tv_loss: 0.01709900051355362\n",
      "iteration 3071, dc_loss: 0.018355920910835266, tv_loss: 0.01709904335439205\n",
      "iteration 3072, dc_loss: 0.018355926498770714, tv_loss: 0.01709899678826332\n",
      "iteration 3073, dc_loss: 0.01835591159760952, tv_loss: 0.01709887385368347\n",
      "iteration 3074, dc_loss: 0.01835589110851288, tv_loss: 0.0170989241451025\n",
      "iteration 3075, dc_loss: 0.018355876207351685, tv_loss: 0.017098966985940933\n",
      "iteration 3076, dc_loss: 0.018355844542384148, tv_loss: 0.01709895394742489\n",
      "iteration 3077, dc_loss: 0.01835576817393303, tv_loss: 0.01709905080497265\n",
      "iteration 3078, dc_loss: 0.018355710431933403, tv_loss: 0.017098894342780113\n",
      "iteration 3079, dc_loss: 0.018355704843997955, tv_loss: 0.01709885522723198\n",
      "iteration 3080, dc_loss: 0.018355734646320343, tv_loss: 0.01709878258407116\n",
      "iteration 3081, dc_loss: 0.01835574582219124, tv_loss: 0.01709895394742489\n",
      "iteration 3082, dc_loss: 0.018355699256062508, tv_loss: 0.017098868265748024\n",
      "iteration 3083, dc_loss: 0.01835566945374012, tv_loss: 0.017098557204008102\n",
      "iteration 3084, dc_loss: 0.01835566945374012, tv_loss: 0.017098579555749893\n",
      "iteration 3085, dc_loss: 0.018355662003159523, tv_loss: 0.01709871180355549\n",
      "iteration 3086, dc_loss: 0.018355658277869225, tv_loss: 0.01709866151213646\n",
      "iteration 3087, dc_loss: 0.018355676904320717, tv_loss: 0.01709839701652527\n",
      "iteration 3088, dc_loss: 0.018355729058384895, tv_loss: 0.017098261043429375\n",
      "iteration 3089, dc_loss: 0.01835576817393303, tv_loss: 0.01709838956594467\n",
      "iteration 3090, dc_loss: 0.01835574209690094, tv_loss: 0.017098555341362953\n",
      "iteration 3091, dc_loss: 0.018355704843997955, tv_loss: 0.017098454758524895\n",
      "iteration 3092, dc_loss: 0.018355676904320717, tv_loss: 0.01709829829633236\n",
      "iteration 3093, dc_loss: 0.01835566945374012, tv_loss: 0.017098277807235718\n",
      "iteration 3094, dc_loss: 0.018355654552578926, tv_loss: 0.017098357900977135\n",
      "iteration 3095, dc_loss: 0.018355634063482285, tv_loss: 0.017098307609558105\n",
      "iteration 3096, dc_loss: 0.018355611711740494, tv_loss: 0.01709812693297863\n",
      "iteration 3097, dc_loss: 0.018355602398514748, tv_loss: 0.017098069190979004\n",
      "iteration 3098, dc_loss: 0.018355585634708405, tv_loss: 0.01709822565317154\n",
      "iteration 3099, dc_loss: 0.01835552044212818, tv_loss: 0.01709819585084915\n",
      "iteration 3100, dc_loss: 0.018355438485741615, tv_loss: 0.01709802635014057\n",
      "iteration 3101, dc_loss: 0.01835540495812893, tv_loss: 0.01709800399839878\n",
      "iteration 3102, dc_loss: 0.018355408683419228, tv_loss: 0.01709807850420475\n",
      "iteration 3103, dc_loss: 0.01835542730987072, tv_loss: 0.0170981977134943\n",
      "iteration 3104, dc_loss: 0.018355460837483406, tv_loss: 0.017097948119044304\n",
      "iteration 3105, dc_loss: 0.01835549809038639, tv_loss: 0.017097853124141693\n",
      "iteration 3106, dc_loss: 0.01835552230477333, tv_loss: 0.017097966745495796\n",
      "iteration 3107, dc_loss: 0.0183554757386446, tv_loss: 0.017098024487495422\n",
      "iteration 3108, dc_loss: 0.018355416133999825, tv_loss: 0.01709792949259281\n",
      "iteration 3109, dc_loss: 0.018355345353484154, tv_loss: 0.01709803007543087\n",
      "iteration 3110, dc_loss: 0.018355293199419975, tv_loss: 0.01709817908704281\n",
      "iteration 3111, dc_loss: 0.01835528574883938, tv_loss: 0.017097987234592438\n",
      "iteration 3112, dc_loss: 0.01835528574883938, tv_loss: 0.01709798350930214\n",
      "iteration 3113, dc_loss: 0.018355336040258408, tv_loss: 0.0170978344976902\n",
      "iteration 3114, dc_loss: 0.0183553546667099, tv_loss: 0.01709791086614132\n",
      "iteration 3115, dc_loss: 0.01835532672703266, tv_loss: 0.017097916454076767\n",
      "iteration 3116, dc_loss: 0.018355287611484528, tv_loss: 0.017097821459174156\n",
      "iteration 3117, dc_loss: 0.018355218693614006, tv_loss: 0.017097847536206245\n",
      "iteration 3118, dc_loss: 0.018355172127485275, tv_loss: 0.017097940668463707\n",
      "iteration 3119, dc_loss: 0.01835513301193714, tv_loss: 0.017097949981689453\n",
      "iteration 3120, dc_loss: 0.01835513673722744, tv_loss: 0.017097800970077515\n",
      "iteration 3121, dc_loss: 0.018355149775743484, tv_loss: 0.01709766685962677\n",
      "iteration 3122, dc_loss: 0.01835513301193714, tv_loss: 0.017097661271691322\n",
      "iteration 3123, dc_loss: 0.018355118110775948, tv_loss: 0.017097627744078636\n",
      "iteration 3124, dc_loss: 0.018355127424001694, tv_loss: 0.017097515985369682\n",
      "iteration 3125, dc_loss: 0.018355131149291992, tv_loss: 0.01709747314453125\n",
      "iteration 3126, dc_loss: 0.018355166539549828, tv_loss: 0.017097441479563713\n",
      "iteration 3127, dc_loss: 0.018355166539549828, tv_loss: 0.017097290605306625\n",
      "iteration 3128, dc_loss: 0.018355190753936768, tv_loss: 0.017097361385822296\n",
      "iteration 3129, dc_loss: 0.01835515908896923, tv_loss: 0.017097534611821175\n",
      "iteration 3130, dc_loss: 0.018355099484324455, tv_loss: 0.01709732413291931\n",
      "iteration 3131, dc_loss: 0.018355047330260277, tv_loss: 0.017097344622015953\n",
      "iteration 3132, dc_loss: 0.018354997038841248, tv_loss: 0.017097672447562218\n",
      "iteration 3133, dc_loss: 0.018354978412389755, tv_loss: 0.01709754578769207\n",
      "iteration 3134, dc_loss: 0.018354954198002815, tv_loss: 0.017097486183047295\n",
      "iteration 3135, dc_loss: 0.01835497096180916, tv_loss: 0.017097443342208862\n",
      "iteration 3136, dc_loss: 0.018354976549744606, tv_loss: 0.017097530886530876\n",
      "iteration 3137, dc_loss: 0.018354948610067368, tv_loss: 0.017097417265176773\n",
      "iteration 3138, dc_loss: 0.01835494488477707, tv_loss: 0.017097285017371178\n",
      "iteration 3139, dc_loss: 0.01835496351122856, tv_loss: 0.017097199335694313\n",
      "iteration 3140, dc_loss: 0.01835499331355095, tv_loss: 0.017097165808081627\n",
      "iteration 3141, dc_loss: 0.018355030566453934, tv_loss: 0.017097020521759987\n",
      "iteration 3142, dc_loss: 0.01835513487458229, tv_loss: 0.017096709460020065\n",
      "iteration 3143, dc_loss: 0.01835516095161438, tv_loss: 0.01709660142660141\n",
      "iteration 3144, dc_loss: 0.018355118110775948, tv_loss: 0.01709667593240738\n",
      "iteration 3145, dc_loss: 0.018355032429099083, tv_loss: 0.017096862196922302\n",
      "iteration 3146, dc_loss: 0.0183548741042614, tv_loss: 0.017097048461437225\n",
      "iteration 3147, dc_loss: 0.018354695290327072, tv_loss: 0.01709705963730812\n",
      "iteration 3148, dc_loss: 0.01835458166897297, tv_loss: 0.01709737628698349\n",
      "iteration 3149, dc_loss: 0.018354548141360283, tv_loss: 0.017097407951951027\n",
      "iteration 3150, dc_loss: 0.01835460774600506, tv_loss: 0.01709718070924282\n",
      "iteration 3151, dc_loss: 0.01835469715297222, tv_loss: 0.01709696464240551\n",
      "iteration 3152, dc_loss: 0.018354767933487892, tv_loss: 0.017097005620598793\n",
      "iteration 3153, dc_loss: 0.018354788422584534, tv_loss: 0.017097052186727524\n",
      "iteration 3154, dc_loss: 0.018354782834649086, tv_loss: 0.017096882686018944\n",
      "iteration 3155, dc_loss: 0.018354743719100952, tv_loss: 0.01709679700434208\n",
      "iteration 3156, dc_loss: 0.01835467666387558, tv_loss: 0.01709689572453499\n",
      "iteration 3157, dc_loss: 0.018354643136262894, tv_loss: 0.01709691993892193\n",
      "iteration 3158, dc_loss: 0.018354639410972595, tv_loss: 0.017096973955631256\n",
      "iteration 3159, dc_loss: 0.018354643136262894, tv_loss: 0.01709686778485775\n",
      "iteration 3160, dc_loss: 0.01835463009774685, tv_loss: 0.01709672249853611\n",
      "iteration 3161, dc_loss: 0.018354594707489014, tv_loss: 0.017096787691116333\n",
      "iteration 3162, dc_loss: 0.01835458353161812, tv_loss: 0.017096826806664467\n",
      "iteration 3163, dc_loss: 0.01835460588335991, tv_loss: 0.017096679657697678\n",
      "iteration 3164, dc_loss: 0.01835460774600506, tv_loss: 0.01709653064608574\n",
      "iteration 3165, dc_loss: 0.018354594707489014, tv_loss: 0.01709654927253723\n",
      "iteration 3166, dc_loss: 0.018354596570134163, tv_loss: 0.017096590250730515\n",
      "iteration 3167, dc_loss: 0.018354568630456924, tv_loss: 0.017096618190407753\n",
      "iteration 3168, dc_loss: 0.01835457794368267, tv_loss: 0.01709660328924656\n",
      "iteration 3169, dc_loss: 0.018354546278715134, tv_loss: 0.017096636816859245\n",
      "iteration 3170, dc_loss: 0.018354536965489388, tv_loss: 0.017096620053052902\n",
      "iteration 3171, dc_loss: 0.018354538828134537, tv_loss: 0.017096443101763725\n",
      "iteration 3172, dc_loss: 0.018354536965489388, tv_loss: 0.01709636114537716\n",
      "iteration 3173, dc_loss: 0.018354514613747597, tv_loss: 0.017096349969506264\n",
      "iteration 3174, dc_loss: 0.01835448108613491, tv_loss: 0.017096612602472305\n",
      "iteration 3175, dc_loss: 0.018354423344135284, tv_loss: 0.017096543684601784\n",
      "iteration 3176, dc_loss: 0.018354402855038643, tv_loss: 0.01709628850221634\n",
      "iteration 3177, dc_loss: 0.01835441030561924, tv_loss: 0.01709640771150589\n",
      "iteration 3178, dc_loss: 0.01835441030561924, tv_loss: 0.017096392810344696\n",
      "iteration 3179, dc_loss: 0.0183543898165226, tv_loss: 0.017096256837248802\n",
      "iteration 3180, dc_loss: 0.018354365602135658, tv_loss: 0.017096053808927536\n",
      "iteration 3181, dc_loss: 0.018354343250393867, tv_loss: 0.017096111550927162\n",
      "iteration 3182, dc_loss: 0.018354328349232674, tv_loss: 0.017096204683184624\n",
      "iteration 3183, dc_loss: 0.01835431531071663, tv_loss: 0.017096228897571564\n",
      "iteration 3184, dc_loss: 0.01835430972278118, tv_loss: 0.01709636114537716\n",
      "iteration 3185, dc_loss: 0.01835433952510357, tv_loss: 0.017096176743507385\n",
      "iteration 3186, dc_loss: 0.018354365602135658, tv_loss: 0.017096061259508133\n",
      "iteration 3187, dc_loss: 0.018354382365942, tv_loss: 0.017096195369958878\n",
      "iteration 3188, dc_loss: 0.018354399129748344, tv_loss: 0.017096148803830147\n",
      "iteration 3189, dc_loss: 0.018354346975684166, tv_loss: 0.01709611527621746\n",
      "iteration 3190, dc_loss: 0.018354326486587524, tv_loss: 0.017096059396862984\n",
      "iteration 3191, dc_loss: 0.01835428923368454, tv_loss: 0.017095906659960747\n",
      "iteration 3192, dc_loss: 0.01835424266755581, tv_loss: 0.01709592528641224\n",
      "iteration 3193, dc_loss: 0.018354171887040138, tv_loss: 0.017096126452088356\n",
      "iteration 3194, dc_loss: 0.01835414208471775, tv_loss: 0.017096050083637238\n",
      "iteration 3195, dc_loss: 0.018354130908846855, tv_loss: 0.01709592342376709\n",
      "iteration 3196, dc_loss: 0.01835414581000805, tv_loss: 0.01709587872028351\n",
      "iteration 3197, dc_loss: 0.018354153260588646, tv_loss: 0.017095882445573807\n",
      "iteration 3198, dc_loss: 0.01835414208471775, tv_loss: 0.01709582842886448\n",
      "iteration 3199, dc_loss: 0.018354153260588646, tv_loss: 0.017095845192670822\n",
      "iteration 3200, dc_loss: 0.018354151397943497, tv_loss: 0.017095768824219704\n",
      "iteration 3201, dc_loss: 0.0183541402220726, tv_loss: 0.017095796763896942\n",
      "iteration 3202, dc_loss: 0.018354134634137154, tv_loss: 0.01709582470357418\n",
      "iteration 3203, dc_loss: 0.01835412159562111, tv_loss: 0.017095791175961494\n",
      "iteration 3204, dc_loss: 0.018354102969169617, tv_loss: 0.017095688730478287\n",
      "iteration 3205, dc_loss: 0.01835409365594387, tv_loss: 0.017095712944865227\n",
      "iteration 3206, dc_loss: 0.018354082480072975, tv_loss: 0.017095686867833138\n",
      "iteration 3207, dc_loss: 0.01835406944155693, tv_loss: 0.01709568127989769\n",
      "iteration 3208, dc_loss: 0.01835402101278305, tv_loss: 0.017095599323511124\n",
      "iteration 3209, dc_loss: 0.01835399493575096, tv_loss: 0.01709558069705963\n",
      "iteration 3210, dc_loss: 0.018353985622525215, tv_loss: 0.01709575764834881\n",
      "iteration 3211, dc_loss: 0.018353980034589767, tv_loss: 0.01709577813744545\n",
      "iteration 3212, dc_loss: 0.018353937193751335, tv_loss: 0.017095552757382393\n",
      "iteration 3213, dc_loss: 0.01835387386381626, tv_loss: 0.017095595598220825\n",
      "iteration 3214, dc_loss: 0.018353832885622978, tv_loss: 0.017095649614930153\n",
      "iteration 3215, dc_loss: 0.01835385523736477, tv_loss: 0.017095619812607765\n",
      "iteration 3216, dc_loss: 0.018353870138525963, tv_loss: 0.017095603048801422\n",
      "iteration 3217, dc_loss: 0.018353896215558052, tv_loss: 0.017095502465963364\n",
      "iteration 3218, dc_loss: 0.018353940919041634, tv_loss: 0.017095474526286125\n",
      "iteration 3219, dc_loss: 0.018353965133428574, tv_loss: 0.017095278948545456\n",
      "iteration 3220, dc_loss: 0.018353983759880066, tv_loss: 0.01709521934390068\n",
      "iteration 3221, dc_loss: 0.018353968858718872, tv_loss: 0.017095239832997322\n",
      "iteration 3222, dc_loss: 0.018353944644331932, tv_loss: 0.017095297574996948\n",
      "iteration 3223, dc_loss: 0.018353896215558052, tv_loss: 0.017095370218157768\n",
      "iteration 3224, dc_loss: 0.01835390366613865, tv_loss: 0.017095454037189484\n",
      "iteration 3225, dc_loss: 0.018353892490267754, tv_loss: 0.017095444723963737\n",
      "iteration 3226, dc_loss: 0.01835385151207447, tv_loss: 0.017095282673835754\n",
      "iteration 3227, dc_loss: 0.018353775143623352, tv_loss: 0.017095239832997322\n",
      "iteration 3228, dc_loss: 0.01835370995104313, tv_loss: 0.017095398157835007\n",
      "iteration 3229, dc_loss: 0.018353715538978577, tv_loss: 0.01709553226828575\n",
      "iteration 3230, dc_loss: 0.018353719264268875, tv_loss: 0.017095258459448814\n",
      "iteration 3231, dc_loss: 0.018353745341300964, tv_loss: 0.017095062881708145\n",
      "iteration 3232, dc_loss: 0.018353775143623352, tv_loss: 0.01709498092532158\n",
      "iteration 3233, dc_loss: 0.018353741616010666, tv_loss: 0.017095020040869713\n",
      "iteration 3234, dc_loss: 0.01835370436310768, tv_loss: 0.017095286399126053\n",
      "iteration 3235, dc_loss: 0.01835365779697895, tv_loss: 0.017095055431127548\n",
      "iteration 3236, dc_loss: 0.01835360750555992, tv_loss: 0.01709507405757904\n",
      "iteration 3237, dc_loss: 0.018353600054979324, tv_loss: 0.017095211893320084\n",
      "iteration 3238, dc_loss: 0.018353616818785667, tv_loss: 0.017095133662223816\n",
      "iteration 3239, dc_loss: 0.018353650346398354, tv_loss: 0.017095066606998444\n",
      "iteration 3240, dc_loss: 0.018353644758462906, tv_loss: 0.017095059156417847\n",
      "iteration 3241, dc_loss: 0.018353614956140518, tv_loss: 0.01709497720003128\n",
      "iteration 3242, dc_loss: 0.018353592604398727, tv_loss: 0.017094923183321953\n",
      "iteration 3243, dc_loss: 0.018353549763560295, tv_loss: 0.017095118761062622\n",
      "iteration 3244, dc_loss: 0.01835351623594761, tv_loss: 0.017095087096095085\n",
      "iteration 3245, dc_loss: 0.01835354045033455, tv_loss: 0.017094828188419342\n",
      "iteration 3246, dc_loss: 0.018353557214140892, tv_loss: 0.0170949287712574\n",
      "iteration 3247, dc_loss: 0.01835356280207634, tv_loss: 0.017094861716032028\n",
      "iteration 3248, dc_loss: 0.018353531137108803, tv_loss: 0.017094803974032402\n",
      "iteration 3249, dc_loss: 0.018353525549173355, tv_loss: 0.017094988375902176\n",
      "iteration 3250, dc_loss: 0.018353551626205444, tv_loss: 0.01709497906267643\n",
      "iteration 3251, dc_loss: 0.01835358329117298, tv_loss: 0.017094874754548073\n",
      "iteration 3252, dc_loss: 0.018353603780269623, tv_loss: 0.01709473505616188\n",
      "iteration 3253, dc_loss: 0.018353601917624474, tv_loss: 0.017094794660806656\n",
      "iteration 3254, dc_loss: 0.018353568390011787, tv_loss: 0.017094770446419716\n",
      "iteration 3255, dc_loss: 0.018353518098592758, tv_loss: 0.017094764858484268\n",
      "iteration 3256, dc_loss: 0.018353469669818878, tv_loss: 0.017094891518354416\n",
      "iteration 3257, dc_loss: 0.018353387713432312, tv_loss: 0.017094822600483894\n",
      "iteration 3258, dc_loss: 0.018353324383497238, tv_loss: 0.017094934359192848\n",
      "iteration 3259, dc_loss: 0.018353240564465523, tv_loss: 0.017094891518354416\n",
      "iteration 3260, dc_loss: 0.01835322380065918, tv_loss: 0.01709495298564434\n",
      "iteration 3261, dc_loss: 0.01835324801504612, tv_loss: 0.017094939947128296\n",
      "iteration 3262, dc_loss: 0.018353315070271492, tv_loss: 0.017094869166612625\n",
      "iteration 3263, dc_loss: 0.018353324383497238, tv_loss: 0.01709461212158203\n",
      "iteration 3264, dc_loss: 0.01835331879556179, tv_loss: 0.01709473505616188\n",
      "iteration 3265, dc_loss: 0.0183532927185297, tv_loss: 0.01709488220512867\n",
      "iteration 3266, dc_loss: 0.018353251740336418, tv_loss: 0.017094794660806656\n",
      "iteration 3267, dc_loss: 0.018353233113884926, tv_loss: 0.017094818875193596\n",
      "iteration 3268, dc_loss: 0.018353233113884926, tv_loss: 0.01709485426545143\n",
      "iteration 3269, dc_loss: 0.018353212624788284, tv_loss: 0.017094887793064117\n",
      "iteration 3270, dc_loss: 0.018353234976530075, tv_loss: 0.01709490269422531\n",
      "iteration 3271, dc_loss: 0.018353227525949478, tv_loss: 0.017094720155000687\n",
      "iteration 3272, dc_loss: 0.01835324801504612, tv_loss: 0.017094647511839867\n",
      "iteration 3273, dc_loss: 0.018353277817368507, tv_loss: 0.01709459349513054\n",
      "iteration 3274, dc_loss: 0.01835329830646515, tv_loss: 0.017094463109970093\n",
      "iteration 3275, dc_loss: 0.018353283405303955, tv_loss: 0.017094360664486885\n",
      "iteration 3276, dc_loss: 0.018353277817368507, tv_loss: 0.017094401642680168\n",
      "iteration 3277, dc_loss: 0.018353240564465523, tv_loss: 0.017094343900680542\n",
      "iteration 3278, dc_loss: 0.018353171646595, tv_loss: 0.017094265669584274\n",
      "iteration 3279, dc_loss: 0.018353112041950226, tv_loss: 0.017094334587454796\n",
      "iteration 3280, dc_loss: 0.01835310086607933, tv_loss: 0.017094360664486885\n",
      "iteration 3281, dc_loss: 0.01835310086607933, tv_loss: 0.017094343900680542\n",
      "iteration 3282, dc_loss: 0.01835312508046627, tv_loss: 0.01709410920739174\n",
      "iteration 3283, dc_loss: 0.018353158608078957, tv_loss: 0.01709406077861786\n",
      "iteration 3284, dc_loss: 0.01835314929485321, tv_loss: 0.01709417626261711\n",
      "iteration 3285, dc_loss: 0.018353130668401718, tv_loss: 0.017094088718295097\n",
      "iteration 3286, dc_loss: 0.018353108316659927, tv_loss: 0.017093995586037636\n",
      "iteration 3287, dc_loss: 0.018353093415498734, tv_loss: 0.017094112932682037\n",
      "iteration 3288, dc_loss: 0.01835307478904724, tv_loss: 0.017094168812036514\n",
      "iteration 3289, dc_loss: 0.01835307478904724, tv_loss: 0.017094099894165993\n",
      "iteration 3290, dc_loss: 0.01835305616259575, tv_loss: 0.017093976959586143\n",
      "iteration 3291, dc_loss: 0.018353017047047615, tv_loss: 0.017094017937779427\n",
      "iteration 3292, dc_loss: 0.018352964892983437, tv_loss: 0.01709398813545704\n",
      "iteration 3293, dc_loss: 0.018352974206209183, tv_loss: 0.017093921080231667\n",
      "iteration 3294, dc_loss: 0.01835300214588642, tv_loss: 0.017094064503908157\n",
      "iteration 3295, dc_loss: 0.018353044986724854, tv_loss: 0.017093874514102936\n",
      "iteration 3296, dc_loss: 0.01835310272872448, tv_loss: 0.017093803733587265\n",
      "iteration 3297, dc_loss: 0.018353117629885674, tv_loss: 0.01709393784403801\n",
      "iteration 3298, dc_loss: 0.018353082239627838, tv_loss: 0.017093772068619728\n",
      "iteration 3299, dc_loss: 0.018353041261434555, tv_loss: 0.017093824222683907\n",
      "iteration 3300, dc_loss: 0.018353024497628212, tv_loss: 0.017093801870942116\n",
      "iteration 3301, dc_loss: 0.01835300773382187, tv_loss: 0.017093831673264503\n",
      "iteration 3302, dc_loss: 0.018352970480918884, tv_loss: 0.01709383726119995\n",
      "iteration 3303, dc_loss: 0.018352923914790154, tv_loss: 0.017093729227781296\n",
      "iteration 3304, dc_loss: 0.018352894112467766, tv_loss: 0.017093703150749207\n",
      "iteration 3305, dc_loss: 0.01835286058485508, tv_loss: 0.017093727365136147\n",
      "iteration 3306, dc_loss: 0.01835283450782299, tv_loss: 0.017093734815716743\n",
      "iteration 3307, dc_loss: 0.018352847546339035, tv_loss: 0.0170937180519104\n",
      "iteration 3308, dc_loss: 0.018352879211306572, tv_loss: 0.01709379069507122\n",
      "iteration 3309, dc_loss: 0.018352873623371124, tv_loss: 0.017093533650040627\n",
      "iteration 3310, dc_loss: 0.01835285685956478, tv_loss: 0.017093632370233536\n",
      "iteration 3311, dc_loss: 0.018352795392274857, tv_loss: 0.017093632370233536\n",
      "iteration 3312, dc_loss: 0.018352733924984932, tv_loss: 0.017093634232878685\n",
      "iteration 3313, dc_loss: 0.018352685496211052, tv_loss: 0.017093796283006668\n",
      "iteration 3314, dc_loss: 0.01835266500711441, tv_loss: 0.01709374226629734\n",
      "iteration 3315, dc_loss: 0.018352685496211052, tv_loss: 0.017093732953071594\n",
      "iteration 3316, dc_loss: 0.018352728337049484, tv_loss: 0.017093593254685402\n",
      "iteration 3317, dc_loss: 0.018352774903178215, tv_loss: 0.017093475908041\n",
      "iteration 3318, dc_loss: 0.018352774903178215, tv_loss: 0.017093533650040627\n",
      "iteration 3319, dc_loss: 0.018352722749114037, tv_loss: 0.017093520611524582\n",
      "iteration 3320, dc_loss: 0.018352676182985306, tv_loss: 0.017093440517783165\n",
      "iteration 3321, dc_loss: 0.018352678045630455, tv_loss: 0.017093589529395103\n",
      "iteration 3322, dc_loss: 0.018352694809436798, tv_loss: 0.01709355227649212\n",
      "iteration 3323, dc_loss: 0.01835273951292038, tv_loss: 0.017093362286686897\n",
      "iteration 3324, dc_loss: 0.018352767452597618, tv_loss: 0.017093349248170853\n",
      "iteration 3325, dc_loss: 0.018352771177887917, tv_loss: 0.017093218863010406\n",
      "iteration 3326, dc_loss: 0.018352730199694633, tv_loss: 0.017093144357204437\n",
      "iteration 3327, dc_loss: 0.018352670595049858, tv_loss: 0.01709337905049324\n",
      "iteration 3328, dc_loss: 0.018352586776018143, tv_loss: 0.01709344983100891\n",
      "iteration 3329, dc_loss: 0.018352540209889412, tv_loss: 0.017093351110816002\n",
      "iteration 3330, dc_loss: 0.018352549523115158, tv_loss: 0.017093222588300705\n",
      "iteration 3331, dc_loss: 0.01835257187485695, tv_loss: 0.017093215137720108\n",
      "iteration 3332, dc_loss: 0.018352583050727844, tv_loss: 0.01709332875907421\n",
      "iteration 3333, dc_loss: 0.018352579325437546, tv_loss: 0.01709326170384884\n",
      "iteration 3334, dc_loss: 0.018352603539824486, tv_loss: 0.01709299348294735\n",
      "iteration 3335, dc_loss: 0.01835256814956665, tv_loss: 0.017093027010560036\n",
      "iteration 3336, dc_loss: 0.018352510407567024, tv_loss: 0.01709318533539772\n",
      "iteration 3337, dc_loss: 0.018352461978793144, tv_loss: 0.017093278467655182\n",
      "iteration 3338, dc_loss: 0.018352439627051353, tv_loss: 0.017093146219849586\n",
      "iteration 3339, dc_loss: 0.018352465704083443, tv_loss: 0.017093095928430557\n",
      "iteration 3340, dc_loss: 0.01835251972079277, tv_loss: 0.017093133181333542\n",
      "iteration 3341, dc_loss: 0.01835254766047001, tv_loss: 0.017093179747462273\n",
      "iteration 3342, dc_loss: 0.018352564424276352, tv_loss: 0.01709309220314026\n",
      "iteration 3343, dc_loss: 0.01835252344608307, tv_loss: 0.01709311455488205\n",
      "iteration 3344, dc_loss: 0.018352458253502846, tv_loss: 0.017093129456043243\n",
      "iteration 3345, dc_loss: 0.018352409824728966, tv_loss: 0.017093291506171227\n",
      "iteration 3346, dc_loss: 0.018352357670664787, tv_loss: 0.017093339934945107\n",
      "iteration 3347, dc_loss: 0.018352344632148743, tv_loss: 0.01709311455488205\n",
      "iteration 3348, dc_loss: 0.018352368846535683, tv_loss: 0.017093073576688766\n",
      "iteration 3349, dc_loss: 0.018352404236793518, tv_loss: 0.01709306426346302\n",
      "iteration 3350, dc_loss: 0.018352456390857697, tv_loss: 0.017093077301979065\n",
      "iteration 3351, dc_loss: 0.018352486193180084, tv_loss: 0.017092905938625336\n",
      "iteration 3352, dc_loss: 0.01835249364376068, tv_loss: 0.017092891037464142\n",
      "iteration 3353, dc_loss: 0.01835246942937374, tv_loss: 0.017092809081077576\n",
      "iteration 3354, dc_loss: 0.01835242286324501, tv_loss: 0.017092842608690262\n",
      "iteration 3355, dc_loss: 0.01835237629711628, tv_loss: 0.01709306798875332\n",
      "iteration 3356, dc_loss: 0.018352320417761803, tv_loss: 0.017092904075980186\n",
      "iteration 3357, dc_loss: 0.018352290615439415, tv_loss: 0.017092807218432426\n",
      "iteration 3358, dc_loss: 0.018352286890149117, tv_loss: 0.017093027010560036\n",
      "iteration 3359, dc_loss: 0.01835227943956852, tv_loss: 0.017092853784561157\n",
      "iteration 3360, dc_loss: 0.018352290615439415, tv_loss: 0.01709275133907795\n",
      "iteration 3361, dc_loss: 0.018352337181568146, tv_loss: 0.017092715948820114\n",
      "iteration 3362, dc_loss: 0.01835237257182598, tv_loss: 0.017092568799853325\n",
      "iteration 3363, dc_loss: 0.01835239864885807, tv_loss: 0.017092542722821236\n",
      "iteration 3364, dc_loss: 0.018352355808019638, tv_loss: 0.017092479392886162\n",
      "iteration 3365, dc_loss: 0.018352283164858818, tv_loss: 0.017092572525143623\n",
      "iteration 3366, dc_loss: 0.018352242186665535, tv_loss: 0.01709260419011116\n",
      "iteration 3367, dc_loss: 0.018352249637246132, tv_loss: 0.017092501744627953\n",
      "iteration 3368, dc_loss: 0.01835222728550434, tv_loss: 0.017092518508434296\n",
      "iteration 3369, dc_loss: 0.01835222728550434, tv_loss: 0.017092593014240265\n",
      "iteration 3370, dc_loss: 0.01835225522518158, tv_loss: 0.01709248311817646\n",
      "iteration 3371, dc_loss: 0.01835225149989128, tv_loss: 0.017092419788241386\n",
      "iteration 3372, dc_loss: 0.018352216109633446, tv_loss: 0.01709253340959549\n",
      "iteration 3373, dc_loss: 0.01835220493376255, tv_loss: 0.01709253340959549\n",
      "iteration 3374, dc_loss: 0.01835220865905285, tv_loss: 0.017092568799853325\n",
      "iteration 3375, dc_loss: 0.018352216109633446, tv_loss: 0.017092447727918625\n",
      "iteration 3376, dc_loss: 0.018352245911955833, tv_loss: 0.017092473804950714\n",
      "iteration 3377, dc_loss: 0.018352249637246132, tv_loss: 0.017092466354370117\n",
      "iteration 3378, dc_loss: 0.018352195620536804, tv_loss: 0.017092550173401833\n",
      "iteration 3379, dc_loss: 0.01835213042795658, tv_loss: 0.017092619091272354\n",
      "iteration 3380, dc_loss: 0.018352100625634193, tv_loss: 0.01709246076643467\n",
      "iteration 3381, dc_loss: 0.018352100625634193, tv_loss: 0.017092470079660416\n",
      "iteration 3382, dc_loss: 0.01835216023027897, tv_loss: 0.017092512920498848\n",
      "iteration 3383, dc_loss: 0.018352217972278595, tv_loss: 0.017092453315854073\n",
      "iteration 3384, dc_loss: 0.018352219834923744, tv_loss: 0.017092358320951462\n",
      "iteration 3385, dc_loss: 0.018352126702666283, tv_loss: 0.017092255875468254\n",
      "iteration 3386, dc_loss: 0.018352029845118523, tv_loss: 0.017092423513531685\n",
      "iteration 3387, dc_loss: 0.0183519646525383, tv_loss: 0.017092503607273102\n",
      "iteration 3388, dc_loss: 0.018351972103118896, tv_loss: 0.017092552036046982\n",
      "iteration 3389, dc_loss: 0.018351973965764046, tv_loss: 0.01709231734275818\n",
      "iteration 3390, dc_loss: 0.018351977691054344, tv_loss: 0.017092347145080566\n",
      "iteration 3391, dc_loss: 0.018351981416344643, tv_loss: 0.017092518508434296\n",
      "iteration 3392, dc_loss: 0.018352016806602478, tv_loss: 0.01709236577153206\n",
      "iteration 3393, dc_loss: 0.018352044746279716, tv_loss: 0.01709206961095333\n",
      "iteration 3394, dc_loss: 0.018352068960666656, tv_loss: 0.017092129215598106\n",
      "iteration 3395, dc_loss: 0.018352067098021507, tv_loss: 0.017092159017920494\n",
      "iteration 3396, dc_loss: 0.018352050334215164, tv_loss: 0.01709207147359848\n",
      "iteration 3397, dc_loss: 0.01835199072957039, tv_loss: 0.0170921441167593\n",
      "iteration 3398, dc_loss: 0.018351931124925613, tv_loss: 0.017092272639274597\n",
      "iteration 3399, dc_loss: 0.01835191249847412, tv_loss: 0.017092321068048477\n",
      "iteration 3400, dc_loss: 0.018351877108216286, tv_loss: 0.017092257738113403\n",
      "iteration 3401, dc_loss: 0.018351884558796883, tv_loss: 0.017092064023017883\n",
      "iteration 3402, dc_loss: 0.018351923674345016, tv_loss: 0.017092054709792137\n",
      "iteration 3403, dc_loss: 0.01835193857550621, tv_loss: 0.017092237249016762\n",
      "iteration 3404, dc_loss: 0.01835191436111927, tv_loss: 0.01709209382534027\n",
      "iteration 3405, dc_loss: 0.01835191436111927, tv_loss: 0.01709200069308281\n",
      "iteration 3406, dc_loss: 0.01835191808640957, tv_loss: 0.017092138528823853\n",
      "iteration 3407, dc_loss: 0.018351906910538673, tv_loss: 0.01709204725921154\n",
      "iteration 3408, dc_loss: 0.018351895734667778, tv_loss: 0.017091980203986168\n",
      "iteration 3409, dc_loss: 0.01835186779499054, tv_loss: 0.01709195412695408\n",
      "iteration 3410, dc_loss: 0.018351849168539047, tv_loss: 0.01709188148379326\n",
      "iteration 3411, dc_loss: 0.018351836130023003, tv_loss: 0.01709195040166378\n",
      "iteration 3412, dc_loss: 0.018351798877120018, tv_loss: 0.017091987654566765\n",
      "iteration 3413, dc_loss: 0.01835176907479763, tv_loss: 0.017092060297727585\n",
      "iteration 3414, dc_loss: 0.01835174299776554, tv_loss: 0.017091907560825348\n",
      "iteration 3415, dc_loss: 0.018351778388023376, tv_loss: 0.017091896384954453\n",
      "iteration 3416, dc_loss: 0.01835179701447487, tv_loss: 0.01709202490746975\n",
      "iteration 3417, dc_loss: 0.01835179142653942, tv_loss: 0.017091888934373856\n",
      "iteration 3418, dc_loss: 0.018351752310991287, tv_loss: 0.01709183305501938\n",
      "iteration 3419, dc_loss: 0.018351726233959198, tv_loss: 0.017092039808630943\n",
      "iteration 3420, dc_loss: 0.018351690843701363, tv_loss: 0.01709192618727684\n",
      "iteration 3421, dc_loss: 0.018351653590798378, tv_loss: 0.01709190383553505\n",
      "iteration 3422, dc_loss: 0.018351607024669647, tv_loss: 0.017092108726501465\n",
      "iteration 3423, dc_loss: 0.0183516014367342, tv_loss: 0.017092153429985046\n",
      "iteration 3424, dc_loss: 0.01835160329937935, tv_loss: 0.017091961577534676\n",
      "iteration 3425, dc_loss: 0.018351610749959946, tv_loss: 0.017091775313019753\n",
      "iteration 3426, dc_loss: 0.018351607024669647, tv_loss: 0.017091847956180573\n",
      "iteration 3427, dc_loss: 0.01835157908499241, tv_loss: 0.017091821879148483\n",
      "iteration 3428, dc_loss: 0.01835152506828308, tv_loss: 0.017091941088438034\n",
      "iteration 3429, dc_loss: 0.018351495265960693, tv_loss: 0.017092090100049973\n",
      "iteration 3430, dc_loss: 0.018351512029767036, tv_loss: 0.017091965302824974\n",
      "iteration 3431, dc_loss: 0.018351539969444275, tv_loss: 0.017091630026698112\n",
      "iteration 3432, dc_loss: 0.01835159957408905, tv_loss: 0.017091600224375725\n",
      "iteration 3433, dc_loss: 0.018351653590798378, tv_loss: 0.017091622576117516\n",
      "iteration 3434, dc_loss: 0.01835167407989502, tv_loss: 0.01709153689444065\n",
      "iteration 3435, dc_loss: 0.018351688981056213, tv_loss: 0.017091579735279083\n",
      "iteration 3436, dc_loss: 0.018351716920733452, tv_loss: 0.017091505229473114\n",
      "iteration 3437, dc_loss: 0.018351737409830093, tv_loss: 0.017091432586312294\n",
      "iteration 3438, dc_loss: 0.018351685255765915, tv_loss: 0.017091402783989906\n",
      "iteration 3439, dc_loss: 0.01835162378847599, tv_loss: 0.017091555520892143\n",
      "iteration 3440, dc_loss: 0.018351571634411812, tv_loss: 0.017091507092118263\n",
      "iteration 3441, dc_loss: 0.01835150085389614, tv_loss: 0.017091482877731323\n",
      "iteration 3442, dc_loss: 0.01835147850215435, tv_loss: 0.017091579735279083\n",
      "iteration 3443, dc_loss: 0.01835145428776741, tv_loss: 0.017091602087020874\n",
      "iteration 3444, dc_loss: 0.018351448699831963, tv_loss: 0.01709134317934513\n",
      "iteration 3445, dc_loss: 0.018351469188928604, tv_loss: 0.017091454938054085\n",
      "iteration 3446, dc_loss: 0.018351489678025246, tv_loss: 0.017091456800699234\n",
      "iteration 3447, dc_loss: 0.018351493403315544, tv_loss: 0.017091337591409683\n",
      "iteration 3448, dc_loss: 0.018351474776864052, tv_loss: 0.017091307789087296\n",
      "iteration 3449, dc_loss: 0.01835145801305771, tv_loss: 0.01709129475057125\n",
      "iteration 3450, dc_loss: 0.018351420760154724, tv_loss: 0.017091404646635056\n",
      "iteration 3451, dc_loss: 0.018351418897509575, tv_loss: 0.017091434448957443\n",
      "iteration 3452, dc_loss: 0.018351402133703232, tv_loss: 0.01709139719605446\n",
      "iteration 3453, dc_loss: 0.018351394683122635, tv_loss: 0.017091257497668266\n",
      "iteration 3454, dc_loss: 0.01835138164460659, tv_loss: 0.017091255635023117\n",
      "iteration 3455, dc_loss: 0.018351327627897263, tv_loss: 0.017091458663344383\n",
      "iteration 3456, dc_loss: 0.018351292237639427, tv_loss: 0.017091577872633934\n",
      "iteration 3457, dc_loss: 0.018351279199123383, tv_loss: 0.017091499641537666\n",
      "iteration 3458, dc_loss: 0.018351329490542412, tv_loss: 0.017091315239667892\n",
      "iteration 3459, dc_loss: 0.01835140399634838, tv_loss: 0.017091164365410805\n",
      "iteration 3460, dc_loss: 0.018351472914218903, tv_loss: 0.0170911718159914\n",
      "iteration 3461, dc_loss: 0.018351461738348007, tv_loss: 0.017091179266572\n",
      "iteration 3462, dc_loss: 0.018351398408412933, tv_loss: 0.017091233283281326\n",
      "iteration 3463, dc_loss: 0.018351292237639427, tv_loss: 0.017091238871216774\n",
      "iteration 3464, dc_loss: 0.018351227045059204, tv_loss: 0.017091209068894386\n",
      "iteration 3465, dc_loss: 0.018351227045059204, tv_loss: 0.01709134317934513\n",
      "iteration 3466, dc_loss: 0.01835126429796219, tv_loss: 0.017091182991862297\n",
      "iteration 3467, dc_loss: 0.01835133135318756, tv_loss: 0.017091186717152596\n",
      "iteration 3468, dc_loss: 0.01835135743021965, tv_loss: 0.01709122583270073\n",
      "iteration 3469, dc_loss: 0.018351368606090546, tv_loss: 0.017091017216444016\n",
      "iteration 3470, dc_loss: 0.01835135743021965, tv_loss: 0.017091017216444016\n",
      "iteration 3471, dc_loss: 0.01835133694112301, tv_loss: 0.017091022804379463\n",
      "iteration 3472, dc_loss: 0.018351292237639427, tv_loss: 0.017091158777475357\n",
      "iteration 3473, dc_loss: 0.018351208418607712, tv_loss: 0.017091212794184685\n",
      "iteration 3474, dc_loss: 0.018351120874285698, tv_loss: 0.01709125004708767\n",
      "iteration 3475, dc_loss: 0.018351085484027863, tv_loss: 0.0170913003385067\n",
      "iteration 3476, dc_loss: 0.018351130187511444, tv_loss: 0.01709120161831379\n",
      "iteration 3477, dc_loss: 0.018351200968027115, tv_loss: 0.017091065645217896\n",
      "iteration 3478, dc_loss: 0.01835126429796219, tv_loss: 0.01709110103547573\n",
      "iteration 3479, dc_loss: 0.018351292237639427, tv_loss: 0.01709095761179924\n",
      "iteration 3480, dc_loss: 0.018351299688220024, tv_loss: 0.017090899869799614\n",
      "iteration 3481, dc_loss: 0.018351273611187935, tv_loss: 0.01709084026515484\n",
      "iteration 3482, dc_loss: 0.018351206555962563, tv_loss: 0.017090944573283195\n",
      "iteration 3483, dc_loss: 0.01835111901164055, tv_loss: 0.017091024667024612\n",
      "iteration 3484, dc_loss: 0.018351083621382713, tv_loss: 0.01709103398025036\n",
      "iteration 3485, dc_loss: 0.018351079896092415, tv_loss: 0.017091039568185806\n",
      "iteration 3486, dc_loss: 0.018351104110479355, tv_loss: 0.017091132700443268\n",
      "iteration 3487, dc_loss: 0.01835114322602749, tv_loss: 0.01709108054637909\n",
      "iteration 3488, dc_loss: 0.01835118606686592, tv_loss: 0.017090868204832077\n",
      "iteration 3489, dc_loss: 0.018351206555962563, tv_loss: 0.01709079183638096\n",
      "iteration 3490, dc_loss: 0.018351227045059204, tv_loss: 0.017090938985347748\n",
      "iteration 3491, dc_loss: 0.018351227045059204, tv_loss: 0.017090924084186554\n",
      "iteration 3492, dc_loss: 0.01835121214389801, tv_loss: 0.017090855166316032\n",
      "iteration 3493, dc_loss: 0.018351174890995026, tv_loss: 0.01709088869392872\n",
      "iteration 3494, dc_loss: 0.018351128324866295, tv_loss: 0.017090817913413048\n",
      "iteration 3495, dc_loss: 0.01835109479725361, tv_loss: 0.017090823501348495\n",
      "iteration 3496, dc_loss: 0.018351050093770027, tv_loss: 0.017090944573283195\n",
      "iteration 3497, dc_loss: 0.01835104636847973, tv_loss: 0.01709069311618805\n",
      "iteration 3498, dc_loss: 0.018351055681705475, tv_loss: 0.017090758308768272\n",
      "iteration 3499, dc_loss: 0.018351076170802116, tv_loss: 0.0170908086001873\n",
      "iteration 3500, dc_loss: 0.018351087346673012, tv_loss: 0.01709063909947872\n",
      "iteration 3501, dc_loss: 0.0183511171489954, tv_loss: 0.017090532928705215\n",
      "iteration 3502, dc_loss: 0.018351104110479355, tv_loss: 0.01709066331386566\n",
      "iteration 3503, dc_loss: 0.01835106872022152, tv_loss: 0.017090732231736183\n",
      "iteration 3504, dc_loss: 0.0183509960770607, tv_loss: 0.017090607434511185\n",
      "iteration 3505, dc_loss: 0.018350906670093536, tv_loss: 0.017090681940317154\n",
      "iteration 3506, dc_loss: 0.018350878730416298, tv_loss: 0.017090730369091034\n",
      "iteration 3507, dc_loss: 0.018350914120674133, tv_loss: 0.017090745270252228\n",
      "iteration 3508, dc_loss: 0.018351005390286446, tv_loss: 0.017090801149606705\n",
      "iteration 3509, dc_loss: 0.01835107058286667, tv_loss: 0.017090527340769768\n",
      "iteration 3510, dc_loss: 0.018351098522543907, tv_loss: 0.01709046959877014\n",
      "iteration 3511, dc_loss: 0.018351083621382713, tv_loss: 0.017090503126382828\n",
      "iteration 3512, dc_loss: 0.018351005390286446, tv_loss: 0.017090527340769768\n",
      "iteration 3513, dc_loss: 0.018350902944803238, tv_loss: 0.01709047518670559\n",
      "iteration 3514, dc_loss: 0.018350858241319656, tv_loss: 0.017090601846575737\n",
      "iteration 3515, dc_loss: 0.018350845202803612, tv_loss: 0.01709071919322014\n",
      "iteration 3516, dc_loss: 0.018350820988416672, tv_loss: 0.017090698704123497\n",
      "iteration 3517, dc_loss: 0.01835080422461033, tv_loss: 0.017090611159801483\n",
      "iteration 3518, dc_loss: 0.018350787460803986, tv_loss: 0.017090601846575737\n",
      "iteration 3519, dc_loss: 0.018350817263126373, tv_loss: 0.017090559005737305\n",
      "iteration 3520, dc_loss: 0.018350861966609955, tv_loss: 0.01709052361547947\n",
      "iteration 3521, dc_loss: 0.018350908532738686, tv_loss: 0.017090624198317528\n",
      "iteration 3522, dc_loss: 0.01835092529654503, tv_loss: 0.017090335488319397\n",
      "iteration 3523, dc_loss: 0.018350914120674133, tv_loss: 0.017090247943997383\n",
      "iteration 3524, dc_loss: 0.018350886180996895, tv_loss: 0.01709040068089962\n",
      "iteration 3525, dc_loss: 0.018350834026932716, tv_loss: 0.0170904528349638\n",
      "iteration 3526, dc_loss: 0.01835082471370697, tv_loss: 0.01709037460386753\n",
      "iteration 3527, dc_loss: 0.018350811675190926, tv_loss: 0.0170904528349638\n",
      "iteration 3528, dc_loss: 0.018350809812545776, tv_loss: 0.017090419307351112\n",
      "iteration 3529, dc_loss: 0.01835082657635212, tv_loss: 0.01709025539457798\n",
      "iteration 3530, dc_loss: 0.018350863829255104, tv_loss: 0.01709030196070671\n",
      "iteration 3531, dc_loss: 0.018350856378674507, tv_loss: 0.01709035038948059\n",
      "iteration 3532, dc_loss: 0.018350820988416672, tv_loss: 0.01709025539457798\n",
      "iteration 3533, dc_loss: 0.018350750207901, tv_loss: 0.017090268433094025\n",
      "iteration 3534, dc_loss: 0.018350709229707718, tv_loss: 0.017090294510126114\n",
      "iteration 3535, dc_loss: 0.018350698053836823, tv_loss: 0.017090240493416786\n",
      "iteration 3536, dc_loss: 0.01835067942738533, tv_loss: 0.01709040254354477\n",
      "iteration 3537, dc_loss: 0.01835065521299839, tv_loss: 0.017090562731027603\n",
      "iteration 3538, dc_loss: 0.018350660800933838, tv_loss: 0.01709038019180298\n",
      "iteration 3539, dc_loss: 0.018350696191191673, tv_loss: 0.01709011010825634\n",
      "iteration 3540, dc_loss: 0.018350737169384956, tv_loss: 0.017090318724513054\n",
      "iteration 3541, dc_loss: 0.018350740894675255, tv_loss: 0.017090314999222755\n",
      "iteration 3542, dc_loss: 0.018350766971707344, tv_loss: 0.017090223729610443\n",
      "iteration 3543, dc_loss: 0.018350763246417046, tv_loss: 0.017090095207095146\n",
      "iteration 3544, dc_loss: 0.018350740894675255, tv_loss: 0.017090102657675743\n",
      "iteration 3545, dc_loss: 0.018350690603256226, tv_loss: 0.017090072855353355\n",
      "iteration 3546, dc_loss: 0.018350638449192047, tv_loss: 0.017090054228901863\n",
      "iteration 3547, dc_loss: 0.018350616097450256, tv_loss: 0.01709015481173992\n",
      "iteration 3548, dc_loss: 0.018350552767515182, tv_loss: 0.01709017902612686\n",
      "iteration 3549, dc_loss: 0.018350519239902496, tv_loss: 0.01709022931754589\n",
      "iteration 3550, dc_loss: 0.018350522965192795, tv_loss: 0.01709010824561119\n",
      "iteration 3551, dc_loss: 0.018350522965192795, tv_loss: 0.01709018088877201\n",
      "iteration 3552, dc_loss: 0.01835053041577339, tv_loss: 0.017090165987610817\n",
      "iteration 3553, dc_loss: 0.018350552767515182, tv_loss: 0.0170900858938694\n",
      "iteration 3554, dc_loss: 0.018350597470998764, tv_loss: 0.017090020701289177\n",
      "iteration 3555, dc_loss: 0.018350638449192047, tv_loss: 0.01708991453051567\n",
      "iteration 3556, dc_loss: 0.01835065335035324, tv_loss: 0.017089879140257835\n",
      "iteration 3557, dc_loss: 0.018350645899772644, tv_loss: 0.017089979723095894\n",
      "iteration 3558, dc_loss: 0.018350597470998764, tv_loss: 0.017089828848838806\n",
      "iteration 3559, dc_loss: 0.01835057884454727, tv_loss: 0.017089828848838806\n",
      "iteration 3560, dc_loss: 0.018350601196289062, tv_loss: 0.017090048640966415\n",
      "iteration 3561, dc_loss: 0.018350595608353615, tv_loss: 0.0170898474752903\n",
      "iteration 3562, dc_loss: 0.01835056208074093, tv_loss: 0.0170898474752903\n",
      "iteration 3563, dc_loss: 0.018350493162870407, tv_loss: 0.01709000952541828\n",
      "iteration 3564, dc_loss: 0.01835046149790287, tv_loss: 0.017090225592255592\n",
      "iteration 3565, dc_loss: 0.01835043728351593, tv_loss: 0.017090125009417534\n",
      "iteration 3566, dc_loss: 0.018350394442677498, tv_loss: 0.017090043053030968\n",
      "iteration 3567, dc_loss: 0.018350373953580856, tv_loss: 0.017089970409870148\n",
      "iteration 3568, dc_loss: 0.01835036836564541, tv_loss: 0.01708996668457985\n",
      "iteration 3569, dc_loss: 0.018350400030612946, tv_loss: 0.017090097069740295\n",
      "iteration 3570, dc_loss: 0.018350424244999886, tv_loss: 0.017089948058128357\n",
      "iteration 3571, dc_loss: 0.018350442871451378, tv_loss: 0.01708982139825821\n",
      "iteration 3572, dc_loss: 0.018350454047322273, tv_loss: 0.01708972454071045\n",
      "iteration 3573, dc_loss: 0.018350474536418915, tv_loss: 0.017089752480387688\n",
      "iteration 3574, dc_loss: 0.01835046522319317, tv_loss: 0.017089642584323883\n",
      "iteration 3575, dc_loss: 0.018350444734096527, tv_loss: 0.01708969660103321\n",
      "iteration 3576, dc_loss: 0.018350429832935333, tv_loss: 0.01708977296948433\n",
      "iteration 3577, dc_loss: 0.018350396305322647, tv_loss: 0.01708984188735485\n",
      "iteration 3578, dc_loss: 0.018350346013903618, tv_loss: 0.01708958111703396\n",
      "iteration 3579, dc_loss: 0.01835034415125847, tv_loss: 0.01708984561264515\n",
      "iteration 3580, dc_loss: 0.0183503869920969, tv_loss: 0.017089959233999252\n",
      "iteration 3581, dc_loss: 0.018350442871451378, tv_loss: 0.01708960346877575\n",
      "iteration 3582, dc_loss: 0.018350467085838318, tv_loss: 0.0170895978808403\n",
      "iteration 3583, dc_loss: 0.018350448459386826, tv_loss: 0.017089711502194405\n",
      "iteration 3584, dc_loss: 0.018350429832935333, tv_loss: 0.017089473083615303\n",
      "iteration 3585, dc_loss: 0.018350377678871155, tv_loss: 0.01708962954580784\n",
      "iteration 3586, dc_loss: 0.01835031993687153, tv_loss: 0.017089765518903732\n",
      "iteration 3587, dc_loss: 0.018350303173065186, tv_loss: 0.017089754343032837\n",
      "iteration 3588, dc_loss: 0.018350331112742424, tv_loss: 0.017089569941163063\n",
      "iteration 3589, dc_loss: 0.018350400030612946, tv_loss: 0.01708952523767948\n",
      "iteration 3590, dc_loss: 0.018350480124354362, tv_loss: 0.017089420929551125\n",
      "iteration 3591, dc_loss: 0.018350519239902496, tv_loss: 0.017089281231164932\n",
      "iteration 3592, dc_loss: 0.018350519239902496, tv_loss: 0.01708938367664814\n",
      "iteration 3593, dc_loss: 0.01835048571228981, tv_loss: 0.017089340835809708\n",
      "iteration 3594, dc_loss: 0.018350394442677498, tv_loss: 0.017089294269680977\n",
      "iteration 3595, dc_loss: 0.01835028827190399, tv_loss: 0.017089517787098885\n",
      "iteration 3596, dc_loss: 0.018350224941968918, tv_loss: 0.017089635133743286\n",
      "iteration 3597, dc_loss: 0.01835021749138832, tv_loss: 0.017089609056711197\n",
      "iteration 3598, dc_loss: 0.018350230529904366, tv_loss: 0.01708955690264702\n",
      "iteration 3599, dc_loss: 0.0183502659201622, tv_loss: 0.017089536413550377\n",
      "iteration 3600, dc_loss: 0.018350301310420036, tv_loss: 0.017089499160647392\n",
      "iteration 3601, dc_loss: 0.01835031621158123, tv_loss: 0.017089465633034706\n",
      "iteration 3602, dc_loss: 0.01835031434893608, tv_loss: 0.017089510336518288\n",
      "iteration 3603, dc_loss: 0.01835029572248459, tv_loss: 0.0170893557369709\n",
      "iteration 3604, dc_loss: 0.01835029199719429, tv_loss: 0.01708940789103508\n",
      "iteration 3605, dc_loss: 0.018350260332226753, tv_loss: 0.017089486122131348\n",
      "iteration 3606, dc_loss: 0.01835023984313011, tv_loss: 0.0170894768089056\n",
      "iteration 3607, dc_loss: 0.018350230529904366, tv_loss: 0.017089353874325752\n",
      "iteration 3608, dc_loss: 0.01835021749138832, tv_loss: 0.01708940789103508\n",
      "iteration 3609, dc_loss: 0.018350202590227127, tv_loss: 0.01708955504000187\n",
      "iteration 3610, dc_loss: 0.01835014298558235, tv_loss: 0.01708952710032463\n",
      "iteration 3611, dc_loss: 0.01835012249648571, tv_loss: 0.01708950102329254\n",
      "iteration 3612, dc_loss: 0.018350141122937202, tv_loss: 0.017089510336518288\n",
      "iteration 3613, dc_loss: 0.018350163474678993, tv_loss: 0.01708953268826008\n",
      "iteration 3614, dc_loss: 0.018350157886743546, tv_loss: 0.017089279368519783\n",
      "iteration 3615, dc_loss: 0.01835014298558235, tv_loss: 0.017089199274778366\n",
      "iteration 3616, dc_loss: 0.018350113183259964, tv_loss: 0.017089344561100006\n",
      "iteration 3617, dc_loss: 0.018350152298808098, tv_loss: 0.01708926633000374\n",
      "iteration 3618, dc_loss: 0.018350208178162575, tv_loss: 0.017089232802391052\n",
      "iteration 3619, dc_loss: 0.018350226804614067, tv_loss: 0.017089247703552246\n",
      "iteration 3620, dc_loss: 0.018350211903452873, tv_loss: 0.017089182510972023\n",
      "iteration 3621, dc_loss: 0.01835014671087265, tv_loss: 0.017089340835809708\n",
      "iteration 3622, dc_loss: 0.018350088968873024, tv_loss: 0.017089370638132095\n",
      "iteration 3623, dc_loss: 0.01835004612803459, tv_loss: 0.017089270055294037\n",
      "iteration 3624, dc_loss: 0.018350033089518547, tv_loss: 0.01708943396806717\n",
      "iteration 3625, dc_loss: 0.018350033089518547, tv_loss: 0.01708946004509926\n",
      "iteration 3626, dc_loss: 0.01835002563893795, tv_loss: 0.017089400440454483\n",
      "iteration 3627, dc_loss: 0.01835004799067974, tv_loss: 0.017089353874325752\n",
      "iteration 3628, dc_loss: 0.018350083380937576, tv_loss: 0.017089201137423515\n",
      "iteration 3629, dc_loss: 0.01835010014474392, tv_loss: 0.017089281231164932\n",
      "iteration 3630, dc_loss: 0.018350079655647278, tv_loss: 0.017089247703552246\n",
      "iteration 3631, dc_loss: 0.018350059166550636, tv_loss: 0.01708921603858471\n",
      "iteration 3632, dc_loss: 0.018350036814808846, tv_loss: 0.017089087516069412\n",
      "iteration 3633, dc_loss: 0.0183500275015831, tv_loss: 0.017089152708649635\n",
      "iteration 3634, dc_loss: 0.018350042402744293, tv_loss: 0.017088977620005608\n",
      "iteration 3635, dc_loss: 0.01835004985332489, tv_loss: 0.017088910564780235\n",
      "iteration 3636, dc_loss: 0.01835005171597004, tv_loss: 0.01708894781768322\n",
      "iteration 3637, dc_loss: 0.018350018188357353, tv_loss: 0.017088910564780235\n",
      "iteration 3638, dc_loss: 0.01834995485842228, tv_loss: 0.017088908702135086\n",
      "iteration 3639, dc_loss: 0.018349934369325638, tv_loss: 0.017089052125811577\n",
      "iteration 3640, dc_loss: 0.018349945545196533, tv_loss: 0.017089080065488815\n",
      "iteration 3641, dc_loss: 0.018349964171648026, tv_loss: 0.017088821157813072\n",
      "iteration 3642, dc_loss: 0.018349982798099518, tv_loss: 0.017088843509554863\n",
      "iteration 3643, dc_loss: 0.018349982798099518, tv_loss: 0.01708906516432762\n",
      "iteration 3644, dc_loss: 0.018350007012486458, tv_loss: 0.01708899810910225\n",
      "iteration 3645, dc_loss: 0.018350020051002502, tv_loss: 0.017088931053876877\n",
      "iteration 3646, dc_loss: 0.01835000514984131, tv_loss: 0.017088817432522774\n",
      "iteration 3647, dc_loss: 0.01834992505609989, tv_loss: 0.017088953405618668\n",
      "iteration 3648, dc_loss: 0.01834988221526146, tv_loss: 0.017089160159230232\n",
      "iteration 3649, dc_loss: 0.01834985241293907, tv_loss: 0.017089027911424637\n",
      "iteration 3650, dc_loss: 0.018349839374423027, tv_loss: 0.017088860273361206\n",
      "iteration 3651, dc_loss: 0.01834985986351967, tv_loss: 0.017088811844587326\n",
      "iteration 3652, dc_loss: 0.018349895253777504, tv_loss: 0.017089009284973145\n",
      "iteration 3653, dc_loss: 0.01834995299577713, tv_loss: 0.017088795080780983\n",
      "iteration 3654, dc_loss: 0.01834997348487377, tv_loss: 0.01708887703716755\n",
      "iteration 3655, dc_loss: 0.01834997721016407, tv_loss: 0.017088698223233223\n",
      "iteration 3656, dc_loss: 0.018349947407841682, tv_loss: 0.017088579013943672\n",
      "iteration 3657, dc_loss: 0.018349939957261086, tv_loss: 0.01708865538239479\n",
      "iteration 3658, dc_loss: 0.01834994927048683, tv_loss: 0.017088809981942177\n",
      "iteration 3659, dc_loss: 0.018349889665842056, tv_loss: 0.017088817432522774\n",
      "iteration 3660, dc_loss: 0.01834983564913273, tv_loss: 0.017088929191231728\n",
      "iteration 3661, dc_loss: 0.018349768593907356, tv_loss: 0.017088986933231354\n",
      "iteration 3662, dc_loss: 0.018349718302488327, tv_loss: 0.017088890075683594\n",
      "iteration 3663, dc_loss: 0.01834973506629467, tv_loss: 0.017088918015360832\n",
      "iteration 3664, dc_loss: 0.018349792808294296, tv_loss: 0.017089055851101875\n",
      "iteration 3665, dc_loss: 0.01834983006119728, tv_loss: 0.01708879880607128\n",
      "iteration 3666, dc_loss: 0.01834983564913273, tv_loss: 0.017088809981942177\n",
      "iteration 3667, dc_loss: 0.018349826335906982, tv_loss: 0.017088785767555237\n",
      "iteration 3668, dc_loss: 0.018349820747971535, tv_loss: 0.01708868518471718\n",
      "iteration 3669, dc_loss: 0.01834980398416519, tv_loss: 0.017088783904910088\n",
      "iteration 3670, dc_loss: 0.018349818885326385, tv_loss: 0.017088834196329117\n",
      "iteration 3671, dc_loss: 0.01834981143474579, tv_loss: 0.017088795080780983\n",
      "iteration 3672, dc_loss: 0.01834978722035885, tv_loss: 0.017088808119297028\n",
      "iteration 3673, dc_loss: 0.018349789083003998, tv_loss: 0.017088619992136955\n",
      "iteration 3674, dc_loss: 0.01834980398416519, tv_loss: 0.017088619992136955\n",
      "iteration 3675, dc_loss: 0.01834980584681034, tv_loss: 0.01708870194852352\n",
      "iteration 3676, dc_loss: 0.01834980584681034, tv_loss: 0.017088720574975014\n",
      "iteration 3677, dc_loss: 0.018349796533584595, tv_loss: 0.017088616266846657\n",
      "iteration 3678, dc_loss: 0.0183497816324234, tv_loss: 0.017088649794459343\n",
      "iteration 3679, dc_loss: 0.018349777907133102, tv_loss: 0.017088621854782104\n",
      "iteration 3680, dc_loss: 0.01834975741803646, tv_loss: 0.01708863116800785\n",
      "iteration 3681, dc_loss: 0.01834975928068161, tv_loss: 0.017088668420910835\n",
      "iteration 3682, dc_loss: 0.01834980398416519, tv_loss: 0.017088508233428\n",
      "iteration 3683, dc_loss: 0.018349887803196907, tv_loss: 0.01708844304084778\n",
      "iteration 3684, dc_loss: 0.018349898979067802, tv_loss: 0.017088348045945168\n",
      "iteration 3685, dc_loss: 0.018349891528487206, tv_loss: 0.01708848588168621\n",
      "iteration 3686, dc_loss: 0.01834983564913273, tv_loss: 0.017088396474719048\n",
      "iteration 3687, dc_loss: 0.018349789083003998, tv_loss: 0.017088482156395912\n",
      "iteration 3688, dc_loss: 0.018349770456552505, tv_loss: 0.017088450491428375\n",
      "iteration 3689, dc_loss: 0.018349742516875267, tv_loss: 0.017088400200009346\n",
      "iteration 3690, dc_loss: 0.01834965869784355, tv_loss: 0.0170886293053627\n",
      "iteration 3691, dc_loss: 0.0183495432138443, tv_loss: 0.017088769003748894\n",
      "iteration 3692, dc_loss: 0.018349463120102882, tv_loss: 0.017088858410716057\n",
      "iteration 3693, dc_loss: 0.01834944449365139, tv_loss: 0.017088977620005608\n",
      "iteration 3694, dc_loss: 0.018349485471844673, tv_loss: 0.017088674008846283\n",
      "iteration 3695, dc_loss: 0.018349539488554, tv_loss: 0.017088497057557106\n",
      "iteration 3696, dc_loss: 0.0183496605604887, tv_loss: 0.017088336870074272\n",
      "iteration 3697, dc_loss: 0.01834975928068161, tv_loss: 0.017088433727622032\n",
      "iteration 3698, dc_loss: 0.018349772319197655, tv_loss: 0.01708831824362278\n",
      "iteration 3699, dc_loss: 0.01834973879158497, tv_loss: 0.017088141292333603\n",
      "iteration 3700, dc_loss: 0.01834969036281109, tv_loss: 0.01708831824362278\n",
      "iteration 3701, dc_loss: 0.018349647521972656, tv_loss: 0.01708846166729927\n",
      "iteration 3702, dc_loss: 0.01834961026906967, tv_loss: 0.017088502645492554\n",
      "iteration 3703, dc_loss: 0.01834958791732788, tv_loss: 0.017088456079363823\n",
      "iteration 3704, dc_loss: 0.01834956742823124, tv_loss: 0.01708848774433136\n",
      "iteration 3705, dc_loss: 0.018349526450037956, tv_loss: 0.017088554799556732\n",
      "iteration 3706, dc_loss: 0.018349481746554375, tv_loss: 0.01708853617310524\n",
      "iteration 3707, dc_loss: 0.018349433317780495, tv_loss: 0.017088361084461212\n",
      "iteration 3708, dc_loss: 0.01834944821894169, tv_loss: 0.017088530585169792\n",
      "iteration 3709, dc_loss: 0.018349505960941315, tv_loss: 0.01708846352994442\n",
      "iteration 3710, dc_loss: 0.01834956370294094, tv_loss: 0.017088305205106735\n",
      "iteration 3711, dc_loss: 0.018349599093198776, tv_loss: 0.017088353633880615\n",
      "iteration 3712, dc_loss: 0.01834956370294094, tv_loss: 0.01708832196891308\n",
      "iteration 3713, dc_loss: 0.018349539488554, tv_loss: 0.01708834618330002\n",
      "iteration 3714, dc_loss: 0.018349500373005867, tv_loss: 0.017088526859879494\n",
      "iteration 3715, dc_loss: 0.018349453806877136, tv_loss: 0.017088504508137703\n",
      "iteration 3716, dc_loss: 0.018349463120102882, tv_loss: 0.017088482156395912\n",
      "iteration 3717, dc_loss: 0.01834949292242527, tv_loss: 0.017088457942008972\n",
      "iteration 3718, dc_loss: 0.018349479883909225, tv_loss: 0.017088476568460464\n",
      "iteration 3719, dc_loss: 0.018349461257457733, tv_loss: 0.017088497057557106\n",
      "iteration 3720, dc_loss: 0.018349479883909225, tv_loss: 0.017088428139686584\n",
      "iteration 3721, dc_loss: 0.018349504098296165, tv_loss: 0.017088329419493675\n",
      "iteration 3722, dc_loss: 0.018349483609199524, tv_loss: 0.017088372260332108\n",
      "iteration 3723, dc_loss: 0.018349450081586838, tv_loss: 0.0170883871614933\n",
      "iteration 3724, dc_loss: 0.018349463120102882, tv_loss: 0.017088336870074272\n",
      "iteration 3725, dc_loss: 0.018349463120102882, tv_loss: 0.01708829216659069\n",
      "iteration 3726, dc_loss: 0.018349463120102882, tv_loss: 0.017088202759623528\n",
      "iteration 3727, dc_loss: 0.01834947057068348, tv_loss: 0.017088349908590317\n",
      "iteration 3728, dc_loss: 0.018349451944231987, tv_loss: 0.01708832010626793\n",
      "iteration 3729, dc_loss: 0.018349438905715942, tv_loss: 0.017088046297430992\n",
      "iteration 3730, dc_loss: 0.018349410966038704, tv_loss: 0.01708824746310711\n",
      "iteration 3731, dc_loss: 0.01834937185049057, tv_loss: 0.017088333144783974\n",
      "iteration 3732, dc_loss: 0.018349355086684227, tv_loss: 0.0170882660895586\n",
      "iteration 3733, dc_loss: 0.01834934949874878, tv_loss: 0.017088141292333603\n",
      "iteration 3734, dc_loss: 0.018349356949329376, tv_loss: 0.017088264226913452\n",
      "iteration 3735, dc_loss: 0.01834936812520027, tv_loss: 0.01708834059536457\n",
      "iteration 3736, dc_loss: 0.018349386751651764, tv_loss: 0.01708824373781681\n",
      "iteration 3737, dc_loss: 0.018349356949329376, tv_loss: 0.017088135704398155\n",
      "iteration 3738, dc_loss: 0.01834931969642639, tv_loss: 0.017088213935494423\n",
      "iteration 3739, dc_loss: 0.018349312245845795, tv_loss: 0.017088232561945915\n",
      "iteration 3740, dc_loss: 0.018349289894104004, tv_loss: 0.017088167369365692\n",
      "iteration 3741, dc_loss: 0.01834924891591072, tv_loss: 0.0170881524682045\n",
      "iteration 3742, dc_loss: 0.01834925450384617, tv_loss: 0.017088040709495544\n",
      "iteration 3743, dc_loss: 0.0183493010699749, tv_loss: 0.01708807982504368\n",
      "iteration 3744, dc_loss: 0.018349334597587585, tv_loss: 0.0170881450176239\n",
      "iteration 3745, dc_loss: 0.018349366262555122, tv_loss: 0.017087973654270172\n",
      "iteration 3746, dc_loss: 0.0183494221419096, tv_loss: 0.01708776131272316\n",
      "iteration 3747, dc_loss: 0.01834944076836109, tv_loss: 0.01708783209323883\n",
      "iteration 3748, dc_loss: 0.018349409103393555, tv_loss: 0.017087969928979874\n",
      "iteration 3749, dc_loss: 0.01834939420223236, tv_loss: 0.017087986692786217\n",
      "iteration 3750, dc_loss: 0.018349358811974525, tv_loss: 0.017087796702980995\n",
      "iteration 3751, dc_loss: 0.01834930293262005, tv_loss: 0.017088085412979126\n",
      "iteration 3752, dc_loss: 0.01834925077855587, tv_loss: 0.017088187858462334\n",
      "iteration 3753, dc_loss: 0.018349215388298035, tv_loss: 0.017088187858462334\n",
      "iteration 3754, dc_loss: 0.01834917813539505, tv_loss: 0.01708800345659256\n",
      "iteration 3755, dc_loss: 0.01834920048713684, tv_loss: 0.017088046297430992\n",
      "iteration 3756, dc_loss: 0.018349258229136467, tv_loss: 0.017087996006011963\n",
      "iteration 3757, dc_loss: 0.018349329009652138, tv_loss: 0.01708795689046383\n",
      "iteration 3758, dc_loss: 0.01834934949874878, tv_loss: 0.017087828367948532\n",
      "iteration 3759, dc_loss: 0.018349340185523033, tv_loss: 0.017087716609239578\n",
      "iteration 3760, dc_loss: 0.01834931969642639, tv_loss: 0.017087791115045547\n",
      "iteration 3761, dc_loss: 0.018349243327975273, tv_loss: 0.017087949439883232\n",
      "iteration 3762, dc_loss: 0.018349172547459602, tv_loss: 0.017088033258914948\n",
      "iteration 3763, dc_loss: 0.01834913156926632, tv_loss: 0.017087921500205994\n",
      "iteration 3764, dc_loss: 0.01834910735487938, tv_loss: 0.017087867483496666\n",
      "iteration 3765, dc_loss: 0.01834910362958908, tv_loss: 0.017088033258914948\n",
      "iteration 3766, dc_loss: 0.018349122256040573, tv_loss: 0.017087901011109352\n",
      "iteration 3767, dc_loss: 0.018349146470427513, tv_loss: 0.017087843269109726\n",
      "iteration 3768, dc_loss: 0.0183491799980402, tv_loss: 0.01708778738975525\n",
      "iteration 3769, dc_loss: 0.018349191173911095, tv_loss: 0.017087722197175026\n",
      "iteration 3770, dc_loss: 0.018349172547459602, tv_loss: 0.017087658867239952\n",
      "iteration 3771, dc_loss: 0.01834915019571781, tv_loss: 0.017087768763303757\n",
      "iteration 3772, dc_loss: 0.018349118530750275, tv_loss: 0.017087945714592934\n",
      "iteration 3773, dc_loss: 0.01834907755255699, tv_loss: 0.01708788238465786\n",
      "iteration 3774, dc_loss: 0.018349042162299156, tv_loss: 0.01708785630762577\n",
      "iteration 3775, dc_loss: 0.01834907941520214, tv_loss: 0.01708787865936756\n",
      "iteration 3776, dc_loss: 0.018349148333072662, tv_loss: 0.017087871208786964\n",
      "iteration 3777, dc_loss: 0.018349194899201393, tv_loss: 0.01708783395588398\n",
      "iteration 3778, dc_loss: 0.01834919862449169, tv_loss: 0.017087731510400772\n",
      "iteration 3779, dc_loss: 0.018349161371588707, tv_loss: 0.017087768763303757\n",
      "iteration 3780, dc_loss: 0.01834912598133087, tv_loss: 0.017087940126657486\n",
      "iteration 3781, dc_loss: 0.018349124118685722, tv_loss: 0.017087820917367935\n",
      "iteration 3782, dc_loss: 0.01834910921752453, tv_loss: 0.01708786003291607\n",
      "iteration 3783, dc_loss: 0.018349122256040573, tv_loss: 0.01708788424730301\n",
      "iteration 3784, dc_loss: 0.01834912784397602, tv_loss: 0.017087692394852638\n",
      "iteration 3785, dc_loss: 0.01834910921752453, tv_loss: 0.017087746411561966\n",
      "iteration 3786, dc_loss: 0.018349135294556618, tv_loss: 0.017087804153561592\n",
      "iteration 3787, dc_loss: 0.01834915205836296, tv_loss: 0.017087705433368683\n",
      "iteration 3788, dc_loss: 0.018349124118685722, tv_loss: 0.017087722197175026\n",
      "iteration 3789, dc_loss: 0.018349071964621544, tv_loss: 0.01708778738975525\n",
      "iteration 3790, dc_loss: 0.01834902912378311, tv_loss: 0.017087846994400024\n",
      "iteration 3791, dc_loss: 0.018349038437008858, tv_loss: 0.017087841406464577\n",
      "iteration 3792, dc_loss: 0.01834901235997677, tv_loss: 0.01708780601620674\n",
      "iteration 3793, dc_loss: 0.01834901049733162, tv_loss: 0.01708770915865898\n",
      "iteration 3794, dc_loss: 0.018349014222621918, tv_loss: 0.01708771102130413\n",
      "iteration 3795, dc_loss: 0.018349019810557365, tv_loss: 0.017087792977690697\n",
      "iteration 3796, dc_loss: 0.01834898814558983, tv_loss: 0.01708776131272316\n",
      "iteration 3797, dc_loss: 0.018348941579461098, tv_loss: 0.017087722197175026\n",
      "iteration 3798, dc_loss: 0.018348919227719307, tv_loss: 0.0170876607298851\n",
      "iteration 3799, dc_loss: 0.018348926678299904, tv_loss: 0.017087779939174652\n",
      "iteration 3800, dc_loss: 0.01834896206855774, tv_loss: 0.017087705433368683\n",
      "iteration 3801, dc_loss: 0.018348993733525276, tv_loss: 0.017087679356336594\n",
      "iteration 3802, dc_loss: 0.01834903657436371, tv_loss: 0.017087573185563087\n",
      "iteration 3803, dc_loss: 0.018349064514040947, tv_loss: 0.017087457701563835\n",
      "iteration 3804, dc_loss: 0.018349073827266693, tv_loss: 0.01708735153079033\n",
      "iteration 3805, dc_loss: 0.018349120393395424, tv_loss: 0.01708746701478958\n",
      "iteration 3806, dc_loss: 0.018349122256040573, tv_loss: 0.01708742417395115\n",
      "iteration 3807, dc_loss: 0.01834907755255699, tv_loss: 0.017087556421756744\n",
      "iteration 3808, dc_loss: 0.018349027261137962, tv_loss: 0.01708751730620861\n",
      "iteration 3809, dc_loss: 0.018348973244428635, tv_loss: 0.017087487503886223\n",
      "iteration 3810, dc_loss: 0.018348872661590576, tv_loss: 0.01708785817027092\n",
      "iteration 3811, dc_loss: 0.018348809331655502, tv_loss: 0.017087871208786964\n",
      "iteration 3812, dc_loss: 0.018348779529333115, tv_loss: 0.017087796702980995\n",
      "iteration 3813, dc_loss: 0.01834878884255886, tv_loss: 0.017087822780013084\n",
      "iteration 3814, dc_loss: 0.018348827958106995, tv_loss: 0.017087768763303757\n",
      "iteration 3815, dc_loss: 0.01834889128804207, tv_loss: 0.017087597399950027\n",
      "iteration 3816, dc_loss: 0.018348976969718933, tv_loss: 0.017087480053305626\n",
      "iteration 3817, dc_loss: 0.01834905333817005, tv_loss: 0.017087379470467567\n",
      "iteration 3818, dc_loss: 0.01834908314049244, tv_loss: 0.017087304964661598\n",
      "iteration 3819, dc_loss: 0.018349098041653633, tv_loss: 0.017087390646338463\n",
      "iteration 3820, dc_loss: 0.018349064514040947, tv_loss: 0.017087312415242195\n",
      "iteration 3821, dc_loss: 0.01834898069500923, tv_loss: 0.017087385058403015\n",
      "iteration 3822, dc_loss: 0.01834889128804207, tv_loss: 0.017087562009692192\n",
      "iteration 3823, dc_loss: 0.018348796293139458, tv_loss: 0.01708770915865898\n",
      "iteration 3824, dc_loss: 0.018348734825849533, tv_loss: 0.017087671905755997\n",
      "iteration 3825, dc_loss: 0.01834876649081707, tv_loss: 0.01708768866956234\n",
      "iteration 3826, dc_loss: 0.01834881864488125, tv_loss: 0.017087586224079132\n",
      "iteration 3827, dc_loss: 0.018348872661590576, tv_loss: 0.017087487503886223\n",
      "iteration 3828, dc_loss: 0.018348874524235725, tv_loss: 0.01708747074007988\n",
      "iteration 3829, dc_loss: 0.018348878249526024, tv_loss: 0.01708758994936943\n",
      "iteration 3830, dc_loss: 0.01834888383746147, tv_loss: 0.017087450250983238\n",
      "iteration 3831, dc_loss: 0.018348941579461098, tv_loss: 0.017087455838918686\n",
      "iteration 3832, dc_loss: 0.01834898069500923, tv_loss: 0.017087487503886223\n",
      "iteration 3833, dc_loss: 0.018348990008234978, tv_loss: 0.017087290063500404\n",
      "iteration 3834, dc_loss: 0.018348952755331993, tv_loss: 0.017087267711758614\n",
      "iteration 3835, dc_loss: 0.018348926678299904, tv_loss: 0.01708744652569294\n",
      "iteration 3836, dc_loss: 0.01834886707365513, tv_loss: 0.01708742044866085\n",
      "iteration 3837, dc_loss: 0.018348850309848785, tv_loss: 0.01708732172846794\n",
      "iteration 3838, dc_loss: 0.018348850309848785, tv_loss: 0.01708737388253212\n",
      "iteration 3839, dc_loss: 0.018348854035139084, tv_loss: 0.017087381333112717\n",
      "iteration 3840, dc_loss: 0.018348829820752144, tv_loss: 0.017087498679757118\n",
      "iteration 3841, dc_loss: 0.018348773941397667, tv_loss: 0.017087388783693314\n",
      "iteration 3842, dc_loss: 0.01834869757294655, tv_loss: 0.017087457701563835\n",
      "iteration 3843, dc_loss: 0.018348658457398415, tv_loss: 0.01708766259253025\n",
      "iteration 3844, dc_loss: 0.01834867335855961, tv_loss: 0.017087696120142937\n",
      "iteration 3845, dc_loss: 0.018348701298236847, tv_loss: 0.017087429761886597\n",
      "iteration 3846, dc_loss: 0.01834874413907528, tv_loss: 0.017087440937757492\n",
      "iteration 3847, dc_loss: 0.01834878884255886, tv_loss: 0.01708749309182167\n",
      "iteration 3848, dc_loss: 0.018348820507526398, tv_loss: 0.017087610438466072\n",
      "iteration 3849, dc_loss: 0.01834879443049431, tv_loss: 0.017087433487176895\n",
      "iteration 3850, dc_loss: 0.018348781391978264, tv_loss: 0.017087306827306747\n",
      "iteration 3851, dc_loss: 0.018348781391978264, tv_loss: 0.017087433487176895\n",
      "iteration 3852, dc_loss: 0.01834879443049431, tv_loss: 0.017087340354919434\n",
      "iteration 3853, dc_loss: 0.018348854035139084, tv_loss: 0.017087284475564957\n",
      "iteration 3854, dc_loss: 0.01834888383746147, tv_loss: 0.017087318003177643\n",
      "iteration 3855, dc_loss: 0.018348846584558487, tv_loss: 0.017087282612919807\n",
      "iteration 3856, dc_loss: 0.018348785117268562, tv_loss: 0.017087310552597046\n",
      "iteration 3857, dc_loss: 0.01834874227643013, tv_loss: 0.017087390646338463\n",
      "iteration 3858, dc_loss: 0.018348703160881996, tv_loss: 0.01708727516233921\n",
      "iteration 3859, dc_loss: 0.018348688259720802, tv_loss: 0.017087368294596672\n",
      "iteration 3860, dc_loss: 0.01834871806204319, tv_loss: 0.017087427899241447\n",
      "iteration 3861, dc_loss: 0.018348710611462593, tv_loss: 0.017087392508983612\n",
      "iteration 3862, dc_loss: 0.018348703160881996, tv_loss: 0.01708720251917839\n",
      "iteration 3863, dc_loss: 0.01834867335855961, tv_loss: 0.017087280750274658\n",
      "iteration 3864, dc_loss: 0.018348662182688713, tv_loss: 0.017087405547499657\n",
      "iteration 3865, dc_loss: 0.018348675221204758, tv_loss: 0.017087174579501152\n",
      "iteration 3866, dc_loss: 0.018348712474107742, tv_loss: 0.017087042331695557\n",
      "iteration 3867, dc_loss: 0.018348710611462593, tv_loss: 0.017087148502469063\n",
      "iteration 3868, dc_loss: 0.018348699435591698, tv_loss: 0.017087198793888092\n",
      "iteration 3869, dc_loss: 0.01834866777062416, tv_loss: 0.017087232321500778\n",
      "iteration 3870, dc_loss: 0.018348662182688713, tv_loss: 0.017087232321500778\n",
      "iteration 3871, dc_loss: 0.018348654732108116, tv_loss: 0.017087124288082123\n",
      "iteration 3872, dc_loss: 0.01834866590797901, tv_loss: 0.017087291926145554\n",
      "iteration 3873, dc_loss: 0.01834867149591446, tv_loss: 0.017087137326598167\n",
      "iteration 3874, dc_loss: 0.018348682671785355, tv_loss: 0.017087208107113838\n",
      "iteration 3875, dc_loss: 0.018348688259720802, tv_loss: 0.017087239772081375\n",
      "iteration 3876, dc_loss: 0.018348658457398415, tv_loss: 0.017087217420339584\n",
      "iteration 3877, dc_loss: 0.01834859885275364, tv_loss: 0.01708725467324257\n",
      "iteration 3878, dc_loss: 0.018348556011915207, tv_loss: 0.017087245360016823\n",
      "iteration 3879, dc_loss: 0.018348554149270058, tv_loss: 0.01708715222775936\n",
      "iteration 3880, dc_loss: 0.018348578363656998, tv_loss: 0.017087168991565704\n",
      "iteration 3881, dc_loss: 0.018348602578043938, tv_loss: 0.017087088897824287\n",
      "iteration 3882, dc_loss: 0.018348634243011475, tv_loss: 0.017086930572986603\n",
      "iteration 3883, dc_loss: 0.01834864728152752, tv_loss: 0.01708696037530899\n",
      "iteration 3884, dc_loss: 0.018348660320043564, tv_loss: 0.017086990177631378\n",
      "iteration 3885, dc_loss: 0.01834862492978573, tv_loss: 0.017087040469050407\n",
      "iteration 3886, dc_loss: 0.01834857277572155, tv_loss: 0.01708701252937317\n",
      "iteration 3887, dc_loss: 0.018348533660173416, tv_loss: 0.017087046056985855\n",
      "iteration 3888, dc_loss: 0.018348505720496178, tv_loss: 0.017086990177631378\n",
      "iteration 3889, dc_loss: 0.018348535522818565, tv_loss: 0.01708700880408287\n",
      "iteration 3890, dc_loss: 0.018348582088947296, tv_loss: 0.017086824402213097\n",
      "iteration 3891, dc_loss: 0.018348637968301773, tv_loss: 0.017086762934923172\n",
      "iteration 3892, dc_loss: 0.01834869384765625, tv_loss: 0.01708674244582653\n",
      "iteration 3893, dc_loss: 0.01834869012236595, tv_loss: 0.017086869105696678\n",
      "iteration 3894, dc_loss: 0.018348660320043564, tv_loss: 0.01708683744072914\n",
      "iteration 3895, dc_loss: 0.01834859326481819, tv_loss: 0.017086835578083992\n",
      "iteration 3896, dc_loss: 0.018348541110754013, tv_loss: 0.017086787149310112\n",
      "iteration 3897, dc_loss: 0.01834849640727043, tv_loss: 0.017087005078792572\n",
      "iteration 3898, dc_loss: 0.018348485231399536, tv_loss: 0.01708700694143772\n",
      "iteration 3899, dc_loss: 0.01834845170378685, tv_loss: 0.01708696223795414\n",
      "iteration 3900, dc_loss: 0.018348461017012596, tv_loss: 0.01708703674376011\n",
      "iteration 3901, dc_loss: 0.018348531797528267, tv_loss: 0.017086951062083244\n",
      "iteration 3902, dc_loss: 0.0183485709130764, tv_loss: 0.01708688773214817\n",
      "iteration 3903, dc_loss: 0.01834857650101185, tv_loss: 0.017087027430534363\n",
      "iteration 3904, dc_loss: 0.01834857277572155, tv_loss: 0.017086857929825783\n",
      "iteration 3905, dc_loss: 0.01834857277572155, tv_loss: 0.01708688773214817\n",
      "iteration 3906, dc_loss: 0.01834854669868946, tv_loss: 0.017087111249566078\n",
      "iteration 3907, dc_loss: 0.01834847591817379, tv_loss: 0.01708715781569481\n",
      "iteration 3908, dc_loss: 0.018348410725593567, tv_loss: 0.017087114974856377\n",
      "iteration 3909, dc_loss: 0.01834840513765812, tv_loss: 0.0170871801674366\n",
      "iteration 3910, dc_loss: 0.018348421901464462, tv_loss: 0.017087118700146675\n",
      "iteration 3911, dc_loss: 0.01834844797849655, tv_loss: 0.01708701066672802\n",
      "iteration 3912, dc_loss: 0.018348483368754387, tv_loss: 0.01708698831498623\n",
      "iteration 3913, dc_loss: 0.01834852434694767, tv_loss: 0.017086997628211975\n",
      "iteration 3914, dc_loss: 0.01834855042397976, tv_loss: 0.017086992040276527\n",
      "iteration 3915, dc_loss: 0.018348531797528267, tv_loss: 0.017086906358599663\n",
      "iteration 3916, dc_loss: 0.018348481506109238, tv_loss: 0.017086999490857124\n",
      "iteration 3917, dc_loss: 0.018348457291722298, tv_loss: 0.01708691567182541\n",
      "iteration 3918, dc_loss: 0.018348442390561104, tv_loss: 0.017086951062083244\n",
      "iteration 3919, dc_loss: 0.01834842748939991, tv_loss: 0.01708712987601757\n",
      "iteration 3920, dc_loss: 0.01834842562675476, tv_loss: 0.017086969688534737\n",
      "iteration 3921, dc_loss: 0.0183484498411417, tv_loss: 0.017086969688534737\n",
      "iteration 3922, dc_loss: 0.01834849640727043, tv_loss: 0.017086820676922798\n",
      "iteration 3923, dc_loss: 0.01834854669868946, tv_loss: 0.017086809501051903\n",
      "iteration 3924, dc_loss: 0.018348578363656998, tv_loss: 0.01708681508898735\n",
      "iteration 3925, dc_loss: 0.01834862492978573, tv_loss: 0.017086610198020935\n",
      "iteration 3926, dc_loss: 0.018348656594753265, tv_loss: 0.017086610198020935\n",
      "iteration 3927, dc_loss: 0.01834859512746334, tv_loss: 0.017086734995245934\n",
      "iteration 3928, dc_loss: 0.01834847405552864, tv_loss: 0.017086751759052277\n",
      "iteration 3929, dc_loss: 0.018348366022109985, tv_loss: 0.017086971551179886\n",
      "iteration 3930, dc_loss: 0.018348325043916702, tv_loss: 0.017086993902921677\n",
      "iteration 3931, dc_loss: 0.018348360434174538, tv_loss: 0.01708686724305153\n",
      "iteration 3932, dc_loss: 0.01834840327501297, tv_loss: 0.01708684302866459\n",
      "iteration 3933, dc_loss: 0.01834842376410961, tv_loss: 0.017086701467633247\n",
      "iteration 3934, dc_loss: 0.018348393961787224, tv_loss: 0.01708674617111683\n",
      "iteration 3935, dc_loss: 0.01834833063185215, tv_loss: 0.01708688773214817\n",
      "iteration 3936, dc_loss: 0.01834828406572342, tv_loss: 0.017086876556277275\n",
      "iteration 3937, dc_loss: 0.018348300829529762, tv_loss: 0.01708696410059929\n",
      "iteration 3938, dc_loss: 0.018348338082432747, tv_loss: 0.017086701467633247\n",
      "iteration 3939, dc_loss: 0.01834835857152939, tv_loss: 0.017086632549762726\n",
      "iteration 3940, dc_loss: 0.018348360434174538, tv_loss: 0.01708679087460041\n",
      "iteration 3941, dc_loss: 0.01834832690656185, tv_loss: 0.017086762934923172\n",
      "iteration 3942, dc_loss: 0.01834825985133648, tv_loss: 0.01708664558827877\n",
      "iteration 3943, dc_loss: 0.018348217010498047, tv_loss: 0.01708693616092205\n",
      "iteration 3944, dc_loss: 0.018348226323723793, tv_loss: 0.017086785286664963\n",
      "iteration 3945, dc_loss: 0.018348239362239838, tv_loss: 0.017086610198020935\n",
      "iteration 3946, dc_loss: 0.018348274752497673, tv_loss: 0.017086591571569443\n",
      "iteration 3947, dc_loss: 0.018348325043916702, tv_loss: 0.01708652265369892\n",
      "iteration 3948, dc_loss: 0.018348323181271553, tv_loss: 0.01708647981286049\n",
      "iteration 3949, dc_loss: 0.018348313868045807, tv_loss: 0.01708661951124668\n",
      "iteration 3950, dc_loss: 0.01834830455482006, tv_loss: 0.017086487263441086\n",
      "iteration 3951, dc_loss: 0.018348298966884613, tv_loss: 0.017086492851376534\n",
      "iteration 3952, dc_loss: 0.018348287791013718, tv_loss: 0.01708667352795601\n",
      "iteration 3953, dc_loss: 0.01834831014275551, tv_loss: 0.017086641862988472\n",
      "iteration 3954, dc_loss: 0.018348317593336105, tv_loss: 0.017086362466216087\n",
      "iteration 3955, dc_loss: 0.01834835857152939, tv_loss: 0.0170862078666687\n",
      "iteration 3956, dc_loss: 0.018348386511206627, tv_loss: 0.017086373642086983\n",
      "iteration 3957, dc_loss: 0.01834842376410961, tv_loss: 0.017086541280150414\n",
      "iteration 3958, dc_loss: 0.01834840327501297, tv_loss: 0.01708633452653885\n",
      "iteration 3959, dc_loss: 0.018348323181271553, tv_loss: 0.017086394131183624\n",
      "iteration 3960, dc_loss: 0.01834825985133648, tv_loss: 0.017086712643504143\n",
      "iteration 3961, dc_loss: 0.018348241224884987, tv_loss: 0.017086725682020187\n",
      "iteration 3962, dc_loss: 0.018348241224884987, tv_loss: 0.017086578533053398\n",
      "iteration 3963, dc_loss: 0.018348276615142822, tv_loss: 0.01708669774234295\n",
      "iteration 3964, dc_loss: 0.018348321318626404, tv_loss: 0.017086682841181755\n",
      "iteration 3965, dc_loss: 0.01834835484623909, tv_loss: 0.017086606472730637\n",
      "iteration 3966, dc_loss: 0.018348345533013344, tv_loss: 0.017086496576666832\n",
      "iteration 3967, dc_loss: 0.018348319455981255, tv_loss: 0.01708642579615116\n",
      "iteration 3968, dc_loss: 0.018348312005400658, tv_loss: 0.017086485400795937\n",
      "iteration 3969, dc_loss: 0.018348293378949165, tv_loss: 0.01708647795021534\n",
      "iteration 3970, dc_loss: 0.018348269164562225, tv_loss: 0.017086505889892578\n",
      "iteration 3971, dc_loss: 0.018348244950175285, tv_loss: 0.017086535692214966\n",
      "iteration 3972, dc_loss: 0.018348200246691704, tv_loss: 0.017086442559957504\n",
      "iteration 3973, dc_loss: 0.01834816113114357, tv_loss: 0.017086613923311234\n",
      "iteration 3974, dc_loss: 0.018348168581724167, tv_loss: 0.01708666794002056\n",
      "iteration 3975, dc_loss: 0.018348192796111107, tv_loss: 0.01708654873073101\n",
      "iteration 3976, dc_loss: 0.01834820955991745, tv_loss: 0.017086489126086235\n",
      "iteration 3977, dc_loss: 0.018348198384046555, tv_loss: 0.01708657667040825\n",
      "iteration 3978, dc_loss: 0.01834823004901409, tv_loss: 0.017086606472730637\n",
      "iteration 3979, dc_loss: 0.018348287791013718, tv_loss: 0.01708655059337616\n",
      "iteration 3980, dc_loss: 0.0183483324944973, tv_loss: 0.0170864537358284\n",
      "iteration 3981, dc_loss: 0.018348345533013344, tv_loss: 0.017086399719119072\n",
      "iteration 3982, dc_loss: 0.018348321318626404, tv_loss: 0.01708640344440937\n",
      "iteration 3983, dc_loss: 0.018348274752497673, tv_loss: 0.017086569219827652\n",
      "iteration 3984, dc_loss: 0.01834818720817566, tv_loss: 0.01708647608757019\n",
      "iteration 3985, dc_loss: 0.01834813877940178, tv_loss: 0.017086509615182877\n",
      "iteration 3986, dc_loss: 0.018348103389143944, tv_loss: 0.01708669401705265\n",
      "iteration 3987, dc_loss: 0.018348082900047302, tv_loss: 0.017086641862988472\n",
      "iteration 3988, dc_loss: 0.018348105251789093, tv_loss: 0.017086578533053398\n",
      "iteration 3989, dc_loss: 0.018348131328821182, tv_loss: 0.017086535692214966\n",
      "iteration 3990, dc_loss: 0.018348148092627525, tv_loss: 0.017086569219827652\n",
      "iteration 3991, dc_loss: 0.018348148092627525, tv_loss: 0.017086373642086983\n",
      "iteration 3992, dc_loss: 0.01834813505411148, tv_loss: 0.01708652451634407\n",
      "iteration 3993, dc_loss: 0.018348146229982376, tv_loss: 0.017086612060666084\n",
      "iteration 3994, dc_loss: 0.01834813319146633, tv_loss: 0.017086518928408623\n",
      "iteration 3995, dc_loss: 0.01834810897707939, tv_loss: 0.017086556181311607\n",
      "iteration 3996, dc_loss: 0.018348071724176407, tv_loss: 0.01708664558827877\n",
      "iteration 3997, dc_loss: 0.018348047509789467, tv_loss: 0.017086589708924294\n",
      "iteration 3998, dc_loss: 0.01834808848798275, tv_loss: 0.017086582258343697\n",
      "iteration 3999, dc_loss: 0.018348146229982376, tv_loss: 0.017086351290345192\n",
      "iteration 4000, dc_loss: 0.018348217010498047, tv_loss: 0.01708623766899109\n",
      "PSNR Value mt1: 35.77876077524474\n",
      "SSIM Value mt1: 0.7715722252164936\n"
     ]
    }
   ],
   "source": [
    "for dataset, ar in zip(datasets_list, [1, 2, 4, 8]):\n",
    "    scheduler = lambda t: 0.8 ** (t // 400) * 1e-1\n",
    "    optimizer = torchopt.adam(lr=0.1)\n",
    "    kspace, (mean, std), masked_kspace, mask, csm = dataset[15]\n",
    "    field = Grid((640, 368), mean, std)\n",
    "    params, image_list_ADAM = reconstruct(\n",
    "        field,\n",
    "        torch.rand(1, 2),\n",
    "        masked_kspace,\n",
    "        csm,\n",
    "        mask,\n",
    "        alpha=0.005,\n",
    "        optimizer=optimizer,\n",
    "        iterations=4000,\n",
    "        device=torch.device(\"cuda\"),\n",
    "    )\n",
    "    psnr, ssim = Evaluate_MT1(image_gt, image_list_ADAM[-1])\n",
    "    ADAM_list.append((psnr, ssim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[6.0409e-04, 7.4484e-05]]]) tensor([[[1.2870, 1.2171]]])\n",
      "torch.Size([1, 1, 2]) torch.Size([1, 1, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| params.keys(): dict_keys(['net.0.linear.weight', 'net.0.linear.bias', 'net.1.linear.weight', 'net.1.linear.bias', 'net.2.linear.weight', 'net.2.linear.bias', 'net.3.linear.weight', 'net.3.linear.bias', 'net.4.linear.weight', 'net.4.linear.bias', 'net.5.weight', 'net.5.bias'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "iteration 1, dc_loss: 3.18599009513855, tv_loss: 0.0005901351105421782\n",
      "iteration 2, dc_loss: 3.0555496215820312, tv_loss: 0.003078893991187215\n",
      "iteration 3, dc_loss: 2.9740359783172607, tv_loss: 0.005541853606700897\n",
      "iteration 4, dc_loss: 2.9173800945281982, tv_loss: 0.007393249310553074\n",
      "iteration 5, dc_loss: 2.8812050819396973, tv_loss: 0.00861265230923891\n",
      "iteration 6, dc_loss: 2.8570971488952637, tv_loss: 0.009377313777804375\n",
      "iteration 7, dc_loss: 2.8397624492645264, tv_loss: 0.00983405765146017\n",
      "iteration 8, dc_loss: 2.8248579502105713, tv_loss: 0.010136129334568977\n",
      "iteration 9, dc_loss: 2.8094234466552734, tv_loss: 0.010419312864542007\n",
      "iteration 10, dc_loss: 2.793241500854492, tv_loss: 0.010728083550930023\n",
      "iteration 11, dc_loss: 2.7774014472961426, tv_loss: 0.011017663404345512\n",
      "iteration 12, dc_loss: 2.7619478702545166, tv_loss: 0.011285373941063881\n",
      "iteration 13, dc_loss: 2.7462716102600098, tv_loss: 0.01155644841492176\n",
      "iteration 14, dc_loss: 2.7313075065612793, tv_loss: 0.011812151409685612\n",
      "iteration 15, dc_loss: 2.7171573638916016, tv_loss: 0.012041540816426277\n",
      "iteration 16, dc_loss: 2.703204393386841, tv_loss: 0.012238908559083939\n",
      "iteration 17, dc_loss: 2.689452648162842, tv_loss: 0.012408394366502762\n",
      "iteration 18, dc_loss: 2.675945281982422, tv_loss: 0.012566561810672283\n",
      "iteration 19, dc_loss: 2.662456750869751, tv_loss: 0.01271588634699583\n",
      "iteration 20, dc_loss: 2.648873805999756, tv_loss: 0.012841224670410156\n",
      "iteration 21, dc_loss: 2.635471820831299, tv_loss: 0.012928929179906845\n",
      "iteration 22, dc_loss: 2.6223788261413574, tv_loss: 0.012935112230479717\n",
      "iteration 23, dc_loss: 2.6094985008239746, tv_loss: 0.012821940705180168\n",
      "iteration 24, dc_loss: 2.596390962600708, tv_loss: 0.012656895443797112\n",
      "iteration 25, dc_loss: 2.583242654800415, tv_loss: 0.012455599382519722\n",
      "iteration 26, dc_loss: 2.5702950954437256, tv_loss: 0.01213940791785717\n",
      "iteration 27, dc_loss: 2.5572831630706787, tv_loss: 0.011748007498681545\n",
      "iteration 28, dc_loss: 2.544499397277832, tv_loss: 0.011288234032690525\n",
      "iteration 29, dc_loss: 2.531848192214966, tv_loss: 0.010820628143846989\n",
      "iteration 30, dc_loss: 2.519261598587036, tv_loss: 0.010224226862192154\n",
      "iteration 31, dc_loss: 2.5067741870880127, tv_loss: 0.00970772746950388\n",
      "iteration 32, dc_loss: 2.4945995807647705, tv_loss: 0.009157530032098293\n",
      "iteration 33, dc_loss: 2.482656478881836, tv_loss: 0.008974982425570488\n",
      "iteration 34, dc_loss: 2.4727139472961426, tv_loss: 0.008431161753833294\n",
      "iteration 35, dc_loss: 2.464120388031006, tv_loss: 0.00878959521651268\n",
      "iteration 36, dc_loss: 2.449411153793335, tv_loss: 0.0084871556609869\n",
      "iteration 37, dc_loss: 2.4353392124176025, tv_loss: 0.008606518618762493\n",
      "iteration 38, dc_loss: 2.426391363143921, tv_loss: 0.008725350722670555\n",
      "iteration 39, dc_loss: 2.4124033451080322, tv_loss: 0.008704359643161297\n",
      "iteration 40, dc_loss: 2.402641534805298, tv_loss: 0.00866924598813057\n",
      "iteration 41, dc_loss: 2.3907692432403564, tv_loss: 0.008734159171581268\n",
      "iteration 42, dc_loss: 2.378795862197876, tv_loss: 0.00876196101307869\n",
      "iteration 43, dc_loss: 2.3689117431640625, tv_loss: 0.008712631650269032\n",
      "iteration 44, dc_loss: 2.356128692626953, tv_loss: 0.008691265247762203\n",
      "iteration 45, dc_loss: 2.346388101577759, tv_loss: 0.008687683381140232\n",
      "iteration 46, dc_loss: 2.3344805240631104, tv_loss: 0.008660320192575455\n",
      "iteration 47, dc_loss: 2.324181079864502, tv_loss: 0.008651336655020714\n",
      "iteration 48, dc_loss: 2.3133394718170166, tv_loss: 0.008825661614537239\n",
      "iteration 49, dc_loss: 2.3021578788757324, tv_loss: 0.008886387571692467\n",
      "iteration 50, dc_loss: 2.292487859725952, tv_loss: 0.008856762200593948\n",
      "iteration 51, dc_loss: 2.2810161113739014, tv_loss: 0.009036242961883545\n",
      "iteration 52, dc_loss: 2.271014451980591, tv_loss: 0.00914246030151844\n",
      "iteration 53, dc_loss: 2.260601282119751, tv_loss: 0.009143671952188015\n",
      "iteration 54, dc_loss: 2.249986410140991, tv_loss: 0.009265738539397717\n",
      "iteration 55, dc_loss: 2.240107297897339, tv_loss: 0.009402237832546234\n",
      "iteration 56, dc_loss: 2.229515552520752, tv_loss: 0.009450714103877544\n",
      "iteration 57, dc_loss: 2.219741106033325, tv_loss: 0.009526357054710388\n",
      "iteration 58, dc_loss: 2.209629535675049, tv_loss: 0.00966115016490221\n",
      "iteration 59, dc_loss: 2.1994614601135254, tv_loss: 0.009730204939842224\n",
      "iteration 60, dc_loss: 2.189948558807373, tv_loss: 0.00979373138397932\n",
      "iteration 61, dc_loss: 2.1798179149627686, tv_loss: 0.009936897084116936\n",
      "iteration 62, dc_loss: 2.1700687408447266, tv_loss: 0.010028270073235035\n",
      "iteration 63, dc_loss: 2.1606249809265137, tv_loss: 0.010084137320518494\n",
      "iteration 64, dc_loss: 2.150705337524414, tv_loss: 0.010221594013273716\n",
      "iteration 65, dc_loss: 2.1411948204040527, tv_loss: 0.01031937263906002\n",
      "iteration 66, dc_loss: 2.131850004196167, tv_loss: 0.010361842811107635\n",
      "iteration 67, dc_loss: 2.122185707092285, tv_loss: 0.01048495527356863\n",
      "iteration 68, dc_loss: 2.1128194332122803, tv_loss: 0.010578949004411697\n",
      "iteration 69, dc_loss: 2.103624105453491, tv_loss: 0.010631420649588108\n",
      "iteration 70, dc_loss: 2.09420108795166, tv_loss: 0.010756578296422958\n",
      "iteration 71, dc_loss: 2.0849640369415283, tv_loss: 0.010835917666554451\n",
      "iteration 72, dc_loss: 2.075930118560791, tv_loss: 0.010896614752709866\n",
      "iteration 73, dc_loss: 2.066742420196533, tv_loss: 0.011021449230611324\n",
      "iteration 74, dc_loss: 2.0576298236846924, tv_loss: 0.011084775440394878\n",
      "iteration 75, dc_loss: 2.048677682876587, tv_loss: 0.01116675604134798\n",
      "iteration 76, dc_loss: 2.0397539138793945, tv_loss: 0.011275344528257847\n",
      "iteration 77, dc_loss: 2.030834674835205, tv_loss: 0.0113253528252244\n",
      "iteration 78, dc_loss: 2.02192759513855, tv_loss: 0.011422797106206417\n",
      "iteration 79, dc_loss: 2.0131638050079346, tv_loss: 0.01150696724653244\n",
      "iteration 80, dc_loss: 2.00447678565979, tv_loss: 0.011568970046937466\n",
      "iteration 81, dc_loss: 1.9957480430603027, tv_loss: 0.011669335886836052\n",
      "iteration 82, dc_loss: 1.9870761632919312, tv_loss: 0.011732356622815132\n",
      "iteration 83, dc_loss: 1.9784709215164185, tv_loss: 0.011812778189778328\n",
      "iteration 84, dc_loss: 1.9699441194534302, tv_loss: 0.011905764229595661\n",
      "iteration 85, dc_loss: 1.9615230560302734, tv_loss: 0.011953780427575111\n",
      "iteration 86, dc_loss: 1.9530844688415527, tv_loss: 0.012084400281310081\n",
      "iteration 87, dc_loss: 1.9448516368865967, tv_loss: 0.01209881529211998\n",
      "iteration 88, dc_loss: 1.9366753101348877, tv_loss: 0.012265958823263645\n",
      "iteration 89, dc_loss: 1.9289432764053345, tv_loss: 0.012235775589942932\n",
      "iteration 90, dc_loss: 1.9210172891616821, tv_loss: 0.012437449768185616\n",
      "iteration 91, dc_loss: 1.9126032590866089, tv_loss: 0.012387348338961601\n",
      "iteration 92, dc_loss: 1.9035354852676392, tv_loss: 0.012510244734585285\n",
      "iteration 93, dc_loss: 1.8952784538269043, tv_loss: 0.012598011642694473\n",
      "iteration 94, dc_loss: 1.887870192527771, tv_loss: 0.012574722059071064\n",
      "iteration 95, dc_loss: 1.8796862363815308, tv_loss: 0.012776032090187073\n",
      "iteration 96, dc_loss: 1.8711609840393066, tv_loss: 0.012758389115333557\n",
      "iteration 97, dc_loss: 1.863264799118042, tv_loss: 0.012848075479269028\n",
      "iteration 98, dc_loss: 1.8556911945343018, tv_loss: 0.013017207384109497\n",
      "iteration 99, dc_loss: 1.8477120399475098, tv_loss: 0.01303821336477995\n",
      "iteration 100, dc_loss: 1.8395557403564453, tv_loss: 0.013121407479047775\n",
      "iteration 101, dc_loss: 1.8319810628890991, tv_loss: 0.013179175555706024\n",
      "iteration 102, dc_loss: 1.8244222402572632, tv_loss: 0.013217217288911343\n",
      "iteration 103, dc_loss: 1.8165253400802612, tv_loss: 0.01331736333668232\n",
      "iteration 104, dc_loss: 1.8088362216949463, tv_loss: 0.0134229501709342\n",
      "iteration 105, dc_loss: 1.801542043685913, tv_loss: 0.013406112790107727\n",
      "iteration 106, dc_loss: 1.793989658355713, tv_loss: 0.013585359789431095\n",
      "iteration 107, dc_loss: 1.786450982093811, tv_loss: 0.013544431887567043\n",
      "iteration 108, dc_loss: 1.7789920568466187, tv_loss: 0.013656467199325562\n",
      "iteration 109, dc_loss: 1.7716172933578491, tv_loss: 0.013698452152311802\n",
      "iteration 110, dc_loss: 1.7640010118484497, tv_loss: 0.013772054575383663\n",
      "iteration 111, dc_loss: 1.7563732862472534, tv_loss: 0.01378712523728609\n",
      "iteration 112, dc_loss: 1.7488960027694702, tv_loss: 0.013946637511253357\n",
      "iteration 113, dc_loss: 1.7417081594467163, tv_loss: 0.013983284123241901\n",
      "iteration 114, dc_loss: 1.7344584465026855, tv_loss: 0.014066719450056553\n",
      "iteration 115, dc_loss: 1.727284550666809, tv_loss: 0.01414538361132145\n",
      "iteration 116, dc_loss: 1.7203823328018188, tv_loss: 0.014096551574766636\n",
      "iteration 117, dc_loss: 1.7132868766784668, tv_loss: 0.014291960746049881\n",
      "iteration 118, dc_loss: 1.7061842679977417, tv_loss: 0.014236310496926308\n",
      "iteration 119, dc_loss: 1.6988359689712524, tv_loss: 0.014380580745637417\n",
      "iteration 120, dc_loss: 1.6916598081588745, tv_loss: 0.014445919543504715\n",
      "iteration 121, dc_loss: 1.6845752000808716, tv_loss: 0.014472504146397114\n",
      "iteration 122, dc_loss: 1.677569031715393, tv_loss: 0.014558385126292706\n",
      "iteration 123, dc_loss: 1.6708011627197266, tv_loss: 0.014567899517714977\n",
      "iteration 124, dc_loss: 1.6640238761901855, tv_loss: 0.014666017144918442\n",
      "iteration 125, dc_loss: 1.6571866273880005, tv_loss: 0.014673878438770771\n",
      "iteration 126, dc_loss: 1.6501402854919434, tv_loss: 0.014811795204877853\n",
      "iteration 127, dc_loss: 1.6432732343673706, tv_loss: 0.014825805090367794\n",
      "iteration 128, dc_loss: 1.636472463607788, tv_loss: 0.014921065419912338\n",
      "iteration 129, dc_loss: 1.6298017501831055, tv_loss: 0.01497166883200407\n",
      "iteration 130, dc_loss: 1.6231908798217773, tv_loss: 0.014985131099820137\n",
      "iteration 131, dc_loss: 1.6165649890899658, tv_loss: 0.015113763511180878\n",
      "iteration 132, dc_loss: 1.6101094484329224, tv_loss: 0.015068882144987583\n",
      "iteration 133, dc_loss: 1.6034311056137085, tv_loss: 0.015219349414110184\n",
      "iteration 134, dc_loss: 1.5967917442321777, tv_loss: 0.015196215361356735\n",
      "iteration 135, dc_loss: 1.5900801420211792, tv_loss: 0.01529223844408989\n",
      "iteration 136, dc_loss: 1.583535075187683, tv_loss: 0.015359483659267426\n",
      "iteration 137, dc_loss: 1.5771595239639282, tv_loss: 0.015364500693976879\n",
      "iteration 138, dc_loss: 1.5706864595413208, tv_loss: 0.015506413765251637\n",
      "iteration 139, dc_loss: 1.5644383430480957, tv_loss: 0.015457980334758759\n",
      "iteration 140, dc_loss: 1.5580569505691528, tv_loss: 0.015603385865688324\n",
      "iteration 141, dc_loss: 1.5518443584442139, tv_loss: 0.015561561100184917\n",
      "iteration 142, dc_loss: 1.5454453229904175, tv_loss: 0.015677206218242645\n",
      "iteration 143, dc_loss: 1.5391144752502441, tv_loss: 0.015673842281103134\n",
      "iteration 144, dc_loss: 1.5327153205871582, tv_loss: 0.01577039249241352\n",
      "iteration 145, dc_loss: 1.5264338254928589, tv_loss: 0.015814905986189842\n",
      "iteration 146, dc_loss: 1.5201963186264038, tv_loss: 0.0158787053078413\n",
      "iteration 147, dc_loss: 1.5140721797943115, tv_loss: 0.01594649627804756\n",
      "iteration 148, dc_loss: 1.5080676078796387, tv_loss: 0.015956968069076538\n",
      "iteration 149, dc_loss: 1.5020349025726318, tv_loss: 0.0160702895373106\n",
      "iteration 150, dc_loss: 1.4961568117141724, tv_loss: 0.01601698249578476\n",
      "iteration 151, dc_loss: 1.4900859594345093, tv_loss: 0.01622241549193859\n",
      "iteration 152, dc_loss: 1.484229564666748, tv_loss: 0.016065333038568497\n",
      "iteration 153, dc_loss: 1.4779647588729858, tv_loss: 0.01630750671029091\n",
      "iteration 154, dc_loss: 1.471893072128296, tv_loss: 0.016166090965270996\n",
      "iteration 155, dc_loss: 1.4656957387924194, tv_loss: 0.016324535012245178\n",
      "iteration 156, dc_loss: 1.459905982017517, tv_loss: 0.016376931220293045\n",
      "iteration 157, dc_loss: 1.4544010162353516, tv_loss: 0.01638026535511017\n",
      "iteration 158, dc_loss: 1.4486217498779297, tv_loss: 0.016601575538516045\n",
      "iteration 159, dc_loss: 1.4429203271865845, tv_loss: 0.016412461176514626\n",
      "iteration 160, dc_loss: 1.4367741346359253, tv_loss: 0.016622591763734818\n",
      "iteration 161, dc_loss: 1.4309974908828735, tv_loss: 0.01659870147705078\n",
      "iteration 162, dc_loss: 1.4254769086837769, tv_loss: 0.016604993492364883\n",
      "iteration 163, dc_loss: 1.4198423624038696, tv_loss: 0.016815487295389175\n",
      "iteration 164, dc_loss: 1.4141031503677368, tv_loss: 0.016642749309539795\n",
      "iteration 165, dc_loss: 1.4079630374908447, tv_loss: 0.016800977289676666\n",
      "iteration 166, dc_loss: 1.4021738767623901, tv_loss: 0.016833702102303505\n",
      "iteration 167, dc_loss: 1.396748661994934, tv_loss: 0.016859013587236404\n",
      "iteration 168, dc_loss: 1.391204833984375, tv_loss: 0.01707432046532631\n",
      "iteration 169, dc_loss: 1.3856761455535889, tv_loss: 0.016963955014944077\n",
      "iteration 170, dc_loss: 1.3799818754196167, tv_loss: 0.017025591805577278\n",
      "iteration 171, dc_loss: 1.374444603919983, tv_loss: 0.017094045877456665\n",
      "iteration 172, dc_loss: 1.3690592050552368, tv_loss: 0.017161916941404343\n",
      "iteration 173, dc_loss: 1.3637264966964722, tv_loss: 0.017192451283335686\n",
      "iteration 174, dc_loss: 1.3581691980361938, tv_loss: 0.01732293888926506\n",
      "iteration 175, dc_loss: 1.3529331684112549, tv_loss: 0.01716538518667221\n",
      "iteration 176, dc_loss: 1.3472473621368408, tv_loss: 0.017466191202402115\n",
      "iteration 177, dc_loss: 1.3419530391693115, tv_loss: 0.017301570624113083\n",
      "iteration 178, dc_loss: 1.3364142179489136, tv_loss: 0.017464477568864822\n",
      "iteration 179, dc_loss: 1.3311046361923218, tv_loss: 0.017476247623562813\n",
      "iteration 180, dc_loss: 1.3258955478668213, tv_loss: 0.01743529550731182\n",
      "iteration 181, dc_loss: 1.3204518556594849, tv_loss: 0.017597485333681107\n",
      "iteration 182, dc_loss: 1.3152347803115845, tv_loss: 0.01754855550825596\n",
      "iteration 183, dc_loss: 1.3099766969680786, tv_loss: 0.01765955239534378\n",
      "iteration 184, dc_loss: 1.3048434257507324, tv_loss: 0.017725616693496704\n",
      "iteration 185, dc_loss: 1.2998253107070923, tv_loss: 0.017713623121380806\n",
      "iteration 186, dc_loss: 1.2946444749832153, tv_loss: 0.01778450421988964\n",
      "iteration 187, dc_loss: 1.2894352674484253, tv_loss: 0.017853321507573128\n",
      "iteration 188, dc_loss: 1.2844130992889404, tv_loss: 0.017777306959033012\n",
      "iteration 189, dc_loss: 1.2792291641235352, tv_loss: 0.01800837181508541\n",
      "iteration 190, dc_loss: 1.2744545936584473, tv_loss: 0.01779673807322979\n",
      "iteration 191, dc_loss: 1.2691943645477295, tv_loss: 0.0180830005556345\n",
      "iteration 192, dc_loss: 1.2642096281051636, tv_loss: 0.017919505015015602\n",
      "iteration 193, dc_loss: 1.2589458227157593, tv_loss: 0.01809602975845337\n",
      "iteration 194, dc_loss: 1.253904104232788, tv_loss: 0.018138818442821503\n",
      "iteration 195, dc_loss: 1.248986005783081, tv_loss: 0.01813546195626259\n",
      "iteration 196, dc_loss: 1.2439604997634888, tv_loss: 0.018266232684254646\n",
      "iteration 197, dc_loss: 1.2392115592956543, tv_loss: 0.018180236220359802\n",
      "iteration 198, dc_loss: 1.2342329025268555, tv_loss: 0.018369516357779503\n",
      "iteration 199, dc_loss: 1.2295875549316406, tv_loss: 0.018253248184919357\n",
      "iteration 200, dc_loss: 1.2246339321136475, tv_loss: 0.018480857834219933\n",
      "iteration 201, dc_loss: 1.2201080322265625, tv_loss: 0.018271295353770256\n",
      "iteration 202, dc_loss: 1.2151570320129395, tv_loss: 0.018587272614240646\n",
      "iteration 203, dc_loss: 1.2104754447937012, tv_loss: 0.018340101465582848\n",
      "iteration 204, dc_loss: 1.2052654027938843, tv_loss: 0.01862766221165657\n",
      "iteration 205, dc_loss: 1.2004066705703735, tv_loss: 0.01852111518383026\n",
      "iteration 206, dc_loss: 1.1956145763397217, tv_loss: 0.018603196367621422\n",
      "iteration 207, dc_loss: 1.191036343574524, tv_loss: 0.018715640529990196\n",
      "iteration 208, dc_loss: 1.1866257190704346, tv_loss: 0.018604643642902374\n",
      "iteration 209, dc_loss: 1.1818013191223145, tv_loss: 0.0188087597489357\n",
      "iteration 210, dc_loss: 1.1770379543304443, tv_loss: 0.018707619979977608\n",
      "iteration 211, dc_loss: 1.172160267829895, tv_loss: 0.018786808475852013\n",
      "iteration 212, dc_loss: 1.1673980951309204, tv_loss: 0.01889842003583908\n",
      "iteration 213, dc_loss: 1.162993311882019, tv_loss: 0.018792346119880676\n",
      "iteration 214, dc_loss: 1.158333659172058, tv_loss: 0.01906454935669899\n",
      "iteration 215, dc_loss: 1.1540724039077759, tv_loss: 0.01884288340806961\n",
      "iteration 216, dc_loss: 1.1493518352508545, tv_loss: 0.01911001279950142\n",
      "iteration 217, dc_loss: 1.1448590755462646, tv_loss: 0.018962843343615532\n",
      "iteration 218, dc_loss: 1.1400279998779297, tv_loss: 0.019120212644338608\n",
      "iteration 219, dc_loss: 1.1355844736099243, tv_loss: 0.019080093130469322\n",
      "iteration 220, dc_loss: 1.1310938596725464, tv_loss: 0.01918044313788414\n",
      "iteration 221, dc_loss: 1.1267169713974, tv_loss: 0.019188718870282173\n",
      "iteration 222, dc_loss: 1.1221808195114136, tv_loss: 0.01921243779361248\n",
      "iteration 223, dc_loss: 1.1176177263259888, tv_loss: 0.019269920885562897\n",
      "iteration 224, dc_loss: 1.1132534742355347, tv_loss: 0.01923835277557373\n",
      "iteration 225, dc_loss: 1.1088237762451172, tv_loss: 0.019376149401068687\n",
      "iteration 226, dc_loss: 1.1045641899108887, tv_loss: 0.019318468868732452\n",
      "iteration 227, dc_loss: 1.1000324487686157, tv_loss: 0.019450703635811806\n",
      "iteration 228, dc_loss: 1.0956367254257202, tv_loss: 0.019418345764279366\n",
      "iteration 229, dc_loss: 1.0912892818450928, tv_loss: 0.019443226978182793\n",
      "iteration 230, dc_loss: 1.0869678258895874, tv_loss: 0.019568422809243202\n",
      "iteration 231, dc_loss: 1.0829375982284546, tv_loss: 0.01948818564414978\n",
      "iteration 232, dc_loss: 1.0785804986953735, tv_loss: 0.019712276756763458\n",
      "iteration 233, dc_loss: 1.0746556520462036, tv_loss: 0.019495446234941483\n",
      "iteration 234, dc_loss: 1.0702351331710815, tv_loss: 0.01981062814593315\n",
      "iteration 235, dc_loss: 1.066327691078186, tv_loss: 0.019549204036593437\n",
      "iteration 236, dc_loss: 1.0619728565216064, tv_loss: 0.01988503336906433\n",
      "iteration 237, dc_loss: 1.0581200122833252, tv_loss: 0.019607847556471825\n",
      "iteration 238, dc_loss: 1.053884744644165, tv_loss: 0.019922683015465736\n",
      "iteration 239, dc_loss: 1.0500324964523315, tv_loss: 0.019658641889691353\n",
      "iteration 240, dc_loss: 1.0456465482711792, tv_loss: 0.01992131397128105\n",
      "iteration 241, dc_loss: 1.0415654182434082, tv_loss: 0.019777053967118263\n",
      "iteration 242, dc_loss: 1.0373364686965942, tv_loss: 0.0198679082095623\n",
      "iteration 243, dc_loss: 1.033407211303711, tv_loss: 0.020016519352793694\n",
      "iteration 244, dc_loss: 1.029555082321167, tv_loss: 0.019828995689749718\n",
      "iteration 245, dc_loss: 1.0252794027328491, tv_loss: 0.020201368257403374\n",
      "iteration 246, dc_loss: 1.0211827754974365, tv_loss: 0.01988409273326397\n",
      "iteration 247, dc_loss: 1.0167226791381836, tv_loss: 0.020112786442041397\n",
      "iteration 248, dc_loss: 1.0128536224365234, tv_loss: 0.020106641575694084\n",
      "iteration 249, dc_loss: 1.009149193763733, tv_loss: 0.020095497369766235\n",
      "iteration 250, dc_loss: 1.005210280418396, tv_loss: 0.020321261137723923\n",
      "iteration 251, dc_loss: 1.0010673999786377, tv_loss: 0.02006869949400425\n",
      "iteration 252, dc_loss: 0.9967174530029297, tv_loss: 0.02030966989696026\n",
      "iteration 253, dc_loss: 0.9931604266166687, tv_loss: 0.020188435912132263\n",
      "iteration 254, dc_loss: 0.9891746640205383, tv_loss: 0.020380552858114243\n",
      "iteration 255, dc_loss: 0.9851621389389038, tv_loss: 0.020375775173306465\n",
      "iteration 256, dc_loss: 0.9810680150985718, tv_loss: 0.02033715695142746\n",
      "iteration 257, dc_loss: 0.9772002100944519, tv_loss: 0.020446691662073135\n",
      "iteration 258, dc_loss: 0.9736632704734802, tv_loss: 0.020332269370555878\n",
      "iteration 259, dc_loss: 0.9695236086845398, tv_loss: 0.020602775737643242\n",
      "iteration 260, dc_loss: 0.9657140970230103, tv_loss: 0.020419664680957794\n",
      "iteration 261, dc_loss: 0.9618518948554993, tv_loss: 0.0205619465559721\n",
      "iteration 262, dc_loss: 0.9581822752952576, tv_loss: 0.02056564949452877\n",
      "iteration 263, dc_loss: 0.9544469714164734, tv_loss: 0.020590607076883316\n",
      "iteration 264, dc_loss: 0.9505032300949097, tv_loss: 0.02074998803436756\n",
      "iteration 265, dc_loss: 0.9470304846763611, tv_loss: 0.020595449954271317\n",
      "iteration 266, dc_loss: 0.9432939291000366, tv_loss: 0.02083667740225792\n",
      "iteration 267, dc_loss: 0.9399418830871582, tv_loss: 0.020580563694238663\n",
      "iteration 268, dc_loss: 0.9361394047737122, tv_loss: 0.020949533209204674\n",
      "iteration 269, dc_loss: 0.932891845703125, tv_loss: 0.020659931004047394\n",
      "iteration 270, dc_loss: 0.9291087985038757, tv_loss: 0.02101716212928295\n",
      "iteration 271, dc_loss: 0.9255600571632385, tv_loss: 0.020720714703202248\n",
      "iteration 272, dc_loss: 0.9214401841163635, tv_loss: 0.021031267940998077\n",
      "iteration 273, dc_loss: 0.9177759289741516, tv_loss: 0.020902229472994804\n",
      "iteration 274, dc_loss: 0.9141497611999512, tv_loss: 0.020958933979272842\n",
      "iteration 275, dc_loss: 0.9106592535972595, tv_loss: 0.021045813336968422\n",
      "iteration 276, dc_loss: 0.9071837663650513, tv_loss: 0.020914226770401\n",
      "iteration 277, dc_loss: 0.9034729599952698, tv_loss: 0.02117510326206684\n",
      "iteration 278, dc_loss: 0.9002509117126465, tv_loss: 0.020975181832909584\n",
      "iteration 279, dc_loss: 0.8965209126472473, tv_loss: 0.02126873843371868\n",
      "iteration 280, dc_loss: 0.8930468559265137, tv_loss: 0.021094929426908493\n",
      "iteration 281, dc_loss: 0.8894026875495911, tv_loss: 0.02120727300643921\n",
      "iteration 282, dc_loss: 0.8858881592750549, tv_loss: 0.02123899571597576\n",
      "iteration 283, dc_loss: 0.8825388550758362, tv_loss: 0.021208373829722404\n",
      "iteration 284, dc_loss: 0.8790128827095032, tv_loss: 0.021383950486779213\n",
      "iteration 285, dc_loss: 0.8757839202880859, tv_loss: 0.021249152719974518\n",
      "iteration 286, dc_loss: 0.872248113155365, tv_loss: 0.02146000787615776\n",
      "iteration 287, dc_loss: 0.8691940307617188, tv_loss: 0.021258540451526642\n",
      "iteration 288, dc_loss: 0.8657863736152649, tv_loss: 0.021580899134278297\n",
      "iteration 289, dc_loss: 0.8627762198448181, tv_loss: 0.021286342293024063\n",
      "iteration 290, dc_loss: 0.8591707944869995, tv_loss: 0.021683773025870323\n",
      "iteration 291, dc_loss: 0.8561678528785706, tv_loss: 0.021295446902513504\n",
      "iteration 292, dc_loss: 0.8525043725967407, tv_loss: 0.021730341017246246\n",
      "iteration 293, dc_loss: 0.8492022752761841, tv_loss: 0.021403035148978233\n",
      "iteration 294, dc_loss: 0.8454422354698181, tv_loss: 0.021699240431189537\n",
      "iteration 295, dc_loss: 0.8422096967697144, tv_loss: 0.021661950275301933\n",
      "iteration 296, dc_loss: 0.8390628695487976, tv_loss: 0.021586239337921143\n",
      "iteration 297, dc_loss: 0.8357544541358948, tv_loss: 0.021837268024683\n",
      "iteration 298, dc_loss: 0.8330131769180298, tv_loss: 0.021544795483350754\n",
      "iteration 299, dc_loss: 0.8296746015548706, tv_loss: 0.02196982130408287\n",
      "iteration 300, dc_loss: 0.8267547488212585, tv_loss: 0.02162344940006733\n",
      "iteration 301, dc_loss: 0.8233773112297058, tv_loss: 0.021951623260974884\n",
      "iteration 302, dc_loss: 0.820175290107727, tv_loss: 0.021760055795311928\n",
      "iteration 303, dc_loss: 0.8166463375091553, tv_loss: 0.021930484101176262\n",
      "iteration 304, dc_loss: 0.8132758140563965, tv_loss: 0.021923212334513664\n",
      "iteration 305, dc_loss: 0.8102344274520874, tv_loss: 0.02189113385975361\n",
      "iteration 306, dc_loss: 0.8071534037590027, tv_loss: 0.022089924663305283\n",
      "iteration 307, dc_loss: 0.8044774532318115, tv_loss: 0.021857041865587234\n",
      "iteration 308, dc_loss: 0.8013194799423218, tv_loss: 0.022246049717068672\n",
      "iteration 309, dc_loss: 0.7984391450881958, tv_loss: 0.021861758083105087\n",
      "iteration 310, dc_loss: 0.7949236035346985, tv_loss: 0.0222945399582386\n",
      "iteration 311, dc_loss: 0.7919192910194397, tv_loss: 0.02196686528623104\n",
      "iteration 312, dc_loss: 0.7884936332702637, tv_loss: 0.022198054939508438\n",
      "iteration 313, dc_loss: 0.7853638529777527, tv_loss: 0.022195708006620407\n",
      "iteration 314, dc_loss: 0.7824749946594238, tv_loss: 0.022122494876384735\n",
      "iteration 315, dc_loss: 0.7794017195701599, tv_loss: 0.02236281894147396\n",
      "iteration 316, dc_loss: 0.7767702341079712, tv_loss: 0.022079341113567352\n",
      "iteration 317, dc_loss: 0.7736494541168213, tv_loss: 0.02246195264160633\n",
      "iteration 318, dc_loss: 0.7706935405731201, tv_loss: 0.0221202801913023\n",
      "iteration 319, dc_loss: 0.767342746257782, tv_loss: 0.02240651845932007\n",
      "iteration 320, dc_loss: 0.7641868591308594, tv_loss: 0.022342834621667862\n",
      "iteration 321, dc_loss: 0.7613245248794556, tv_loss: 0.022358587011694908\n",
      "iteration 322, dc_loss: 0.7582551836967468, tv_loss: 0.02255033515393734\n",
      "iteration 323, dc_loss: 0.7559319734573364, tv_loss: 0.022258754819631577\n",
      "iteration 324, dc_loss: 0.7532669305801392, tv_loss: 0.02274165488779545\n",
      "iteration 325, dc_loss: 0.7509987950325012, tv_loss: 0.022321568801999092\n",
      "iteration 326, dc_loss: 0.7474086880683899, tv_loss: 0.02269362471997738\n",
      "iteration 327, dc_loss: 0.743995726108551, tv_loss: 0.022383634001016617\n",
      "iteration 328, dc_loss: 0.74085932970047, tv_loss: 0.022599415853619576\n",
      "iteration 329, dc_loss: 0.7382611036300659, tv_loss: 0.022654356434941292\n",
      "iteration 330, dc_loss: 0.7355563044548035, tv_loss: 0.02250712364912033\n",
      "iteration 331, dc_loss: 0.7322065830230713, tv_loss: 0.022708777338266373\n",
      "iteration 332, dc_loss: 0.7294002771377563, tv_loss: 0.022544868290424347\n",
      "iteration 333, dc_loss: 0.7263867855072021, tv_loss: 0.022766847163438797\n",
      "iteration 334, dc_loss: 0.7234390377998352, tv_loss: 0.022764327004551888\n",
      "iteration 335, dc_loss: 0.7206114530563354, tv_loss: 0.02277022786438465\n",
      "iteration 336, dc_loss: 0.7178122997283936, tv_loss: 0.02284143678843975\n",
      "iteration 337, dc_loss: 0.7151510715484619, tv_loss: 0.022675255313515663\n",
      "iteration 338, dc_loss: 0.7120075821876526, tv_loss: 0.02286490425467491\n",
      "iteration 339, dc_loss: 0.7090892195701599, tv_loss: 0.022878844290971756\n",
      "iteration 340, dc_loss: 0.7063602209091187, tv_loss: 0.022913746535778046\n",
      "iteration 341, dc_loss: 0.7036901116371155, tv_loss: 0.022990722209215164\n",
      "iteration 342, dc_loss: 0.7010469436645508, tv_loss: 0.022896889597177505\n",
      "iteration 343, dc_loss: 0.6981642842292786, tv_loss: 0.022997455671429634\n",
      "iteration 344, dc_loss: 0.695361852645874, tv_loss: 0.023038510233163834\n",
      "iteration 345, dc_loss: 0.6928558945655823, tv_loss: 0.022969800978899002\n",
      "iteration 346, dc_loss: 0.6901815533638, tv_loss: 0.023215917870402336\n",
      "iteration 347, dc_loss: 0.6881518363952637, tv_loss: 0.02287323959171772\n",
      "iteration 348, dc_loss: 0.6859006285667419, tv_loss: 0.023392723873257637\n",
      "iteration 349, dc_loss: 0.6844744682312012, tv_loss: 0.022826623171567917\n",
      "iteration 350, dc_loss: 0.6831427812576294, tv_loss: 0.02358117513358593\n",
      "iteration 351, dc_loss: 0.6795827150344849, tv_loss: 0.022841745987534523\n",
      "iteration 352, dc_loss: 0.6753624081611633, tv_loss: 0.02332795411348343\n",
      "iteration 353, dc_loss: 0.6722946763038635, tv_loss: 0.02320433035492897\n",
      "iteration 354, dc_loss: 0.6707560420036316, tv_loss: 0.022999728098511696\n",
      "iteration 355, dc_loss: 0.6685587763786316, tv_loss: 0.023612894117832184\n",
      "iteration 356, dc_loss: 0.6650803089141846, tv_loss: 0.023270418867468834\n",
      "iteration 357, dc_loss: 0.6626523733139038, tv_loss: 0.023355143144726753\n",
      "iteration 358, dc_loss: 0.6602535247802734, tv_loss: 0.023509308695793152\n",
      "iteration 359, dc_loss: 0.6572660803794861, tv_loss: 0.023159759119153023\n",
      "iteration 360, dc_loss: 0.6548042297363281, tv_loss: 0.023561468347907066\n",
      "iteration 361, dc_loss: 0.652418315410614, tv_loss: 0.02345944568514824\n",
      "iteration 362, dc_loss: 0.6494928002357483, tv_loss: 0.023440824821591377\n",
      "iteration 363, dc_loss: 0.6469306349754333, tv_loss: 0.023686207830905914\n",
      "iteration 364, dc_loss: 0.6448554396629333, tv_loss: 0.023443272337317467\n",
      "iteration 365, dc_loss: 0.6420334577560425, tv_loss: 0.023530030623078346\n",
      "iteration 366, dc_loss: 0.6393674612045288, tv_loss: 0.023687412962317467\n",
      "iteration 367, dc_loss: 0.6370289325714111, tv_loss: 0.023652518168091774\n",
      "iteration 368, dc_loss: 0.6346243619918823, tv_loss: 0.023617984727025032\n",
      "iteration 369, dc_loss: 0.6320062875747681, tv_loss: 0.0236834567040205\n",
      "iteration 370, dc_loss: 0.6295216083526611, tv_loss: 0.023717543110251427\n",
      "iteration 371, dc_loss: 0.6273518800735474, tv_loss: 0.023693544790148735\n",
      "iteration 372, dc_loss: 0.6246564984321594, tv_loss: 0.02383062057197094\n",
      "iteration 373, dc_loss: 0.6222525835037231, tv_loss: 0.023765966296195984\n",
      "iteration 374, dc_loss: 0.6200088858604431, tv_loss: 0.023745786398649216\n",
      "iteration 375, dc_loss: 0.6174132823944092, tv_loss: 0.023866308853030205\n",
      "iteration 376, dc_loss: 0.615190327167511, tv_loss: 0.023835381492972374\n",
      "iteration 377, dc_loss: 0.6127755045890808, tv_loss: 0.023921430110931396\n",
      "iteration 378, dc_loss: 0.6103321313858032, tv_loss: 0.0238726157695055\n",
      "iteration 379, dc_loss: 0.6080988049507141, tv_loss: 0.023887353017926216\n",
      "iteration 380, dc_loss: 0.6057027578353882, tv_loss: 0.02399822138249874\n",
      "iteration 381, dc_loss: 0.6033097505569458, tv_loss: 0.023988332599401474\n",
      "iteration 382, dc_loss: 0.6010500192642212, tv_loss: 0.024005630984902382\n",
      "iteration 383, dc_loss: 0.5988249182701111, tv_loss: 0.023977002128958702\n",
      "iteration 384, dc_loss: 0.5964510440826416, tv_loss: 0.02407977543771267\n",
      "iteration 385, dc_loss: 0.594415009021759, tv_loss: 0.023999741300940514\n",
      "iteration 386, dc_loss: 0.5921684503555298, tv_loss: 0.0242209043353796\n",
      "iteration 387, dc_loss: 0.5903549790382385, tv_loss: 0.02397458255290985\n",
      "iteration 388, dc_loss: 0.5879257321357727, tv_loss: 0.02432679757475853\n",
      "iteration 389, dc_loss: 0.585700273513794, tv_loss: 0.023991841822862625\n",
      "iteration 390, dc_loss: 0.5829538702964783, tv_loss: 0.02431895211338997\n",
      "iteration 391, dc_loss: 0.580948531627655, tv_loss: 0.024119427427649498\n",
      "iteration 392, dc_loss: 0.578997790813446, tv_loss: 0.024275358766317368\n",
      "iteration 393, dc_loss: 0.5770914554595947, tv_loss: 0.024154918268322945\n",
      "iteration 394, dc_loss: 0.574722409248352, tv_loss: 0.024507369846105576\n",
      "iteration 395, dc_loss: 0.5727521777153015, tv_loss: 0.024078916758298874\n",
      "iteration 396, dc_loss: 0.5704064965248108, tv_loss: 0.02457934059202671\n",
      "iteration 397, dc_loss: 0.5686977505683899, tv_loss: 0.0240677110850811\n",
      "iteration 398, dc_loss: 0.5662026405334473, tv_loss: 0.024543587118387222\n",
      "iteration 399, dc_loss: 0.5639340877532959, tv_loss: 0.024199748411774635\n",
      "iteration 400, dc_loss: 0.5615569353103638, tv_loss: 0.024449586868286133\n",
      "iteration 401, dc_loss: 0.5594390034675598, tv_loss: 0.02443559095263481\n",
      "iteration 402, dc_loss: 0.5573612451553345, tv_loss: 0.024421194568276405\n",
      "iteration 403, dc_loss: 0.5557562708854675, tv_loss: 0.024401487782597542\n",
      "iteration 404, dc_loss: 0.5540205240249634, tv_loss: 0.02437683939933777\n",
      "iteration 405, dc_loss: 0.5520751476287842, tv_loss: 0.024484261870384216\n",
      "iteration 406, dc_loss: 0.5505361557006836, tv_loss: 0.024508658796548843\n",
      "iteration 407, dc_loss: 0.5487973093986511, tv_loss: 0.02448539435863495\n",
      "iteration 408, dc_loss: 0.5470460057258606, tv_loss: 0.02455105446279049\n",
      "iteration 409, dc_loss: 0.5453599095344543, tv_loss: 0.024530494585633278\n",
      "iteration 410, dc_loss: 0.5436526536941528, tv_loss: 0.024482574313879013\n",
      "iteration 411, dc_loss: 0.5419387221336365, tv_loss: 0.02457248792052269\n",
      "iteration 412, dc_loss: 0.5402711629867554, tv_loss: 0.024575263261795044\n",
      "iteration 413, dc_loss: 0.5386322140693665, tv_loss: 0.024606553837656975\n",
      "iteration 414, dc_loss: 0.5369148254394531, tv_loss: 0.024716142565011978\n",
      "iteration 415, dc_loss: 0.5352442860603333, tv_loss: 0.024642979726195335\n",
      "iteration 416, dc_loss: 0.5336154699325562, tv_loss: 0.024628274142742157\n",
      "iteration 417, dc_loss: 0.531910240650177, tv_loss: 0.024694764986634254\n",
      "iteration 418, dc_loss: 0.530272364616394, tv_loss: 0.024755343794822693\n",
      "iteration 419, dc_loss: 0.5286591649055481, tv_loss: 0.02474386617541313\n",
      "iteration 420, dc_loss: 0.5269725918769836, tv_loss: 0.024792661890387535\n",
      "iteration 421, dc_loss: 0.5253309607505798, tv_loss: 0.024751782417297363\n",
      "iteration 422, dc_loss: 0.5237610340118408, tv_loss: 0.02473381720483303\n",
      "iteration 423, dc_loss: 0.5220569372177124, tv_loss: 0.024897290393710136\n",
      "iteration 424, dc_loss: 0.5204954147338867, tv_loss: 0.024845771491527557\n",
      "iteration 425, dc_loss: 0.5188683867454529, tv_loss: 0.024827968329191208\n",
      "iteration 426, dc_loss: 0.5172113180160522, tv_loss: 0.024923821911215782\n",
      "iteration 427, dc_loss: 0.515729546546936, tv_loss: 0.024813393130898476\n",
      "iteration 428, dc_loss: 0.5140496492385864, tv_loss: 0.024946123361587524\n",
      "iteration 429, dc_loss: 0.5125876665115356, tv_loss: 0.02495153620839119\n",
      "iteration 430, dc_loss: 0.5111632943153381, tv_loss: 0.024994244799017906\n",
      "iteration 431, dc_loss: 0.5098584294319153, tv_loss: 0.024876978248357773\n",
      "iteration 432, dc_loss: 0.5087711215019226, tv_loss: 0.025126637890934944\n",
      "iteration 433, dc_loss: 0.5071781873703003, tv_loss: 0.02478197030723095\n",
      "iteration 434, dc_loss: 0.5053675770759583, tv_loss: 0.025225616991519928\n",
      "iteration 435, dc_loss: 0.5036640167236328, tv_loss: 0.024934150278568268\n",
      "iteration 436, dc_loss: 0.5019382834434509, tv_loss: 0.02501324750483036\n",
      "iteration 437, dc_loss: 0.5007249116897583, tv_loss: 0.025315668433904648\n",
      "iteration 438, dc_loss: 0.4999065101146698, tv_loss: 0.024813691154122353\n",
      "iteration 439, dc_loss: 0.49821731448173523, tv_loss: 0.025353925302624702\n",
      "iteration 440, dc_loss: 0.4961964190006256, tv_loss: 0.025069693103432655\n",
      "iteration 441, dc_loss: 0.494734525680542, tv_loss: 0.025059925392270088\n",
      "iteration 442, dc_loss: 0.49284934997558594, tv_loss: 0.025326775386929512\n",
      "iteration 443, dc_loss: 0.49167293310165405, tv_loss: 0.025087980553507805\n",
      "iteration 444, dc_loss: 0.49069759249687195, tv_loss: 0.025286270305514336\n",
      "iteration 445, dc_loss: 0.4885529577732086, tv_loss: 0.025169270113110542\n",
      "iteration 446, dc_loss: 0.48679840564727783, tv_loss: 0.025248944759368896\n",
      "iteration 447, dc_loss: 0.48555174469947815, tv_loss: 0.02534548193216324\n",
      "iteration 448, dc_loss: 0.48435524106025696, tv_loss: 0.02522108517587185\n",
      "iteration 449, dc_loss: 0.48272162675857544, tv_loss: 0.02537410520017147\n",
      "iteration 450, dc_loss: 0.4812009334564209, tv_loss: 0.025196988135576248\n",
      "iteration 451, dc_loss: 0.47959044575691223, tv_loss: 0.0254212636500597\n",
      "iteration 452, dc_loss: 0.4781797528266907, tv_loss: 0.025341464206576347\n",
      "iteration 453, dc_loss: 0.47698870301246643, tv_loss: 0.02530580386519432\n",
      "iteration 454, dc_loss: 0.4753613770008087, tv_loss: 0.025608714669942856\n",
      "iteration 455, dc_loss: 0.4740404784679413, tv_loss: 0.025207890197634697\n",
      "iteration 456, dc_loss: 0.47224748134613037, tv_loss: 0.025404030457139015\n",
      "iteration 457, dc_loss: 0.47078263759613037, tv_loss: 0.02556648477911949\n",
      "iteration 458, dc_loss: 0.46973878145217896, tv_loss: 0.02537386864423752\n",
      "iteration 459, dc_loss: 0.46812334656715393, tv_loss: 0.025582650676369667\n",
      "iteration 460, dc_loss: 0.46678927540779114, tv_loss: 0.025434298440814018\n",
      "iteration 461, dc_loss: 0.4654828906059265, tv_loss: 0.02548847533762455\n",
      "iteration 462, dc_loss: 0.4639185667037964, tv_loss: 0.02557677961885929\n",
      "iteration 463, dc_loss: 0.46243059635162354, tv_loss: 0.02557685226202011\n",
      "iteration 464, dc_loss: 0.46123206615448, tv_loss: 0.025554358959197998\n",
      "iteration 465, dc_loss: 0.45993855595588684, tv_loss: 0.02560282126069069\n",
      "iteration 466, dc_loss: 0.45844918489456177, tv_loss: 0.025570891797542572\n",
      "iteration 467, dc_loss: 0.45712605118751526, tv_loss: 0.025571826845407486\n",
      "iteration 468, dc_loss: 0.45574527978897095, tv_loss: 0.02582414820790291\n",
      "iteration 469, dc_loss: 0.4546239376068115, tv_loss: 0.025534771382808685\n",
      "iteration 470, dc_loss: 0.4531856179237366, tv_loss: 0.025621432811021805\n",
      "iteration 471, dc_loss: 0.45165151357650757, tv_loss: 0.025837456807494164\n",
      "iteration 472, dc_loss: 0.45068901777267456, tv_loss: 0.02556915208697319\n",
      "iteration 473, dc_loss: 0.4492710828781128, tv_loss: 0.025817297399044037\n",
      "iteration 474, dc_loss: 0.4479994475841522, tv_loss: 0.025780782103538513\n",
      "iteration 475, dc_loss: 0.44685661792755127, tv_loss: 0.025662101805210114\n",
      "iteration 476, dc_loss: 0.44506752490997314, tv_loss: 0.02578703872859478\n",
      "iteration 477, dc_loss: 0.4437832534313202, tv_loss: 0.025870302692055702\n",
      "iteration 478, dc_loss: 0.4429750144481659, tv_loss: 0.025775684043765068\n",
      "iteration 479, dc_loss: 0.44137948751449585, tv_loss: 0.025821562856435776\n",
      "iteration 480, dc_loss: 0.43979352712631226, tv_loss: 0.02583417296409607\n",
      "iteration 481, dc_loss: 0.43884438276290894, tv_loss: 0.025842927396297455\n",
      "iteration 482, dc_loss: 0.43762609362602234, tv_loss: 0.02598915994167328\n",
      "iteration 483, dc_loss: 0.4361315071582794, tv_loss: 0.025848383083939552\n",
      "iteration 484, dc_loss: 0.4348294138908386, tv_loss: 0.02583857998251915\n",
      "iteration 485, dc_loss: 0.4336276054382324, tv_loss: 0.02600284479558468\n",
      "iteration 486, dc_loss: 0.432583212852478, tv_loss: 0.025877758860588074\n",
      "iteration 487, dc_loss: 0.43096354603767395, tv_loss: 0.02600720338523388\n",
      "iteration 488, dc_loss: 0.42986980080604553, tv_loss: 0.026024233549833298\n",
      "iteration 489, dc_loss: 0.428681880235672, tv_loss: 0.025992415845394135\n",
      "iteration 490, dc_loss: 0.42728742957115173, tv_loss: 0.02593926154077053\n",
      "iteration 491, dc_loss: 0.42596715688705444, tv_loss: 0.02611985057592392\n",
      "iteration 492, dc_loss: 0.42474570870399475, tv_loss: 0.026016144081950188\n",
      "iteration 493, dc_loss: 0.4235096275806427, tv_loss: 0.026059946045279503\n",
      "iteration 494, dc_loss: 0.4221755266189575, tv_loss: 0.02610187791287899\n",
      "iteration 495, dc_loss: 0.4210221767425537, tv_loss: 0.026096977293491364\n",
      "iteration 496, dc_loss: 0.4197004437446594, tv_loss: 0.0261483546346426\n",
      "iteration 497, dc_loss: 0.41848114132881165, tv_loss: 0.026110000908374786\n",
      "iteration 498, dc_loss: 0.4173336923122406, tv_loss: 0.026137065142393112\n",
      "iteration 499, dc_loss: 0.4159848093986511, tv_loss: 0.026177993044257164\n",
      "iteration 500, dc_loss: 0.41482993960380554, tv_loss: 0.0261547788977623\n",
      "iteration 501, dc_loss: 0.4136413335800171, tv_loss: 0.02621578797698021\n",
      "iteration 502, dc_loss: 0.41241735219955444, tv_loss: 0.02626957558095455\n",
      "iteration 503, dc_loss: 0.41119110584259033, tv_loss: 0.0261943768709898\n",
      "iteration 504, dc_loss: 0.40999478101730347, tv_loss: 0.026196632534265518\n",
      "iteration 505, dc_loss: 0.40881070494651794, tv_loss: 0.026331176981329918\n",
      "iteration 506, dc_loss: 0.4077085852622986, tv_loss: 0.026213111355900764\n",
      "iteration 507, dc_loss: 0.40642791986465454, tv_loss: 0.02636166475713253\n",
      "iteration 508, dc_loss: 0.4055066406726837, tv_loss: 0.026248132809996605\n",
      "iteration 509, dc_loss: 0.40424612164497375, tv_loss: 0.026440707966685295\n",
      "iteration 510, dc_loss: 0.4035053849220276, tv_loss: 0.02617906779050827\n",
      "iteration 511, dc_loss: 0.4023963212966919, tv_loss: 0.026594622060656548\n",
      "iteration 512, dc_loss: 0.40203526616096497, tv_loss: 0.026099154725670815\n",
      "iteration 513, dc_loss: 0.4009826183319092, tv_loss: 0.026675837114453316\n",
      "iteration 514, dc_loss: 0.399882435798645, tv_loss: 0.026063749566674232\n",
      "iteration 515, dc_loss: 0.3978135287761688, tv_loss: 0.02655290625989437\n",
      "iteration 516, dc_loss: 0.3963438868522644, tv_loss: 0.026280602440238\n",
      "iteration 517, dc_loss: 0.39519065618515015, tv_loss: 0.02634504809975624\n",
      "iteration 518, dc_loss: 0.3941737115383148, tv_loss: 0.0266144797205925\n",
      "iteration 519, dc_loss: 0.3936721980571747, tv_loss: 0.026394840329885483\n",
      "iteration 520, dc_loss: 0.3922503590583801, tv_loss: 0.02672063745558262\n",
      "iteration 521, dc_loss: 0.3911909759044647, tv_loss: 0.02629079669713974\n",
      "iteration 522, dc_loss: 0.3897171914577484, tv_loss: 0.026491042226552963\n",
      "iteration 523, dc_loss: 0.38863202929496765, tv_loss: 0.0266355462372303\n",
      "iteration 524, dc_loss: 0.3878951668739319, tv_loss: 0.026482634246349335\n",
      "iteration 525, dc_loss: 0.3863832354545593, tv_loss: 0.026720987632870674\n",
      "iteration 526, dc_loss: 0.3854460120201111, tv_loss: 0.02649090439081192\n",
      "iteration 527, dc_loss: 0.3844231963157654, tv_loss: 0.02655130997300148\n",
      "iteration 528, dc_loss: 0.3831233084201813, tv_loss: 0.02666938304901123\n",
      "iteration 529, dc_loss: 0.38221147656440735, tv_loss: 0.026630694046616554\n",
      "iteration 530, dc_loss: 0.3809702396392822, tv_loss: 0.026793764904141426\n",
      "iteration 531, dc_loss: 0.380215048789978, tv_loss: 0.02660899981856346\n",
      "iteration 532, dc_loss: 0.37953296303749084, tv_loss: 0.026713307946920395\n",
      "iteration 533, dc_loss: 0.3782319128513336, tv_loss: 0.026677383109927177\n",
      "iteration 534, dc_loss: 0.377165824174881, tv_loss: 0.026775602251291275\n",
      "iteration 535, dc_loss: 0.3760702610015869, tv_loss: 0.0267926137894392\n",
      "iteration 536, dc_loss: 0.37494492530822754, tv_loss: 0.026647470891475677\n",
      "iteration 537, dc_loss: 0.3736371099948883, tv_loss: 0.026793407276272774\n",
      "iteration 538, dc_loss: 0.3726887106895447, tv_loss: 0.026754392310976982\n",
      "iteration 539, dc_loss: 0.37164393067359924, tv_loss: 0.026880452409386635\n",
      "iteration 540, dc_loss: 0.37044665217399597, tv_loss: 0.026872264221310616\n",
      "iteration 541, dc_loss: 0.3694608509540558, tv_loss: 0.026729673147201538\n",
      "iteration 542, dc_loss: 0.36840882897377014, tv_loss: 0.02688797563314438\n",
      "iteration 543, dc_loss: 0.36746782064437866, tv_loss: 0.026825198903679848\n",
      "iteration 544, dc_loss: 0.3662552237510681, tv_loss: 0.02700350061058998\n",
      "iteration 545, dc_loss: 0.3652665615081787, tv_loss: 0.026838399469852448\n",
      "iteration 546, dc_loss: 0.3642449975013733, tv_loss: 0.026861025020480156\n",
      "iteration 547, dc_loss: 0.3632308542728424, tv_loss: 0.026854123920202255\n",
      "iteration 548, dc_loss: 0.3621310889720917, tv_loss: 0.02695683017373085\n",
      "iteration 549, dc_loss: 0.36111772060394287, tv_loss: 0.027092916890978813\n",
      "iteration 550, dc_loss: 0.36027243733406067, tv_loss: 0.026870297268033028\n",
      "iteration 551, dc_loss: 0.359116792678833, tv_loss: 0.027009539306163788\n",
      "iteration 552, dc_loss: 0.35822391510009766, tv_loss: 0.026950618252158165\n",
      "iteration 553, dc_loss: 0.3571333587169647, tv_loss: 0.027175256982445717\n",
      "iteration 554, dc_loss: 0.3561722934246063, tv_loss: 0.02698471024632454\n",
      "iteration 555, dc_loss: 0.35515859723091125, tv_loss: 0.027034439146518707\n",
      "iteration 556, dc_loss: 0.3542289137840271, tv_loss: 0.027184708043932915\n",
      "iteration 557, dc_loss: 0.3531634509563446, tv_loss: 0.027081839740276337\n",
      "iteration 558, dc_loss: 0.3522111177444458, tv_loss: 0.02703924849629402\n",
      "iteration 559, dc_loss: 0.35124480724334717, tv_loss: 0.02717350609600544\n",
      "iteration 560, dc_loss: 0.3503095507621765, tv_loss: 0.027175139635801315\n",
      "iteration 561, dc_loss: 0.3493741452693939, tv_loss: 0.02707969769835472\n",
      "iteration 562, dc_loss: 0.3483388423919678, tv_loss: 0.027124697342514992\n",
      "iteration 563, dc_loss: 0.3474823534488678, tv_loss: 0.027166780084371567\n",
      "iteration 564, dc_loss: 0.3464537262916565, tv_loss: 0.027308296412229538\n",
      "iteration 565, dc_loss: 0.3456484377384186, tv_loss: 0.02710050716996193\n",
      "iteration 566, dc_loss: 0.34464070200920105, tv_loss: 0.027258917689323425\n",
      "iteration 567, dc_loss: 0.34401801228523254, tv_loss: 0.02720453590154648\n",
      "iteration 568, dc_loss: 0.3430345952510834, tv_loss: 0.027454394847154617\n",
      "iteration 569, dc_loss: 0.3427124321460724, tv_loss: 0.02696683257818222\n",
      "iteration 570, dc_loss: 0.341776043176651, tv_loss: 0.02754131704568863\n",
      "iteration 571, dc_loss: 0.34162190556526184, tv_loss: 0.02697799913585186\n",
      "iteration 572, dc_loss: 0.3404136002063751, tv_loss: 0.027654819190502167\n",
      "iteration 573, dc_loss: 0.3392044007778168, tv_loss: 0.027047857642173767\n",
      "iteration 574, dc_loss: 0.33730626106262207, tv_loss: 0.02738766185939312\n",
      "iteration 575, dc_loss: 0.3363199830055237, tv_loss: 0.02737528458237648\n",
      "iteration 576, dc_loss: 0.3359260857105255, tv_loss: 0.02734256722033024\n",
      "iteration 577, dc_loss: 0.3349936604499817, tv_loss: 0.02753172628581524\n",
      "iteration 578, dc_loss: 0.33441346883773804, tv_loss: 0.02721365913748741\n",
      "iteration 579, dc_loss: 0.3330906629562378, tv_loss: 0.027748936787247658\n",
      "iteration 580, dc_loss: 0.33213385939598083, tv_loss: 0.027255438268184662\n",
      "iteration 581, dc_loss: 0.3311563730239868, tv_loss: 0.027495624497532845\n",
      "iteration 582, dc_loss: 0.3303031325340271, tv_loss: 0.027738995850086212\n",
      "iteration 583, dc_loss: 0.32991451025009155, tv_loss: 0.02724388614296913\n",
      "iteration 584, dc_loss: 0.3286750018596649, tv_loss: 0.02786232717335224\n",
      "iteration 585, dc_loss: 0.32764583826065063, tv_loss: 0.027487091720104218\n",
      "iteration 586, dc_loss: 0.32688087224960327, tv_loss: 0.027589675039052963\n",
      "iteration 587, dc_loss: 0.32590022683143616, tv_loss: 0.02762719802558422\n",
      "iteration 588, dc_loss: 0.32531291246414185, tv_loss: 0.027493786066770554\n",
      "iteration 589, dc_loss: 0.32433491945266724, tv_loss: 0.02794484980404377\n",
      "iteration 590, dc_loss: 0.3235139846801758, tv_loss: 0.02754129283130169\n",
      "iteration 591, dc_loss: 0.3226240277290344, tv_loss: 0.028034457936882973\n",
      "iteration 592, dc_loss: 0.3216607868671417, tv_loss: 0.027711614966392517\n",
      "iteration 593, dc_loss: 0.3210507035255432, tv_loss: 0.027897652238607407\n",
      "iteration 594, dc_loss: 0.3198989927768707, tv_loss: 0.027940580621361732\n",
      "iteration 595, dc_loss: 0.31941965222358704, tv_loss: 0.027849823236465454\n",
      "iteration 596, dc_loss: 0.3183761239051819, tv_loss: 0.02788774110376835\n",
      "iteration 597, dc_loss: 0.3178788721561432, tv_loss: 0.027825944125652313\n",
      "iteration 598, dc_loss: 0.31679192185401917, tv_loss: 0.027884406968951225\n",
      "iteration 599, dc_loss: 0.3159521222114563, tv_loss: 0.028045691549777985\n",
      "iteration 600, dc_loss: 0.31511837244033813, tv_loss: 0.02782599627971649\n",
      "iteration 601, dc_loss: 0.3142869472503662, tv_loss: 0.028010189533233643\n",
      "iteration 602, dc_loss: 0.3135391175746918, tv_loss: 0.02788301184773445\n",
      "iteration 603, dc_loss: 0.312786728143692, tv_loss: 0.028010640293359756\n",
      "iteration 604, dc_loss: 0.3119175136089325, tv_loss: 0.027834340929985046\n",
      "iteration 605, dc_loss: 0.31099629402160645, tv_loss: 0.028127970173954964\n",
      "iteration 606, dc_loss: 0.31030046939849854, tv_loss: 0.027881938964128494\n",
      "iteration 607, dc_loss: 0.3093760907649994, tv_loss: 0.02807587757706642\n",
      "iteration 608, dc_loss: 0.3086232841014862, tv_loss: 0.027897251769900322\n",
      "iteration 609, dc_loss: 0.3077886402606964, tv_loss: 0.028027448803186417\n",
      "iteration 610, dc_loss: 0.3069293200969696, tv_loss: 0.0279841385781765\n",
      "iteration 611, dc_loss: 0.30623993277549744, tv_loss: 0.028027644380927086\n",
      "iteration 612, dc_loss: 0.30535176396369934, tv_loss: 0.028032401576638222\n",
      "iteration 613, dc_loss: 0.3047749400138855, tv_loss: 0.02803060971200466\n",
      "iteration 614, dc_loss: 0.30384963750839233, tv_loss: 0.02809702418744564\n",
      "iteration 615, dc_loss: 0.3032224774360657, tv_loss: 0.028065921738743782\n",
      "iteration 616, dc_loss: 0.3023342490196228, tv_loss: 0.028112957254052162\n",
      "iteration 617, dc_loss: 0.3019043505191803, tv_loss: 0.027982940897345543\n",
      "iteration 618, dc_loss: 0.3009311258792877, tv_loss: 0.02822553552687168\n",
      "iteration 619, dc_loss: 0.30045196413993835, tv_loss: 0.028019951656460762\n",
      "iteration 620, dc_loss: 0.2993985116481781, tv_loss: 0.0282141100615263\n",
      "iteration 621, dc_loss: 0.2989117503166199, tv_loss: 0.028008930385112762\n",
      "iteration 622, dc_loss: 0.29787036776542664, tv_loss: 0.028215626254677773\n",
      "iteration 623, dc_loss: 0.29722875356674194, tv_loss: 0.028136886656284332\n",
      "iteration 624, dc_loss: 0.2963017225265503, tv_loss: 0.028158998116850853\n",
      "iteration 625, dc_loss: 0.2956126928329468, tv_loss: 0.028140773996710777\n",
      "iteration 626, dc_loss: 0.29471728205680847, tv_loss: 0.028198126703500748\n",
      "iteration 627, dc_loss: 0.2940222918987274, tv_loss: 0.028253108263015747\n",
      "iteration 628, dc_loss: 0.2933528423309326, tv_loss: 0.028131328523159027\n",
      "iteration 629, dc_loss: 0.2925567626953125, tv_loss: 0.02830437570810318\n",
      "iteration 630, dc_loss: 0.29196271300315857, tv_loss: 0.028117679059505463\n",
      "iteration 631, dc_loss: 0.2910870909690857, tv_loss: 0.028394171968102455\n",
      "iteration 632, dc_loss: 0.29059433937072754, tv_loss: 0.028103601187467575\n",
      "iteration 633, dc_loss: 0.28974711894989014, tv_loss: 0.028402676805853844\n",
      "iteration 634, dc_loss: 0.2893278896808624, tv_loss: 0.0281192846596241\n",
      "iteration 635, dc_loss: 0.28847944736480713, tv_loss: 0.02847752906382084\n",
      "iteration 636, dc_loss: 0.2880508303642273, tv_loss: 0.028095491230487823\n",
      "iteration 637, dc_loss: 0.2871134579181671, tv_loss: 0.028520073741674423\n",
      "iteration 638, dc_loss: 0.28674933314323425, tv_loss: 0.028100239112973213\n",
      "iteration 639, dc_loss: 0.28571248054504395, tv_loss: 0.02857161872088909\n",
      "iteration 640, dc_loss: 0.28526389598846436, tv_loss: 0.028134092688560486\n",
      "iteration 641, dc_loss: 0.2841964364051819, tv_loss: 0.028504984453320503\n",
      "iteration 642, dc_loss: 0.2836056351661682, tv_loss: 0.0282374806702137\n",
      "iteration 643, dc_loss: 0.28269556164741516, tv_loss: 0.028469445183873177\n",
      "iteration 644, dc_loss: 0.28204378485679626, tv_loss: 0.028327230364084244\n",
      "iteration 645, dc_loss: 0.2813778221607208, tv_loss: 0.028408421203494072\n",
      "iteration 646, dc_loss: 0.2806370258331299, tv_loss: 0.028407558798789978\n",
      "iteration 647, dc_loss: 0.28009486198425293, tv_loss: 0.028408624231815338\n",
      "iteration 648, dc_loss: 0.27931392192840576, tv_loss: 0.028463544324040413\n",
      "iteration 649, dc_loss: 0.27887141704559326, tv_loss: 0.02837917022407055\n",
      "iteration 650, dc_loss: 0.27800747752189636, tv_loss: 0.028524965047836304\n",
      "iteration 651, dc_loss: 0.277576208114624, tv_loss: 0.02841058000922203\n",
      "iteration 652, dc_loss: 0.27672672271728516, tv_loss: 0.028541522100567818\n",
      "iteration 653, dc_loss: 0.27625349164009094, tv_loss: 0.0284101739525795\n",
      "iteration 654, dc_loss: 0.2753753364086151, tv_loss: 0.028563618659973145\n",
      "iteration 655, dc_loss: 0.27484264969825745, tv_loss: 0.028485868126153946\n",
      "iteration 656, dc_loss: 0.27407360076904297, tv_loss: 0.028517715632915497\n",
      "iteration 657, dc_loss: 0.2735047936439514, tv_loss: 0.028514351695775986\n",
      "iteration 658, dc_loss: 0.2727492153644562, tv_loss: 0.02854406274855137\n",
      "iteration 659, dc_loss: 0.27211639285087585, tv_loss: 0.028575357049703598\n",
      "iteration 660, dc_loss: 0.27143752574920654, tv_loss: 0.0285059604793787\n",
      "iteration 661, dc_loss: 0.2707920968532562, tv_loss: 0.028625447303056717\n",
      "iteration 662, dc_loss: 0.27023300528526306, tv_loss: 0.02850424312055111\n",
      "iteration 663, dc_loss: 0.26953497529029846, tv_loss: 0.028705326840281487\n",
      "iteration 664, dc_loss: 0.2690806984901428, tv_loss: 0.02846376784145832\n",
      "iteration 665, dc_loss: 0.268292099237442, tv_loss: 0.028747908771038055\n",
      "iteration 666, dc_loss: 0.26792722940444946, tv_loss: 0.02847021073102951\n",
      "iteration 667, dc_loss: 0.2671424150466919, tv_loss: 0.028824135661125183\n",
      "iteration 668, dc_loss: 0.26700881123542786, tv_loss: 0.02841958776116371\n",
      "iteration 669, dc_loss: 0.26625627279281616, tv_loss: 0.02892334572970867\n",
      "iteration 670, dc_loss: 0.26641547679901123, tv_loss: 0.0283778328448534\n",
      "iteration 671, dc_loss: 0.2655583322048187, tv_loss: 0.029039563611149788\n",
      "iteration 672, dc_loss: 0.26532572507858276, tv_loss: 0.028376542031764984\n",
      "iteration 673, dc_loss: 0.26386505365371704, tv_loss: 0.028949396684765816\n",
      "iteration 674, dc_loss: 0.26333341002464294, tv_loss: 0.028544481843709946\n",
      "iteration 675, dc_loss: 0.2623281478881836, tv_loss: 0.028798338025808334\n",
      "iteration 676, dc_loss: 0.2618007957935333, tv_loss: 0.028694503009319305\n",
      "iteration 677, dc_loss: 0.2613445222377777, tv_loss: 0.028672978281974792\n",
      "iteration 678, dc_loss: 0.2606865465641022, tv_loss: 0.028876645490527153\n",
      "iteration 679, dc_loss: 0.2603514492511749, tv_loss: 0.028609471395611763\n",
      "iteration 680, dc_loss: 0.25944480299949646, tv_loss: 0.02885722555220127\n",
      "iteration 681, dc_loss: 0.2588151693344116, tv_loss: 0.028743872418999672\n",
      "iteration 682, dc_loss: 0.2581588923931122, tv_loss: 0.028741557151079178\n",
      "iteration 683, dc_loss: 0.2574392259120941, tv_loss: 0.028967928141355515\n",
      "iteration 684, dc_loss: 0.2573372721672058, tv_loss: 0.028582092374563217\n",
      "iteration 685, dc_loss: 0.2564304769039154, tv_loss: 0.028994828462600708\n",
      "iteration 686, dc_loss: 0.2560707628726959, tv_loss: 0.02867402881383896\n",
      "iteration 687, dc_loss: 0.25517329573631287, tv_loss: 0.028909046202898026\n",
      "iteration 688, dc_loss: 0.2546986937522888, tv_loss: 0.02871681936085224\n",
      "iteration 689, dc_loss: 0.2540479600429535, tv_loss: 0.028821248561143875\n",
      "iteration 690, dc_loss: 0.25352713465690613, tv_loss: 0.02887803316116333\n",
      "iteration 691, dc_loss: 0.2530914843082428, tv_loss: 0.028725609183311462\n",
      "iteration 692, dc_loss: 0.25246062874794006, tv_loss: 0.02890082076191902\n",
      "iteration 693, dc_loss: 0.25188854336738586, tv_loss: 0.02871660701930523\n",
      "iteration 694, dc_loss: 0.25121039152145386, tv_loss: 0.028941968455910683\n",
      "iteration 695, dc_loss: 0.25060829520225525, tv_loss: 0.028793832287192345\n",
      "iteration 696, dc_loss: 0.2501840889453888, tv_loss: 0.028794238343834877\n",
      "iteration 697, dc_loss: 0.2495085746049881, tv_loss: 0.028917884454131126\n",
      "iteration 698, dc_loss: 0.24914711713790894, tv_loss: 0.02878553606569767\n",
      "iteration 699, dc_loss: 0.24845106899738312, tv_loss: 0.028937174007296562\n",
      "iteration 700, dc_loss: 0.2480294555425644, tv_loss: 0.028821343556046486\n",
      "iteration 701, dc_loss: 0.24730537831783295, tv_loss: 0.028945783153176308\n",
      "iteration 702, dc_loss: 0.2469004988670349, tv_loss: 0.02887219563126564\n",
      "iteration 703, dc_loss: 0.24627700448036194, tv_loss: 0.028927216306328773\n",
      "iteration 704, dc_loss: 0.24580855667591095, tv_loss: 0.028894972056150436\n",
      "iteration 705, dc_loss: 0.2452470064163208, tv_loss: 0.028951212763786316\n",
      "iteration 706, dc_loss: 0.24474841356277466, tv_loss: 0.0288968775421381\n",
      "iteration 707, dc_loss: 0.24416673183441162, tv_loss: 0.028977351263165474\n",
      "iteration 708, dc_loss: 0.24371251463890076, tv_loss: 0.028910892084240913\n",
      "iteration 709, dc_loss: 0.2431574910879135, tv_loss: 0.028973422944545746\n",
      "iteration 710, dc_loss: 0.24269956350326538, tv_loss: 0.028916245326399803\n",
      "iteration 711, dc_loss: 0.24216750264167786, tv_loss: 0.029012111946940422\n",
      "iteration 712, dc_loss: 0.24174143373966217, tv_loss: 0.028931157663464546\n",
      "iteration 713, dc_loss: 0.24112056195735931, tv_loss: 0.029059557244181633\n",
      "iteration 714, dc_loss: 0.24080482125282288, tv_loss: 0.028884677216410637\n",
      "iteration 715, dc_loss: 0.24011605978012085, tv_loss: 0.029146049171686172\n",
      "iteration 716, dc_loss: 0.24001866579055786, tv_loss: 0.028855640441179276\n",
      "iteration 717, dc_loss: 0.23937909305095673, tv_loss: 0.02924511209130287\n",
      "iteration 718, dc_loss: 0.2395399808883667, tv_loss: 0.02880382165312767\n",
      "iteration 719, dc_loss: 0.2389449179172516, tv_loss: 0.029359400272369385\n",
      "iteration 720, dc_loss: 0.23917639255523682, tv_loss: 0.028736619278788567\n",
      "iteration 721, dc_loss: 0.2380785346031189, tv_loss: 0.02939334325492382\n",
      "iteration 722, dc_loss: 0.2375960350036621, tv_loss: 0.02883005142211914\n",
      "iteration 723, dc_loss: 0.23627693951129913, tv_loss: 0.02923290617763996\n",
      "iteration 724, dc_loss: 0.23575036227703094, tv_loss: 0.029081720858812332\n",
      "iteration 725, dc_loss: 0.2355266809463501, tv_loss: 0.02897581458091736\n",
      "iteration 726, dc_loss: 0.23496145009994507, tv_loss: 0.029327496886253357\n",
      "iteration 727, dc_loss: 0.23492582142353058, tv_loss: 0.028887949883937836\n",
      "iteration 728, dc_loss: 0.2338765561580658, tv_loss: 0.0293401088565588\n",
      "iteration 729, dc_loss: 0.23349477350711823, tv_loss: 0.029035938903689384\n",
      "iteration 730, dc_loss: 0.23285160958766937, tv_loss: 0.029162172228097916\n",
      "iteration 731, dc_loss: 0.23235978186130524, tv_loss: 0.02927195280790329\n",
      "iteration 732, dc_loss: 0.23219981789588928, tv_loss: 0.029049133881926537\n",
      "iteration 733, dc_loss: 0.23139537870883942, tv_loss: 0.029339630156755447\n",
      "iteration 734, dc_loss: 0.2310853898525238, tv_loss: 0.02911602519452572\n",
      "iteration 735, dc_loss: 0.2304777204990387, tv_loss: 0.029238266870379448\n",
      "iteration 736, dc_loss: 0.22999419271945953, tv_loss: 0.029293213039636612\n",
      "iteration 737, dc_loss: 0.2296541929244995, tv_loss: 0.029207386076450348\n",
      "iteration 738, dc_loss: 0.2291007936000824, tv_loss: 0.029321417212486267\n",
      "iteration 739, dc_loss: 0.22877100110054016, tv_loss: 0.029186412692070007\n",
      "iteration 740, dc_loss: 0.22814984619617462, tv_loss: 0.029311111196875572\n",
      "iteration 741, dc_loss: 0.2277219444513321, tv_loss: 0.029236797243356705\n",
      "iteration 742, dc_loss: 0.22721271216869354, tv_loss: 0.02933989278972149\n",
      "iteration 743, dc_loss: 0.22677619755268097, tv_loss: 0.02933092787861824\n",
      "iteration 744, dc_loss: 0.2264605164527893, tv_loss: 0.02925034984946251\n",
      "iteration 745, dc_loss: 0.22586660087108612, tv_loss: 0.029387662187218666\n",
      "iteration 746, dc_loss: 0.2255602777004242, tv_loss: 0.029280206188559532\n",
      "iteration 747, dc_loss: 0.224956676363945, tv_loss: 0.02940966747701168\n",
      "iteration 748, dc_loss: 0.22460444271564484, tv_loss: 0.029333049431443214\n",
      "iteration 749, dc_loss: 0.2241530865430832, tv_loss: 0.02934662438929081\n",
      "iteration 750, dc_loss: 0.22370870411396027, tv_loss: 0.029411710798740387\n",
      "iteration 751, dc_loss: 0.2233058661222458, tv_loss: 0.02933981828391552\n",
      "iteration 752, dc_loss: 0.22277912497520447, tv_loss: 0.02944408915936947\n",
      "iteration 753, dc_loss: 0.2224293351173401, tv_loss: 0.029358556494116783\n",
      "iteration 754, dc_loss: 0.2219908982515335, tv_loss: 0.029418103396892548\n",
      "iteration 755, dc_loss: 0.22158131003379822, tv_loss: 0.029392775148153305\n",
      "iteration 756, dc_loss: 0.22113168239593506, tv_loss: 0.029416879639029503\n",
      "iteration 757, dc_loss: 0.22071871161460876, tv_loss: 0.02945004589855671\n",
      "iteration 758, dc_loss: 0.22031445801258087, tv_loss: 0.02945994772017002\n",
      "iteration 759, dc_loss: 0.21987448632717133, tv_loss: 0.029485784471035004\n",
      "iteration 760, dc_loss: 0.21954578161239624, tv_loss: 0.029399218037724495\n",
      "iteration 761, dc_loss: 0.21909423172473907, tv_loss: 0.029495103284716606\n",
      "iteration 762, dc_loss: 0.21883293986320496, tv_loss: 0.02941063605248928\n",
      "iteration 763, dc_loss: 0.21831241250038147, tv_loss: 0.029614130035042763\n",
      "iteration 764, dc_loss: 0.2181306630373001, tv_loss: 0.029384354129433632\n",
      "iteration 765, dc_loss: 0.21748070418834686, tv_loss: 0.02963941916823387\n",
      "iteration 766, dc_loss: 0.21735379099845886, tv_loss: 0.029360605403780937\n",
      "iteration 767, dc_loss: 0.2167094647884369, tv_loss: 0.02966468222439289\n",
      "iteration 768, dc_loss: 0.2167392075061798, tv_loss: 0.029340840876102448\n",
      "iteration 769, dc_loss: 0.21620367467403412, tv_loss: 0.029778432101011276\n",
      "iteration 770, dc_loss: 0.2162625640630722, tv_loss: 0.029361432418227196\n",
      "iteration 771, dc_loss: 0.21551963686943054, tv_loss: 0.029783129692077637\n",
      "iteration 772, dc_loss: 0.2152750939130783, tv_loss: 0.029357071965932846\n",
      "iteration 773, dc_loss: 0.21437977254390717, tv_loss: 0.02971513196825981\n",
      "iteration 774, dc_loss: 0.21412600576877594, tv_loss: 0.029495136812329292\n",
      "iteration 775, dc_loss: 0.21356794238090515, tv_loss: 0.02967221476137638\n",
      "iteration 776, dc_loss: 0.2132166475057602, tv_loss: 0.029603848233819008\n",
      "iteration 777, dc_loss: 0.21277455985546112, tv_loss: 0.029563290998339653\n",
      "iteration 778, dc_loss: 0.21228493750095367, tv_loss: 0.029661990702152252\n",
      "iteration 779, dc_loss: 0.2120824009180069, tv_loss: 0.029529908671975136\n",
      "iteration 780, dc_loss: 0.21153146028518677, tv_loss: 0.02974162995815277\n",
      "iteration 781, dc_loss: 0.21134357154369354, tv_loss: 0.02950403466820717\n",
      "iteration 782, dc_loss: 0.21068644523620605, tv_loss: 0.02975919470191002\n",
      "iteration 783, dc_loss: 0.21052560210227966, tv_loss: 0.02953881397843361\n",
      "iteration 784, dc_loss: 0.21001151204109192, tv_loss: 0.02971985749900341\n",
      "iteration 785, dc_loss: 0.20977555215358734, tv_loss: 0.029595257714390755\n",
      "iteration 786, dc_loss: 0.2092641144990921, tv_loss: 0.029699435457587242\n",
      "iteration 787, dc_loss: 0.208860844373703, tv_loss: 0.029698368161916733\n",
      "iteration 788, dc_loss: 0.2084457278251648, tv_loss: 0.029759466648101807\n",
      "iteration 789, dc_loss: 0.20808054506778717, tv_loss: 0.02972257509827614\n",
      "iteration 790, dc_loss: 0.20775164663791656, tv_loss: 0.02967754378914833\n",
      "iteration 791, dc_loss: 0.20733146369457245, tv_loss: 0.029737962409853935\n",
      "iteration 792, dc_loss: 0.20706132054328918, tv_loss: 0.029673507437109947\n",
      "iteration 793, dc_loss: 0.2065763622522354, tv_loss: 0.029832474887371063\n",
      "iteration 794, dc_loss: 0.2064495086669922, tv_loss: 0.029639223590493202\n",
      "iteration 795, dc_loss: 0.20596536993980408, tv_loss: 0.02985144779086113\n",
      "iteration 796, dc_loss: 0.20593738555908203, tv_loss: 0.029599348083138466\n",
      "iteration 797, dc_loss: 0.20538417994976044, tv_loss: 0.029920315369963646\n",
      "iteration 798, dc_loss: 0.20545241236686707, tv_loss: 0.029600799083709717\n",
      "iteration 799, dc_loss: 0.20482099056243896, tv_loss: 0.03005344048142433\n",
      "iteration 800, dc_loss: 0.2048979252576828, tv_loss: 0.029576243832707405\n",
      "iteration 801, dc_loss: 0.2040814906358719, tv_loss: 0.030026236549019814\n",
      "iteration 802, dc_loss: 0.20365101099014282, tv_loss: 0.029690949246287346\n",
      "iteration 803, dc_loss: 0.20339736342430115, tv_loss: 0.02970431186258793\n",
      "iteration 804, dc_loss: 0.2031193971633911, tv_loss: 0.029990587383508682\n",
      "iteration 805, dc_loss: 0.2028283029794693, tv_loss: 0.029709987342357635\n",
      "iteration 806, dc_loss: 0.20246946811676025, tv_loss: 0.029770467430353165\n",
      "iteration 807, dc_loss: 0.2022000104188919, tv_loss: 0.029953403398394585\n",
      "iteration 808, dc_loss: 0.20201274752616882, tv_loss: 0.0297379270195961\n",
      "iteration 809, dc_loss: 0.2015708088874817, tv_loss: 0.029830103740096092\n",
      "iteration 810, dc_loss: 0.20130741596221924, tv_loss: 0.02993830293416977\n",
      "iteration 811, dc_loss: 0.20117270946502686, tv_loss: 0.029777392745018005\n",
      "iteration 812, dc_loss: 0.2007245570421219, tv_loss: 0.02986142411828041\n",
      "iteration 813, dc_loss: 0.20047079026699066, tv_loss: 0.02991037629544735\n",
      "iteration 814, dc_loss: 0.2003544569015503, tv_loss: 0.02978767268359661\n",
      "iteration 815, dc_loss: 0.19988736510276794, tv_loss: 0.02993014268577099\n",
      "iteration 816, dc_loss: 0.19965408742427826, tv_loss: 0.029929302632808685\n",
      "iteration 817, dc_loss: 0.19950510561466217, tv_loss: 0.029835674911737442\n",
      "iteration 818, dc_loss: 0.1990990787744522, tv_loss: 0.029956011101603508\n",
      "iteration 819, dc_loss: 0.1988617330789566, tv_loss: 0.029918024316430092\n",
      "iteration 820, dc_loss: 0.19868703186511993, tv_loss: 0.02984943985939026\n",
      "iteration 821, dc_loss: 0.19829247891902924, tv_loss: 0.030015863478183746\n",
      "iteration 822, dc_loss: 0.19810166954994202, tv_loss: 0.02992040477693081\n",
      "iteration 823, dc_loss: 0.19786182045936584, tv_loss: 0.029911333695054054\n",
      "iteration 824, dc_loss: 0.19752010703086853, tv_loss: 0.029983634129166603\n",
      "iteration 825, dc_loss: 0.19732746481895447, tv_loss: 0.029934758320450783\n",
      "iteration 826, dc_loss: 0.19704194366931915, tv_loss: 0.029917405918240547\n",
      "iteration 827, dc_loss: 0.1967596560716629, tv_loss: 0.02997581474483013\n",
      "iteration 828, dc_loss: 0.19657982885837555, tv_loss: 0.02992687188088894\n",
      "iteration 829, dc_loss: 0.19623418152332306, tv_loss: 0.02998100221157074\n",
      "iteration 830, dc_loss: 0.19599859416484833, tv_loss: 0.02997584268450737\n",
      "iteration 831, dc_loss: 0.19580262899398804, tv_loss: 0.02994040586054325\n",
      "iteration 832, dc_loss: 0.19547809660434723, tv_loss: 0.03000485524535179\n",
      "iteration 833, dc_loss: 0.19526490569114685, tv_loss: 0.02997981198132038\n",
      "iteration 834, dc_loss: 0.19503487646579742, tv_loss: 0.030024634674191475\n",
      "iteration 835, dc_loss: 0.19471819698810577, tv_loss: 0.030089808627963066\n",
      "iteration 836, dc_loss: 0.1945178359746933, tv_loss: 0.029975280165672302\n",
      "iteration 837, dc_loss: 0.19425886869430542, tv_loss: 0.029995888471603394\n",
      "iteration 838, dc_loss: 0.19397424161434174, tv_loss: 0.030085546895861626\n",
      "iteration 839, dc_loss: 0.19379326701164246, tv_loss: 0.030024459585547447\n",
      "iteration 840, dc_loss: 0.1935022920370102, tv_loss: 0.030065009370446205\n",
      "iteration 841, dc_loss: 0.1932675689458847, tv_loss: 0.030035926029086113\n",
      "iteration 842, dc_loss: 0.19302316009998322, tv_loss: 0.03004472889006138\n",
      "iteration 843, dc_loss: 0.19276180863380432, tv_loss: 0.030074890702962875\n",
      "iteration 844, dc_loss: 0.1925458461046219, tv_loss: 0.03003392182290554\n",
      "iteration 845, dc_loss: 0.1923009753227234, tv_loss: 0.030038785189390182\n",
      "iteration 846, dc_loss: 0.19202634692192078, tv_loss: 0.03007683716714382\n",
      "iteration 847, dc_loss: 0.1918320655822754, tv_loss: 0.0300621185451746\n",
      "iteration 848, dc_loss: 0.19154883921146393, tv_loss: 0.030140673741698265\n",
      "iteration 849, dc_loss: 0.1913410723209381, tv_loss: 0.030115459114313126\n",
      "iteration 850, dc_loss: 0.19109855592250824, tv_loss: 0.030081847682595253\n",
      "iteration 851, dc_loss: 0.19083279371261597, tv_loss: 0.030109860002994537\n",
      "iteration 852, dc_loss: 0.19062770903110504, tv_loss: 0.03008853644132614\n",
      "iteration 853, dc_loss: 0.1903679072856903, tv_loss: 0.030105585232377052\n",
      "iteration 854, dc_loss: 0.19013965129852295, tv_loss: 0.03011184185743332\n",
      "iteration 855, dc_loss: 0.18990856409072876, tv_loss: 0.030146632343530655\n",
      "iteration 856, dc_loss: 0.18968968093395233, tv_loss: 0.0301821231842041\n",
      "iteration 857, dc_loss: 0.18944096565246582, tv_loss: 0.030150560662150383\n",
      "iteration 858, dc_loss: 0.18921704590320587, tv_loss: 0.03012208640575409\n",
      "iteration 859, dc_loss: 0.1889617145061493, tv_loss: 0.03015952929854393\n",
      "iteration 860, dc_loss: 0.18875111639499664, tv_loss: 0.030143311247229576\n",
      "iteration 861, dc_loss: 0.18851354718208313, tv_loss: 0.030149297788739204\n",
      "iteration 862, dc_loss: 0.18828652799129486, tv_loss: 0.030162585899233818\n",
      "iteration 863, dc_loss: 0.18806901574134827, tv_loss: 0.03020314872264862\n",
      "iteration 864, dc_loss: 0.18781903386116028, tv_loss: 0.030229708179831505\n",
      "iteration 865, dc_loss: 0.18761219084262848, tv_loss: 0.030175942927598953\n",
      "iteration 866, dc_loss: 0.18736723065376282, tv_loss: 0.03019336611032486\n",
      "iteration 867, dc_loss: 0.18716168403625488, tv_loss: 0.03017488121986389\n",
      "iteration 868, dc_loss: 0.18694353103637695, tv_loss: 0.030181128531694412\n",
      "iteration 869, dc_loss: 0.1866801530122757, tv_loss: 0.03030143864452839\n",
      "iteration 870, dc_loss: 0.1865013688802719, tv_loss: 0.03021552786231041\n",
      "iteration 871, dc_loss: 0.18624833226203918, tv_loss: 0.030226968228816986\n",
      "iteration 872, dc_loss: 0.18605421483516693, tv_loss: 0.030231870710849762\n",
      "iteration 873, dc_loss: 0.18581977486610413, tv_loss: 0.030306361615657806\n",
      "iteration 874, dc_loss: 0.18556062877178192, tv_loss: 0.03026478923857212\n",
      "iteration 875, dc_loss: 0.1853940337896347, tv_loss: 0.030232343822717667\n",
      "iteration 876, dc_loss: 0.18515343964099884, tv_loss: 0.030287226662039757\n",
      "iteration 877, dc_loss: 0.18493276834487915, tv_loss: 0.030323738232254982\n",
      "iteration 878, dc_loss: 0.18473504483699799, tv_loss: 0.030232425779104233\n",
      "iteration 879, dc_loss: 0.18449734151363373, tv_loss: 0.030334601178765297\n",
      "iteration 880, dc_loss: 0.1842873990535736, tv_loss: 0.030280444771051407\n",
      "iteration 881, dc_loss: 0.18407317996025085, tv_loss: 0.03031039610505104\n",
      "iteration 882, dc_loss: 0.18385276198387146, tv_loss: 0.03030042164027691\n",
      "iteration 883, dc_loss: 0.18365919589996338, tv_loss: 0.030371075496077538\n",
      "iteration 884, dc_loss: 0.1834266483783722, tv_loss: 0.030329624190926552\n",
      "iteration 885, dc_loss: 0.18323595821857452, tv_loss: 0.03033045493066311\n",
      "iteration 886, dc_loss: 0.18300320208072662, tv_loss: 0.030400477349758148\n",
      "iteration 887, dc_loss: 0.1828029900789261, tv_loss: 0.030288904905319214\n",
      "iteration 888, dc_loss: 0.18258284032344818, tv_loss: 0.030430955812335014\n",
      "iteration 889, dc_loss: 0.1823524385690689, tv_loss: 0.030374042689800262\n",
      "iteration 890, dc_loss: 0.18218322098255157, tv_loss: 0.030403079465031624\n",
      "iteration 891, dc_loss: 0.18194548785686493, tv_loss: 0.030410796403884888\n",
      "iteration 892, dc_loss: 0.18178535997867584, tv_loss: 0.03036242164671421\n",
      "iteration 893, dc_loss: 0.18153482675552368, tv_loss: 0.030395036563277245\n",
      "iteration 894, dc_loss: 0.1813146024942398, tv_loss: 0.030428601428866386\n",
      "iteration 895, dc_loss: 0.18112340569496155, tv_loss: 0.03041602484881878\n",
      "iteration 896, dc_loss: 0.18094412982463837, tv_loss: 0.030458111315965652\n",
      "iteration 897, dc_loss: 0.18074004352092743, tv_loss: 0.03035796619951725\n",
      "iteration 898, dc_loss: 0.1805051565170288, tv_loss: 0.030504172667860985\n",
      "iteration 899, dc_loss: 0.18028096854686737, tv_loss: 0.03047833777964115\n",
      "iteration 900, dc_loss: 0.18014980852603912, tv_loss: 0.03042491525411606\n",
      "iteration 901, dc_loss: 0.17993633449077606, tv_loss: 0.030401980504393578\n",
      "iteration 902, dc_loss: 0.17970241606235504, tv_loss: 0.03047172538936138\n",
      "iteration 903, dc_loss: 0.17947816848754883, tv_loss: 0.030446652323007584\n",
      "iteration 904, dc_loss: 0.17934276163578033, tv_loss: 0.030470220372080803\n",
      "iteration 905, dc_loss: 0.17910456657409668, tv_loss: 0.03043762780725956\n",
      "iteration 906, dc_loss: 0.17889505624771118, tv_loss: 0.030542641878128052\n",
      "iteration 907, dc_loss: 0.17868271470069885, tv_loss: 0.03048553876578808\n",
      "iteration 908, dc_loss: 0.17855946719646454, tv_loss: 0.030502278357744217\n",
      "iteration 909, dc_loss: 0.178331658244133, tv_loss: 0.03047262318432331\n",
      "iteration 910, dc_loss: 0.17809134721755981, tv_loss: 0.03057103604078293\n",
      "iteration 911, dc_loss: 0.177887961268425, tv_loss: 0.0305547546595335\n",
      "iteration 912, dc_loss: 0.17775565385818481, tv_loss: 0.030482428148388863\n",
      "iteration 913, dc_loss: 0.17756353318691254, tv_loss: 0.03049699030816555\n",
      "iteration 914, dc_loss: 0.17733106017112732, tv_loss: 0.030490491539239883\n",
      "iteration 915, dc_loss: 0.1771192103624344, tv_loss: 0.03054964914917946\n",
      "iteration 916, dc_loss: 0.17693285644054413, tv_loss: 0.030475899577140808\n",
      "iteration 917, dc_loss: 0.1767749935388565, tv_loss: 0.030524391680955887\n",
      "iteration 918, dc_loss: 0.17658551037311554, tv_loss: 0.03046496957540512\n",
      "iteration 919, dc_loss: 0.17633600533008575, tv_loss: 0.030506525188684464\n",
      "iteration 920, dc_loss: 0.17618682980537415, tv_loss: 0.03050791658461094\n",
      "iteration 921, dc_loss: 0.17598223686218262, tv_loss: 0.03052385523915291\n",
      "iteration 922, dc_loss: 0.17579978704452515, tv_loss: 0.030504239723086357\n",
      "iteration 923, dc_loss: 0.17560110986232758, tv_loss: 0.030503107234835625\n",
      "iteration 924, dc_loss: 0.17543604969978333, tv_loss: 0.030559992417693138\n",
      "iteration 925, dc_loss: 0.17522743344306946, tv_loss: 0.030533811077475548\n",
      "iteration 926, dc_loss: 0.17503944039344788, tv_loss: 0.030542880296707153\n",
      "iteration 927, dc_loss: 0.17482781410217285, tv_loss: 0.030586205422878265\n",
      "iteration 928, dc_loss: 0.17470811307430267, tv_loss: 0.030543534085154533\n",
      "iteration 929, dc_loss: 0.17449133098125458, tv_loss: 0.030512915924191475\n",
      "iteration 930, dc_loss: 0.17430272698402405, tv_loss: 0.030605128034949303\n",
      "iteration 931, dc_loss: 0.1741146445274353, tv_loss: 0.030587784945964813\n",
      "iteration 932, dc_loss: 0.17400434613227844, tv_loss: 0.030540119856595993\n",
      "iteration 933, dc_loss: 0.17384101450443268, tv_loss: 0.030572550371289253\n",
      "iteration 934, dc_loss: 0.17372450232505798, tv_loss: 0.030650269240140915\n",
      "iteration 935, dc_loss: 0.17368605732917786, tv_loss: 0.030565669760107994\n",
      "iteration 936, dc_loss: 0.17358362674713135, tv_loss: 0.030645102262496948\n",
      "iteration 937, dc_loss: 0.17353036999702454, tv_loss: 0.03053668513894081\n",
      "iteration 938, dc_loss: 0.1731431782245636, tv_loss: 0.03071954846382141\n",
      "iteration 939, dc_loss: 0.17316897213459015, tv_loss: 0.030488746240735054\n",
      "iteration 940, dc_loss: 0.17281058430671692, tv_loss: 0.03078582137823105\n",
      "iteration 941, dc_loss: 0.17298728227615356, tv_loss: 0.03040725365281105\n",
      "iteration 942, dc_loss: 0.1723974496126175, tv_loss: 0.03082290105521679\n",
      "iteration 943, dc_loss: 0.1723187267780304, tv_loss: 0.030517973005771637\n",
      "iteration 944, dc_loss: 0.17200183868408203, tv_loss: 0.030640024691820145\n",
      "iteration 945, dc_loss: 0.17196355760097504, tv_loss: 0.030659442767500877\n",
      "iteration 946, dc_loss: 0.17206203937530518, tv_loss: 0.030548086389899254\n",
      "iteration 947, dc_loss: 0.17199859023094177, tv_loss: 0.030755897983908653\n",
      "iteration 948, dc_loss: 0.171782985329628, tv_loss: 0.03060351312160492\n",
      "iteration 949, dc_loss: 0.17152024805545807, tv_loss: 0.030670100823044777\n",
      "iteration 950, dc_loss: 0.17134439945220947, tv_loss: 0.030726054683327675\n",
      "iteration 951, dc_loss: 0.1711716502904892, tv_loss: 0.030661454424262047\n",
      "iteration 952, dc_loss: 0.17099477350711823, tv_loss: 0.030601253733038902\n",
      "iteration 953, dc_loss: 0.1705760955810547, tv_loss: 0.030738992616534233\n",
      "iteration 954, dc_loss: 0.17049485445022583, tv_loss: 0.030659455806016922\n",
      "iteration 955, dc_loss: 0.170259490609169, tv_loss: 0.030691616237163544\n",
      "iteration 956, dc_loss: 0.17005375027656555, tv_loss: 0.030685847625136375\n",
      "iteration 957, dc_loss: 0.16990944743156433, tv_loss: 0.030601758509874344\n",
      "iteration 958, dc_loss: 0.16962414979934692, tv_loss: 0.030773531645536423\n",
      "iteration 959, dc_loss: 0.16957536339759827, tv_loss: 0.0305885449051857\n",
      "iteration 960, dc_loss: 0.16923777759075165, tv_loss: 0.030752386897802353\n",
      "iteration 961, dc_loss: 0.16911832988262177, tv_loss: 0.030646774917840958\n",
      "iteration 962, dc_loss: 0.16889874637126923, tv_loss: 0.03072192147374153\n",
      "iteration 963, dc_loss: 0.16871480643749237, tv_loss: 0.030700240284204483\n",
      "iteration 964, dc_loss: 0.16856692731380463, tv_loss: 0.030663853511214256\n",
      "iteration 965, dc_loss: 0.1683378964662552, tv_loss: 0.030679475516080856\n",
      "iteration 966, dc_loss: 0.1681816428899765, tv_loss: 0.030735492706298828\n",
      "iteration 967, dc_loss: 0.16797704994678497, tv_loss: 0.03073381818830967\n",
      "iteration 968, dc_loss: 0.16786184906959534, tv_loss: 0.030698981136083603\n",
      "iteration 969, dc_loss: 0.16761845350265503, tv_loss: 0.03075418807566166\n",
      "iteration 970, dc_loss: 0.16757480800151825, tv_loss: 0.030686968937516212\n",
      "iteration 971, dc_loss: 0.16725821793079376, tv_loss: 0.030796557664871216\n",
      "iteration 972, dc_loss: 0.16722825169563293, tv_loss: 0.03067593090236187\n",
      "iteration 973, dc_loss: 0.1669534295797348, tv_loss: 0.030747950077056885\n",
      "iteration 974, dc_loss: 0.16681614518165588, tv_loss: 0.030760271474719048\n",
      "iteration 975, dc_loss: 0.16661134362220764, tv_loss: 0.030764490365982056\n",
      "iteration 976, dc_loss: 0.16651411354541779, tv_loss: 0.030714591965079308\n",
      "iteration 977, dc_loss: 0.16627639532089233, tv_loss: 0.030758563429117203\n",
      "iteration 978, dc_loss: 0.1661357581615448, tv_loss: 0.030774027109146118\n",
      "iteration 979, dc_loss: 0.1659543514251709, tv_loss: 0.030766863375902176\n",
      "iteration 980, dc_loss: 0.1658395677804947, tv_loss: 0.0307467021048069\n",
      "iteration 981, dc_loss: 0.16566136479377747, tv_loss: 0.030737657099962234\n",
      "iteration 982, dc_loss: 0.16544613242149353, tv_loss: 0.03083585388958454\n",
      "iteration 983, dc_loss: 0.16535653173923492, tv_loss: 0.030747583135962486\n",
      "iteration 984, dc_loss: 0.1651555299758911, tv_loss: 0.030806677415966988\n",
      "iteration 985, dc_loss: 0.16506914794445038, tv_loss: 0.03072272427380085\n",
      "iteration 986, dc_loss: 0.16481183469295502, tv_loss: 0.030849065631628036\n",
      "iteration 987, dc_loss: 0.16476154327392578, tv_loss: 0.030738288536667824\n",
      "iteration 988, dc_loss: 0.1645536720752716, tv_loss: 0.030830882489681244\n",
      "iteration 989, dc_loss: 0.16451044380664825, tv_loss: 0.03071841225028038\n",
      "iteration 990, dc_loss: 0.164220929145813, tv_loss: 0.030927814543247223\n",
      "iteration 991, dc_loss: 0.16434024274349213, tv_loss: 0.030707022175192833\n",
      "iteration 992, dc_loss: 0.16404975950717926, tv_loss: 0.03097079135477543\n",
      "iteration 993, dc_loss: 0.16430747509002686, tv_loss: 0.030689924955368042\n",
      "iteration 994, dc_loss: 0.16391536593437195, tv_loss: 0.03103034943342209\n",
      "iteration 995, dc_loss: 0.1642693728208542, tv_loss: 0.03061722032725811\n",
      "iteration 996, dc_loss: 0.163801372051239, tv_loss: 0.03108050674200058\n",
      "iteration 997, dc_loss: 0.1639711707830429, tv_loss: 0.030645912513136864\n",
      "iteration 998, dc_loss: 0.16324861347675323, tv_loss: 0.031053299084305763\n",
      "iteration 999, dc_loss: 0.16310854256153107, tv_loss: 0.030737027525901794\n",
      "iteration 1000, dc_loss: 0.16272439062595367, tv_loss: 0.030852889642119408\n",
      "iteration 1001, dc_loss: 0.1625576615333557, tv_loss: 0.030885230749845505\n",
      "iteration 1002, dc_loss: 0.1626649796962738, tv_loss: 0.0307364072650671\n",
      "iteration 1003, dc_loss: 0.16241800785064697, tv_loss: 0.03098905459046364\n",
      "iteration 1004, dc_loss: 0.1625223308801651, tv_loss: 0.03076285310089588\n",
      "iteration 1005, dc_loss: 0.1620520055294037, tv_loss: 0.03099961206316948\n",
      "iteration 1006, dc_loss: 0.161963552236557, tv_loss: 0.030805818736553192\n",
      "iteration 1007, dc_loss: 0.16169202327728271, tv_loss: 0.03087960183620453\n",
      "iteration 1008, dc_loss: 0.16150765120983124, tv_loss: 0.030969342216849327\n",
      "iteration 1009, dc_loss: 0.16159428656101227, tv_loss: 0.03078043833374977\n",
      "iteration 1010, dc_loss: 0.16131766140460968, tv_loss: 0.03102514147758484\n",
      "iteration 1011, dc_loss: 0.16143196821212769, tv_loss: 0.03072914108633995\n",
      "iteration 1012, dc_loss: 0.16096574068069458, tv_loss: 0.03099946677684784\n",
      "iteration 1013, dc_loss: 0.16086633503437042, tv_loss: 0.030943602323532104\n",
      "iteration 1014, dc_loss: 0.160663902759552, tv_loss: 0.0309520922601223\n",
      "iteration 1015, dc_loss: 0.16049370169639587, tv_loss: 0.03094896674156189\n",
      "iteration 1016, dc_loss: 0.16056112945079803, tv_loss: 0.03092622198164463\n",
      "iteration 1017, dc_loss: 0.16023440659046173, tv_loss: 0.031041275709867477\n",
      "iteration 1018, dc_loss: 0.1602049171924591, tv_loss: 0.030905110761523247\n",
      "iteration 1019, dc_loss: 0.1599491536617279, tv_loss: 0.031068699434399605\n",
      "iteration 1020, dc_loss: 0.15982089936733246, tv_loss: 0.0309299249202013\n",
      "iteration 1021, dc_loss: 0.15968088805675507, tv_loss: 0.030952293425798416\n",
      "iteration 1022, dc_loss: 0.15948019921779633, tv_loss: 0.031117353588342667\n",
      "iteration 1023, dc_loss: 0.15947365760803223, tv_loss: 0.030889842659235\n",
      "iteration 1024, dc_loss: 0.15923190116882324, tv_loss: 0.031102461740374565\n",
      "iteration 1025, dc_loss: 0.15921509265899658, tv_loss: 0.03094968944787979\n",
      "iteration 1026, dc_loss: 0.15894533693790436, tv_loss: 0.031018376350402832\n",
      "iteration 1027, dc_loss: 0.1588490605354309, tv_loss: 0.031085427850484848\n",
      "iteration 1028, dc_loss: 0.15865828096866608, tv_loss: 0.031009087339043617\n",
      "iteration 1029, dc_loss: 0.15856443345546722, tv_loss: 0.03110821731388569\n",
      "iteration 1030, dc_loss: 0.15846037864685059, tv_loss: 0.03095887042582035\n",
      "iteration 1031, dc_loss: 0.1582178771495819, tv_loss: 0.031171578913927078\n",
      "iteration 1032, dc_loss: 0.15815624594688416, tv_loss: 0.03105224296450615\n",
      "iteration 1033, dc_loss: 0.157980814576149, tv_loss: 0.03108675591647625\n",
      "iteration 1034, dc_loss: 0.15797100961208344, tv_loss: 0.031063558533787727\n",
      "iteration 1035, dc_loss: 0.1577185094356537, tv_loss: 0.031080586835741997\n",
      "iteration 1036, dc_loss: 0.15762804448604584, tv_loss: 0.031123215332627296\n",
      "iteration 1037, dc_loss: 0.15742915868759155, tv_loss: 0.031116699799895287\n",
      "iteration 1038, dc_loss: 0.15735812485218048, tv_loss: 0.031119292601943016\n",
      "iteration 1039, dc_loss: 0.1572079211473465, tv_loss: 0.0310894176363945\n",
      "iteration 1040, dc_loss: 0.15710437297821045, tv_loss: 0.031065842136740685\n",
      "iteration 1041, dc_loss: 0.1569504290819168, tv_loss: 0.031156696379184723\n",
      "iteration 1042, dc_loss: 0.15679579973220825, tv_loss: 0.031101422384381294\n",
      "iteration 1043, dc_loss: 0.15671944618225098, tv_loss: 0.031143758445978165\n",
      "iteration 1044, dc_loss: 0.15662391483783722, tv_loss: 0.03103303536772728\n",
      "iteration 1045, dc_loss: 0.15641680359840393, tv_loss: 0.03116917796432972\n",
      "iteration 1046, dc_loss: 0.15632610023021698, tv_loss: 0.031120648607611656\n",
      "iteration 1047, dc_loss: 0.15611740946769714, tv_loss: 0.031169166788458824\n",
      "iteration 1048, dc_loss: 0.15618067979812622, tv_loss: 0.03107469715178013\n",
      "iteration 1049, dc_loss: 0.1558753401041031, tv_loss: 0.03120231255888939\n",
      "iteration 1050, dc_loss: 0.15596479177474976, tv_loss: 0.031076373532414436\n",
      "iteration 1051, dc_loss: 0.1556311547756195, tv_loss: 0.031270187348127365\n",
      "iteration 1052, dc_loss: 0.1558283418416977, tv_loss: 0.031029097735881805\n",
      "iteration 1053, dc_loss: 0.15549704432487488, tv_loss: 0.0312645249068737\n",
      "iteration 1054, dc_loss: 0.15568728744983673, tv_loss: 0.030945276841521263\n",
      "iteration 1055, dc_loss: 0.1552484929561615, tv_loss: 0.03132934123277664\n",
      "iteration 1056, dc_loss: 0.1553374081850052, tv_loss: 0.030952302739024162\n",
      "iteration 1057, dc_loss: 0.15495184063911438, tv_loss: 0.03130418062210083\n",
      "iteration 1058, dc_loss: 0.15492703020572662, tv_loss: 0.03103032521903515\n",
      "iteration 1059, dc_loss: 0.15465444326400757, tv_loss: 0.03116236813366413\n",
      "iteration 1060, dc_loss: 0.15451103448867798, tv_loss: 0.03119230829179287\n",
      "iteration 1061, dc_loss: 0.1544765830039978, tv_loss: 0.031065139919519424\n",
      "iteration 1062, dc_loss: 0.15426042675971985, tv_loss: 0.03134847432374954\n",
      "iteration 1063, dc_loss: 0.15439411997795105, tv_loss: 0.031014492735266685\n",
      "iteration 1064, dc_loss: 0.15406493842601776, tv_loss: 0.03140341490507126\n",
      "iteration 1065, dc_loss: 0.1541437953710556, tv_loss: 0.03107628785073757\n",
      "iteration 1066, dc_loss: 0.15374897420406342, tv_loss: 0.03141644597053528\n",
      "iteration 1067, dc_loss: 0.15386736392974854, tv_loss: 0.03099382109940052\n",
      "iteration 1068, dc_loss: 0.15352651476860046, tv_loss: 0.03131292015314102\n",
      "iteration 1069, dc_loss: 0.15345507860183716, tv_loss: 0.031108934432268143\n",
      "iteration 1070, dc_loss: 0.15324105322360992, tv_loss: 0.031246325001120567\n",
      "iteration 1071, dc_loss: 0.15317940711975098, tv_loss: 0.031181594356894493\n",
      "iteration 1072, dc_loss: 0.15304400026798248, tv_loss: 0.031113652512431145\n",
      "iteration 1073, dc_loss: 0.15285871922969818, tv_loss: 0.031355198472738266\n",
      "iteration 1074, dc_loss: 0.15279506146907806, tv_loss: 0.031155480071902275\n",
      "iteration 1075, dc_loss: 0.15265120565891266, tv_loss: 0.031336650252342224\n",
      "iteration 1076, dc_loss: 0.15254636108875275, tv_loss: 0.031161010265350342\n",
      "iteration 1077, dc_loss: 0.15236589312553406, tv_loss: 0.031370725482702255\n",
      "iteration 1078, dc_loss: 0.1522616147994995, tv_loss: 0.031166374683380127\n",
      "iteration 1079, dc_loss: 0.15213386714458466, tv_loss: 0.03136064112186432\n",
      "iteration 1080, dc_loss: 0.15203014016151428, tv_loss: 0.03115462325513363\n",
      "iteration 1081, dc_loss: 0.15192654728889465, tv_loss: 0.0314064621925354\n",
      "iteration 1082, dc_loss: 0.15178102254867554, tv_loss: 0.0312584713101387\n",
      "iteration 1083, dc_loss: 0.15168240666389465, tv_loss: 0.031450022011995316\n",
      "iteration 1084, dc_loss: 0.15154466032981873, tv_loss: 0.03140915557742119\n",
      "iteration 1085, dc_loss: 0.1514217108488083, tv_loss: 0.031309738755226135\n",
      "iteration 1086, dc_loss: 0.15131157636642456, tv_loss: 0.03136640787124634\n",
      "iteration 1087, dc_loss: 0.15120382606983185, tv_loss: 0.03126600384712219\n",
      "iteration 1088, dc_loss: 0.15113984048366547, tv_loss: 0.03122679702937603\n",
      "iteration 1089, dc_loss: 0.1509646475315094, tv_loss: 0.03136886656284332\n",
      "iteration 1090, dc_loss: 0.15097017586231232, tv_loss: 0.03116978518664837\n",
      "iteration 1091, dc_loss: 0.1507931649684906, tv_loss: 0.031496401876211166\n",
      "iteration 1092, dc_loss: 0.15098553895950317, tv_loss: 0.03112747333943844\n",
      "iteration 1093, dc_loss: 0.15075358748435974, tv_loss: 0.03165208175778389\n",
      "iteration 1094, dc_loss: 0.15109996497631073, tv_loss: 0.031250037252902985\n",
      "iteration 1095, dc_loss: 0.15076856315135956, tv_loss: 0.03153255209326744\n",
      "iteration 1096, dc_loss: 0.15106335282325745, tv_loss: 0.031092574819922447\n",
      "iteration 1097, dc_loss: 0.15042880177497864, tv_loss: 0.031533561646938324\n",
      "iteration 1098, dc_loss: 0.15042607486248016, tv_loss: 0.03117148019373417\n",
      "iteration 1099, dc_loss: 0.14990945160388947, tv_loss: 0.031412530690431595\n",
      "iteration 1100, dc_loss: 0.1498367339372635, tv_loss: 0.03122694417834282\n",
      "iteration 1101, dc_loss: 0.1497853547334671, tv_loss: 0.03133562579751015\n",
      "iteration 1102, dc_loss: 0.14968042075634003, tv_loss: 0.031379181891679764\n",
      "iteration 1103, dc_loss: 0.14981292188167572, tv_loss: 0.03127739951014519\n",
      "iteration 1104, dc_loss: 0.14937177300453186, tv_loss: 0.03136400878429413\n",
      "iteration 1105, dc_loss: 0.14929735660552979, tv_loss: 0.03140350058674812\n",
      "iteration 1106, dc_loss: 0.14906656742095947, tv_loss: 0.03142350912094116\n",
      "iteration 1107, dc_loss: 0.14899922907352448, tv_loss: 0.03146069124341011\n",
      "iteration 1108, dc_loss: 0.14904174208641052, tv_loss: 0.03134845942258835\n",
      "iteration 1109, dc_loss: 0.14881935715675354, tv_loss: 0.03143511340022087\n",
      "iteration 1110, dc_loss: 0.14883466064929962, tv_loss: 0.03133438900113106\n",
      "iteration 1111, dc_loss: 0.14849336445331573, tv_loss: 0.03141823410987854\n",
      "iteration 1112, dc_loss: 0.14858274161815643, tv_loss: 0.031245065852999687\n",
      "iteration 1113, dc_loss: 0.14831110835075378, tv_loss: 0.031446103006601334\n",
      "iteration 1114, dc_loss: 0.14827342331409454, tv_loss: 0.031351689249277115\n",
      "iteration 1115, dc_loss: 0.14816135168075562, tv_loss: 0.03142254427075386\n",
      "iteration 1116, dc_loss: 0.14800001680850983, tv_loss: 0.03134724497795105\n",
      "iteration 1117, dc_loss: 0.14791251718997955, tv_loss: 0.03146883100271225\n",
      "iteration 1118, dc_loss: 0.14774397015571594, tv_loss: 0.03148747235536575\n",
      "iteration 1119, dc_loss: 0.1477290838956833, tv_loss: 0.031375668942928314\n",
      "iteration 1120, dc_loss: 0.14755551517009735, tv_loss: 0.03137483820319176\n",
      "iteration 1121, dc_loss: 0.14745159447193146, tv_loss: 0.03138197958469391\n",
      "iteration 1122, dc_loss: 0.14725564420223236, tv_loss: 0.03137950226664543\n",
      "iteration 1123, dc_loss: 0.14721183478832245, tv_loss: 0.03141419589519501\n",
      "iteration 1124, dc_loss: 0.14706255495548248, tv_loss: 0.031343527138233185\n",
      "iteration 1125, dc_loss: 0.14700716733932495, tv_loss: 0.031476251780986786\n",
      "iteration 1126, dc_loss: 0.14682017266750336, tv_loss: 0.03143729642033577\n",
      "iteration 1127, dc_loss: 0.14674925804138184, tv_loss: 0.03147055208683014\n",
      "iteration 1128, dc_loss: 0.14664390683174133, tv_loss: 0.031419988721609116\n",
      "iteration 1129, dc_loss: 0.14649458229541779, tv_loss: 0.031454410403966904\n",
      "iteration 1130, dc_loss: 0.14642898738384247, tv_loss: 0.03137516602873802\n",
      "iteration 1131, dc_loss: 0.14629815518856049, tv_loss: 0.03144608065485954\n",
      "iteration 1132, dc_loss: 0.14615552127361298, tv_loss: 0.0314217135310173\n",
      "iteration 1133, dc_loss: 0.14609451591968536, tv_loss: 0.031439948827028275\n",
      "iteration 1134, dc_loss: 0.14592254161834717, tv_loss: 0.031437940895557404\n",
      "iteration 1135, dc_loss: 0.14594390988349915, tv_loss: 0.03139522299170494\n",
      "iteration 1136, dc_loss: 0.14570127427577972, tv_loss: 0.031453948467969894\n",
      "iteration 1137, dc_loss: 0.14569959044456482, tv_loss: 0.03146389499306679\n",
      "iteration 1138, dc_loss: 0.1454974114894867, tv_loss: 0.03148939460515976\n",
      "iteration 1139, dc_loss: 0.14554759860038757, tv_loss: 0.03141595423221588\n",
      "iteration 1140, dc_loss: 0.14532458782196045, tv_loss: 0.03148488327860832\n",
      "iteration 1141, dc_loss: 0.14538022875785828, tv_loss: 0.031418364495038986\n",
      "iteration 1142, dc_loss: 0.14513561129570007, tv_loss: 0.031544044613838196\n",
      "iteration 1143, dc_loss: 0.14528509974479675, tv_loss: 0.03137921541929245\n",
      "iteration 1144, dc_loss: 0.14500659704208374, tv_loss: 0.03156397491693497\n",
      "iteration 1145, dc_loss: 0.1451699286699295, tv_loss: 0.031357377767562866\n",
      "iteration 1146, dc_loss: 0.1448136568069458, tv_loss: 0.031610991805791855\n",
      "iteration 1147, dc_loss: 0.14503520727157593, tv_loss: 0.03132438659667969\n",
      "iteration 1148, dc_loss: 0.14461737871170044, tv_loss: 0.03157750889658928\n",
      "iteration 1149, dc_loss: 0.14478017389774323, tv_loss: 0.03138306364417076\n",
      "iteration 1150, dc_loss: 0.14442117512226105, tv_loss: 0.03162486106157303\n",
      "iteration 1151, dc_loss: 0.14462536573410034, tv_loss: 0.031375277787446976\n",
      "iteration 1152, dc_loss: 0.14433683454990387, tv_loss: 0.03159772604703903\n",
      "iteration 1153, dc_loss: 0.14441010355949402, tv_loss: 0.03139946609735489\n",
      "iteration 1154, dc_loss: 0.1440735012292862, tv_loss: 0.031560514122247696\n",
      "iteration 1155, dc_loss: 0.14411082863807678, tv_loss: 0.03140292689204216\n",
      "iteration 1156, dc_loss: 0.14377547800540924, tv_loss: 0.03155064955353737\n",
      "iteration 1157, dc_loss: 0.14377890527248383, tv_loss: 0.031495895236730576\n",
      "iteration 1158, dc_loss: 0.1435651034116745, tv_loss: 0.031524792313575745\n",
      "iteration 1159, dc_loss: 0.14349091053009033, tv_loss: 0.03149857372045517\n",
      "iteration 1160, dc_loss: 0.14337879419326782, tv_loss: 0.0314534567296505\n",
      "iteration 1161, dc_loss: 0.1432318091392517, tv_loss: 0.03157676011323929\n",
      "iteration 1162, dc_loss: 0.14324186742305756, tv_loss: 0.03143705427646637\n",
      "iteration 1163, dc_loss: 0.14306679368019104, tv_loss: 0.03159071505069733\n",
      "iteration 1164, dc_loss: 0.14308175444602966, tv_loss: 0.031426336616277695\n",
      "iteration 1165, dc_loss: 0.14285317063331604, tv_loss: 0.03159865736961365\n",
      "iteration 1166, dc_loss: 0.14283490180969238, tv_loss: 0.031464267522096634\n",
      "iteration 1167, dc_loss: 0.14265504479408264, tv_loss: 0.03157367929816246\n",
      "iteration 1168, dc_loss: 0.142637699842453, tv_loss: 0.0314481146633625\n",
      "iteration 1169, dc_loss: 0.14245298504829407, tv_loss: 0.03158634528517723\n",
      "iteration 1170, dc_loss: 0.14244309067726135, tv_loss: 0.031466610729694366\n",
      "iteration 1171, dc_loss: 0.14227569103240967, tv_loss: 0.03159172087907791\n",
      "iteration 1172, dc_loss: 0.1422928422689438, tv_loss: 0.03143622726202011\n",
      "iteration 1173, dc_loss: 0.14205493032932281, tv_loss: 0.03164249658584595\n",
      "iteration 1174, dc_loss: 0.1421002894639969, tv_loss: 0.03146300092339516\n",
      "iteration 1175, dc_loss: 0.14187774062156677, tv_loss: 0.03161320090293884\n",
      "iteration 1176, dc_loss: 0.14195115864276886, tv_loss: 0.03142965957522392\n",
      "iteration 1177, dc_loss: 0.14170153439044952, tv_loss: 0.031647831201553345\n",
      "iteration 1178, dc_loss: 0.14181852340698242, tv_loss: 0.03142917528748512\n",
      "iteration 1179, dc_loss: 0.1415843963623047, tv_loss: 0.031652119010686874\n",
      "iteration 1180, dc_loss: 0.14174333214759827, tv_loss: 0.031390778720378876\n",
      "iteration 1181, dc_loss: 0.14145462214946747, tv_loss: 0.031725089997053146\n",
      "iteration 1182, dc_loss: 0.14169032871723175, tv_loss: 0.03140058368444443\n",
      "iteration 1183, dc_loss: 0.14133796095848083, tv_loss: 0.03171933442354202\n",
      "iteration 1184, dc_loss: 0.14159585535526276, tv_loss: 0.0313459075987339\n",
      "iteration 1185, dc_loss: 0.14111457765102386, tv_loss: 0.03175961971282959\n",
      "iteration 1186, dc_loss: 0.14125755429267883, tv_loss: 0.03138855844736099\n",
      "iteration 1187, dc_loss: 0.1408623456954956, tv_loss: 0.031678128987550735\n",
      "iteration 1188, dc_loss: 0.14086434245109558, tv_loss: 0.03144736588001251\n",
      "iteration 1189, dc_loss: 0.14060421288013458, tv_loss: 0.03163071721792221\n",
      "iteration 1190, dc_loss: 0.14056400954723358, tv_loss: 0.031555354595184326\n",
      "iteration 1191, dc_loss: 0.14055900275707245, tv_loss: 0.03153444081544876\n",
      "iteration 1192, dc_loss: 0.14039310812950134, tv_loss: 0.031602442264556885\n",
      "iteration 1193, dc_loss: 0.14041200280189514, tv_loss: 0.03155761584639549\n",
      "iteration 1194, dc_loss: 0.14022620022296906, tv_loss: 0.03162180259823799\n",
      "iteration 1195, dc_loss: 0.1401900053024292, tv_loss: 0.031553253531455994\n",
      "iteration 1196, dc_loss: 0.14001351594924927, tv_loss: 0.031576383858919144\n",
      "iteration 1197, dc_loss: 0.13992471992969513, tv_loss: 0.031579408794641495\n",
      "iteration 1198, dc_loss: 0.13977931439876556, tv_loss: 0.03159528970718384\n",
      "iteration 1199, dc_loss: 0.13972146809101105, tv_loss: 0.031598154455423355\n",
      "iteration 1200, dc_loss: 0.13964217901229858, tv_loss: 0.031557343900203705\n",
      "iteration 1201, dc_loss: 0.1395302563905716, tv_loss: 0.031631167978048325\n",
      "iteration 1202, dc_loss: 0.1394556611776352, tv_loss: 0.03158780559897423\n",
      "iteration 1203, dc_loss: 0.13936688005924225, tv_loss: 0.031545307487249374\n",
      "iteration 1204, dc_loss: 0.13932423293590546, tv_loss: 0.031566694378852844\n",
      "iteration 1205, dc_loss: 0.1392294019460678, tv_loss: 0.031520549207925797\n",
      "iteration 1206, dc_loss: 0.1391293704509735, tv_loss: 0.03157833591103554\n",
      "iteration 1207, dc_loss: 0.13909263908863068, tv_loss: 0.03150807321071625\n",
      "iteration 1208, dc_loss: 0.13903792202472687, tv_loss: 0.031509313732385635\n",
      "iteration 1209, dc_loss: 0.13892462849617004, tv_loss: 0.03154057264328003\n",
      "iteration 1210, dc_loss: 0.13885508477687836, tv_loss: 0.03152410313487053\n",
      "iteration 1211, dc_loss: 0.13880908489227295, tv_loss: 0.03153283894062042\n",
      "iteration 1212, dc_loss: 0.13873210549354553, tv_loss: 0.03149786219000816\n",
      "iteration 1213, dc_loss: 0.13867348432540894, tv_loss: 0.03153040632605553\n",
      "iteration 1214, dc_loss: 0.1385788768529892, tv_loss: 0.031514864414930344\n",
      "iteration 1215, dc_loss: 0.13851532340049744, tv_loss: 0.03153971955180168\n",
      "iteration 1216, dc_loss: 0.1384652853012085, tv_loss: 0.031504109501838684\n",
      "iteration 1217, dc_loss: 0.13836202025413513, tv_loss: 0.03153379261493683\n",
      "iteration 1218, dc_loss: 0.13829369843006134, tv_loss: 0.031556136906147\n",
      "iteration 1219, dc_loss: 0.13826459646224976, tv_loss: 0.03148828446865082\n",
      "iteration 1220, dc_loss: 0.138168603181839, tv_loss: 0.031539853662252426\n",
      "iteration 1221, dc_loss: 0.13809223473072052, tv_loss: 0.031534116715192795\n",
      "iteration 1222, dc_loss: 0.13804705440998077, tv_loss: 0.03150977939367294\n",
      "iteration 1223, dc_loss: 0.1379612386226654, tv_loss: 0.03154486417770386\n",
      "iteration 1224, dc_loss: 0.13788796961307526, tv_loss: 0.031531721353530884\n",
      "iteration 1225, dc_loss: 0.13783195614814758, tv_loss: 0.031531501561403275\n",
      "iteration 1226, dc_loss: 0.13776662945747375, tv_loss: 0.03153209760785103\n",
      "iteration 1227, dc_loss: 0.13769160211086273, tv_loss: 0.03153117746114731\n",
      "iteration 1228, dc_loss: 0.13762161135673523, tv_loss: 0.03153924271464348\n",
      "iteration 1229, dc_loss: 0.13756366074085236, tv_loss: 0.03153849393129349\n",
      "iteration 1230, dc_loss: 0.13748909533023834, tv_loss: 0.031530968844890594\n",
      "iteration 1231, dc_loss: 0.13741955161094666, tv_loss: 0.031542059034109116\n",
      "iteration 1232, dc_loss: 0.13736280798912048, tv_loss: 0.03153826668858528\n",
      "iteration 1233, dc_loss: 0.13728344440460205, tv_loss: 0.031540803611278534\n",
      "iteration 1234, dc_loss: 0.13722051680088043, tv_loss: 0.031540095806121826\n",
      "iteration 1235, dc_loss: 0.13716645538806915, tv_loss: 0.03153783082962036\n",
      "iteration 1236, dc_loss: 0.13707943260669708, tv_loss: 0.03155062347650528\n",
      "iteration 1237, dc_loss: 0.13701894879341125, tv_loss: 0.03154336288571358\n",
      "iteration 1238, dc_loss: 0.13696320354938507, tv_loss: 0.03154733404517174\n",
      "iteration 1239, dc_loss: 0.13688139617443085, tv_loss: 0.03155475854873657\n",
      "iteration 1240, dc_loss: 0.13683375716209412, tv_loss: 0.03152993693947792\n",
      "iteration 1241, dc_loss: 0.136759951710701, tv_loss: 0.031551893800497055\n",
      "iteration 1242, dc_loss: 0.13667501509189606, tv_loss: 0.03157094493508339\n",
      "iteration 1243, dc_loss: 0.1366395354270935, tv_loss: 0.031535275280475616\n",
      "iteration 1244, dc_loss: 0.13655857741832733, tv_loss: 0.03155054897069931\n",
      "iteration 1245, dc_loss: 0.13648945093154907, tv_loss: 0.031558338552713394\n",
      "iteration 1246, dc_loss: 0.13643980026245117, tv_loss: 0.03154619038105011\n",
      "iteration 1247, dc_loss: 0.13635388016700745, tv_loss: 0.031566448509693146\n",
      "iteration 1248, dc_loss: 0.13630318641662598, tv_loss: 0.031552109867334366\n",
      "iteration 1249, dc_loss: 0.13623575866222382, tv_loss: 0.03155643492937088\n",
      "iteration 1250, dc_loss: 0.13615906238555908, tv_loss: 0.031573906540870667\n",
      "iteration 1251, dc_loss: 0.13612115383148193, tv_loss: 0.03155166283249855\n",
      "iteration 1252, dc_loss: 0.13602380454540253, tv_loss: 0.03157968074083328\n",
      "iteration 1253, dc_loss: 0.13598878681659698, tv_loss: 0.03155042231082916\n",
      "iteration 1254, dc_loss: 0.13590089976787567, tv_loss: 0.03157541900873184\n",
      "iteration 1255, dc_loss: 0.13585031032562256, tv_loss: 0.031563784927129745\n",
      "iteration 1256, dc_loss: 0.1357806921005249, tv_loss: 0.031571291387081146\n",
      "iteration 1257, dc_loss: 0.13571743667125702, tv_loss: 0.03157351538538933\n",
      "iteration 1258, dc_loss: 0.13564811646938324, tv_loss: 0.031581901013851166\n",
      "iteration 1259, dc_loss: 0.1356021761894226, tv_loss: 0.03156613931059837\n",
      "iteration 1260, dc_loss: 0.13550959527492523, tv_loss: 0.03159608319401741\n",
      "iteration 1261, dc_loss: 0.13547293841838837, tv_loss: 0.031570736318826675\n",
      "iteration 1262, dc_loss: 0.13539253175258636, tv_loss: 0.03158898651599884\n",
      "iteration 1263, dc_loss: 0.13533949851989746, tv_loss: 0.03158232197165489\n",
      "iteration 1264, dc_loss: 0.1352708786725998, tv_loss: 0.03159358352422714\n",
      "iteration 1265, dc_loss: 0.13520507514476776, tv_loss: 0.031599145382642746\n",
      "iteration 1266, dc_loss: 0.13514791429042816, tv_loss: 0.0315948948264122\n",
      "iteration 1267, dc_loss: 0.13507001101970673, tv_loss: 0.03160751238465309\n",
      "iteration 1268, dc_loss: 0.13503246009349823, tv_loss: 0.03158370777964592\n",
      "iteration 1269, dc_loss: 0.13494521379470825, tv_loss: 0.03161131963133812\n",
      "iteration 1270, dc_loss: 0.13490059971809387, tv_loss: 0.031596217304468155\n",
      "iteration 1271, dc_loss: 0.13482125103473663, tv_loss: 0.03161276504397392\n",
      "iteration 1272, dc_loss: 0.13477720320224762, tv_loss: 0.03159669414162636\n",
      "iteration 1273, dc_loss: 0.1346973031759262, tv_loss: 0.03161453828215599\n",
      "iteration 1274, dc_loss: 0.13464853167533875, tv_loss: 0.031606048345565796\n",
      "iteration 1275, dc_loss: 0.1345723569393158, tv_loss: 0.03162367641925812\n",
      "iteration 1276, dc_loss: 0.13452281057834625, tv_loss: 0.031610891222953796\n",
      "iteration 1277, dc_loss: 0.13444945216178894, tv_loss: 0.031623244285583496\n",
      "iteration 1278, dc_loss: 0.13439908623695374, tv_loss: 0.03160737082362175\n",
      "iteration 1279, dc_loss: 0.13432903587818146, tv_loss: 0.03161712735891342\n",
      "iteration 1280, dc_loss: 0.13427335023880005, tv_loss: 0.03161788359284401\n",
      "iteration 1281, dc_loss: 0.13419897854328156, tv_loss: 0.03163870424032211\n",
      "iteration 1282, dc_loss: 0.1341531127691269, tv_loss: 0.03162362053990364\n",
      "iteration 1283, dc_loss: 0.1340802013874054, tv_loss: 0.03162772208452225\n",
      "iteration 1284, dc_loss: 0.13402923941612244, tv_loss: 0.031615495681762695\n",
      "iteration 1285, dc_loss: 0.13395152986049652, tv_loss: 0.03163357824087143\n",
      "iteration 1286, dc_loss: 0.1339104175567627, tv_loss: 0.031621839851140976\n",
      "iteration 1287, dc_loss: 0.13382723927497864, tv_loss: 0.03165290132164955\n",
      "iteration 1288, dc_loss: 0.13378718495368958, tv_loss: 0.031630512326955795\n",
      "iteration 1289, dc_loss: 0.1337069272994995, tv_loss: 0.0316447839140892\n",
      "iteration 1290, dc_loss: 0.13367141783237457, tv_loss: 0.03161798045039177\n",
      "iteration 1291, dc_loss: 0.13358213007450104, tv_loss: 0.03164883702993393\n",
      "iteration 1292, dc_loss: 0.13354536890983582, tv_loss: 0.0316307507455349\n",
      "iteration 1293, dc_loss: 0.13346178829669952, tv_loss: 0.031659726053476334\n",
      "iteration 1294, dc_loss: 0.13342900574207306, tv_loss: 0.03163589909672737\n",
      "iteration 1295, dc_loss: 0.13334280252456665, tv_loss: 0.03166057914495468\n",
      "iteration 1296, dc_loss: 0.13331197202205658, tv_loss: 0.03163012117147446\n",
      "iteration 1297, dc_loss: 0.13322219252586365, tv_loss: 0.03166550025343895\n",
      "iteration 1298, dc_loss: 0.13320095837116241, tv_loss: 0.03163450211286545\n",
      "iteration 1299, dc_loss: 0.13310091197490692, tv_loss: 0.03168407082557678\n",
      "iteration 1300, dc_loss: 0.13309268653392792, tv_loss: 0.03162793815135956\n",
      "iteration 1301, dc_loss: 0.13299089670181274, tv_loss: 0.031678877770900726\n",
      "iteration 1302, dc_loss: 0.1329893171787262, tv_loss: 0.031622085720300674\n",
      "iteration 1303, dc_loss: 0.13285814225673676, tv_loss: 0.03170594945549965\n",
      "iteration 1304, dc_loss: 0.13287432491779327, tv_loss: 0.0316283255815506\n",
      "iteration 1305, dc_loss: 0.13273698091506958, tv_loss: 0.031699299812316895\n",
      "iteration 1306, dc_loss: 0.1327345073223114, tv_loss: 0.03163254261016846\n",
      "iteration 1307, dc_loss: 0.13262443244457245, tv_loss: 0.03168574348092079\n",
      "iteration 1308, dc_loss: 0.13260428607463837, tv_loss: 0.031652405858039856\n",
      "iteration 1309, dc_loss: 0.13250522315502167, tv_loss: 0.031692806631326675\n",
      "iteration 1310, dc_loss: 0.13247299194335938, tv_loss: 0.03166402131319046\n",
      "iteration 1311, dc_loss: 0.1323908418416977, tv_loss: 0.03168368712067604\n",
      "iteration 1312, dc_loss: 0.13235490024089813, tv_loss: 0.03166040778160095\n",
      "iteration 1313, dc_loss: 0.13227230310440063, tv_loss: 0.03168150410056114\n",
      "iteration 1314, dc_loss: 0.13222813606262207, tv_loss: 0.03167420253157616\n",
      "iteration 1315, dc_loss: 0.13216102123260498, tv_loss: 0.031694553792476654\n",
      "iteration 1316, dc_loss: 0.13212038576602936, tv_loss: 0.031676966696977615\n",
      "iteration 1317, dc_loss: 0.1320469230413437, tv_loss: 0.03168432042002678\n",
      "iteration 1318, dc_loss: 0.13199393451213837, tv_loss: 0.031679145991802216\n",
      "iteration 1319, dc_loss: 0.1319354623556137, tv_loss: 0.03168559819459915\n",
      "iteration 1320, dc_loss: 0.13187623023986816, tv_loss: 0.031691208481788635\n",
      "iteration 1321, dc_loss: 0.13183896243572235, tv_loss: 0.031684309244155884\n",
      "iteration 1322, dc_loss: 0.13176028430461884, tv_loss: 0.03171626478433609\n",
      "iteration 1323, dc_loss: 0.13174766302108765, tv_loss: 0.03167225793004036\n",
      "iteration 1324, dc_loss: 0.1316504180431366, tv_loss: 0.0317254401743412\n",
      "iteration 1325, dc_loss: 0.1316564381122589, tv_loss: 0.031660642474889755\n",
      "iteration 1326, dc_loss: 0.13153886795043945, tv_loss: 0.03174285963177681\n",
      "iteration 1327, dc_loss: 0.13157488405704498, tv_loss: 0.03164875507354736\n",
      "iteration 1328, dc_loss: 0.13141269981861115, tv_loss: 0.031765833497047424\n",
      "iteration 1329, dc_loss: 0.1314815729856491, tv_loss: 0.031635582447052\n",
      "iteration 1330, dc_loss: 0.1312911957502365, tv_loss: 0.03179062530398369\n",
      "iteration 1331, dc_loss: 0.13138827681541443, tv_loss: 0.031626101583242416\n",
      "iteration 1332, dc_loss: 0.1311747133731842, tv_loss: 0.03177982568740845\n",
      "iteration 1333, dc_loss: 0.13126584887504578, tv_loss: 0.03162171319127083\n",
      "iteration 1334, dc_loss: 0.13105782866477966, tv_loss: 0.031779706478118896\n",
      "iteration 1335, dc_loss: 0.13111986219882965, tv_loss: 0.031661953777074814\n",
      "iteration 1336, dc_loss: 0.13094472885131836, tv_loss: 0.03176609054207802\n",
      "iteration 1337, dc_loss: 0.13096974790096283, tv_loss: 0.03166915476322174\n",
      "iteration 1338, dc_loss: 0.13084061443805695, tv_loss: 0.031741201877593994\n",
      "iteration 1339, dc_loss: 0.13084492087364197, tv_loss: 0.031684406101703644\n",
      "iteration 1340, dc_loss: 0.13072672486305237, tv_loss: 0.031753089278936386\n",
      "iteration 1341, dc_loss: 0.13070929050445557, tv_loss: 0.03170308843255043\n",
      "iteration 1342, dc_loss: 0.13061396777629852, tv_loss: 0.03173868730664253\n",
      "iteration 1343, dc_loss: 0.13061173260211945, tv_loss: 0.03168906271457672\n",
      "iteration 1344, dc_loss: 0.13049939274787903, tv_loss: 0.031753309071063995\n",
      "iteration 1345, dc_loss: 0.13049177825450897, tv_loss: 0.031706374138593674\n",
      "iteration 1346, dc_loss: 0.13038982450962067, tv_loss: 0.03175776079297066\n",
      "iteration 1347, dc_loss: 0.13038718700408936, tv_loss: 0.03169848397374153\n",
      "iteration 1348, dc_loss: 0.130281463265419, tv_loss: 0.0317554734647274\n",
      "iteration 1349, dc_loss: 0.1302730292081833, tv_loss: 0.031714845448732376\n",
      "iteration 1350, dc_loss: 0.13018381595611572, tv_loss: 0.03176445886492729\n",
      "iteration 1351, dc_loss: 0.1301804929971695, tv_loss: 0.031714167445898056\n",
      "iteration 1352, dc_loss: 0.1300724595785141, tv_loss: 0.03177587315440178\n",
      "iteration 1353, dc_loss: 0.13008618354797363, tv_loss: 0.03171071037650108\n",
      "iteration 1354, dc_loss: 0.1299680769443512, tv_loss: 0.031789589673280716\n",
      "iteration 1355, dc_loss: 0.12998777627944946, tv_loss: 0.03171490505337715\n",
      "iteration 1356, dc_loss: 0.12985797226428986, tv_loss: 0.03179246187210083\n",
      "iteration 1357, dc_loss: 0.12987610697746277, tv_loss: 0.03171126917004585\n",
      "iteration 1358, dc_loss: 0.12974931299686432, tv_loss: 0.03178945928812027\n",
      "iteration 1359, dc_loss: 0.12978968024253845, tv_loss: 0.03168606385588646\n",
      "iteration 1360, dc_loss: 0.12963458895683289, tv_loss: 0.03179730474948883\n",
      "iteration 1361, dc_loss: 0.1296779215335846, tv_loss: 0.03170895203948021\n",
      "iteration 1362, dc_loss: 0.12952548265457153, tv_loss: 0.03182642534375191\n",
      "iteration 1363, dc_loss: 0.1295861005783081, tv_loss: 0.0316927544772625\n",
      "iteration 1364, dc_loss: 0.12941999733448029, tv_loss: 0.03180255368351936\n",
      "iteration 1365, dc_loss: 0.12946076691150665, tv_loss: 0.03170478716492653\n",
      "iteration 1366, dc_loss: 0.12931610643863678, tv_loss: 0.03180946037173271\n",
      "iteration 1367, dc_loss: 0.129372239112854, tv_loss: 0.03169927000999451\n",
      "iteration 1368, dc_loss: 0.12920428812503815, tv_loss: 0.0318312793970108\n",
      "iteration 1369, dc_loss: 0.1292569786310196, tv_loss: 0.031712163239717484\n",
      "iteration 1370, dc_loss: 0.12910692393779755, tv_loss: 0.031819868832826614\n",
      "iteration 1371, dc_loss: 0.1291780173778534, tv_loss: 0.0316917784512043\n",
      "iteration 1372, dc_loss: 0.12899470329284668, tv_loss: 0.031840503215789795\n",
      "iteration 1373, dc_loss: 0.1290818750858307, tv_loss: 0.031704094260931015\n",
      "iteration 1374, dc_loss: 0.1288980394601822, tv_loss: 0.03186464309692383\n",
      "iteration 1375, dc_loss: 0.1290104240179062, tv_loss: 0.03170722350478172\n",
      "iteration 1376, dc_loss: 0.12879164516925812, tv_loss: 0.03186938539147377\n",
      "iteration 1377, dc_loss: 0.12889336049556732, tv_loss: 0.03170839324593544\n",
      "iteration 1378, dc_loss: 0.12871867418289185, tv_loss: 0.031849075108766556\n",
      "iteration 1379, dc_loss: 0.12883524596691132, tv_loss: 0.03170052543282509\n",
      "iteration 1380, dc_loss: 0.12861180305480957, tv_loss: 0.03186332806944847\n",
      "iteration 1381, dc_loss: 0.12879468500614166, tv_loss: 0.03170716390013695\n",
      "iteration 1382, dc_loss: 0.12863443791866302, tv_loss: 0.031876422464847565\n",
      "iteration 1383, dc_loss: 0.12890945374965668, tv_loss: 0.03167429566383362\n",
      "iteration 1384, dc_loss: 0.1286364644765854, tv_loss: 0.031922414898872375\n",
      "iteration 1385, dc_loss: 0.12885668873786926, tv_loss: 0.031655147671699524\n",
      "iteration 1386, dc_loss: 0.12844489514827728, tv_loss: 0.031945474445819855\n",
      "iteration 1387, dc_loss: 0.12862426042556763, tv_loss: 0.031673725694417953\n",
      "iteration 1388, dc_loss: 0.12828287482261658, tv_loss: 0.03189477697014809\n",
      "iteration 1389, dc_loss: 0.1282796710729599, tv_loss: 0.03176886588335037\n",
      "iteration 1390, dc_loss: 0.12818893790245056, tv_loss: 0.031798105686903\n",
      "iteration 1391, dc_loss: 0.12811151146888733, tv_loss: 0.031814273446798325\n",
      "iteration 1392, dc_loss: 0.12814611196517944, tv_loss: 0.03175297752022743\n",
      "iteration 1393, dc_loss: 0.12796543538570404, tv_loss: 0.03189406916499138\n",
      "iteration 1394, dc_loss: 0.12810485064983368, tv_loss: 0.031711723655462265\n",
      "iteration 1395, dc_loss: 0.1278647929430008, tv_loss: 0.0318942666053772\n",
      "iteration 1396, dc_loss: 0.1279231309890747, tv_loss: 0.0317465141415596\n",
      "iteration 1397, dc_loss: 0.12776675820350647, tv_loss: 0.03182891756296158\n",
      "iteration 1398, dc_loss: 0.12771625816822052, tv_loss: 0.03180159628391266\n",
      "iteration 1399, dc_loss: 0.1276703029870987, tv_loss: 0.03180483356118202\n",
      "iteration 1400, dc_loss: 0.12759508192539215, tv_loss: 0.03182678297162056\n",
      "iteration 1401, dc_loss: 0.12762577831745148, tv_loss: 0.03175465017557144\n",
      "iteration 1402, dc_loss: 0.12746810913085938, tv_loss: 0.03187352418899536\n",
      "iteration 1403, dc_loss: 0.12751330435276031, tv_loss: 0.03177435323596001\n",
      "iteration 1404, dc_loss: 0.12735851109027863, tv_loss: 0.031874023377895355\n",
      "iteration 1405, dc_loss: 0.12738534808158875, tv_loss: 0.03177710622549057\n",
      "iteration 1406, dc_loss: 0.12726373970508575, tv_loss: 0.03184002265334129\n",
      "iteration 1407, dc_loss: 0.12724138796329498, tv_loss: 0.03181082382798195\n",
      "iteration 1408, dc_loss: 0.1271623969078064, tv_loss: 0.03184245899319649\n",
      "iteration 1409, dc_loss: 0.1271117776632309, tv_loss: 0.03184103965759277\n",
      "iteration 1410, dc_loss: 0.1270957589149475, tv_loss: 0.031807903200387955\n",
      "iteration 1411, dc_loss: 0.127004012465477, tv_loss: 0.03184875473380089\n",
      "iteration 1412, dc_loss: 0.12700004875659943, tv_loss: 0.03180130571126938\n",
      "iteration 1413, dc_loss: 0.1268882155418396, tv_loss: 0.03186136111617088\n",
      "iteration 1414, dc_loss: 0.12691256403923035, tv_loss: 0.03179585933685303\n",
      "iteration 1415, dc_loss: 0.1267888993024826, tv_loss: 0.03187514469027519\n",
      "iteration 1416, dc_loss: 0.1268114596605301, tv_loss: 0.03179076686501503\n",
      "iteration 1417, dc_loss: 0.12668301165103912, tv_loss: 0.03187655285000801\n",
      "iteration 1418, dc_loss: 0.12672457098960876, tv_loss: 0.03179209679365158\n",
      "iteration 1419, dc_loss: 0.12658417224884033, tv_loss: 0.03190147504210472\n",
      "iteration 1420, dc_loss: 0.12662561237812042, tv_loss: 0.031799253076314926\n",
      "iteration 1421, dc_loss: 0.12650041282176971, tv_loss: 0.03188157081604004\n",
      "iteration 1422, dc_loss: 0.12651818990707397, tv_loss: 0.03181980922818184\n",
      "iteration 1423, dc_loss: 0.1263931542634964, tv_loss: 0.031897444278001785\n",
      "iteration 1424, dc_loss: 0.126456156373024, tv_loss: 0.03178989887237549\n",
      "iteration 1425, dc_loss: 0.12628905475139618, tv_loss: 0.03192108869552612\n",
      "iteration 1426, dc_loss: 0.12635931372642517, tv_loss: 0.03179607167840004\n",
      "iteration 1427, dc_loss: 0.12619778513908386, tv_loss: 0.03191033750772476\n",
      "iteration 1428, dc_loss: 0.12626172602176666, tv_loss: 0.031803395599126816\n",
      "iteration 1429, dc_loss: 0.1261134296655655, tv_loss: 0.03190293163061142\n",
      "iteration 1430, dc_loss: 0.126181498169899, tv_loss: 0.0317930206656456\n",
      "iteration 1431, dc_loss: 0.12599022686481476, tv_loss: 0.03195470944046974\n",
      "iteration 1432, dc_loss: 0.1261049211025238, tv_loss: 0.03178888559341431\n",
      "iteration 1433, dc_loss: 0.12592047452926636, tv_loss: 0.03193584829568863\n",
      "iteration 1434, dc_loss: 0.12602223455905914, tv_loss: 0.03177877888083458\n",
      "iteration 1435, dc_loss: 0.125822514295578, tv_loss: 0.03193755820393562\n",
      "iteration 1436, dc_loss: 0.12592938542366028, tv_loss: 0.03179053217172623\n",
      "iteration 1437, dc_loss: 0.12572842836380005, tv_loss: 0.03196470066905022\n",
      "iteration 1438, dc_loss: 0.125871941447258, tv_loss: 0.03177798539400101\n",
      "iteration 1439, dc_loss: 0.12565740942955017, tv_loss: 0.03195436671376228\n",
      "iteration 1440, dc_loss: 0.1257733404636383, tv_loss: 0.03179435804486275\n",
      "iteration 1441, dc_loss: 0.1255834549665451, tv_loss: 0.031959351152181625\n",
      "iteration 1442, dc_loss: 0.1256815642118454, tv_loss: 0.03179698437452316\n",
      "iteration 1443, dc_loss: 0.12547896802425385, tv_loss: 0.031954631209373474\n",
      "iteration 1444, dc_loss: 0.12555930018424988, tv_loss: 0.03180622681975365\n",
      "iteration 1445, dc_loss: 0.12536479532718658, tv_loss: 0.03195101395249367\n",
      "iteration 1446, dc_loss: 0.12542608380317688, tv_loss: 0.03182671591639519\n",
      "iteration 1447, dc_loss: 0.12525032460689545, tv_loss: 0.03193572163581848\n",
      "iteration 1448, dc_loss: 0.12531110644340515, tv_loss: 0.0318414568901062\n",
      "iteration 1449, dc_loss: 0.1251554787158966, tv_loss: 0.03194154426455498\n",
      "iteration 1450, dc_loss: 0.12520600855350494, tv_loss: 0.03184638172388077\n",
      "iteration 1451, dc_loss: 0.12505482137203217, tv_loss: 0.03192567452788353\n",
      "iteration 1452, dc_loss: 0.12511524558067322, tv_loss: 0.031855978071689606\n",
      "iteration 1453, dc_loss: 0.12497968226671219, tv_loss: 0.03193489462137222\n",
      "iteration 1454, dc_loss: 0.1250612735748291, tv_loss: 0.03184223175048828\n",
      "iteration 1455, dc_loss: 0.12488792836666107, tv_loss: 0.03197381645441055\n",
      "iteration 1456, dc_loss: 0.12501858174800873, tv_loss: 0.03183463215827942\n",
      "iteration 1457, dc_loss: 0.12483371049165726, tv_loss: 0.031984277069568634\n",
      "iteration 1458, dc_loss: 0.12499606609344482, tv_loss: 0.031796153634786606\n",
      "iteration 1459, dc_loss: 0.12473795562982559, tv_loss: 0.03201449289917946\n",
      "iteration 1460, dc_loss: 0.12493179738521576, tv_loss: 0.03178107738494873\n",
      "iteration 1461, dc_loss: 0.12464062869548798, tv_loss: 0.03202851116657257\n",
      "iteration 1462, dc_loss: 0.12481953948736191, tv_loss: 0.03178649768233299\n",
      "iteration 1463, dc_loss: 0.12452532351016998, tv_loss: 0.03202633932232857\n",
      "iteration 1464, dc_loss: 0.12466287612915039, tv_loss: 0.03181827813386917\n",
      "iteration 1465, dc_loss: 0.12442762404680252, tv_loss: 0.03198336064815521\n",
      "iteration 1466, dc_loss: 0.12448287755250931, tv_loss: 0.031859420239925385\n",
      "iteration 1467, dc_loss: 0.12431301921606064, tv_loss: 0.03196226432919502\n",
      "iteration 1468, dc_loss: 0.12433207780122757, tv_loss: 0.03188510984182358\n",
      "iteration 1469, dc_loss: 0.12424244731664658, tv_loss: 0.031937263906002045\n",
      "iteration 1470, dc_loss: 0.1242070123553276, tv_loss: 0.031913332641124725\n",
      "iteration 1471, dc_loss: 0.12418998032808304, tv_loss: 0.03188407048583031\n",
      "iteration 1472, dc_loss: 0.12407650798559189, tv_loss: 0.03197290375828743\n",
      "iteration 1473, dc_loss: 0.12412811070680618, tv_loss: 0.031883303076028824\n",
      "iteration 1474, dc_loss: 0.12397797405719757, tv_loss: 0.031976085156202316\n",
      "iteration 1475, dc_loss: 0.1240740641951561, tv_loss: 0.03185863420367241\n",
      "iteration 1476, dc_loss: 0.12388158589601517, tv_loss: 0.03199344873428345\n",
      "iteration 1477, dc_loss: 0.12397141754627228, tv_loss: 0.03185562416911125\n",
      "iteration 1478, dc_loss: 0.12381193041801453, tv_loss: 0.03198520466685295\n",
      "iteration 1479, dc_loss: 0.123845174908638, tv_loss: 0.03189903125166893\n",
      "iteration 1480, dc_loss: 0.12371253222227097, tv_loss: 0.03196662664413452\n",
      "iteration 1481, dc_loss: 0.12377164512872696, tv_loss: 0.031883951276540756\n",
      "iteration 1482, dc_loss: 0.12364384531974792, tv_loss: 0.03197106346487999\n",
      "iteration 1483, dc_loss: 0.12366168946027756, tv_loss: 0.03190502151846886\n",
      "iteration 1484, dc_loss: 0.1235794872045517, tv_loss: 0.03195277974009514\n",
      "iteration 1485, dc_loss: 0.12359212338924408, tv_loss: 0.03190737962722778\n",
      "iteration 1486, dc_loss: 0.1234765499830246, tv_loss: 0.03200157731771469\n",
      "iteration 1487, dc_loss: 0.12355928122997284, tv_loss: 0.03187768906354904\n",
      "iteration 1488, dc_loss: 0.12341102957725525, tv_loss: 0.03199271112680435\n",
      "iteration 1489, dc_loss: 0.12349660694599152, tv_loss: 0.03187426179647446\n",
      "iteration 1490, dc_loss: 0.12336255609989166, tv_loss: 0.032003723084926605\n",
      "iteration 1491, dc_loss: 0.12344317883253098, tv_loss: 0.03187686949968338\n",
      "iteration 1492, dc_loss: 0.1232498437166214, tv_loss: 0.03204498440027237\n",
      "iteration 1493, dc_loss: 0.12342014163732529, tv_loss: 0.0318392850458622\n",
      "iteration 1494, dc_loss: 0.12318317592144012, tv_loss: 0.03204972296953201\n",
      "iteration 1495, dc_loss: 0.1233559399843216, tv_loss: 0.031835153698921204\n",
      "iteration 1496, dc_loss: 0.12306243926286697, tv_loss: 0.032087355852127075\n",
      "iteration 1497, dc_loss: 0.12328193336725235, tv_loss: 0.031820014119148254\n",
      "iteration 1498, dc_loss: 0.12297580391168594, tv_loss: 0.03208240866661072\n",
      "iteration 1499, dc_loss: 0.12318602204322815, tv_loss: 0.03184878081083298\n",
      "iteration 1500, dc_loss: 0.12288009375333786, tv_loss: 0.032051634043455124\n",
      "iteration 1501, dc_loss: 0.12299875169992447, tv_loss: 0.031872011721134186\n",
      "iteration 1502, dc_loss: 0.12281794100999832, tv_loss: 0.03200226649641991\n",
      "iteration 1503, dc_loss: 0.1228509396314621, tv_loss: 0.031920868903398514\n",
      "iteration 1504, dc_loss: 0.12274445593357086, tv_loss: 0.032009683549404144\n",
      "iteration 1505, dc_loss: 0.12275506556034088, tv_loss: 0.031950850039720535\n",
      "iteration 1506, dc_loss: 0.12265907227993011, tv_loss: 0.03199543058872223\n",
      "iteration 1507, dc_loss: 0.12261123210191727, tv_loss: 0.03195340186357498\n",
      "iteration 1508, dc_loss: 0.122556172311306, tv_loss: 0.03195852041244507\n",
      "iteration 1509, dc_loss: 0.12248099595308304, tv_loss: 0.031985726207494736\n",
      "iteration 1510, dc_loss: 0.12248940020799637, tv_loss: 0.03193933516740799\n",
      "iteration 1511, dc_loss: 0.12237673997879028, tv_loss: 0.032034698873758316\n",
      "iteration 1512, dc_loss: 0.12244842946529388, tv_loss: 0.03191768750548363\n",
      "iteration 1513, dc_loss: 0.12227676808834076, tv_loss: 0.032060109078884125\n",
      "iteration 1514, dc_loss: 0.12239498645067215, tv_loss: 0.031900156289339066\n",
      "iteration 1515, dc_loss: 0.12222356349229813, tv_loss: 0.032059766352176666\n",
      "iteration 1516, dc_loss: 0.12227418273687363, tv_loss: 0.03194069862365723\n",
      "iteration 1517, dc_loss: 0.12212061882019043, tv_loss: 0.03202047199010849\n",
      "iteration 1518, dc_loss: 0.12214765697717667, tv_loss: 0.03194978088140488\n",
      "iteration 1519, dc_loss: 0.1220279186964035, tv_loss: 0.032028887420892715\n",
      "iteration 1520, dc_loss: 0.12204264104366302, tv_loss: 0.03197984769940376\n",
      "iteration 1521, dc_loss: 0.12195570766925812, tv_loss: 0.03200218454003334\n",
      "iteration 1522, dc_loss: 0.12194162607192993, tv_loss: 0.03197092562913895\n",
      "iteration 1523, dc_loss: 0.12186678498983383, tv_loss: 0.03200671449303627\n",
      "iteration 1524, dc_loss: 0.12185637652873993, tv_loss: 0.031979359686374664\n",
      "iteration 1525, dc_loss: 0.12178909778594971, tv_loss: 0.03200848400592804\n",
      "iteration 1526, dc_loss: 0.12177661061286926, tv_loss: 0.03197575360536575\n",
      "iteration 1527, dc_loss: 0.12168861925601959, tv_loss: 0.03200765699148178\n",
      "iteration 1528, dc_loss: 0.12167444825172424, tv_loss: 0.03197851404547691\n",
      "iteration 1529, dc_loss: 0.12160276621580124, tv_loss: 0.032005708664655685\n",
      "iteration 1530, dc_loss: 0.12160413712263107, tv_loss: 0.031976744532585144\n",
      "iteration 1531, dc_loss: 0.12151020765304565, tv_loss: 0.03202510625123978\n",
      "iteration 1532, dc_loss: 0.12155834585428238, tv_loss: 0.03194769471883774\n",
      "iteration 1533, dc_loss: 0.12142205983400345, tv_loss: 0.03206484392285347\n",
      "iteration 1534, dc_loss: 0.1214994564652443, tv_loss: 0.03195464611053467\n",
      "iteration 1535, dc_loss: 0.12136059999465942, tv_loss: 0.0320645235478878\n",
      "iteration 1536, dc_loss: 0.12148793786764145, tv_loss: 0.03192640841007233\n",
      "iteration 1537, dc_loss: 0.12132950127124786, tv_loss: 0.032093629240989685\n",
      "iteration 1538, dc_loss: 0.1215582862496376, tv_loss: 0.03189125284552574\n",
      "iteration 1539, dc_loss: 0.12135269492864609, tv_loss: 0.03218276426196098\n",
      "iteration 1540, dc_loss: 0.12166823446750641, tv_loss: 0.03185388073325157\n",
      "iteration 1541, dc_loss: 0.12142561376094818, tv_loss: 0.03220191225409508\n",
      "iteration 1542, dc_loss: 0.12171874195337296, tv_loss: 0.031833015382289886\n",
      "iteration 1543, dc_loss: 0.12128189951181412, tv_loss: 0.03221907839179039\n",
      "iteration 1544, dc_loss: 0.12144444137811661, tv_loss: 0.031849365681409836\n",
      "iteration 1545, dc_loss: 0.12100523710250854, tv_loss: 0.032132260501384735\n",
      "iteration 1546, dc_loss: 0.12105986475944519, tv_loss: 0.03194141015410423\n",
      "iteration 1547, dc_loss: 0.12088612467050552, tv_loss: 0.03204748407006264\n",
      "iteration 1548, dc_loss: 0.12087991088628769, tv_loss: 0.03204195201396942\n",
      "iteration 1549, dc_loss: 0.1209859848022461, tv_loss: 0.031945254653692245\n",
      "iteration 1550, dc_loss: 0.12083060294389725, tv_loss: 0.03212140500545502\n",
      "iteration 1551, dc_loss: 0.12098128348588943, tv_loss: 0.0319090373814106\n",
      "iteration 1552, dc_loss: 0.12070753425359726, tv_loss: 0.03212588280439377\n",
      "iteration 1553, dc_loss: 0.12080999463796616, tv_loss: 0.0319298580288887\n",
      "iteration 1554, dc_loss: 0.12058541923761368, tv_loss: 0.03209303691983223\n",
      "iteration 1555, dc_loss: 0.12065248936414719, tv_loss: 0.03198311850428581\n",
      "iteration 1556, dc_loss: 0.12053809314966202, tv_loss: 0.03206567093729973\n",
      "iteration 1557, dc_loss: 0.12051725387573242, tv_loss: 0.03204300254583359\n",
      "iteration 1558, dc_loss: 0.12052465230226517, tv_loss: 0.03199123218655586\n",
      "iteration 1559, dc_loss: 0.12041543424129486, tv_loss: 0.032059092074632645\n",
      "iteration 1560, dc_loss: 0.12044057995080948, tv_loss: 0.03198673576116562\n",
      "iteration 1561, dc_loss: 0.12029974162578583, tv_loss: 0.03208692744374275\n",
      "iteration 1562, dc_loss: 0.12035111337900162, tv_loss: 0.031999580562114716\n",
      "iteration 1563, dc_loss: 0.12022886425256729, tv_loss: 0.0320817194879055\n",
      "iteration 1564, dc_loss: 0.12026536464691162, tv_loss: 0.031990714371204376\n",
      "iteration 1565, dc_loss: 0.12015897035598755, tv_loss: 0.032055191695690155\n",
      "iteration 1566, dc_loss: 0.12014047801494598, tv_loss: 0.032034799456596375\n",
      "iteration 1567, dc_loss: 0.12010790407657623, tv_loss: 0.032029882073402405\n",
      "iteration 1568, dc_loss: 0.12005843222141266, tv_loss: 0.03205917775630951\n",
      "iteration 1569, dc_loss: 0.12002250552177429, tv_loss: 0.032047759741544724\n",
      "iteration 1570, dc_loss: 0.11994554847478867, tv_loss: 0.032074954360723495\n",
      "iteration 1571, dc_loss: 0.1199287548661232, tv_loss: 0.032031893730163574\n",
      "iteration 1572, dc_loss: 0.11987526714801788, tv_loss: 0.03204378858208656\n",
      "iteration 1573, dc_loss: 0.1198631152510643, tv_loss: 0.03203262761235237\n",
      "iteration 1574, dc_loss: 0.11979010701179504, tv_loss: 0.03209519013762474\n",
      "iteration 1575, dc_loss: 0.11978597939014435, tv_loss: 0.03204070404171944\n",
      "iteration 1576, dc_loss: 0.11972149461507797, tv_loss: 0.03206159546971321\n",
      "iteration 1577, dc_loss: 0.11971535533666611, tv_loss: 0.032038431614637375\n",
      "iteration 1578, dc_loss: 0.11963111162185669, tv_loss: 0.0320899598300457\n",
      "iteration 1579, dc_loss: 0.11962538957595825, tv_loss: 0.03206044062972069\n",
      "iteration 1580, dc_loss: 0.11956482380628586, tv_loss: 0.03206348419189453\n",
      "iteration 1581, dc_loss: 0.11953727155923843, tv_loss: 0.032063573598861694\n",
      "iteration 1582, dc_loss: 0.11947960406541824, tv_loss: 0.03207496926188469\n",
      "iteration 1583, dc_loss: 0.11948462575674057, tv_loss: 0.03204880654811859\n",
      "iteration 1584, dc_loss: 0.1194053590297699, tv_loss: 0.032088518142700195\n",
      "iteration 1585, dc_loss: 0.1194295883178711, tv_loss: 0.032027535140514374\n",
      "iteration 1586, dc_loss: 0.11933860182762146, tv_loss: 0.03210614249110222\n",
      "iteration 1587, dc_loss: 0.11941883713006973, tv_loss: 0.03200007975101471\n",
      "iteration 1588, dc_loss: 0.11927769333124161, tv_loss: 0.03216153755784035\n",
      "iteration 1589, dc_loss: 0.11942467838525772, tv_loss: 0.03200426325201988\n",
      "iteration 1590, dc_loss: 0.11929522454738617, tv_loss: 0.0321640707552433\n",
      "iteration 1591, dc_loss: 0.11952943354845047, tv_loss: 0.03194504976272583\n",
      "iteration 1592, dc_loss: 0.11932186782360077, tv_loss: 0.032232340425252914\n",
      "iteration 1593, dc_loss: 0.11964967101812363, tv_loss: 0.0319029837846756\n",
      "iteration 1594, dc_loss: 0.11929251998662949, tv_loss: 0.03230699151754379\n",
      "iteration 1595, dc_loss: 0.11964325606822968, tv_loss: 0.03185626491904259\n",
      "iteration 1596, dc_loss: 0.11913476139307022, tv_loss: 0.03228873386979103\n",
      "iteration 1597, dc_loss: 0.11937388777732849, tv_loss: 0.03189682960510254\n",
      "iteration 1598, dc_loss: 0.1189308762550354, tv_loss: 0.032197389751672745\n",
      "iteration 1599, dc_loss: 0.11895684152841568, tv_loss: 0.03202557936310768\n",
      "iteration 1600, dc_loss: 0.11886110156774521, tv_loss: 0.03205699473619461\n",
      "iteration 1601, dc_loss: 0.11874812096357346, tv_loss: 0.03216409310698509\n",
      "iteration 1602, dc_loss: 0.11888400465250015, tv_loss: 0.03198833018541336\n",
      "iteration 1603, dc_loss: 0.11871755123138428, tv_loss: 0.03206603229045868\n",
      "iteration 1604, dc_loss: 0.11866084486246109, tv_loss: 0.032128311693668365\n",
      "iteration 1605, dc_loss: 0.11877133697271347, tv_loss: 0.031993426382541656\n",
      "iteration 1606, dc_loss: 0.11860620230436325, tv_loss: 0.032090432941913605\n",
      "iteration 1607, dc_loss: 0.11856181174516678, tv_loss: 0.03212640807032585\n",
      "iteration 1608, dc_loss: 0.11866351217031479, tv_loss: 0.03200652077794075\n",
      "iteration 1609, dc_loss: 0.11851972341537476, tv_loss: 0.03209507465362549\n",
      "iteration 1610, dc_loss: 0.11847046762704849, tv_loss: 0.03210935369133949\n",
      "iteration 1611, dc_loss: 0.11855024844408035, tv_loss: 0.03201127052307129\n",
      "iteration 1612, dc_loss: 0.1184251457452774, tv_loss: 0.03209064528346062\n",
      "iteration 1613, dc_loss: 0.11839329451322556, tv_loss: 0.03208727389574051\n",
      "iteration 1614, dc_loss: 0.1184311956167221, tv_loss: 0.03203997761011124\n",
      "iteration 1615, dc_loss: 0.11833476275205612, tv_loss: 0.03211710974574089\n",
      "iteration 1616, dc_loss: 0.11832023411989212, tv_loss: 0.03209000453352928\n",
      "iteration 1617, dc_loss: 0.11831899732351303, tv_loss: 0.03205960988998413\n",
      "iteration 1618, dc_loss: 0.11823984980583191, tv_loss: 0.03211040794849396\n",
      "iteration 1619, dc_loss: 0.11824770271778107, tv_loss: 0.0320592001080513\n",
      "iteration 1620, dc_loss: 0.11821280419826508, tv_loss: 0.032069131731987\n",
      "iteration 1621, dc_loss: 0.11815148591995239, tv_loss: 0.03211551159620285\n",
      "iteration 1622, dc_loss: 0.11815588176250458, tv_loss: 0.0320749506354332\n",
      "iteration 1623, dc_loss: 0.11811815947294235, tv_loss: 0.03208602964878082\n",
      "iteration 1624, dc_loss: 0.1180654913187027, tv_loss: 0.032102592289447784\n",
      "iteration 1625, dc_loss: 0.11807253211736679, tv_loss: 0.03206523507833481\n",
      "iteration 1626, dc_loss: 0.11802099645137787, tv_loss: 0.03208300843834877\n",
      "iteration 1627, dc_loss: 0.11798158288002014, tv_loss: 0.03209509700536728\n",
      "iteration 1628, dc_loss: 0.11798155307769775, tv_loss: 0.03206750005483627\n",
      "iteration 1629, dc_loss: 0.11792679131031036, tv_loss: 0.03209413215517998\n",
      "iteration 1630, dc_loss: 0.11789858341217041, tv_loss: 0.03209913522005081\n",
      "iteration 1631, dc_loss: 0.1178898960351944, tv_loss: 0.0320969857275486\n",
      "iteration 1632, dc_loss: 0.11783906817436218, tv_loss: 0.03211307153105736\n",
      "iteration 1633, dc_loss: 0.11782269924879074, tv_loss: 0.03209015727043152\n",
      "iteration 1634, dc_loss: 0.11779429018497467, tv_loss: 0.032082609832286835\n",
      "iteration 1635, dc_loss: 0.11774709075689316, tv_loss: 0.032110221683979034\n",
      "iteration 1636, dc_loss: 0.11773697286844254, tv_loss: 0.032101474702358246\n",
      "iteration 1637, dc_loss: 0.11769381910562515, tv_loss: 0.03211481124162674\n",
      "iteration 1638, dc_loss: 0.11766593903303146, tv_loss: 0.03211572393774986\n",
      "iteration 1639, dc_loss: 0.1176556795835495, tv_loss: 0.03208489716053009\n",
      "iteration 1640, dc_loss: 0.1176123321056366, tv_loss: 0.03210156038403511\n",
      "iteration 1641, dc_loss: 0.11758453398942947, tv_loss: 0.032117802649736404\n",
      "iteration 1642, dc_loss: 0.1175532117486, tv_loss: 0.03212601691484451\n",
      "iteration 1643, dc_loss: 0.11752082407474518, tv_loss: 0.03210584446787834\n",
      "iteration 1644, dc_loss: 0.11749663949012756, tv_loss: 0.03211134672164917\n",
      "iteration 1645, dc_loss: 0.11747299879789352, tv_loss: 0.03210512176156044\n",
      "iteration 1646, dc_loss: 0.1174393892288208, tv_loss: 0.032122354954481125\n",
      "iteration 1647, dc_loss: 0.1174086332321167, tv_loss: 0.032117970287799835\n",
      "iteration 1648, dc_loss: 0.11738467216491699, tv_loss: 0.03210627660155296\n",
      "iteration 1649, dc_loss: 0.11735189706087112, tv_loss: 0.032107796519994736\n",
      "iteration 1650, dc_loss: 0.11732016503810883, tv_loss: 0.03212186321616173\n",
      "iteration 1651, dc_loss: 0.11730079352855682, tv_loss: 0.03211264684796333\n",
      "iteration 1652, dc_loss: 0.11726997047662735, tv_loss: 0.0321185365319252\n",
      "iteration 1653, dc_loss: 0.11723851412534714, tv_loss: 0.03211616724729538\n",
      "iteration 1654, dc_loss: 0.11720772832632065, tv_loss: 0.03211034834384918\n",
      "iteration 1655, dc_loss: 0.11718158423900604, tv_loss: 0.03210952505469322\n",
      "iteration 1656, dc_loss: 0.1171555146574974, tv_loss: 0.032109566032886505\n",
      "iteration 1657, dc_loss: 0.11712843924760818, tv_loss: 0.0321073941886425\n",
      "iteration 1658, dc_loss: 0.11709585040807724, tv_loss: 0.03211045637726784\n",
      "iteration 1659, dc_loss: 0.11707023531198502, tv_loss: 0.03211188316345215\n",
      "iteration 1660, dc_loss: 0.11703016608953476, tv_loss: 0.03212646767497063\n",
      "iteration 1661, dc_loss: 0.11701418459415436, tv_loss: 0.03211187198758125\n",
      "iteration 1662, dc_loss: 0.1169978454709053, tv_loss: 0.032098811119794846\n",
      "iteration 1663, dc_loss: 0.11694569885730743, tv_loss: 0.03212004154920578\n",
      "iteration 1664, dc_loss: 0.1169285699725151, tv_loss: 0.03211518004536629\n",
      "iteration 1665, dc_loss: 0.11690202355384827, tv_loss: 0.03213643655180931\n",
      "iteration 1666, dc_loss: 0.11686694622039795, tv_loss: 0.032130103558301926\n",
      "iteration 1667, dc_loss: 0.11684462428092957, tv_loss: 0.03213559463620186\n",
      "iteration 1668, dc_loss: 0.1168099045753479, tv_loss: 0.032128773629665375\n",
      "iteration 1669, dc_loss: 0.11679065227508545, tv_loss: 0.032120730727910995\n",
      "iteration 1670, dc_loss: 0.11676183342933655, tv_loss: 0.03212493658065796\n",
      "iteration 1671, dc_loss: 0.11672040820121765, tv_loss: 0.03215761110186577\n",
      "iteration 1672, dc_loss: 0.11670452356338501, tv_loss: 0.03213353455066681\n",
      "iteration 1673, dc_loss: 0.11667854338884354, tv_loss: 0.03212619945406914\n",
      "iteration 1674, dc_loss: 0.1166391521692276, tv_loss: 0.03214981406927109\n",
      "iteration 1675, dc_loss: 0.1166260614991188, tv_loss: 0.03213632479310036\n",
      "iteration 1676, dc_loss: 0.1165933832526207, tv_loss: 0.032140620052814484\n",
      "iteration 1677, dc_loss: 0.11655232310295105, tv_loss: 0.03215663507580757\n",
      "iteration 1678, dc_loss: 0.11654037982225418, tv_loss: 0.032129138708114624\n",
      "iteration 1679, dc_loss: 0.11650697886943817, tv_loss: 0.03215067461133003\n",
      "iteration 1680, dc_loss: 0.11647852510213852, tv_loss: 0.03215295076370239\n",
      "iteration 1681, dc_loss: 0.11645881086587906, tv_loss: 0.032131992280483246\n",
      "iteration 1682, dc_loss: 0.11642779409885406, tv_loss: 0.032135169953107834\n",
      "iteration 1683, dc_loss: 0.11639009416103363, tv_loss: 0.03217165917158127\n",
      "iteration 1684, dc_loss: 0.11635906249284744, tv_loss: 0.032155703753232956\n",
      "iteration 1685, dc_loss: 0.11634275317192078, tv_loss: 0.03214867040514946\n",
      "iteration 1686, dc_loss: 0.11632364243268967, tv_loss: 0.03214488923549652\n",
      "iteration 1687, dc_loss: 0.11628935486078262, tv_loss: 0.032157108187675476\n",
      "iteration 1688, dc_loss: 0.11626458168029785, tv_loss: 0.032150477170944214\n",
      "iteration 1689, dc_loss: 0.11622457951307297, tv_loss: 0.032163217663764954\n",
      "iteration 1690, dc_loss: 0.11619396507740021, tv_loss: 0.032156772911548615\n",
      "iteration 1691, dc_loss: 0.11618129909038544, tv_loss: 0.032160282135009766\n",
      "iteration 1692, dc_loss: 0.11614689230918884, tv_loss: 0.032150670886039734\n",
      "iteration 1693, dc_loss: 0.11612319201231003, tv_loss: 0.032146748155355453\n",
      "iteration 1694, dc_loss: 0.11610274016857147, tv_loss: 0.03214925527572632\n",
      "iteration 1695, dc_loss: 0.11605602502822876, tv_loss: 0.03218337148427963\n",
      "iteration 1696, dc_loss: 0.1160431057214737, tv_loss: 0.03215567395091057\n",
      "iteration 1697, dc_loss: 0.1160164326429367, tv_loss: 0.03214893862605095\n",
      "iteration 1698, dc_loss: 0.11597613245248795, tv_loss: 0.03217534348368645\n",
      "iteration 1699, dc_loss: 0.11596919596195221, tv_loss: 0.032144512981176376\n",
      "iteration 1700, dc_loss: 0.11593575030565262, tv_loss: 0.0321689210832119\n",
      "iteration 1701, dc_loss: 0.11589238047599792, tv_loss: 0.032177045941352844\n",
      "iteration 1702, dc_loss: 0.11587739735841751, tv_loss: 0.032154761254787445\n",
      "iteration 1703, dc_loss: 0.11585284024477005, tv_loss: 0.0321807824075222\n",
      "iteration 1704, dc_loss: 0.1158197745680809, tv_loss: 0.032162513583898544\n",
      "iteration 1705, dc_loss: 0.11579355597496033, tv_loss: 0.03216484561562538\n",
      "iteration 1706, dc_loss: 0.11578094959259033, tv_loss: 0.032166045159101486\n",
      "iteration 1707, dc_loss: 0.11573434621095657, tv_loss: 0.03217972069978714\n",
      "iteration 1708, dc_loss: 0.11569540947675705, tv_loss: 0.032190993428230286\n",
      "iteration 1709, dc_loss: 0.11570292711257935, tv_loss: 0.032154783606529236\n",
      "iteration 1710, dc_loss: 0.11566699296236038, tv_loss: 0.03216227889060974\n",
      "iteration 1711, dc_loss: 0.1156233698129654, tv_loss: 0.03218870982527733\n",
      "iteration 1712, dc_loss: 0.11560999602079391, tv_loss: 0.03216633200645447\n",
      "iteration 1713, dc_loss: 0.11557973176240921, tv_loss: 0.0321638360619545\n",
      "iteration 1714, dc_loss: 0.11555537581443787, tv_loss: 0.03217050060629845\n",
      "iteration 1715, dc_loss: 0.11552967131137848, tv_loss: 0.032165151089429855\n",
      "iteration 1716, dc_loss: 0.11549016088247299, tv_loss: 0.032182477414608\n",
      "iteration 1717, dc_loss: 0.11548247188329697, tv_loss: 0.03216314688324928\n",
      "iteration 1718, dc_loss: 0.11544211208820343, tv_loss: 0.03216826915740967\n",
      "iteration 1719, dc_loss: 0.11540795862674713, tv_loss: 0.0321822427213192\n",
      "iteration 1720, dc_loss: 0.11540526151657104, tv_loss: 0.03215983510017395\n",
      "iteration 1721, dc_loss: 0.1153656542301178, tv_loss: 0.03217281401157379\n",
      "iteration 1722, dc_loss: 0.11533092707395554, tv_loss: 0.032175853848457336\n",
      "iteration 1723, dc_loss: 0.11531248688697815, tv_loss: 0.03216820955276489\n",
      "iteration 1724, dc_loss: 0.11528806388378143, tv_loss: 0.032173238694667816\n",
      "iteration 1725, dc_loss: 0.11526165902614594, tv_loss: 0.03216969594359398\n",
      "iteration 1726, dc_loss: 0.11523003876209259, tv_loss: 0.032173555344343185\n",
      "iteration 1727, dc_loss: 0.1152065321803093, tv_loss: 0.03217492997646332\n",
      "iteration 1728, dc_loss: 0.11518436670303345, tv_loss: 0.03218187391757965\n",
      "iteration 1729, dc_loss: 0.11515283584594727, tv_loss: 0.032199595123529434\n",
      "iteration 1730, dc_loss: 0.1151147410273552, tv_loss: 0.03219832479953766\n",
      "iteration 1731, dc_loss: 0.11510536074638367, tv_loss: 0.032173555344343185\n",
      "iteration 1732, dc_loss: 0.11507860571146011, tv_loss: 0.032176390290260315\n",
      "iteration 1733, dc_loss: 0.11505357176065445, tv_loss: 0.03218061476945877\n",
      "iteration 1734, dc_loss: 0.1150163933634758, tv_loss: 0.032198283821344376\n",
      "iteration 1735, dc_loss: 0.11498726904392242, tv_loss: 0.03221546486020088\n",
      "iteration 1736, dc_loss: 0.1149810403585434, tv_loss: 0.032177574932575226\n",
      "iteration 1737, dc_loss: 0.1149427518248558, tv_loss: 0.03218863531947136\n",
      "iteration 1738, dc_loss: 0.11491077393293381, tv_loss: 0.032202377915382385\n",
      "iteration 1739, dc_loss: 0.11489729583263397, tv_loss: 0.03220447897911072\n",
      "iteration 1740, dc_loss: 0.11486589163541794, tv_loss: 0.032200396060943604\n",
      "iteration 1741, dc_loss: 0.1148306354880333, tv_loss: 0.0322018526494503\n",
      "iteration 1742, dc_loss: 0.11481256037950516, tv_loss: 0.032205980271101\n",
      "iteration 1743, dc_loss: 0.11480139940977097, tv_loss: 0.03219896927475929\n",
      "iteration 1744, dc_loss: 0.11475901305675507, tv_loss: 0.03219829872250557\n",
      "iteration 1745, dc_loss: 0.11472515761852264, tv_loss: 0.03221392259001732\n",
      "iteration 1746, dc_loss: 0.11470387876033783, tv_loss: 0.03220774233341217\n",
      "iteration 1747, dc_loss: 0.11469223350286484, tv_loss: 0.03220365568995476\n",
      "iteration 1748, dc_loss: 0.11466176807880402, tv_loss: 0.032189127057790756\n",
      "iteration 1749, dc_loss: 0.11463075876235962, tv_loss: 0.0322011336684227\n",
      "iteration 1750, dc_loss: 0.11459766328334808, tv_loss: 0.032233476638793945\n",
      "iteration 1751, dc_loss: 0.11458389461040497, tv_loss: 0.032195985317230225\n",
      "iteration 1752, dc_loss: 0.11455363035202026, tv_loss: 0.032207418233156204\n",
      "iteration 1753, dc_loss: 0.11452298611402512, tv_loss: 0.03221715986728668\n",
      "iteration 1754, dc_loss: 0.1144988015294075, tv_loss: 0.03220392391085625\n",
      "iteration 1755, dc_loss: 0.11448696255683899, tv_loss: 0.032215289771556854\n",
      "iteration 1756, dc_loss: 0.11445031315088272, tv_loss: 0.03219855576753616\n",
      "iteration 1757, dc_loss: 0.11441808938980103, tv_loss: 0.03222203254699707\n",
      "iteration 1758, dc_loss: 0.11441131681203842, tv_loss: 0.032213788479566574\n",
      "iteration 1759, dc_loss: 0.11436711996793747, tv_loss: 0.03221411630511284\n",
      "iteration 1760, dc_loss: 0.11434316635131836, tv_loss: 0.03221677616238594\n",
      "iteration 1761, dc_loss: 0.11432152986526489, tv_loss: 0.03223010525107384\n",
      "iteration 1762, dc_loss: 0.11431152373552322, tv_loss: 0.032205868512392044\n",
      "iteration 1763, dc_loss: 0.1142711341381073, tv_loss: 0.0322147011756897\n",
      "iteration 1764, dc_loss: 0.11423873901367188, tv_loss: 0.032225266098976135\n",
      "iteration 1765, dc_loss: 0.11421669274568558, tv_loss: 0.03223090618848801\n",
      "iteration 1766, dc_loss: 0.1142033264040947, tv_loss: 0.032227374613285065\n",
      "iteration 1767, dc_loss: 0.11417626589536667, tv_loss: 0.03220900520682335\n",
      "iteration 1768, dc_loss: 0.1141369566321373, tv_loss: 0.032232578843832016\n",
      "iteration 1769, dc_loss: 0.11411745846271515, tv_loss: 0.03223784267902374\n",
      "iteration 1770, dc_loss: 0.11409617960453033, tv_loss: 0.032229866832494736\n",
      "iteration 1771, dc_loss: 0.11407370865345001, tv_loss: 0.03221874311566353\n",
      "iteration 1772, dc_loss: 0.11404116451740265, tv_loss: 0.03225070238113403\n",
      "iteration 1773, dc_loss: 0.11401191353797913, tv_loss: 0.032240305095911026\n",
      "iteration 1774, dc_loss: 0.11399739980697632, tv_loss: 0.032224491238594055\n",
      "iteration 1775, dc_loss: 0.11397118866443634, tv_loss: 0.03224770352244377\n",
      "iteration 1776, dc_loss: 0.11393488943576813, tv_loss: 0.03225287050008774\n",
      "iteration 1777, dc_loss: 0.11393067985773087, tv_loss: 0.032229360193014145\n",
      "iteration 1778, dc_loss: 0.11388927698135376, tv_loss: 0.03223973512649536\n",
      "iteration 1779, dc_loss: 0.11386284977197647, tv_loss: 0.03226907551288605\n",
      "iteration 1780, dc_loss: 0.11383714526891708, tv_loss: 0.03223050385713577\n",
      "iteration 1781, dc_loss: 0.11383137851953506, tv_loss: 0.032241374254226685\n",
      "iteration 1782, dc_loss: 0.11378693580627441, tv_loss: 0.03225187212228775\n",
      "iteration 1783, dc_loss: 0.11377034336328506, tv_loss: 0.032232947647571564\n",
      "iteration 1784, dc_loss: 0.11374039947986603, tv_loss: 0.032246481627225876\n",
      "iteration 1785, dc_loss: 0.1137244924902916, tv_loss: 0.03224480152130127\n",
      "iteration 1786, dc_loss: 0.11368538439273834, tv_loss: 0.032259609550237656\n",
      "iteration 1787, dc_loss: 0.11368536204099655, tv_loss: 0.03221484273672104\n",
      "iteration 1788, dc_loss: 0.11364619433879852, tv_loss: 0.03225822001695633\n",
      "iteration 1789, dc_loss: 0.11364742368459702, tv_loss: 0.0322248637676239\n",
      "iteration 1790, dc_loss: 0.11358720809221268, tv_loss: 0.032253675162792206\n",
      "iteration 1791, dc_loss: 0.11361191421747208, tv_loss: 0.032220810651779175\n",
      "iteration 1792, dc_loss: 0.11355292052030563, tv_loss: 0.03228175640106201\n",
      "iteration 1793, dc_loss: 0.11358797550201416, tv_loss: 0.03221040964126587\n",
      "iteration 1794, dc_loss: 0.1135246753692627, tv_loss: 0.03226478397846222\n",
      "iteration 1795, dc_loss: 0.11357218772172928, tv_loss: 0.03219697251915932\n",
      "iteration 1796, dc_loss: 0.1134691834449768, tv_loss: 0.03230476379394531\n",
      "iteration 1797, dc_loss: 0.11352825164794922, tv_loss: 0.032224494963884354\n",
      "iteration 1798, dc_loss: 0.11341391503810883, tv_loss: 0.03228209540247917\n",
      "iteration 1799, dc_loss: 0.11344809830188751, tv_loss: 0.03220342472195625\n",
      "iteration 1800, dc_loss: 0.11335429549217224, tv_loss: 0.032290466129779816\n",
      "iteration 1801, dc_loss: 0.11337009072303772, tv_loss: 0.03222206234931946\n",
      "iteration 1802, dc_loss: 0.113289013504982, tv_loss: 0.032269034534692764\n",
      "iteration 1803, dc_loss: 0.11327923089265823, tv_loss: 0.032257676124572754\n",
      "iteration 1804, dc_loss: 0.11326955258846283, tv_loss: 0.03226019814610481\n",
      "iteration 1805, dc_loss: 0.11322514712810516, tv_loss: 0.03226732835173607\n",
      "iteration 1806, dc_loss: 0.11323628574609756, tv_loss: 0.032219842076301575\n",
      "iteration 1807, dc_loss: 0.11317914724349976, tv_loss: 0.032290395349264145\n",
      "iteration 1808, dc_loss: 0.11321523785591125, tv_loss: 0.03223100304603577\n",
      "iteration 1809, dc_loss: 0.11311338096857071, tv_loss: 0.032299868762493134\n",
      "iteration 1810, dc_loss: 0.11317801475524902, tv_loss: 0.032207198441028595\n",
      "iteration 1811, dc_loss: 0.11309615522623062, tv_loss: 0.03228001296520233\n",
      "iteration 1812, dc_loss: 0.11311080306768417, tv_loss: 0.0322260707616806\n",
      "iteration 1813, dc_loss: 0.11303433030843735, tv_loss: 0.03228797763586044\n",
      "iteration 1814, dc_loss: 0.11305848509073257, tv_loss: 0.03221410512924194\n",
      "iteration 1815, dc_loss: 0.11297378689050674, tv_loss: 0.03226882219314575\n",
      "iteration 1816, dc_loss: 0.11298154294490814, tv_loss: 0.032246120274066925\n",
      "iteration 1817, dc_loss: 0.1129252091050148, tv_loss: 0.03228488937020302\n",
      "iteration 1818, dc_loss: 0.11293574422597885, tv_loss: 0.03223167732357979\n",
      "iteration 1819, dc_loss: 0.11286834627389908, tv_loss: 0.032275982201099396\n",
      "iteration 1820, dc_loss: 0.11286803334951401, tv_loss: 0.03224823996424675\n",
      "iteration 1821, dc_loss: 0.11283589899539948, tv_loss: 0.032269664108753204\n",
      "iteration 1822, dc_loss: 0.11282949894666672, tv_loss: 0.03225865960121155\n",
      "iteration 1823, dc_loss: 0.11278585344552994, tv_loss: 0.03226161748170853\n",
      "iteration 1824, dc_loss: 0.112764373421669, tv_loss: 0.03225920721888542\n",
      "iteration 1825, dc_loss: 0.1127488762140274, tv_loss: 0.03226200118660927\n",
      "iteration 1826, dc_loss: 0.11272010207176208, tv_loss: 0.032284028828144073\n",
      "iteration 1827, dc_loss: 0.11270154267549515, tv_loss: 0.03225747123360634\n",
      "iteration 1828, dc_loss: 0.11266479641199112, tv_loss: 0.03227463737130165\n",
      "iteration 1829, dc_loss: 0.11266464740037918, tv_loss: 0.03225437179207802\n",
      "iteration 1830, dc_loss: 0.11261686682701111, tv_loss: 0.03227591887116432\n",
      "iteration 1831, dc_loss: 0.11262504756450653, tv_loss: 0.03226253017783165\n",
      "iteration 1832, dc_loss: 0.11255466192960739, tv_loss: 0.032280098646879196\n",
      "iteration 1833, dc_loss: 0.11256826668977737, tv_loss: 0.032264359295368195\n",
      "iteration 1834, dc_loss: 0.11252236366271973, tv_loss: 0.03229647874832153\n",
      "iteration 1835, dc_loss: 0.11253809928894043, tv_loss: 0.03224628046154976\n",
      "iteration 1836, dc_loss: 0.11246329545974731, tv_loss: 0.032297201454639435\n",
      "iteration 1837, dc_loss: 0.11250291764736176, tv_loss: 0.03224778175354004\n",
      "iteration 1838, dc_loss: 0.11243383586406708, tv_loss: 0.03231373056769371\n",
      "iteration 1839, dc_loss: 0.11250894516706467, tv_loss: 0.0322362519800663\n",
      "iteration 1840, dc_loss: 0.11242369562387466, tv_loss: 0.03232136368751526\n",
      "iteration 1841, dc_loss: 0.11252503097057343, tv_loss: 0.03221486136317253\n",
      "iteration 1842, dc_loss: 0.11243552714586258, tv_loss: 0.03234701231122017\n",
      "iteration 1843, dc_loss: 0.11259011179208755, tv_loss: 0.0321962833404541\n",
      "iteration 1844, dc_loss: 0.11241511255502701, tv_loss: 0.03238164260983467\n",
      "iteration 1845, dc_loss: 0.11257100850343704, tv_loss: 0.03217200189828873\n",
      "iteration 1846, dc_loss: 0.11232704669237137, tv_loss: 0.03236886486411095\n",
      "iteration 1847, dc_loss: 0.11244408786296844, tv_loss: 0.03219359740614891\n",
      "iteration 1848, dc_loss: 0.1122196763753891, tv_loss: 0.03235685080289841\n",
      "iteration 1849, dc_loss: 0.11229877173900604, tv_loss: 0.03221779316663742\n",
      "iteration 1850, dc_loss: 0.11217176914215088, tv_loss: 0.03229229897260666\n",
      "iteration 1851, dc_loss: 0.1121368482708931, tv_loss: 0.032290536910295486\n",
      "iteration 1852, dc_loss: 0.11219809204339981, tv_loss: 0.032220326364040375\n",
      "iteration 1853, dc_loss: 0.11207249015569687, tv_loss: 0.03234996274113655\n",
      "iteration 1854, dc_loss: 0.11221348494291306, tv_loss: 0.032201528549194336\n",
      "iteration 1855, dc_loss: 0.11203783750534058, tv_loss: 0.0323636420071125\n",
      "iteration 1856, dc_loss: 0.11215001344680786, tv_loss: 0.03220449388027191\n",
      "iteration 1857, dc_loss: 0.1119808778166771, tv_loss: 0.03233572468161583\n",
      "iteration 1858, dc_loss: 0.11199957877397537, tv_loss: 0.03225408494472504\n",
      "iteration 1859, dc_loss: 0.11195708811283112, tv_loss: 0.0322674922645092\n",
      "iteration 1860, dc_loss: 0.11190379410982132, tv_loss: 0.03230312094092369\n",
      "iteration 1861, dc_loss: 0.11194095015525818, tv_loss: 0.032252196222543716\n",
      "iteration 1862, dc_loss: 0.11185135692358017, tv_loss: 0.03232589364051819\n",
      "iteration 1863, dc_loss: 0.11190865933895111, tv_loss: 0.03224297985434532\n",
      "iteration 1864, dc_loss: 0.11180820316076279, tv_loss: 0.032321102917194366\n",
      "iteration 1865, dc_loss: 0.11184658110141754, tv_loss: 0.03226012736558914\n",
      "iteration 1866, dc_loss: 0.11179094016551971, tv_loss: 0.032291512936353683\n",
      "iteration 1867, dc_loss: 0.11177613586187363, tv_loss: 0.03229188546538353\n",
      "iteration 1868, dc_loss: 0.11175607144832611, tv_loss: 0.03229830786585808\n",
      "iteration 1869, dc_loss: 0.11172318458557129, tv_loss: 0.032302893698215485\n",
      "iteration 1870, dc_loss: 0.11169607192277908, tv_loss: 0.032284606248140335\n",
      "iteration 1871, dc_loss: 0.11165457963943481, tv_loss: 0.03229539468884468\n",
      "iteration 1872, dc_loss: 0.11164107918739319, tv_loss: 0.03228285536170006\n",
      "iteration 1873, dc_loss: 0.11161680519580841, tv_loss: 0.0322839580476284\n",
      "iteration 1874, dc_loss: 0.11158858984708786, tv_loss: 0.0323021374642849\n",
      "iteration 1875, dc_loss: 0.11159292608499527, tv_loss: 0.032284341752529144\n",
      "iteration 1876, dc_loss: 0.1115579679608345, tv_loss: 0.032315388321876526\n",
      "iteration 1877, dc_loss: 0.11152645945549011, tv_loss: 0.03230694308876991\n",
      "iteration 1878, dc_loss: 0.11149675399065018, tv_loss: 0.032308485358953476\n",
      "iteration 1879, dc_loss: 0.11149744689464569, tv_loss: 0.03228341415524483\n",
      "iteration 1880, dc_loss: 0.11146795004606247, tv_loss: 0.03229875490069389\n",
      "iteration 1881, dc_loss: 0.11142229288816452, tv_loss: 0.03233477473258972\n",
      "iteration 1882, dc_loss: 0.11144693195819855, tv_loss: 0.03228900209069252\n",
      "iteration 1883, dc_loss: 0.1113738864660263, tv_loss: 0.03233276307582855\n",
      "iteration 1884, dc_loss: 0.11140002310276031, tv_loss: 0.03227613493800163\n",
      "iteration 1885, dc_loss: 0.11133739352226257, tv_loss: 0.03232672065496445\n",
      "iteration 1886, dc_loss: 0.11136820912361145, tv_loss: 0.03228411450982094\n",
      "iteration 1887, dc_loss: 0.11128610372543335, tv_loss: 0.03234229236841202\n",
      "iteration 1888, dc_loss: 0.11130575835704803, tv_loss: 0.03229065239429474\n",
      "iteration 1889, dc_loss: 0.11127211153507233, tv_loss: 0.0323074534535408\n",
      "iteration 1890, dc_loss: 0.11126182973384857, tv_loss: 0.03230494633316994\n",
      "iteration 1891, dc_loss: 0.11122111976146698, tv_loss: 0.032324593514204025\n",
      "iteration 1892, dc_loss: 0.11121857166290283, tv_loss: 0.03231744468212128\n",
      "iteration 1893, dc_loss: 0.11117599159479141, tv_loss: 0.03232286497950554\n",
      "iteration 1894, dc_loss: 0.1111929789185524, tv_loss: 0.032283421605825424\n",
      "iteration 1895, dc_loss: 0.11111295968294144, tv_loss: 0.03235101327300072\n",
      "iteration 1896, dc_loss: 0.11116687208414078, tv_loss: 0.03227446600794792\n",
      "iteration 1897, dc_loss: 0.11107762902975082, tv_loss: 0.032371584326028824\n",
      "iteration 1898, dc_loss: 0.11118146032094955, tv_loss: 0.032253704965114594\n",
      "iteration 1899, dc_loss: 0.1110660582780838, tv_loss: 0.03238428384065628\n",
      "iteration 1900, dc_loss: 0.11122589558362961, tv_loss: 0.032230328768491745\n",
      "iteration 1901, dc_loss: 0.11109257489442825, tv_loss: 0.032427169382572174\n",
      "iteration 1902, dc_loss: 0.11127755790948868, tv_loss: 0.03221357986330986\n",
      "iteration 1903, dc_loss: 0.11107882857322693, tv_loss: 0.032426659017801285\n",
      "iteration 1904, dc_loss: 0.1112336814403534, tv_loss: 0.03220357373356819\n",
      "iteration 1905, dc_loss: 0.11096595227718353, tv_loss: 0.032425280660390854\n",
      "iteration 1906, dc_loss: 0.11109531670808792, tv_loss: 0.032209500670433044\n",
      "iteration 1907, dc_loss: 0.11085525900125504, tv_loss: 0.03240257501602173\n",
      "iteration 1908, dc_loss: 0.11094512045383453, tv_loss: 0.03225809335708618\n",
      "iteration 1909, dc_loss: 0.1108323410153389, tv_loss: 0.03234304487705231\n",
      "iteration 1910, dc_loss: 0.11082792282104492, tv_loss: 0.03231319785118103\n",
      "iteration 1911, dc_loss: 0.11084023118019104, tv_loss: 0.032289471477270126\n",
      "iteration 1912, dc_loss: 0.11073937267065048, tv_loss: 0.03239147737622261\n",
      "iteration 1913, dc_loss: 0.11081614345312119, tv_loss: 0.03229294344782829\n",
      "iteration 1914, dc_loss: 0.11069026589393616, tv_loss: 0.032383665442466736\n",
      "iteration 1915, dc_loss: 0.11080127209424973, tv_loss: 0.0322602242231369\n",
      "iteration 1916, dc_loss: 0.11067108064889908, tv_loss: 0.0323726162314415\n",
      "iteration 1917, dc_loss: 0.11072279512882233, tv_loss: 0.032305702567100525\n",
      "iteration 1918, dc_loss: 0.11062412708997726, tv_loss: 0.03235534951090813\n",
      "iteration 1919, dc_loss: 0.11062729358673096, tv_loss: 0.032308027148246765\n",
      "iteration 1920, dc_loss: 0.11058071255683899, tv_loss: 0.03232612833380699\n",
      "iteration 1921, dc_loss: 0.1105373352766037, tv_loss: 0.03236822783946991\n",
      "iteration 1922, dc_loss: 0.11057521402835846, tv_loss: 0.03230395168066025\n",
      "iteration 1923, dc_loss: 0.11051324009895325, tv_loss: 0.03236207365989685\n",
      "iteration 1924, dc_loss: 0.1105244979262352, tv_loss: 0.03232238069176674\n",
      "iteration 1925, dc_loss: 0.11047067493200302, tv_loss: 0.03235412389039993\n",
      "iteration 1926, dc_loss: 0.11048558354377747, tv_loss: 0.03232599422335625\n",
      "iteration 1927, dc_loss: 0.11040280014276505, tv_loss: 0.03235769644379616\n",
      "iteration 1928, dc_loss: 0.1103985607624054, tv_loss: 0.03232914209365845\n",
      "iteration 1929, dc_loss: 0.11037524044513702, tv_loss: 0.03234310448169708\n",
      "iteration 1930, dc_loss: 0.11036093533039093, tv_loss: 0.03234720230102539\n",
      "iteration 1931, dc_loss: 0.11033755540847778, tv_loss: 0.032345157116651535\n",
      "iteration 1932, dc_loss: 0.1103309914469719, tv_loss: 0.032343752682209015\n",
      "iteration 1933, dc_loss: 0.11031053215265274, tv_loss: 0.03233272582292557\n",
      "iteration 1934, dc_loss: 0.11029179394245148, tv_loss: 0.03232257440686226\n",
      "iteration 1935, dc_loss: 0.11023085564374924, tv_loss: 0.032365404069423676\n",
      "iteration 1936, dc_loss: 0.1102309376001358, tv_loss: 0.032331887632608414\n",
      "iteration 1937, dc_loss: 0.11017133295536041, tv_loss: 0.03236560523509979\n",
      "iteration 1938, dc_loss: 0.11022858321666718, tv_loss: 0.0323067307472229\n",
      "iteration 1939, dc_loss: 0.11013081669807434, tv_loss: 0.03239165246486664\n",
      "iteration 1940, dc_loss: 0.11020555347204208, tv_loss: 0.032283347100019455\n",
      "iteration 1941, dc_loss: 0.11010995507240295, tv_loss: 0.03238116204738617\n",
      "iteration 1942, dc_loss: 0.11017286032438278, tv_loss: 0.032315488904714584\n",
      "iteration 1943, dc_loss: 0.1100793331861496, tv_loss: 0.03239438310265541\n",
      "iteration 1944, dc_loss: 0.11019162088632584, tv_loss: 0.03228138014674187\n",
      "iteration 1945, dc_loss: 0.11006247252225876, tv_loss: 0.032413896173238754\n",
      "iteration 1946, dc_loss: 0.11020366102457047, tv_loss: 0.032273080199956894\n",
      "iteration 1947, dc_loss: 0.1100638285279274, tv_loss: 0.032438550144433975\n",
      "iteration 1948, dc_loss: 0.11023330688476562, tv_loss: 0.03225648030638695\n",
      "iteration 1949, dc_loss: 0.11003187298774719, tv_loss: 0.03245773911476135\n",
      "iteration 1950, dc_loss: 0.11017338931560516, tv_loss: 0.03224679082632065\n",
      "iteration 1951, dc_loss: 0.10991868376731873, tv_loss: 0.03245864808559418\n",
      "iteration 1952, dc_loss: 0.11005236208438873, tv_loss: 0.03227502852678299\n",
      "iteration 1953, dc_loss: 0.10983607918024063, tv_loss: 0.03243373706936836\n",
      "iteration 1954, dc_loss: 0.10989350825548172, tv_loss: 0.03231704980134964\n",
      "iteration 1955, dc_loss: 0.10981635749340057, tv_loss: 0.0323559045791626\n",
      "iteration 1956, dc_loss: 0.10979011654853821, tv_loss: 0.032363779842853546\n",
      "iteration 1957, dc_loss: 0.10981317609548569, tv_loss: 0.03234631568193436\n",
      "iteration 1958, dc_loss: 0.10973453521728516, tv_loss: 0.03242523968219757\n",
      "iteration 1959, dc_loss: 0.1098354384303093, tv_loss: 0.0322885662317276\n",
      "iteration 1960, dc_loss: 0.10967753827571869, tv_loss: 0.032427262514829636\n",
      "iteration 1961, dc_loss: 0.10978379100561142, tv_loss: 0.032306969165802\n",
      "iteration 1962, dc_loss: 0.10966654866933823, tv_loss: 0.032418910413980484\n",
      "iteration 1963, dc_loss: 0.10972657799720764, tv_loss: 0.03232235461473465\n",
      "iteration 1964, dc_loss: 0.10962175577878952, tv_loss: 0.032394230365753174\n",
      "iteration 1965, dc_loss: 0.10965516418218613, tv_loss: 0.03234105557203293\n",
      "iteration 1966, dc_loss: 0.10961101204156876, tv_loss: 0.03238336369395256\n",
      "iteration 1967, dc_loss: 0.1095820963382721, tv_loss: 0.032381217926740646\n",
      "iteration 1968, dc_loss: 0.10953759402036667, tv_loss: 0.032372862100601196\n",
      "iteration 1969, dc_loss: 0.10951986908912659, tv_loss: 0.03235511854290962\n",
      "iteration 1970, dc_loss: 0.10948846489191055, tv_loss: 0.0323798805475235\n",
      "iteration 1971, dc_loss: 0.10946771502494812, tv_loss: 0.03238280490040779\n",
      "iteration 1972, dc_loss: 0.10945075750350952, tv_loss: 0.0323653519153595\n",
      "iteration 1973, dc_loss: 0.10942891240119934, tv_loss: 0.03236810117959976\n",
      "iteration 1974, dc_loss: 0.10941079258918762, tv_loss: 0.03238324075937271\n",
      "iteration 1975, dc_loss: 0.10940241068601608, tv_loss: 0.03237904608249664\n",
      "iteration 1976, dc_loss: 0.10937488824129105, tv_loss: 0.0323735848069191\n",
      "iteration 1977, dc_loss: 0.10934053361415863, tv_loss: 0.03237433359026909\n",
      "iteration 1978, dc_loss: 0.109337218105793, tv_loss: 0.032354455441236496\n",
      "iteration 1979, dc_loss: 0.10929442942142487, tv_loss: 0.03238723799586296\n",
      "iteration 1980, dc_loss: 0.10928353667259216, tv_loss: 0.032387178391218185\n",
      "iteration 1981, dc_loss: 0.10924248397350311, tv_loss: 0.032386504113674164\n",
      "iteration 1982, dc_loss: 0.10926435142755508, tv_loss: 0.032352514564991\n",
      "iteration 1983, dc_loss: 0.10920655727386475, tv_loss: 0.032393310219049454\n",
      "iteration 1984, dc_loss: 0.10923916101455688, tv_loss: 0.0323590449988842\n",
      "iteration 1985, dc_loss: 0.10916362702846527, tv_loss: 0.03241470083594322\n",
      "iteration 1986, dc_loss: 0.1092129573225975, tv_loss: 0.0323379747569561\n",
      "iteration 1987, dc_loss: 0.10912998765707016, tv_loss: 0.032406996935606\n",
      "iteration 1988, dc_loss: 0.10920033603906631, tv_loss: 0.03233567997813225\n",
      "iteration 1989, dc_loss: 0.10910007357597351, tv_loss: 0.032446712255477905\n",
      "iteration 1990, dc_loss: 0.10922185331583023, tv_loss: 0.032322049140930176\n",
      "iteration 1991, dc_loss: 0.109110988676548, tv_loss: 0.0324520468711853\n",
      "iteration 1992, dc_loss: 0.10928796976804733, tv_loss: 0.032285645604133606\n",
      "iteration 1993, dc_loss: 0.10914497077465057, tv_loss: 0.032484639436006546\n",
      "iteration 1994, dc_loss: 0.1093728095293045, tv_loss: 0.03224998340010643\n",
      "iteration 1995, dc_loss: 0.10914326459169388, tv_loss: 0.032514676451683044\n",
      "iteration 1996, dc_loss: 0.10936354845762253, tv_loss: 0.0322258286178112\n",
      "iteration 1997, dc_loss: 0.10901722311973572, tv_loss: 0.03252555802464485\n",
      "iteration 1998, dc_loss: 0.10919710993766785, tv_loss: 0.03224779665470123\n",
      "iteration 1999, dc_loss: 0.10888330638408661, tv_loss: 0.03247515857219696\n",
      "iteration 2000, dc_loss: 0.10898695141077042, tv_loss: 0.03230433911085129\n",
      "iteration 2001, dc_loss: 0.10883970558643341, tv_loss: 0.03241254389286041\n",
      "iteration 2002, dc_loss: 0.10880986601114273, tv_loss: 0.03240610659122467\n",
      "iteration 2003, dc_loss: 0.10886967927217484, tv_loss: 0.03233053907752037\n",
      "iteration 2004, dc_loss: 0.10879594087600708, tv_loss: 0.03238075599074364\n",
      "iteration 2005, dc_loss: 0.10877479612827301, tv_loss: 0.03238244354724884\n",
      "iteration 2006, dc_loss: 0.10879760980606079, tv_loss: 0.032357875257730484\n",
      "iteration 2007, dc_loss: 0.10872340202331543, tv_loss: 0.03240276500582695\n",
      "iteration 2008, dc_loss: 0.10871989279985428, tv_loss: 0.03237829729914665\n",
      "iteration 2009, dc_loss: 0.10874955356121063, tv_loss: 0.032341428101062775\n",
      "iteration 2010, dc_loss: 0.10868039727210999, tv_loss: 0.03240617737174034\n",
      "iteration 2011, dc_loss: 0.10867708176374435, tv_loss: 0.032380230724811554\n",
      "iteration 2012, dc_loss: 0.10869630426168442, tv_loss: 0.03234289959073067\n",
      "iteration 2013, dc_loss: 0.1086212545633316, tv_loss: 0.03239616006612778\n",
      "iteration 2014, dc_loss: 0.10862299799919128, tv_loss: 0.03237365931272507\n",
      "iteration 2015, dc_loss: 0.10863620787858963, tv_loss: 0.032363489270210266\n",
      "iteration 2016, dc_loss: 0.10857746750116348, tv_loss: 0.03241613507270813\n",
      "iteration 2017, dc_loss: 0.10858654230833054, tv_loss: 0.03237150236964226\n",
      "iteration 2018, dc_loss: 0.1085686907172203, tv_loss: 0.03235974162817001\n",
      "iteration 2019, dc_loss: 0.10852430760860443, tv_loss: 0.03239718824625015\n",
      "iteration 2020, dc_loss: 0.10854393243789673, tv_loss: 0.032358117401599884\n",
      "iteration 2021, dc_loss: 0.1085130050778389, tv_loss: 0.03236778825521469\n",
      "iteration 2022, dc_loss: 0.108475461602211, tv_loss: 0.032398298382759094\n",
      "iteration 2023, dc_loss: 0.10849255323410034, tv_loss: 0.032376229763031006\n",
      "iteration 2024, dc_loss: 0.10846475511789322, tv_loss: 0.0323774553835392\n",
      "iteration 2025, dc_loss: 0.10843361914157867, tv_loss: 0.032390303909778595\n",
      "iteration 2026, dc_loss: 0.10844749212265015, tv_loss: 0.03236300125718117\n",
      "iteration 2027, dc_loss: 0.1083967536687851, tv_loss: 0.03238848224282265\n",
      "iteration 2028, dc_loss: 0.10837941616773605, tv_loss: 0.03239607810974121\n",
      "iteration 2029, dc_loss: 0.1084042638540268, tv_loss: 0.03237491473555565\n",
      "iteration 2030, dc_loss: 0.10835219919681549, tv_loss: 0.03240574896335602\n",
      "iteration 2031, dc_loss: 0.10834017395973206, tv_loss: 0.03239046409726143\n",
      "iteration 2032, dc_loss: 0.1083453968167305, tv_loss: 0.032363757491111755\n",
      "iteration 2033, dc_loss: 0.10830677300691605, tv_loss: 0.032388340681791306\n",
      "iteration 2034, dc_loss: 0.10829410701990128, tv_loss: 0.03239551559090614\n",
      "iteration 2035, dc_loss: 0.10828220099210739, tv_loss: 0.03240112587809563\n",
      "iteration 2036, dc_loss: 0.10825738310813904, tv_loss: 0.03239436820149422\n",
      "iteration 2037, dc_loss: 0.10825585573911667, tv_loss: 0.03237633779644966\n",
      "iteration 2038, dc_loss: 0.1082330197095871, tv_loss: 0.03238818049430847\n",
      "iteration 2039, dc_loss: 0.10820963233709335, tv_loss: 0.03239250183105469\n",
      "iteration 2040, dc_loss: 0.1082051694393158, tv_loss: 0.03239702060818672\n",
      "iteration 2041, dc_loss: 0.10818447172641754, tv_loss: 0.03239584341645241\n",
      "iteration 2042, dc_loss: 0.10815803706645966, tv_loss: 0.032394565641880035\n",
      "iteration 2043, dc_loss: 0.10816078633069992, tv_loss: 0.03236958757042885\n",
      "iteration 2044, dc_loss: 0.10813240706920624, tv_loss: 0.03239542245864868\n",
      "iteration 2045, dc_loss: 0.10811049491167068, tv_loss: 0.032417818903923035\n",
      "iteration 2046, dc_loss: 0.10811541229486465, tv_loss: 0.03238741308450699\n",
      "iteration 2047, dc_loss: 0.1080814003944397, tv_loss: 0.03239251673221588\n",
      "iteration 2048, dc_loss: 0.10806196182966232, tv_loss: 0.03239237889647484\n",
      "iteration 2049, dc_loss: 0.10806061327457428, tv_loss: 0.032386988401412964\n",
      "iteration 2050, dc_loss: 0.10803480446338654, tv_loss: 0.03240608051419258\n",
      "iteration 2051, dc_loss: 0.1080288216471672, tv_loss: 0.03240859508514404\n",
      "iteration 2052, dc_loss: 0.10800418257713318, tv_loss: 0.03239809349179268\n",
      "iteration 2053, dc_loss: 0.10798383504152298, tv_loss: 0.032393068075180054\n",
      "iteration 2054, dc_loss: 0.1079801544547081, tv_loss: 0.03239012509584427\n",
      "iteration 2055, dc_loss: 0.10796088725328445, tv_loss: 0.03241141140460968\n",
      "iteration 2056, dc_loss: 0.10794167965650558, tv_loss: 0.0324159599840641\n",
      "iteration 2057, dc_loss: 0.10792232304811478, tv_loss: 0.03239917755126953\n",
      "iteration 2058, dc_loss: 0.10790956020355225, tv_loss: 0.032394323498010635\n",
      "iteration 2059, dc_loss: 0.10788600146770477, tv_loss: 0.032406389713287354\n",
      "iteration 2060, dc_loss: 0.10788200795650482, tv_loss: 0.03239874541759491\n",
      "iteration 2061, dc_loss: 0.10786836594343185, tv_loss: 0.03239686042070389\n",
      "iteration 2062, dc_loss: 0.10783690959215164, tv_loss: 0.032414548099040985\n",
      "iteration 2063, dc_loss: 0.10783308744430542, tv_loss: 0.03239312022924423\n",
      "iteration 2064, dc_loss: 0.1078239381313324, tv_loss: 0.0323864184319973\n",
      "iteration 2065, dc_loss: 0.10779213160276413, tv_loss: 0.032418400049209595\n",
      "iteration 2066, dc_loss: 0.10777776688337326, tv_loss: 0.03241477534174919\n",
      "iteration 2067, dc_loss: 0.10776589810848236, tv_loss: 0.03241090849041939\n",
      "iteration 2068, dc_loss: 0.10775834321975708, tv_loss: 0.03239373117685318\n",
      "iteration 2069, dc_loss: 0.10773813724517822, tv_loss: 0.032399795949459076\n",
      "iteration 2070, dc_loss: 0.10770561546087265, tv_loss: 0.032441798597574234\n",
      "iteration 2071, dc_loss: 0.10770183056592941, tv_loss: 0.03241178020834923\n",
      "iteration 2072, dc_loss: 0.10768430680036545, tv_loss: 0.032406799495220184\n",
      "iteration 2073, dc_loss: 0.10767639428377151, tv_loss: 0.032404351979494095\n",
      "iteration 2074, dc_loss: 0.10765952616930008, tv_loss: 0.03240608796477318\n",
      "iteration 2075, dc_loss: 0.10763170570135117, tv_loss: 0.032433848828077316\n",
      "iteration 2076, dc_loss: 0.10762923210859299, tv_loss: 0.03239666670560837\n",
      "iteration 2077, dc_loss: 0.10760817676782608, tv_loss: 0.03241332992911339\n",
      "iteration 2078, dc_loss: 0.10757873952388763, tv_loss: 0.032440707087516785\n",
      "iteration 2079, dc_loss: 0.10758128762245178, tv_loss: 0.032408252358436584\n",
      "iteration 2080, dc_loss: 0.10756795853376389, tv_loss: 0.03240729868412018\n",
      "iteration 2081, dc_loss: 0.10753802955150604, tv_loss: 0.032428305596113205\n",
      "iteration 2082, dc_loss: 0.10752870887517929, tv_loss: 0.032423995435237885\n",
      "iteration 2083, dc_loss: 0.10751548409461975, tv_loss: 0.03241674602031708\n",
      "iteration 2084, dc_loss: 0.10749810189008713, tv_loss: 0.03240848332643509\n",
      "iteration 2085, dc_loss: 0.10747519135475159, tv_loss: 0.03244059160351753\n",
      "iteration 2086, dc_loss: 0.10746242105960846, tv_loss: 0.032423585653305054\n",
      "iteration 2087, dc_loss: 0.10745774954557419, tv_loss: 0.03240035101771355\n",
      "iteration 2088, dc_loss: 0.10743110626935959, tv_loss: 0.032427411526441574\n",
      "iteration 2089, dc_loss: 0.10741335898637772, tv_loss: 0.03243805468082428\n",
      "iteration 2090, dc_loss: 0.1074097603559494, tv_loss: 0.03241760656237602\n",
      "iteration 2091, dc_loss: 0.1073828861117363, tv_loss: 0.03241650387644768\n",
      "iteration 2092, dc_loss: 0.10737072676420212, tv_loss: 0.032428037375211716\n",
      "iteration 2093, dc_loss: 0.10736258327960968, tv_loss: 0.032435331493616104\n",
      "iteration 2094, dc_loss: 0.10734278708696365, tv_loss: 0.032417796552181244\n",
      "iteration 2095, dc_loss: 0.10731613636016846, tv_loss: 0.032430510967969894\n",
      "iteration 2096, dc_loss: 0.10730805993080139, tv_loss: 0.032425303012132645\n",
      "iteration 2097, dc_loss: 0.10729251056909561, tv_loss: 0.03243224695324898\n",
      "iteration 2098, dc_loss: 0.1072755828499794, tv_loss: 0.03242332488298416\n",
      "iteration 2099, dc_loss: 0.10726436227560043, tv_loss: 0.03241639956831932\n",
      "iteration 2100, dc_loss: 0.10725125670433044, tv_loss: 0.03242667391896248\n",
      "iteration 2101, dc_loss: 0.10722694545984268, tv_loss: 0.03243430331349373\n",
      "iteration 2102, dc_loss: 0.10721095651388168, tv_loss: 0.03242949768900871\n",
      "iteration 2103, dc_loss: 0.10720527172088623, tv_loss: 0.032410431653261185\n",
      "iteration 2104, dc_loss: 0.10718492418527603, tv_loss: 0.0324157178401947\n",
      "iteration 2105, dc_loss: 0.10716086626052856, tv_loss: 0.0324297733604908\n",
      "iteration 2106, dc_loss: 0.10715847462415695, tv_loss: 0.032415200024843216\n",
      "iteration 2107, dc_loss: 0.10713972896337509, tv_loss: 0.0324254147708416\n",
      "iteration 2108, dc_loss: 0.1071200966835022, tv_loss: 0.0324246771633625\n",
      "iteration 2109, dc_loss: 0.10710648447275162, tv_loss: 0.032421231269836426\n",
      "iteration 2110, dc_loss: 0.1070883497595787, tv_loss: 0.032416220754384995\n",
      "iteration 2111, dc_loss: 0.10706977546215057, tv_loss: 0.032427165657281876\n",
      "iteration 2112, dc_loss: 0.10706115514039993, tv_loss: 0.032415445894002914\n",
      "iteration 2113, dc_loss: 0.10705017298460007, tv_loss: 0.032408785074949265\n",
      "iteration 2114, dc_loss: 0.10702550411224365, tv_loss: 0.032420869916677475\n",
      "iteration 2115, dc_loss: 0.10700839012861252, tv_loss: 0.03241995349526405\n",
      "iteration 2116, dc_loss: 0.10700284689664841, tv_loss: 0.03241746872663498\n",
      "iteration 2117, dc_loss: 0.10697948187589645, tv_loss: 0.03242635354399681\n",
      "iteration 2118, dc_loss: 0.10696260631084442, tv_loss: 0.03244500979781151\n",
      "iteration 2119, dc_loss: 0.10695347189903259, tv_loss: 0.032428886741399765\n",
      "iteration 2120, dc_loss: 0.10693235695362091, tv_loss: 0.032429806888103485\n",
      "iteration 2121, dc_loss: 0.10691647231578827, tv_loss: 0.03242330625653267\n",
      "iteration 2122, dc_loss: 0.10690376162528992, tv_loss: 0.032416265457868576\n",
      "iteration 2123, dc_loss: 0.1068902388215065, tv_loss: 0.032421354204416275\n",
      "iteration 2124, dc_loss: 0.10687088966369629, tv_loss: 0.03242611885070801\n",
      "iteration 2125, dc_loss: 0.10685436427593231, tv_loss: 0.032434456050395966\n",
      "iteration 2126, dc_loss: 0.10685193538665771, tv_loss: 0.03243153169751167\n",
      "iteration 2127, dc_loss: 0.1068195030093193, tv_loss: 0.03245101496577263\n",
      "iteration 2128, dc_loss: 0.10680980235338211, tv_loss: 0.03242959827184677\n",
      "iteration 2129, dc_loss: 0.10679797828197479, tv_loss: 0.032424360513687134\n",
      "iteration 2130, dc_loss: 0.10677868127822876, tv_loss: 0.0324273407459259\n",
      "iteration 2131, dc_loss: 0.10676314681768417, tv_loss: 0.03243178129196167\n",
      "iteration 2132, dc_loss: 0.10674768686294556, tv_loss: 0.032443828880786896\n",
      "iteration 2133, dc_loss: 0.10673771798610687, tv_loss: 0.03245612606406212\n",
      "iteration 2134, dc_loss: 0.10671591758728027, tv_loss: 0.032439541071653366\n",
      "iteration 2135, dc_loss: 0.10670297592878342, tv_loss: 0.03243125602602959\n",
      "iteration 2136, dc_loss: 0.10668673366308212, tv_loss: 0.03244598209857941\n",
      "iteration 2137, dc_loss: 0.10667446255683899, tv_loss: 0.032456979155540466\n",
      "iteration 2138, dc_loss: 0.10665935277938843, tv_loss: 0.0324380099773407\n",
      "iteration 2139, dc_loss: 0.10663808137178421, tv_loss: 0.032439377158880234\n",
      "iteration 2140, dc_loss: 0.10662616789340973, tv_loss: 0.03244354575872421\n",
      "iteration 2141, dc_loss: 0.1066163033246994, tv_loss: 0.03246057778596878\n",
      "iteration 2142, dc_loss: 0.10659939795732498, tv_loss: 0.03244514763355255\n",
      "iteration 2143, dc_loss: 0.10657583177089691, tv_loss: 0.03244423493742943\n",
      "iteration 2144, dc_loss: 0.10656233876943588, tv_loss: 0.03246011212468147\n",
      "iteration 2145, dc_loss: 0.10654906928539276, tv_loss: 0.03247271478176117\n",
      "iteration 2146, dc_loss: 0.10653842240571976, tv_loss: 0.03244194760918617\n",
      "iteration 2147, dc_loss: 0.10652320086956024, tv_loss: 0.03244289383292198\n",
      "iteration 2148, dc_loss: 0.10649911314249039, tv_loss: 0.032472554594278336\n",
      "iteration 2149, dc_loss: 0.10649305582046509, tv_loss: 0.032456450164318085\n",
      "iteration 2150, dc_loss: 0.10647692531347275, tv_loss: 0.032446760684251785\n",
      "iteration 2151, dc_loss: 0.10645738244056702, tv_loss: 0.03246188908815384\n",
      "iteration 2152, dc_loss: 0.10644707828760147, tv_loss: 0.03246471658349037\n",
      "iteration 2153, dc_loss: 0.1064232587814331, tv_loss: 0.0324554443359375\n",
      "iteration 2154, dc_loss: 0.10641533881425858, tv_loss: 0.03245139122009277\n",
      "iteration 2155, dc_loss: 0.106397345662117, tv_loss: 0.032480835914611816\n",
      "iteration 2156, dc_loss: 0.10638122260570526, tv_loss: 0.03245505318045616\n",
      "iteration 2157, dc_loss: 0.1063738539814949, tv_loss: 0.032456595450639725\n",
      "iteration 2158, dc_loss: 0.10635370016098022, tv_loss: 0.03247600421309471\n",
      "iteration 2159, dc_loss: 0.1063290387392044, tv_loss: 0.032468248158693314\n",
      "iteration 2160, dc_loss: 0.10632585734128952, tv_loss: 0.03245406597852707\n",
      "iteration 2161, dc_loss: 0.10630782693624496, tv_loss: 0.0324690118432045\n",
      "iteration 2162, dc_loss: 0.1062849909067154, tv_loss: 0.0324690155684948\n",
      "iteration 2163, dc_loss: 0.10627736151218414, tv_loss: 0.03245183825492859\n",
      "iteration 2164, dc_loss: 0.10626678168773651, tv_loss: 0.032461319118738174\n",
      "iteration 2165, dc_loss: 0.10624342411756516, tv_loss: 0.03247170150279999\n",
      "iteration 2166, dc_loss: 0.10623812675476074, tv_loss: 0.03245050460100174\n",
      "iteration 2167, dc_loss: 0.10621672868728638, tv_loss: 0.03244940564036369\n",
      "iteration 2168, dc_loss: 0.10619499534368515, tv_loss: 0.03247055411338806\n",
      "iteration 2169, dc_loss: 0.10618452727794647, tv_loss: 0.03246267884969711\n",
      "iteration 2170, dc_loss: 0.10617542266845703, tv_loss: 0.032448671758174896\n",
      "iteration 2171, dc_loss: 0.10615191608667374, tv_loss: 0.03245953842997551\n",
      "iteration 2172, dc_loss: 0.10614100843667984, tv_loss: 0.032454974949359894\n",
      "iteration 2173, dc_loss: 0.10613783448934555, tv_loss: 0.03244572505354881\n",
      "iteration 2174, dc_loss: 0.10611067712306976, tv_loss: 0.03246583417057991\n",
      "iteration 2175, dc_loss: 0.1060868427157402, tv_loss: 0.03246263042092323\n",
      "iteration 2176, dc_loss: 0.10607998073101044, tv_loss: 0.032453812658786774\n",
      "iteration 2177, dc_loss: 0.10607171803712845, tv_loss: 0.032447900623083115\n",
      "iteration 2178, dc_loss: 0.10604441910982132, tv_loss: 0.03246423974633217\n",
      "iteration 2179, dc_loss: 0.10604390501976013, tv_loss: 0.03245152533054352\n",
      "iteration 2180, dc_loss: 0.10602391511201859, tv_loss: 0.032460398972034454\n",
      "iteration 2181, dc_loss: 0.10600364953279495, tv_loss: 0.032457876950502396\n",
      "iteration 2182, dc_loss: 0.10598248243331909, tv_loss: 0.032460685819387436\n",
      "iteration 2183, dc_loss: 0.10597127676010132, tv_loss: 0.032454803586006165\n",
      "iteration 2184, dc_loss: 0.10596668720245361, tv_loss: 0.03245225176215172\n",
      "iteration 2185, dc_loss: 0.10594521462917328, tv_loss: 0.032454151660203934\n",
      "iteration 2186, dc_loss: 0.10593614727258682, tv_loss: 0.032448917627334595\n",
      "iteration 2187, dc_loss: 0.1059160903096199, tv_loss: 0.032458215951919556\n",
      "iteration 2188, dc_loss: 0.10589346289634705, tv_loss: 0.0324629545211792\n",
      "iteration 2189, dc_loss: 0.105891153216362, tv_loss: 0.0324544720351696\n",
      "iteration 2190, dc_loss: 0.10588391125202179, tv_loss: 0.03244280815124512\n",
      "iteration 2191, dc_loss: 0.10585562139749527, tv_loss: 0.03246337175369263\n",
      "iteration 2192, dc_loss: 0.1058458685874939, tv_loss: 0.03246686980128288\n",
      "iteration 2193, dc_loss: 0.10583645850419998, tv_loss: 0.032472964376211166\n",
      "iteration 2194, dc_loss: 0.10583251714706421, tv_loss: 0.03247273340821266\n",
      "iteration 2195, dc_loss: 0.10581687837839127, tv_loss: 0.0324699766933918\n",
      "iteration 2196, dc_loss: 0.10581274330615997, tv_loss: 0.03246254473924637\n",
      "iteration 2197, dc_loss: 0.10579430311918259, tv_loss: 0.03246289864182472\n",
      "iteration 2198, dc_loss: 0.10577623546123505, tv_loss: 0.03246146813035011\n",
      "iteration 2199, dc_loss: 0.10575872659683228, tv_loss: 0.03246394917368889\n",
      "iteration 2200, dc_loss: 0.10574787855148315, tv_loss: 0.032449737191200256\n",
      "iteration 2201, dc_loss: 0.10570953041315079, tv_loss: 0.032473817467689514\n",
      "iteration 2202, dc_loss: 0.1056973859667778, tv_loss: 0.03246396780014038\n",
      "iteration 2203, dc_loss: 0.10568578541278839, tv_loss: 0.03246511518955231\n",
      "iteration 2204, dc_loss: 0.10567469894886017, tv_loss: 0.03246801346540451\n",
      "iteration 2205, dc_loss: 0.10564233362674713, tv_loss: 0.03248722478747368\n",
      "iteration 2206, dc_loss: 0.10564033687114716, tv_loss: 0.03247514367103577\n",
      "iteration 2207, dc_loss: 0.10563072562217712, tv_loss: 0.03246939927339554\n",
      "iteration 2208, dc_loss: 0.10560876131057739, tv_loss: 0.03247544541954994\n",
      "iteration 2209, dc_loss: 0.1056109145283699, tv_loss: 0.03246287256479263\n",
      "iteration 2210, dc_loss: 0.1055862158536911, tv_loss: 0.03247404843568802\n",
      "iteration 2211, dc_loss: 0.10558272898197174, tv_loss: 0.03246032074093819\n",
      "iteration 2212, dc_loss: 0.10555870831012726, tv_loss: 0.03246983140707016\n",
      "iteration 2213, dc_loss: 0.10553467273712158, tv_loss: 0.032481636852025986\n",
      "iteration 2214, dc_loss: 0.10552965849637985, tv_loss: 0.03246862813830376\n",
      "iteration 2215, dc_loss: 0.1055116131901741, tv_loss: 0.0324677936732769\n",
      "iteration 2216, dc_loss: 0.10549385845661163, tv_loss: 0.03247480466961861\n",
      "iteration 2217, dc_loss: 0.10546834766864777, tv_loss: 0.03249244764447212\n",
      "iteration 2218, dc_loss: 0.10546446591615677, tv_loss: 0.032482583075761795\n",
      "iteration 2219, dc_loss: 0.10545208305120468, tv_loss: 0.032479699701070786\n",
      "iteration 2220, dc_loss: 0.10543016344308853, tv_loss: 0.03248099237680435\n",
      "iteration 2221, dc_loss: 0.10541392862796783, tv_loss: 0.03247513622045517\n",
      "iteration 2222, dc_loss: 0.10540222376585007, tv_loss: 0.032472457736730576\n",
      "iteration 2223, dc_loss: 0.10539040714502335, tv_loss: 0.03247293829917908\n",
      "iteration 2224, dc_loss: 0.10537358373403549, tv_loss: 0.032486479729413986\n",
      "iteration 2225, dc_loss: 0.10536440461874008, tv_loss: 0.03247899189591408\n",
      "iteration 2226, dc_loss: 0.1053570806980133, tv_loss: 0.03248036652803421\n",
      "iteration 2227, dc_loss: 0.10534802079200745, tv_loss: 0.03247141093015671\n",
      "iteration 2228, dc_loss: 0.10532259941101074, tv_loss: 0.03247968107461929\n",
      "iteration 2229, dc_loss: 0.105320505797863, tv_loss: 0.03247211501002312\n",
      "iteration 2230, dc_loss: 0.10529648512601852, tv_loss: 0.0324750691652298\n",
      "iteration 2231, dc_loss: 0.1052752211689949, tv_loss: 0.03247577324509621\n",
      "iteration 2232, dc_loss: 0.10526043176651001, tv_loss: 0.03248824179172516\n",
      "iteration 2233, dc_loss: 0.10526236146688461, tv_loss: 0.03246689215302467\n",
      "iteration 2234, dc_loss: 0.10523537546396255, tv_loss: 0.032484691590070724\n",
      "iteration 2235, dc_loss: 0.1052229031920433, tv_loss: 0.032496511936187744\n",
      "iteration 2236, dc_loss: 0.10521004348993301, tv_loss: 0.032509494572877884\n",
      "iteration 2237, dc_loss: 0.10520517826080322, tv_loss: 0.032480109483003616\n",
      "iteration 2238, dc_loss: 0.10517922788858414, tv_loss: 0.03248924762010574\n",
      "iteration 2239, dc_loss: 0.10516837984323502, tv_loss: 0.03248443454504013\n",
      "iteration 2240, dc_loss: 0.10514999181032181, tv_loss: 0.03249795362353325\n",
      "iteration 2241, dc_loss: 0.10513009876012802, tv_loss: 0.03250465542078018\n",
      "iteration 2242, dc_loss: 0.10511479526758194, tv_loss: 0.032485075294971466\n",
      "iteration 2243, dc_loss: 0.10510607063770294, tv_loss: 0.032486677169799805\n",
      "iteration 2244, dc_loss: 0.10507579892873764, tv_loss: 0.032506030052900314\n",
      "iteration 2245, dc_loss: 0.10507513582706451, tv_loss: 0.03249906003475189\n",
      "iteration 2246, dc_loss: 0.10505429655313492, tv_loss: 0.03249097615480423\n",
      "iteration 2247, dc_loss: 0.10505592823028564, tv_loss: 0.032475270330905914\n",
      "iteration 2248, dc_loss: 0.10502193123102188, tv_loss: 0.03251287341117859\n",
      "iteration 2249, dc_loss: 0.10504771023988724, tv_loss: 0.0324983224272728\n",
      "iteration 2250, dc_loss: 0.1050122007727623, tv_loss: 0.0325155146420002\n",
      "iteration 2251, dc_loss: 0.10503820329904556, tv_loss: 0.032476700842380524\n",
      "iteration 2252, dc_loss: 0.10499056428670883, tv_loss: 0.03249913454055786\n",
      "iteration 2253, dc_loss: 0.10500063747167587, tv_loss: 0.032499585300683975\n",
      "iteration 2254, dc_loss: 0.10494847595691681, tv_loss: 0.03252428397536278\n",
      "iteration 2255, dc_loss: 0.10495486110448837, tv_loss: 0.03248489648103714\n",
      "iteration 2256, dc_loss: 0.10491141676902771, tv_loss: 0.0325031504034996\n",
      "iteration 2257, dc_loss: 0.10491801053285599, tv_loss: 0.03249898925423622\n",
      "iteration 2258, dc_loss: 0.1048860102891922, tv_loss: 0.032537322491407394\n",
      "iteration 2259, dc_loss: 0.10490695387125015, tv_loss: 0.03248200938105583\n",
      "iteration 2260, dc_loss: 0.10487455129623413, tv_loss: 0.03251229599118233\n",
      "iteration 2261, dc_loss: 0.10488835722208023, tv_loss: 0.03250217065215111\n",
      "iteration 2262, dc_loss: 0.10487159341573715, tv_loss: 0.032525110989809036\n",
      "iteration 2263, dc_loss: 0.10488484054803848, tv_loss: 0.03247454762458801\n",
      "iteration 2264, dc_loss: 0.10481959581375122, tv_loss: 0.03253309801220894\n",
      "iteration 2265, dc_loss: 0.10487362742424011, tv_loss: 0.03247524052858353\n",
      "iteration 2266, dc_loss: 0.10477455705404282, tv_loss: 0.03255883976817131\n",
      "iteration 2267, dc_loss: 0.10484779626131058, tv_loss: 0.03245394304394722\n",
      "iteration 2268, dc_loss: 0.1047348901629448, tv_loss: 0.03255920112133026\n",
      "iteration 2269, dc_loss: 0.10484066605567932, tv_loss: 0.03245764225721359\n",
      "iteration 2270, dc_loss: 0.1047288030385971, tv_loss: 0.03257138282060623\n",
      "iteration 2271, dc_loss: 0.10484690964221954, tv_loss: 0.0324346199631691\n",
      "iteration 2272, dc_loss: 0.10470188409090042, tv_loss: 0.03255525976419449\n",
      "iteration 2273, dc_loss: 0.10477820038795471, tv_loss: 0.0324469655752182\n",
      "iteration 2274, dc_loss: 0.10463758558034897, tv_loss: 0.032564885914325714\n",
      "iteration 2275, dc_loss: 0.10472294688224792, tv_loss: 0.03244861960411072\n",
      "iteration 2276, dc_loss: 0.10463198274374008, tv_loss: 0.03253146633505821\n",
      "iteration 2277, dc_loss: 0.10466568171977997, tv_loss: 0.03248582035303116\n",
      "iteration 2278, dc_loss: 0.10462358593940735, tv_loss: 0.032527655363082886\n",
      "iteration 2279, dc_loss: 0.10462163388729095, tv_loss: 0.03249914199113846\n",
      "iteration 2280, dc_loss: 0.10458686947822571, tv_loss: 0.03250305727124214\n",
      "iteration 2281, dc_loss: 0.1045491099357605, tv_loss: 0.03251355513930321\n",
      "iteration 2282, dc_loss: 0.10456105321645737, tv_loss: 0.03248429298400879\n",
      "iteration 2283, dc_loss: 0.10452093183994293, tv_loss: 0.03251204639673233\n",
      "iteration 2284, dc_loss: 0.1045435443520546, tv_loss: 0.03248920664191246\n",
      "iteration 2285, dc_loss: 0.10453237593173981, tv_loss: 0.03250506892800331\n",
      "iteration 2286, dc_loss: 0.1045093759894371, tv_loss: 0.0325017124414444\n",
      "iteration 2287, dc_loss: 0.10446734726428986, tv_loss: 0.032515935599803925\n",
      "iteration 2288, dc_loss: 0.104472815990448, tv_loss: 0.032494332641363144\n",
      "iteration 2289, dc_loss: 0.10443875193595886, tv_loss: 0.03251631185412407\n",
      "iteration 2290, dc_loss: 0.10445909202098846, tv_loss: 0.03251071274280548\n",
      "iteration 2291, dc_loss: 0.10442499071359634, tv_loss: 0.03253258019685745\n",
      "iteration 2292, dc_loss: 0.10447219014167786, tv_loss: 0.03248041868209839\n",
      "iteration 2293, dc_loss: 0.10439196974039078, tv_loss: 0.03253484517335892\n",
      "iteration 2294, dc_loss: 0.10440996289253235, tv_loss: 0.032495640218257904\n",
      "iteration 2295, dc_loss: 0.10436493158340454, tv_loss: 0.03253960609436035\n",
      "iteration 2296, dc_loss: 0.10439590364694595, tv_loss: 0.03250977396965027\n",
      "iteration 2297, dc_loss: 0.10435926169157028, tv_loss: 0.032543059438467026\n",
      "iteration 2298, dc_loss: 0.10439830273389816, tv_loss: 0.03248623386025429\n",
      "iteration 2299, dc_loss: 0.10433349013328552, tv_loss: 0.032531242817640305\n",
      "iteration 2300, dc_loss: 0.10434674471616745, tv_loss: 0.032492127269506454\n",
      "iteration 2301, dc_loss: 0.10428151488304138, tv_loss: 0.032546769827604294\n",
      "iteration 2302, dc_loss: 0.1043384000658989, tv_loss: 0.03248400613665581\n",
      "iteration 2303, dc_loss: 0.10426197946071625, tv_loss: 0.032563019543886185\n",
      "iteration 2304, dc_loss: 0.10433433949947357, tv_loss: 0.03248021751642227\n",
      "iteration 2305, dc_loss: 0.10424145311117172, tv_loss: 0.032545305788517\n",
      "iteration 2306, dc_loss: 0.10428158193826675, tv_loss: 0.032468728721141815\n",
      "iteration 2307, dc_loss: 0.10418090224266052, tv_loss: 0.03255319222807884\n",
      "iteration 2308, dc_loss: 0.10424894094467163, tv_loss: 0.03247777000069618\n",
      "iteration 2309, dc_loss: 0.104173943400383, tv_loss: 0.032561566680669785\n",
      "iteration 2310, dc_loss: 0.10421794652938843, tv_loss: 0.03250687196850777\n",
      "iteration 2311, dc_loss: 0.10414616018533707, tv_loss: 0.03254560008645058\n",
      "iteration 2312, dc_loss: 0.10415537655353546, tv_loss: 0.032506171613931656\n",
      "iteration 2313, dc_loss: 0.10411179810762405, tv_loss: 0.03253013640642166\n",
      "iteration 2314, dc_loss: 0.1041298359632492, tv_loss: 0.03250330686569214\n",
      "iteration 2315, dc_loss: 0.10409867763519287, tv_loss: 0.03252079337835312\n",
      "iteration 2316, dc_loss: 0.10409443080425262, tv_loss: 0.03250753879547119\n",
      "iteration 2317, dc_loss: 0.1040496677160263, tv_loss: 0.032528381794691086\n",
      "iteration 2318, dc_loss: 0.1040448397397995, tv_loss: 0.032516639679670334\n",
      "iteration 2319, dc_loss: 0.10401929169893265, tv_loss: 0.03253304585814476\n",
      "iteration 2320, dc_loss: 0.10402791202068329, tv_loss: 0.0325261726975441\n",
      "iteration 2321, dc_loss: 0.10401637107133865, tv_loss: 0.032523058354854584\n",
      "iteration 2322, dc_loss: 0.10398337244987488, tv_loss: 0.03253808245062828\n",
      "iteration 2323, dc_loss: 0.10398958623409271, tv_loss: 0.032516155391931534\n",
      "iteration 2324, dc_loss: 0.10393771529197693, tv_loss: 0.03253668174147606\n",
      "iteration 2325, dc_loss: 0.1039763018488884, tv_loss: 0.03250104933977127\n",
      "iteration 2326, dc_loss: 0.10390898585319519, tv_loss: 0.032559093087911606\n",
      "iteration 2327, dc_loss: 0.10396919399499893, tv_loss: 0.0324878953397274\n",
      "iteration 2328, dc_loss: 0.10390367358922958, tv_loss: 0.032548364251852036\n",
      "iteration 2329, dc_loss: 0.10395010560750961, tv_loss: 0.03248061239719391\n",
      "iteration 2330, dc_loss: 0.10385879874229431, tv_loss: 0.032568663358688354\n",
      "iteration 2331, dc_loss: 0.1039450392127037, tv_loss: 0.03248005732893944\n",
      "iteration 2332, dc_loss: 0.10385921597480774, tv_loss: 0.032571032643318176\n",
      "iteration 2333, dc_loss: 0.10397516191005707, tv_loss: 0.03246839717030525\n",
      "iteration 2334, dc_loss: 0.1038699746131897, tv_loss: 0.03259112685918808\n",
      "iteration 2335, dc_loss: 0.10399926453828812, tv_loss: 0.032453831285238266\n",
      "iteration 2336, dc_loss: 0.1038724035024643, tv_loss: 0.03261217847466469\n",
      "iteration 2337, dc_loss: 0.10401266813278198, tv_loss: 0.03243524581193924\n",
      "iteration 2338, dc_loss: 0.1038370355963707, tv_loss: 0.032606590539216995\n",
      "iteration 2339, dc_loss: 0.10394416004419327, tv_loss: 0.03245209902524948\n",
      "iteration 2340, dc_loss: 0.10375086963176727, tv_loss: 0.03262098878622055\n",
      "iteration 2341, dc_loss: 0.10381581634283066, tv_loss: 0.032504886388778687\n",
      "iteration 2342, dc_loss: 0.10369092226028442, tv_loss: 0.03257297724485397\n",
      "iteration 2343, dc_loss: 0.10371662676334381, tv_loss: 0.032520830631256104\n",
      "iteration 2344, dc_loss: 0.10371417552232742, tv_loss: 0.03253358229994774\n",
      "iteration 2345, dc_loss: 0.10367932915687561, tv_loss: 0.0325719378888607\n",
      "iteration 2346, dc_loss: 0.10372737050056458, tv_loss: 0.032503899186849594\n",
      "iteration 2347, dc_loss: 0.10363861173391342, tv_loss: 0.032576773315668106\n",
      "iteration 2348, dc_loss: 0.1036963015794754, tv_loss: 0.03250405192375183\n",
      "iteration 2349, dc_loss: 0.10360490530729294, tv_loss: 0.0325617752969265\n",
      "iteration 2350, dc_loss: 0.10365089029073715, tv_loss: 0.03249642625451088\n",
      "iteration 2351, dc_loss: 0.10357337445020676, tv_loss: 0.032558634877204895\n",
      "iteration 2352, dc_loss: 0.10359735786914825, tv_loss: 0.032528284937143326\n",
      "iteration 2353, dc_loss: 0.10356738418340683, tv_loss: 0.03255399689078331\n",
      "iteration 2354, dc_loss: 0.10357016324996948, tv_loss: 0.03254503011703491\n",
      "iteration 2355, dc_loss: 0.10353085398674011, tv_loss: 0.032554950565099716\n",
      "iteration 2356, dc_loss: 0.10353146493434906, tv_loss: 0.03252740576863289\n",
      "iteration 2357, dc_loss: 0.1035110205411911, tv_loss: 0.03252946957945824\n",
      "iteration 2358, dc_loss: 0.1034904420375824, tv_loss: 0.03254283592104912\n",
      "iteration 2359, dc_loss: 0.10348428040742874, tv_loss: 0.03252996504306793\n",
      "iteration 2360, dc_loss: 0.10346930474042892, tv_loss: 0.032537370920181274\n",
      "iteration 2361, dc_loss: 0.10346201807260513, tv_loss: 0.03253140300512314\n",
      "iteration 2362, dc_loss: 0.10342579334974289, tv_loss: 0.032557807862758636\n",
      "iteration 2363, dc_loss: 0.10344244539737701, tv_loss: 0.03252753987908363\n",
      "iteration 2364, dc_loss: 0.10339467972517014, tv_loss: 0.03256317228078842\n",
      "iteration 2365, dc_loss: 0.10340265929698944, tv_loss: 0.03253015875816345\n",
      "iteration 2366, dc_loss: 0.10337436944246292, tv_loss: 0.03254610300064087\n",
      "iteration 2367, dc_loss: 0.10336703062057495, tv_loss: 0.03253445401787758\n",
      "iteration 2368, dc_loss: 0.10335636883974075, tv_loss: 0.03253354877233505\n",
      "iteration 2369, dc_loss: 0.10332895070314407, tv_loss: 0.032551493495702744\n",
      "iteration 2370, dc_loss: 0.10333282500505447, tv_loss: 0.032535530626773834\n",
      "iteration 2371, dc_loss: 0.10330330580472946, tv_loss: 0.03257274627685547\n",
      "iteration 2372, dc_loss: 0.10330982506275177, tv_loss: 0.03255036100745201\n",
      "iteration 2373, dc_loss: 0.10327500104904175, tv_loss: 0.03255924955010414\n",
      "iteration 2374, dc_loss: 0.10326743125915527, tv_loss: 0.03254872187972069\n",
      "iteration 2375, dc_loss: 0.10327330976724625, tv_loss: 0.03254220262169838\n",
      "iteration 2376, dc_loss: 0.10323657095432281, tv_loss: 0.032570116221904755\n",
      "iteration 2377, dc_loss: 0.10322995483875275, tv_loss: 0.03256644308567047\n",
      "iteration 2378, dc_loss: 0.10320659726858139, tv_loss: 0.03256302699446678\n",
      "iteration 2379, dc_loss: 0.10322980582714081, tv_loss: 0.03253389149904251\n",
      "iteration 2380, dc_loss: 0.10319029539823532, tv_loss: 0.03256767615675926\n",
      "iteration 2381, dc_loss: 0.10321005433797836, tv_loss: 0.032550010830163956\n",
      "iteration 2382, dc_loss: 0.10318512469530106, tv_loss: 0.03255852684378624\n",
      "iteration 2383, dc_loss: 0.10323654115200043, tv_loss: 0.03252419829368591\n",
      "iteration 2384, dc_loss: 0.10318955034017563, tv_loss: 0.03259028494358063\n",
      "iteration 2385, dc_loss: 0.1032726988196373, tv_loss: 0.03253040462732315\n",
      "iteration 2386, dc_loss: 0.10319487750530243, tv_loss: 0.03260849416255951\n",
      "iteration 2387, dc_loss: 0.1032935157418251, tv_loss: 0.03249560669064522\n",
      "iteration 2388, dc_loss: 0.10317514091730118, tv_loss: 0.0326143279671669\n",
      "iteration 2389, dc_loss: 0.10325907915830612, tv_loss: 0.03250129148364067\n",
      "iteration 2390, dc_loss: 0.10312388837337494, tv_loss: 0.03260648623108864\n",
      "iteration 2391, dc_loss: 0.10319186747074127, tv_loss: 0.032513584941625595\n",
      "iteration 2392, dc_loss: 0.10308326780796051, tv_loss: 0.03261192888021469\n",
      "iteration 2393, dc_loss: 0.10314883291721344, tv_loss: 0.03252203017473221\n",
      "iteration 2394, dc_loss: 0.10306355357170105, tv_loss: 0.03257525712251663\n",
      "iteration 2395, dc_loss: 0.10311097651720047, tv_loss: 0.03251221403479576\n",
      "iteration 2396, dc_loss: 0.10300688445568085, tv_loss: 0.03260822594165802\n",
      "iteration 2397, dc_loss: 0.10304564982652664, tv_loss: 0.03253195807337761\n",
      "iteration 2398, dc_loss: 0.1029597595334053, tv_loss: 0.03259677439928055\n",
      "iteration 2399, dc_loss: 0.10299384593963623, tv_loss: 0.03253856673836708\n",
      "iteration 2400, dc_loss: 0.10295207798480988, tv_loss: 0.03256512060761452\n",
      "iteration 2401, dc_loss: 0.10294409096240997, tv_loss: 0.0325627401471138\n",
      "iteration 2402, dc_loss: 0.10292167961597443, tv_loss: 0.03253544121980667\n",
      "iteration 2403, dc_loss: 0.10289674252271652, tv_loss: 0.03255406767129898\n",
      "iteration 2404, dc_loss: 0.10289810597896576, tv_loss: 0.032561518251895905\n",
      "iteration 2405, dc_loss: 0.10288919508457184, tv_loss: 0.03253105655312538\n",
      "iteration 2406, dc_loss: 0.10286533832550049, tv_loss: 0.03254895284771919\n",
      "iteration 2407, dc_loss: 0.10285436362028122, tv_loss: 0.03255413845181465\n",
      "iteration 2408, dc_loss: 0.10285317897796631, tv_loss: 0.032538238912820816\n",
      "iteration 2409, dc_loss: 0.10282965749502182, tv_loss: 0.032547082751989365\n",
      "iteration 2410, dc_loss: 0.10281746089458466, tv_loss: 0.03255745396018028\n",
      "iteration 2411, dc_loss: 0.10281270742416382, tv_loss: 0.0325467586517334\n",
      "iteration 2412, dc_loss: 0.1027931421995163, tv_loss: 0.032559093087911606\n",
      "iteration 2413, dc_loss: 0.10279051959514618, tv_loss: 0.032548416405916214\n",
      "iteration 2414, dc_loss: 0.10278294235467911, tv_loss: 0.03254012390971184\n",
      "iteration 2415, dc_loss: 0.10275623947381973, tv_loss: 0.032555099576711655\n",
      "iteration 2416, dc_loss: 0.10275214910507202, tv_loss: 0.032557401806116104\n",
      "iteration 2417, dc_loss: 0.10274495929479599, tv_loss: 0.032562755048274994\n",
      "iteration 2418, dc_loss: 0.10273566842079163, tv_loss: 0.03255945444107056\n",
      "iteration 2419, dc_loss: 0.10271964967250824, tv_loss: 0.032550495117902756\n",
      "iteration 2420, dc_loss: 0.10270337760448456, tv_loss: 0.03255374729633331\n",
      "iteration 2421, dc_loss: 0.10270087420940399, tv_loss: 0.03255651518702507\n",
      "iteration 2422, dc_loss: 0.1026896983385086, tv_loss: 0.03256285563111305\n",
      "iteration 2423, dc_loss: 0.10266852378845215, tv_loss: 0.032569754868745804\n",
      "iteration 2424, dc_loss: 0.10266593843698502, tv_loss: 0.032554399222135544\n",
      "iteration 2425, dc_loss: 0.102662593126297, tv_loss: 0.03254237025976181\n",
      "iteration 2426, dc_loss: 0.10263895988464355, tv_loss: 0.03255320340394974\n",
      "iteration 2427, dc_loss: 0.10263767838478088, tv_loss: 0.03254232555627823\n",
      "iteration 2428, dc_loss: 0.10262709110975266, tv_loss: 0.03254581242799759\n",
      "iteration 2429, dc_loss: 0.10259837657213211, tv_loss: 0.03256314620375633\n",
      "iteration 2430, dc_loss: 0.10260678827762604, tv_loss: 0.03255157917737961\n",
      "iteration 2431, dc_loss: 0.10259389877319336, tv_loss: 0.03257045894861221\n",
      "iteration 2432, dc_loss: 0.10257584601640701, tv_loss: 0.03257349506020546\n",
      "iteration 2433, dc_loss: 0.10257342457771301, tv_loss: 0.03255147114396095\n",
      "iteration 2434, dc_loss: 0.10256039351224899, tv_loss: 0.032554369419813156\n",
      "iteration 2435, dc_loss: 0.1025439202785492, tv_loss: 0.032558634877204895\n",
      "iteration 2436, dc_loss: 0.10253816097974777, tv_loss: 0.032561179250478745\n",
      "iteration 2437, dc_loss: 0.10252667218446732, tv_loss: 0.032571934163570404\n",
      "iteration 2438, dc_loss: 0.10251753777265549, tv_loss: 0.0325683169066906\n",
      "iteration 2439, dc_loss: 0.10251384228467941, tv_loss: 0.03255178779363632\n",
      "iteration 2440, dc_loss: 0.10249058157205582, tv_loss: 0.032561369240283966\n",
      "iteration 2441, dc_loss: 0.10248124599456787, tv_loss: 0.03255986422300339\n",
      "iteration 2442, dc_loss: 0.10248009860515594, tv_loss: 0.03255564346909523\n",
      "iteration 2443, dc_loss: 0.1024620309472084, tv_loss: 0.03256503492593765\n",
      "iteration 2444, dc_loss: 0.10245051234960556, tv_loss: 0.03257635235786438\n",
      "iteration 2445, dc_loss: 0.10244100540876389, tv_loss: 0.03257187828421593\n",
      "iteration 2446, dc_loss: 0.10243959724903107, tv_loss: 0.03255580738186836\n",
      "iteration 2447, dc_loss: 0.10242599993944168, tv_loss: 0.03255655989050865\n",
      "iteration 2448, dc_loss: 0.10240311175584793, tv_loss: 0.03256545960903168\n",
      "iteration 2449, dc_loss: 0.1023997887969017, tv_loss: 0.032559581100940704\n",
      "iteration 2450, dc_loss: 0.1023985892534256, tv_loss: 0.03255586326122284\n",
      "iteration 2451, dc_loss: 0.1023770421743393, tv_loss: 0.0325748547911644\n",
      "iteration 2452, dc_loss: 0.10236337035894394, tv_loss: 0.03258570283651352\n",
      "iteration 2453, dc_loss: 0.10236553847789764, tv_loss: 0.03256429731845856\n",
      "iteration 2454, dc_loss: 0.102353036403656, tv_loss: 0.03256041184067726\n",
      "iteration 2455, dc_loss: 0.10233411937952042, tv_loss: 0.03256653621792793\n",
      "iteration 2456, dc_loss: 0.10232596844434738, tv_loss: 0.032567378133535385\n",
      "iteration 2457, dc_loss: 0.10232418030500412, tv_loss: 0.032560136169195175\n",
      "iteration 2458, dc_loss: 0.10230853408575058, tv_loss: 0.03257022425532341\n",
      "iteration 2459, dc_loss: 0.10228893905878067, tv_loss: 0.032582443207502365\n",
      "iteration 2460, dc_loss: 0.10228927433490753, tv_loss: 0.03256812319159508\n",
      "iteration 2461, dc_loss: 0.10228073596954346, tv_loss: 0.03256409987807274\n",
      "iteration 2462, dc_loss: 0.10226043313741684, tv_loss: 0.03257492184638977\n",
      "iteration 2463, dc_loss: 0.10225803405046463, tv_loss: 0.03256417438387871\n",
      "iteration 2464, dc_loss: 0.10224568098783493, tv_loss: 0.03256832808256149\n",
      "iteration 2465, dc_loss: 0.10223179310560226, tv_loss: 0.0325709767639637\n",
      "iteration 2466, dc_loss: 0.10222430527210236, tv_loss: 0.03257139399647713\n",
      "iteration 2467, dc_loss: 0.10221218317747116, tv_loss: 0.03257926553487778\n",
      "iteration 2468, dc_loss: 0.10220767557621002, tv_loss: 0.03257786110043526\n",
      "iteration 2469, dc_loss: 0.10219428688287735, tv_loss: 0.03257410600781441\n",
      "iteration 2470, dc_loss: 0.10217811167240143, tv_loss: 0.03257019817829132\n",
      "iteration 2471, dc_loss: 0.10217150300741196, tv_loss: 0.03256883844733238\n",
      "iteration 2472, dc_loss: 0.10216338187456131, tv_loss: 0.03257543593645096\n",
      "iteration 2473, dc_loss: 0.10215301811695099, tv_loss: 0.03258613869547844\n",
      "iteration 2474, dc_loss: 0.10214366763830185, tv_loss: 0.03257942944765091\n",
      "iteration 2475, dc_loss: 0.10213013738393784, tv_loss: 0.03256956860423088\n",
      "iteration 2476, dc_loss: 0.10211849957704544, tv_loss: 0.03257232904434204\n",
      "iteration 2477, dc_loss: 0.10211169719696045, tv_loss: 0.032570499926805496\n",
      "iteration 2478, dc_loss: 0.10210248827934265, tv_loss: 0.03258366510272026\n",
      "iteration 2479, dc_loss: 0.10209158807992935, tv_loss: 0.03258686140179634\n",
      "iteration 2480, dc_loss: 0.10207698494195938, tv_loss: 0.03258293494582176\n",
      "iteration 2481, dc_loss: 0.10206860303878784, tv_loss: 0.03257107734680176\n",
      "iteration 2482, dc_loss: 0.10205888748168945, tv_loss: 0.03257033973932266\n",
      "iteration 2483, dc_loss: 0.10204781591892242, tv_loss: 0.0325784832239151\n",
      "iteration 2484, dc_loss: 0.10204461961984634, tv_loss: 0.03257713466882706\n",
      "iteration 2485, dc_loss: 0.10202724486589432, tv_loss: 0.032588060945272446\n",
      "iteration 2486, dc_loss: 0.10201095044612885, tv_loss: 0.03258870914578438\n",
      "iteration 2487, dc_loss: 0.10201047360897064, tv_loss: 0.032569438219070435\n",
      "iteration 2488, dc_loss: 0.10200120508670807, tv_loss: 0.032569799572229385\n",
      "iteration 2489, dc_loss: 0.10198123008012772, tv_loss: 0.03257797285914421\n",
      "iteration 2490, dc_loss: 0.1019807904958725, tv_loss: 0.03256799653172493\n",
      "iteration 2491, dc_loss: 0.1019660010933876, tv_loss: 0.03258093446493149\n",
      "iteration 2492, dc_loss: 0.10195380449295044, tv_loss: 0.03258766978979111\n",
      "iteration 2493, dc_loss: 0.10194924473762512, tv_loss: 0.03258822485804558\n",
      "iteration 2494, dc_loss: 0.10193616896867752, tv_loss: 0.03258025273680687\n",
      "iteration 2495, dc_loss: 0.10192293673753738, tv_loss: 0.03258125111460686\n",
      "iteration 2496, dc_loss: 0.10191226750612259, tv_loss: 0.032589953392744064\n",
      "iteration 2497, dc_loss: 0.10191059112548828, tv_loss: 0.032597579061985016\n",
      "iteration 2498, dc_loss: 0.10189726948738098, tv_loss: 0.03258552774786949\n",
      "iteration 2499, dc_loss: 0.10188117623329163, tv_loss: 0.032580967992544174\n",
      "iteration 2500, dc_loss: 0.10187356173992157, tv_loss: 0.03259037435054779\n",
      "iteration 2501, dc_loss: 0.10186374187469482, tv_loss: 0.032597266137599945\n",
      "iteration 2502, dc_loss: 0.10185626894235611, tv_loss: 0.03259271755814552\n",
      "iteration 2503, dc_loss: 0.1018415167927742, tv_loss: 0.032583560794591904\n",
      "iteration 2504, dc_loss: 0.1018371731042862, tv_loss: 0.03258049488067627\n",
      "iteration 2505, dc_loss: 0.1018306240439415, tv_loss: 0.032583389431238174\n",
      "iteration 2506, dc_loss: 0.10180916637182236, tv_loss: 0.03259234502911568\n",
      "iteration 2507, dc_loss: 0.10180085897445679, tv_loss: 0.03259317949414253\n",
      "iteration 2508, dc_loss: 0.10179316997528076, tv_loss: 0.032579224556684494\n",
      "iteration 2509, dc_loss: 0.10178156942129135, tv_loss: 0.03258037567138672\n",
      "iteration 2510, dc_loss: 0.1017778143286705, tv_loss: 0.03257688507437706\n",
      "iteration 2511, dc_loss: 0.10175938159227371, tv_loss: 0.03258781507611275\n",
      "iteration 2512, dc_loss: 0.1017516627907753, tv_loss: 0.032595835626125336\n",
      "iteration 2513, dc_loss: 0.10174597054719925, tv_loss: 0.03259514644742012\n",
      "iteration 2514, dc_loss: 0.1017344743013382, tv_loss: 0.03258984908461571\n",
      "iteration 2515, dc_loss: 0.1017199456691742, tv_loss: 0.032589901238679886\n",
      "iteration 2516, dc_loss: 0.10171136260032654, tv_loss: 0.03258420154452324\n",
      "iteration 2517, dc_loss: 0.1016976460814476, tv_loss: 0.032585740089416504\n",
      "iteration 2518, dc_loss: 0.10169069468975067, tv_loss: 0.032593101263046265\n",
      "iteration 2519, dc_loss: 0.10168585181236267, tv_loss: 0.03259468823671341\n",
      "iteration 2520, dc_loss: 0.10167047381401062, tv_loss: 0.032594695687294006\n",
      "iteration 2521, dc_loss: 0.1016605943441391, tv_loss: 0.032585833221673965\n",
      "iteration 2522, dc_loss: 0.10164476931095123, tv_loss: 0.032601241022348404\n",
      "iteration 2523, dc_loss: 0.10164015740156174, tv_loss: 0.0326034314930439\n",
      "iteration 2524, dc_loss: 0.10163743048906326, tv_loss: 0.032599642872810364\n",
      "iteration 2525, dc_loss: 0.10161779075860977, tv_loss: 0.03259335830807686\n",
      "iteration 2526, dc_loss: 0.10161199420690536, tv_loss: 0.03259454295039177\n",
      "iteration 2527, dc_loss: 0.10160215944051743, tv_loss: 0.03260233253240585\n",
      "iteration 2528, dc_loss: 0.10158701241016388, tv_loss: 0.03261062130331993\n",
      "iteration 2529, dc_loss: 0.10158492624759674, tv_loss: 0.03258625790476799\n",
      "iteration 2530, dc_loss: 0.10157434642314911, tv_loss: 0.03259241580963135\n",
      "iteration 2531, dc_loss: 0.10154929757118225, tv_loss: 0.032622452825307846\n",
      "iteration 2532, dc_loss: 0.10155032575130463, tv_loss: 0.03259987384080887\n",
      "iteration 2533, dc_loss: 0.10155042260885239, tv_loss: 0.032589834183454514\n",
      "iteration 2534, dc_loss: 0.10152082145214081, tv_loss: 0.03261825442314148\n",
      "iteration 2535, dc_loss: 0.10152396559715271, tv_loss: 0.032597050070762634\n",
      "iteration 2536, dc_loss: 0.10151693969964981, tv_loss: 0.03259756788611412\n",
      "iteration 2537, dc_loss: 0.10149629414081573, tv_loss: 0.03260345384478569\n",
      "iteration 2538, dc_loss: 0.10148707777261734, tv_loss: 0.03260669857263565\n",
      "iteration 2539, dc_loss: 0.10147947072982788, tv_loss: 0.03260691091418266\n",
      "iteration 2540, dc_loss: 0.10147230327129364, tv_loss: 0.032596610486507416\n",
      "iteration 2541, dc_loss: 0.10145240277051926, tv_loss: 0.03260505199432373\n",
      "iteration 2542, dc_loss: 0.10144805908203125, tv_loss: 0.032614566385746\n",
      "iteration 2543, dc_loss: 0.10144698619842529, tv_loss: 0.03260399028658867\n",
      "iteration 2544, dc_loss: 0.10142160207033157, tv_loss: 0.03260236978530884\n",
      "iteration 2545, dc_loss: 0.10142023861408234, tv_loss: 0.03261420875787735\n",
      "iteration 2546, dc_loss: 0.10141590237617493, tv_loss: 0.03260641172528267\n",
      "iteration 2547, dc_loss: 0.10139582306146622, tv_loss: 0.03260137140750885\n",
      "iteration 2548, dc_loss: 0.10138638317584991, tv_loss: 0.03261132165789604\n",
      "iteration 2549, dc_loss: 0.1013781949877739, tv_loss: 0.03261127695441246\n",
      "iteration 2550, dc_loss: 0.101361945271492, tv_loss: 0.03261563181877136\n",
      "iteration 2551, dc_loss: 0.10136374086141586, tv_loss: 0.03260056674480438\n",
      "iteration 2552, dc_loss: 0.10135315358638763, tv_loss: 0.03260120376944542\n",
      "iteration 2553, dc_loss: 0.10133275389671326, tv_loss: 0.032609980553388596\n",
      "iteration 2554, dc_loss: 0.10133369266986847, tv_loss: 0.03261365741491318\n",
      "iteration 2555, dc_loss: 0.1013147383928299, tv_loss: 0.032605718821287155\n",
      "iteration 2556, dc_loss: 0.10130993276834488, tv_loss: 0.03260879963636398\n",
      "iteration 2557, dc_loss: 0.10130034387111664, tv_loss: 0.03262094780802727\n",
      "iteration 2558, dc_loss: 0.10128167271614075, tv_loss: 0.03261028602719307\n",
      "iteration 2559, dc_loss: 0.10127879679203033, tv_loss: 0.0326174795627594\n",
      "iteration 2560, dc_loss: 0.1012732982635498, tv_loss: 0.03260987624526024\n",
      "iteration 2561, dc_loss: 0.10126352310180664, tv_loss: 0.03260514512658119\n",
      "iteration 2562, dc_loss: 0.10124295204877853, tv_loss: 0.03262775018811226\n",
      "iteration 2563, dc_loss: 0.10123869776725769, tv_loss: 0.032613303512334824\n",
      "iteration 2564, dc_loss: 0.10123053193092346, tv_loss: 0.03262181580066681\n",
      "iteration 2565, dc_loss: 0.10120794922113419, tv_loss: 0.032630570232868195\n",
      "iteration 2566, dc_loss: 0.10120544582605362, tv_loss: 0.032620035111904144\n",
      "iteration 2567, dc_loss: 0.10120921581983566, tv_loss: 0.032629724591970444\n",
      "iteration 2568, dc_loss: 0.10119125992059708, tv_loss: 0.03260203078389168\n",
      "iteration 2569, dc_loss: 0.10117807239294052, tv_loss: 0.032638974487781525\n",
      "iteration 2570, dc_loss: 0.10116574913263321, tv_loss: 0.0326274111866951\n",
      "iteration 2571, dc_loss: 0.10115482658147812, tv_loss: 0.03261410817503929\n",
      "iteration 2572, dc_loss: 0.10114945471286774, tv_loss: 0.03264148533344269\n",
      "iteration 2573, dc_loss: 0.1011311262845993, tv_loss: 0.03262590616941452\n",
      "iteration 2574, dc_loss: 0.1011335477232933, tv_loss: 0.032623887062072754\n",
      "iteration 2575, dc_loss: 0.1011214330792427, tv_loss: 0.032626982778310776\n",
      "iteration 2576, dc_loss: 0.1011071652173996, tv_loss: 0.03262295573949814\n",
      "iteration 2577, dc_loss: 0.10110379755496979, tv_loss: 0.032618239521980286\n",
      "iteration 2578, dc_loss: 0.10108833014965057, tv_loss: 0.03262430801987648\n",
      "iteration 2579, dc_loss: 0.101071298122406, tv_loss: 0.03264209255576134\n",
      "iteration 2580, dc_loss: 0.1010693684220314, tv_loss: 0.03260333463549614\n",
      "iteration 2581, dc_loss: 0.10106189548969269, tv_loss: 0.032633330672979355\n",
      "iteration 2582, dc_loss: 0.10103810578584671, tv_loss: 0.032633762806653976\n",
      "iteration 2583, dc_loss: 0.10104730725288391, tv_loss: 0.032603345811367035\n",
      "iteration 2584, dc_loss: 0.10102857649326324, tv_loss: 0.03262997046113014\n",
      "iteration 2585, dc_loss: 0.10101339221000671, tv_loss: 0.032621756196022034\n",
      "iteration 2586, dc_loss: 0.10101281106472015, tv_loss: 0.03262884169816971\n",
      "iteration 2587, dc_loss: 0.10100528597831726, tv_loss: 0.03260808065533638\n",
      "iteration 2588, dc_loss: 0.10098395496606827, tv_loss: 0.03263446316123009\n",
      "iteration 2589, dc_loss: 0.1009725034236908, tv_loss: 0.03265032172203064\n",
      "iteration 2590, dc_loss: 0.1009756326675415, tv_loss: 0.03260480612516403\n",
      "iteration 2591, dc_loss: 0.10096514970064163, tv_loss: 0.03265385702252388\n",
      "iteration 2592, dc_loss: 0.10095773637294769, tv_loss: 0.03261001780629158\n",
      "iteration 2593, dc_loss: 0.10093286633491516, tv_loss: 0.03265567868947983\n",
      "iteration 2594, dc_loss: 0.10093921422958374, tv_loss: 0.03262674808502197\n",
      "iteration 2595, dc_loss: 0.10093028098344803, tv_loss: 0.03262786939740181\n",
      "iteration 2596, dc_loss: 0.10092417150735855, tv_loss: 0.03263699635863304\n",
      "iteration 2597, dc_loss: 0.10091305524110794, tv_loss: 0.03263745456933975\n",
      "iteration 2598, dc_loss: 0.10092367976903915, tv_loss: 0.032634422183036804\n",
      "iteration 2599, dc_loss: 0.10090313851833344, tv_loss: 0.03261806070804596\n",
      "iteration 2600, dc_loss: 0.10087886452674866, tv_loss: 0.032682184129953384\n",
      "iteration 2601, dc_loss: 0.10087625682353973, tv_loss: 0.03261689469218254\n",
      "iteration 2602, dc_loss: 0.10085862874984741, tv_loss: 0.03266938403248787\n",
      "iteration 2603, dc_loss: 0.10084810107946396, tv_loss: 0.03263210132718086\n",
      "iteration 2604, dc_loss: 0.10083654522895813, tv_loss: 0.03266531229019165\n",
      "iteration 2605, dc_loss: 0.10082990676164627, tv_loss: 0.03264078497886658\n",
      "iteration 2606, dc_loss: 0.10082953423261642, tv_loss: 0.032654087990522385\n",
      "iteration 2607, dc_loss: 0.10080377012491226, tv_loss: 0.032645631581544876\n",
      "iteration 2608, dc_loss: 0.10078851878643036, tv_loss: 0.03267381712794304\n",
      "iteration 2609, dc_loss: 0.10077685862779617, tv_loss: 0.03264216333627701\n",
      "iteration 2610, dc_loss: 0.10079246759414673, tv_loss: 0.03266018256545067\n",
      "iteration 2611, dc_loss: 0.10077544301748276, tv_loss: 0.03263199329376221\n",
      "iteration 2612, dc_loss: 0.10074524581432343, tv_loss: 0.03268823400139809\n",
      "iteration 2613, dc_loss: 0.10073386132717133, tv_loss: 0.03265933692455292\n",
      "iteration 2614, dc_loss: 0.10075929760932922, tv_loss: 0.03265856206417084\n",
      "iteration 2615, dc_loss: 0.10073816776275635, tv_loss: 0.032641779631376266\n",
      "iteration 2616, dc_loss: 0.1007109135389328, tv_loss: 0.032679688185453415\n",
      "iteration 2617, dc_loss: 0.10068704187870026, tv_loss: 0.03267688676714897\n",
      "iteration 2618, dc_loss: 0.10071255266666412, tv_loss: 0.0326496846973896\n",
      "iteration 2619, dc_loss: 0.10070552676916122, tv_loss: 0.032632213085889816\n",
      "iteration 2620, dc_loss: 0.10067666321992874, tv_loss: 0.03266459330916405\n",
      "iteration 2621, dc_loss: 0.10065823793411255, tv_loss: 0.0326562263071537\n",
      "iteration 2622, dc_loss: 0.10068049281835556, tv_loss: 0.032649241387844086\n",
      "iteration 2623, dc_loss: 0.10065604001283646, tv_loss: 0.032641056925058365\n",
      "iteration 2624, dc_loss: 0.10064289718866348, tv_loss: 0.032675549387931824\n",
      "iteration 2625, dc_loss: 0.10061252117156982, tv_loss: 0.032675500959157944\n",
      "iteration 2626, dc_loss: 0.10064591467380524, tv_loss: 0.032653000205755234\n",
      "iteration 2627, dc_loss: 0.10062366724014282, tv_loss: 0.03265320509672165\n",
      "iteration 2628, dc_loss: 0.1006176620721817, tv_loss: 0.032649870961904526\n",
      "iteration 2629, dc_loss: 0.10058002173900604, tv_loss: 0.03268520534038544\n",
      "iteration 2630, dc_loss: 0.10059724003076553, tv_loss: 0.032639920711517334\n",
      "iteration 2631, dc_loss: 0.10057881474494934, tv_loss: 0.03267235681414604\n",
      "iteration 2632, dc_loss: 0.10058370232582092, tv_loss: 0.032639969140291214\n",
      "iteration 2633, dc_loss: 0.1005358099937439, tv_loss: 0.032673079520463943\n",
      "iteration 2634, dc_loss: 0.1005476638674736, tv_loss: 0.032678063958883286\n",
      "iteration 2635, dc_loss: 0.10054108500480652, tv_loss: 0.03262472152709961\n",
      "iteration 2636, dc_loss: 0.10053239017724991, tv_loss: 0.03268938884139061\n",
      "iteration 2637, dc_loss: 0.10049524903297424, tv_loss: 0.032665934413671494\n",
      "iteration 2638, dc_loss: 0.10050556808710098, tv_loss: 0.0326855406165123\n",
      "iteration 2639, dc_loss: 0.10051065683364868, tv_loss: 0.03264833614230156\n",
      "iteration 2640, dc_loss: 0.10048384219408035, tv_loss: 0.03266472369432449\n",
      "iteration 2641, dc_loss: 0.10046224296092987, tv_loss: 0.032697875052690506\n",
      "iteration 2642, dc_loss: 0.10045500099658966, tv_loss: 0.03265247866511345\n",
      "iteration 2643, dc_loss: 0.10047329217195511, tv_loss: 0.032683249562978745\n",
      "iteration 2644, dc_loss: 0.10043957084417343, tv_loss: 0.032659661024808884\n",
      "iteration 2645, dc_loss: 0.10044381022453308, tv_loss: 0.032684702426195145\n",
      "iteration 2646, dc_loss: 0.10042112320661545, tv_loss: 0.03267605975270271\n",
      "iteration 2647, dc_loss: 0.10041780024766922, tv_loss: 0.03267411142587662\n",
      "iteration 2648, dc_loss: 0.10039941221475601, tv_loss: 0.032692816108465195\n",
      "iteration 2649, dc_loss: 0.10040412843227386, tv_loss: 0.032640453428030014\n",
      "iteration 2650, dc_loss: 0.10038711875677109, tv_loss: 0.03270282223820686\n",
      "iteration 2651, dc_loss: 0.10037305951118469, tv_loss: 0.03265602886676788\n",
      "iteration 2652, dc_loss: 0.10036312788724899, tv_loss: 0.03270767629146576\n",
      "iteration 2653, dc_loss: 0.10036672651767731, tv_loss: 0.03266315534710884\n",
      "iteration 2654, dc_loss: 0.10035338997840881, tv_loss: 0.03268280252814293\n",
      "iteration 2655, dc_loss: 0.10032912343740463, tv_loss: 0.03270423039793968\n",
      "iteration 2656, dc_loss: 0.10033505409955978, tv_loss: 0.03266520798206329\n",
      "iteration 2657, dc_loss: 0.10034219920635223, tv_loss: 0.032697681337594986\n",
      "iteration 2658, dc_loss: 0.10033189505338669, tv_loss: 0.032663147896528244\n",
      "iteration 2659, dc_loss: 0.10030557960271835, tv_loss: 0.03272485360503197\n",
      "iteration 2660, dc_loss: 0.10029944032430649, tv_loss: 0.03267914056777954\n",
      "iteration 2661, dc_loss: 0.1002802699804306, tv_loss: 0.03273928910493851\n",
      "iteration 2662, dc_loss: 0.10029110312461853, tv_loss: 0.03267231211066246\n",
      "iteration 2663, dc_loss: 0.10027366876602173, tv_loss: 0.03271373733878136\n",
      "iteration 2664, dc_loss: 0.10027948021888733, tv_loss: 0.03265848755836487\n",
      "iteration 2665, dc_loss: 0.10023126006126404, tv_loss: 0.03273724764585495\n",
      "iteration 2666, dc_loss: 0.10024293512105942, tv_loss: 0.03268539533019066\n",
      "iteration 2667, dc_loss: 0.1002269759774208, tv_loss: 0.032701924443244934\n",
      "iteration 2668, dc_loss: 0.10024713724851608, tv_loss: 0.03266710788011551\n",
      "iteration 2669, dc_loss: 0.10019862651824951, tv_loss: 0.03270190209150314\n",
      "iteration 2670, dc_loss: 0.10019570589065552, tv_loss: 0.03271433338522911\n",
      "iteration 2671, dc_loss: 0.10018932819366455, tv_loss: 0.03267557546496391\n",
      "iteration 2672, dc_loss: 0.10019765794277191, tv_loss: 0.032719630748033524\n",
      "iteration 2673, dc_loss: 0.10016913712024689, tv_loss: 0.03270205855369568\n",
      "iteration 2674, dc_loss: 0.10015088319778442, tv_loss: 0.03273719549179077\n",
      "iteration 2675, dc_loss: 0.10016463696956635, tv_loss: 0.0326925665140152\n",
      "iteration 2676, dc_loss: 0.10014289617538452, tv_loss: 0.03272625058889389\n",
      "iteration 2677, dc_loss: 0.1001448929309845, tv_loss: 0.03268150985240936\n",
      "iteration 2678, dc_loss: 0.10012305527925491, tv_loss: 0.03274274617433548\n",
      "iteration 2679, dc_loss: 0.10014458745718002, tv_loss: 0.032654035836458206\n",
      "iteration 2680, dc_loss: 0.10010629147291183, tv_loss: 0.03274918720126152\n",
      "iteration 2681, dc_loss: 0.10010506957769394, tv_loss: 0.03267640992999077\n",
      "iteration 2682, dc_loss: 0.10007493942975998, tv_loss: 0.032778725028038025\n",
      "iteration 2683, dc_loss: 0.100101538002491, tv_loss: 0.032683588564395905\n",
      "iteration 2684, dc_loss: 0.10006354004144669, tv_loss: 0.03277629241347313\n",
      "iteration 2685, dc_loss: 0.10005855560302734, tv_loss: 0.0327618271112442\n",
      "iteration 2686, dc_loss: 0.10002555698156357, tv_loss: 0.03273222595453262\n",
      "iteration 2687, dc_loss: 0.1000683531165123, tv_loss: 0.03270874172449112\n",
      "iteration 2688, dc_loss: 0.10001780837774277, tv_loss: 0.032705940306186676\n",
      "iteration 2689, dc_loss: 0.10002044588327408, tv_loss: 0.032710302621126175\n",
      "iteration 2690, dc_loss: 0.0999954342842102, tv_loss: 0.03271881863474846\n",
      "iteration 2691, dc_loss: 0.10003119707107544, tv_loss: 0.032652419060468674\n",
      "iteration 2692, dc_loss: 0.09999478608369827, tv_loss: 0.03273240849375725\n",
      "iteration 2693, dc_loss: 0.09997989982366562, tv_loss: 0.032690417021512985\n",
      "iteration 2694, dc_loss: 0.09996223449707031, tv_loss: 0.032761674374341965\n",
      "iteration 2695, dc_loss: 0.09997650235891342, tv_loss: 0.03268986940383911\n",
      "iteration 2696, dc_loss: 0.09994789212942123, tv_loss: 0.03275701031088829\n",
      "iteration 2697, dc_loss: 0.09992873668670654, tv_loss: 0.03272995352745056\n",
      "iteration 2698, dc_loss: 0.0999215766787529, tv_loss: 0.032756634056568146\n",
      "iteration 2699, dc_loss: 0.09992744028568268, tv_loss: 0.03273525834083557\n",
      "iteration 2700, dc_loss: 0.09991484880447388, tv_loss: 0.0327046737074852\n",
      "iteration 2701, dc_loss: 0.09989146143198013, tv_loss: 0.03272488713264465\n",
      "iteration 2702, dc_loss: 0.09989214688539505, tv_loss: 0.032718852162361145\n",
      "iteration 2703, dc_loss: 0.09988953918218613, tv_loss: 0.0326998308300972\n",
      "iteration 2704, dc_loss: 0.09988183528184891, tv_loss: 0.032731957733631134\n",
      "iteration 2705, dc_loss: 0.09985934942960739, tv_loss: 0.03272143006324768\n",
      "iteration 2706, dc_loss: 0.09987429529428482, tv_loss: 0.03274182975292206\n",
      "iteration 2707, dc_loss: 0.09988390654325485, tv_loss: 0.03269759565591812\n",
      "iteration 2708, dc_loss: 0.09985797107219696, tv_loss: 0.03274422883987427\n",
      "iteration 2709, dc_loss: 0.09982845932245255, tv_loss: 0.03274055942893028\n",
      "iteration 2710, dc_loss: 0.09981436282396317, tv_loss: 0.03275168687105179\n",
      "iteration 2711, dc_loss: 0.09983185678720474, tv_loss: 0.03270478546619415\n",
      "iteration 2712, dc_loss: 0.0997997298836708, tv_loss: 0.032733574509620667\n",
      "iteration 2713, dc_loss: 0.09978923946619034, tv_loss: 0.032720014452934265\n",
      "iteration 2714, dc_loss: 0.09977426379919052, tv_loss: 0.03274533152580261\n",
      "iteration 2715, dc_loss: 0.09980267286300659, tv_loss: 0.032691504806280136\n",
      "iteration 2716, dc_loss: 0.09975985437631607, tv_loss: 0.032766878604888916\n",
      "iteration 2717, dc_loss: 0.09977347403764725, tv_loss: 0.032736726105213165\n",
      "iteration 2718, dc_loss: 0.09974516928195953, tv_loss: 0.032749924808740616\n",
      "iteration 2719, dc_loss: 0.09979544579982758, tv_loss: 0.032684553414583206\n",
      "iteration 2720, dc_loss: 0.09973263740539551, tv_loss: 0.0327431857585907\n",
      "iteration 2721, dc_loss: 0.09975631535053253, tv_loss: 0.03268929570913315\n",
      "iteration 2722, dc_loss: 0.09970611333847046, tv_loss: 0.03277267888188362\n",
      "iteration 2723, dc_loss: 0.09976806491613388, tv_loss: 0.032671086490154266\n",
      "iteration 2724, dc_loss: 0.09969425946474075, tv_loss: 0.03278830274939537\n",
      "iteration 2725, dc_loss: 0.09974666684865952, tv_loss: 0.0327221117913723\n",
      "iteration 2726, dc_loss: 0.09968187659978867, tv_loss: 0.032766055315732956\n",
      "iteration 2727, dc_loss: 0.09976500272750854, tv_loss: 0.03267250582575798\n",
      "iteration 2728, dc_loss: 0.09965772181749344, tv_loss: 0.03276141360402107\n",
      "iteration 2729, dc_loss: 0.09967916458845139, tv_loss: 0.032693471759557724\n",
      "iteration 2730, dc_loss: 0.09962011128664017, tv_loss: 0.03276669234037399\n",
      "iteration 2731, dc_loss: 0.0996701568365097, tv_loss: 0.03268805518746376\n",
      "iteration 2732, dc_loss: 0.09964574128389359, tv_loss: 0.03275737538933754\n",
      "iteration 2733, dc_loss: 0.09963095188140869, tv_loss: 0.032750148326158524\n",
      "iteration 2734, dc_loss: 0.09961271286010742, tv_loss: 0.03273089602589607\n",
      "iteration 2735, dc_loss: 0.09959443658590317, tv_loss: 0.03272099047899246\n",
      "iteration 2736, dc_loss: 0.09960506856441498, tv_loss: 0.032718557864427567\n",
      "iteration 2737, dc_loss: 0.09958945214748383, tv_loss: 0.03273211792111397\n",
      "iteration 2738, dc_loss: 0.09959767013788223, tv_loss: 0.03274025768041611\n",
      "iteration 2739, dc_loss: 0.0995829626917839, tv_loss: 0.03271446377038956\n",
      "iteration 2740, dc_loss: 0.09954750537872314, tv_loss: 0.03274180740118027\n",
      "iteration 2741, dc_loss: 0.09951809048652649, tv_loss: 0.0327516607940197\n",
      "iteration 2742, dc_loss: 0.09952405095100403, tv_loss: 0.03274063766002655\n",
      "iteration 2743, dc_loss: 0.09954848885536194, tv_loss: 0.032703518867492676\n",
      "iteration 2744, dc_loss: 0.09952041506767273, tv_loss: 0.03273681178689003\n",
      "iteration 2745, dc_loss: 0.09949978440999985, tv_loss: 0.03271941468119621\n",
      "iteration 2746, dc_loss: 0.09947703778743744, tv_loss: 0.032749347388744354\n",
      "iteration 2747, dc_loss: 0.09949736297130585, tv_loss: 0.032703593373298645\n",
      "iteration 2748, dc_loss: 0.09947121143341064, tv_loss: 0.03275435417890549\n",
      "iteration 2749, dc_loss: 0.09946177899837494, tv_loss: 0.03275018930435181\n",
      "iteration 2750, dc_loss: 0.0994407907128334, tv_loss: 0.032738614827394485\n",
      "iteration 2751, dc_loss: 0.09945884346961975, tv_loss: 0.03270157054066658\n",
      "iteration 2752, dc_loss: 0.09942413121461868, tv_loss: 0.032738640904426575\n",
      "iteration 2753, dc_loss: 0.09942270815372467, tv_loss: 0.032722506672143936\n",
      "iteration 2754, dc_loss: 0.09940332174301147, tv_loss: 0.0327545665204525\n",
      "iteration 2755, dc_loss: 0.09942331165075302, tv_loss: 0.03269331902265549\n",
      "iteration 2756, dc_loss: 0.09937731176614761, tv_loss: 0.03276250883936882\n",
      "iteration 2757, dc_loss: 0.09937327355146408, tv_loss: 0.03274817392230034\n",
      "iteration 2758, dc_loss: 0.09935737401247025, tv_loss: 0.032740987837314606\n",
      "iteration 2759, dc_loss: 0.09937851876020432, tv_loss: 0.0327095203101635\n",
      "iteration 2760, dc_loss: 0.09934887290000916, tv_loss: 0.03272975981235504\n",
      "iteration 2761, dc_loss: 0.09933266043663025, tv_loss: 0.032722752541303635\n",
      "iteration 2762, dc_loss: 0.09932349622249603, tv_loss: 0.03274663910269737\n",
      "iteration 2763, dc_loss: 0.09934065490961075, tv_loss: 0.03269314765930176\n",
      "iteration 2764, dc_loss: 0.09929930418729782, tv_loss: 0.032767657190561295\n",
      "iteration 2765, dc_loss: 0.09929849952459335, tv_loss: 0.03274455666542053\n",
      "iteration 2766, dc_loss: 0.09927628934383392, tv_loss: 0.03275420516729355\n",
      "iteration 2767, dc_loss: 0.09931333363056183, tv_loss: 0.03270795941352844\n",
      "iteration 2768, dc_loss: 0.09926901012659073, tv_loss: 0.032734375447034836\n",
      "iteration 2769, dc_loss: 0.09926709532737732, tv_loss: 0.03272093087434769\n",
      "iteration 2770, dc_loss: 0.09924395382404327, tv_loss: 0.03275502100586891\n",
      "iteration 2771, dc_loss: 0.09927810728549957, tv_loss: 0.03269082307815552\n",
      "iteration 2772, dc_loss: 0.09922825545072556, tv_loss: 0.032770656049251556\n",
      "iteration 2773, dc_loss: 0.0992484837770462, tv_loss: 0.03273121640086174\n",
      "iteration 2774, dc_loss: 0.0992085188627243, tv_loss: 0.03276554122567177\n",
      "iteration 2775, dc_loss: 0.09927887469530106, tv_loss: 0.032685812562704086\n",
      "iteration 2776, dc_loss: 0.09919574856758118, tv_loss: 0.032771944999694824\n",
      "iteration 2777, dc_loss: 0.09925645589828491, tv_loss: 0.032698579132556915\n",
      "iteration 2778, dc_loss: 0.09918591380119324, tv_loss: 0.03279094770550728\n",
      "iteration 2779, dc_loss: 0.09929365664720535, tv_loss: 0.03266135975718498\n",
      "iteration 2780, dc_loss: 0.0991760715842247, tv_loss: 0.03279811516404152\n",
      "iteration 2781, dc_loss: 0.09924664348363876, tv_loss: 0.032699789851903915\n",
      "iteration 2782, dc_loss: 0.09913615882396698, tv_loss: 0.03279242664575577\n",
      "iteration 2783, dc_loss: 0.09922338277101517, tv_loss: 0.03266973793506622\n",
      "iteration 2784, dc_loss: 0.09911008179187775, tv_loss: 0.0327753983438015\n",
      "iteration 2785, dc_loss: 0.09913738071918488, tv_loss: 0.03272273391485214\n",
      "iteration 2786, dc_loss: 0.0991051122546196, tv_loss: 0.03274876996874809\n",
      "iteration 2787, dc_loss: 0.09912171959877014, tv_loss: 0.032708607614040375\n",
      "iteration 2788, dc_loss: 0.09910742938518524, tv_loss: 0.03273638337850571\n",
      "iteration 2789, dc_loss: 0.09905374050140381, tv_loss: 0.032768357545137405\n",
      "iteration 2790, dc_loss: 0.0990886464715004, tv_loss: 0.03272740915417671\n",
      "iteration 2791, dc_loss: 0.09906773269176483, tv_loss: 0.032728131860494614\n",
      "iteration 2792, dc_loss: 0.09907674044370651, tv_loss: 0.03272987902164459\n",
      "iteration 2793, dc_loss: 0.09903571754693985, tv_loss: 0.03275459259748459\n",
      "iteration 2794, dc_loss: 0.09904355555772781, tv_loss: 0.03273915871977806\n",
      "iteration 2795, dc_loss: 0.09904972463846207, tv_loss: 0.0327143520116806\n",
      "iteration 2796, dc_loss: 0.0990239754319191, tv_loss: 0.032745808362960815\n",
      "iteration 2797, dc_loss: 0.09900383651256561, tv_loss: 0.03275185450911522\n",
      "iteration 2798, dc_loss: 0.0990096777677536, tv_loss: 0.03273984417319298\n",
      "iteration 2799, dc_loss: 0.09902593493461609, tv_loss: 0.0327102355659008\n",
      "iteration 2800, dc_loss: 0.09902435541152954, tv_loss: 0.032735396176576614\n",
      "iteration 2801, dc_loss: 0.09899426251649857, tv_loss: 0.032754767686128616\n",
      "iteration 2802, dc_loss: 0.09897290915250778, tv_loss: 0.032706789672374725\n",
      "iteration 2803, dc_loss: 0.09896756708621979, tv_loss: 0.032701268792152405\n",
      "iteration 2804, dc_loss: 0.09896724671125412, tv_loss: 0.032702215015888214\n",
      "iteration 2805, dc_loss: 0.0989496037364006, tv_loss: 0.03271282836794853\n",
      "iteration 2806, dc_loss: 0.09891746193170547, tv_loss: 0.03270658105611801\n",
      "iteration 2807, dc_loss: 0.09893513470888138, tv_loss: 0.032685287296772\n",
      "iteration 2808, dc_loss: 0.09895116090774536, tv_loss: 0.03268779069185257\n",
      "iteration 2809, dc_loss: 0.09889690577983856, tv_loss: 0.03269035741686821\n",
      "iteration 2810, dc_loss: 0.09889398515224457, tv_loss: 0.03272368758916855\n",
      "iteration 2811, dc_loss: 0.09891137480735779, tv_loss: 0.032674726098775864\n",
      "iteration 2812, dc_loss: 0.09889446198940277, tv_loss: 0.03271164372563362\n",
      "iteration 2813, dc_loss: 0.09887050837278366, tv_loss: 0.03269653394818306\n",
      "iteration 2814, dc_loss: 0.09887675195932388, tv_loss: 0.032709915190935135\n",
      "iteration 2815, dc_loss: 0.09886843711137772, tv_loss: 0.03268983215093613\n",
      "iteration 2816, dc_loss: 0.09885300695896149, tv_loss: 0.032708048820495605\n",
      "iteration 2817, dc_loss: 0.09884607046842575, tv_loss: 0.03269188851118088\n",
      "iteration 2818, dc_loss: 0.0988449975848198, tv_loss: 0.03269829601049423\n",
      "iteration 2819, dc_loss: 0.09884131699800491, tv_loss: 0.03268369287252426\n",
      "iteration 2820, dc_loss: 0.0988185852766037, tv_loss: 0.032699279487133026\n",
      "iteration 2821, dc_loss: 0.09880924224853516, tv_loss: 0.03271083906292915\n",
      "iteration 2822, dc_loss: 0.09881352633237839, tv_loss: 0.03268648311495781\n",
      "iteration 2823, dc_loss: 0.09880728274583817, tv_loss: 0.03270118311047554\n",
      "iteration 2824, dc_loss: 0.09879084676504135, tv_loss: 0.032691631466150284\n",
      "iteration 2825, dc_loss: 0.09878136217594147, tv_loss: 0.0327228307723999\n",
      "iteration 2826, dc_loss: 0.09878131002187729, tv_loss: 0.03268970921635628\n",
      "iteration 2827, dc_loss: 0.09877846390008926, tv_loss: 0.03271220996975899\n",
      "iteration 2828, dc_loss: 0.09876007586717606, tv_loss: 0.03270645812153816\n",
      "iteration 2829, dc_loss: 0.09875175356864929, tv_loss: 0.032718461006879807\n",
      "iteration 2830, dc_loss: 0.09875371307134628, tv_loss: 0.032698772847652435\n",
      "iteration 2831, dc_loss: 0.09874480962753296, tv_loss: 0.03270547091960907\n",
      "iteration 2832, dc_loss: 0.09873543679714203, tv_loss: 0.03269985690712929\n",
      "iteration 2833, dc_loss: 0.09872572124004364, tv_loss: 0.032706841826438904\n",
      "iteration 2834, dc_loss: 0.0987161174416542, tv_loss: 0.032699573785066605\n",
      "iteration 2835, dc_loss: 0.09871627390384674, tv_loss: 0.03270026668906212\n",
      "iteration 2836, dc_loss: 0.09870640188455582, tv_loss: 0.03269351273775101\n",
      "iteration 2837, dc_loss: 0.09869405627250671, tv_loss: 0.03271091356873512\n",
      "iteration 2838, dc_loss: 0.09868979454040527, tv_loss: 0.032692279666662216\n",
      "iteration 2839, dc_loss: 0.0986872985959053, tv_loss: 0.0327090248465538\n",
      "iteration 2840, dc_loss: 0.09867626428604126, tv_loss: 0.032691944390535355\n",
      "iteration 2841, dc_loss: 0.09866832196712494, tv_loss: 0.03271840140223503\n",
      "iteration 2842, dc_loss: 0.09865764528512955, tv_loss: 0.03270428255200386\n",
      "iteration 2843, dc_loss: 0.0986565500497818, tv_loss: 0.032714903354644775\n",
      "iteration 2844, dc_loss: 0.09864848107099533, tv_loss: 0.03270547091960907\n",
      "iteration 2845, dc_loss: 0.09863433241844177, tv_loss: 0.03271232917904854\n",
      "iteration 2846, dc_loss: 0.09863090515136719, tv_loss: 0.03270517662167549\n",
      "iteration 2847, dc_loss: 0.09862802922725677, tv_loss: 0.03269953280687332\n",
      "iteration 2848, dc_loss: 0.09861527383327484, tv_loss: 0.032704487442970276\n",
      "iteration 2849, dc_loss: 0.09860805422067642, tv_loss: 0.0327003188431263\n",
      "iteration 2850, dc_loss: 0.09860401600599289, tv_loss: 0.03269661217927933\n",
      "iteration 2851, dc_loss: 0.09859495609998703, tv_loss: 0.03270173817873001\n",
      "iteration 2852, dc_loss: 0.0985870510339737, tv_loss: 0.03269744664430618\n",
      "iteration 2853, dc_loss: 0.09857963025569916, tv_loss: 0.032706644386053085\n",
      "iteration 2854, dc_loss: 0.09857441484928131, tv_loss: 0.03269451484084129\n",
      "iteration 2855, dc_loss: 0.09856446832418442, tv_loss: 0.03270989656448364\n",
      "iteration 2856, dc_loss: 0.0985577404499054, tv_loss: 0.032699037343263626\n",
      "iteration 2857, dc_loss: 0.09855391085147858, tv_loss: 0.03270462900400162\n",
      "iteration 2858, dc_loss: 0.09854406863451004, tv_loss: 0.032700568437576294\n",
      "iteration 2859, dc_loss: 0.0985330119729042, tv_loss: 0.03270873799920082\n",
      "iteration 2860, dc_loss: 0.09853268414735794, tv_loss: 0.032699406147003174\n",
      "iteration 2861, dc_loss: 0.09852469712495804, tv_loss: 0.032697778195142746\n",
      "iteration 2862, dc_loss: 0.09851323068141937, tv_loss: 0.032699115574359894\n",
      "iteration 2863, dc_loss: 0.09850963205099106, tv_loss: 0.03270157426595688\n",
      "iteration 2864, dc_loss: 0.09850142896175385, tv_loss: 0.032691583037376404\n",
      "iteration 2865, dc_loss: 0.09848970174789429, tv_loss: 0.032710254192352295\n",
      "iteration 2866, dc_loss: 0.09848887473344803, tv_loss: 0.0326901413500309\n",
      "iteration 2867, dc_loss: 0.09848660975694656, tv_loss: 0.03270237147808075\n",
      "iteration 2868, dc_loss: 0.09847117960453033, tv_loss: 0.03269783779978752\n",
      "iteration 2869, dc_loss: 0.09845653176307678, tv_loss: 0.032720573246479034\n",
      "iteration 2870, dc_loss: 0.09845852851867676, tv_loss: 0.032698340713977814\n",
      "iteration 2871, dc_loss: 0.09846051037311554, tv_loss: 0.03270407021045685\n",
      "iteration 2872, dc_loss: 0.09844270348548889, tv_loss: 0.03270338103175163\n",
      "iteration 2873, dc_loss: 0.0984291210770607, tv_loss: 0.032718122005462646\n",
      "iteration 2874, dc_loss: 0.09842979907989502, tv_loss: 0.03270331397652626\n",
      "iteration 2875, dc_loss: 0.09842806309461594, tv_loss: 0.03269962966442108\n",
      "iteration 2876, dc_loss: 0.09841623157262802, tv_loss: 0.032702382653951645\n",
      "iteration 2877, dc_loss: 0.09840337187051773, tv_loss: 0.03270552679896355\n",
      "iteration 2878, dc_loss: 0.09839961677789688, tv_loss: 0.032698605209589005\n",
      "iteration 2879, dc_loss: 0.0984000414609909, tv_loss: 0.0326969288289547\n",
      "iteration 2880, dc_loss: 0.09838636964559555, tv_loss: 0.032694652676582336\n",
      "iteration 2881, dc_loss: 0.09837321192026138, tv_loss: 0.032712168991565704\n",
      "iteration 2882, dc_loss: 0.0983738899230957, tv_loss: 0.03269116207957268\n",
      "iteration 2883, dc_loss: 0.0983719751238823, tv_loss: 0.03270528465509415\n",
      "iteration 2884, dc_loss: 0.09835433959960938, tv_loss: 0.032699551433324814\n",
      "iteration 2885, dc_loss: 0.0983462780714035, tv_loss: 0.03271994739770889\n",
      "iteration 2886, dc_loss: 0.09834491461515427, tv_loss: 0.03269863501191139\n",
      "iteration 2887, dc_loss: 0.09834013879299164, tv_loss: 0.032709408551454544\n",
      "iteration 2888, dc_loss: 0.09832507371902466, tv_loss: 0.032710276544094086\n",
      "iteration 2889, dc_loss: 0.09831968694925308, tv_loss: 0.03270771726965904\n",
      "iteration 2890, dc_loss: 0.09831597656011581, tv_loss: 0.032701775431632996\n",
      "iteration 2891, dc_loss: 0.09830955415964127, tv_loss: 0.032701268792152405\n",
      "iteration 2892, dc_loss: 0.09829734265804291, tv_loss: 0.032703664153814316\n",
      "iteration 2893, dc_loss: 0.09829297661781311, tv_loss: 0.03270091116428375\n",
      "iteration 2894, dc_loss: 0.09828821569681168, tv_loss: 0.03269476443529129\n",
      "iteration 2895, dc_loss: 0.09828108549118042, tv_loss: 0.03270237147808075\n",
      "iteration 2896, dc_loss: 0.09826838970184326, tv_loss: 0.032697468996047974\n",
      "iteration 2897, dc_loss: 0.0982622280716896, tv_loss: 0.03271103650331497\n",
      "iteration 2898, dc_loss: 0.09825978428125381, tv_loss: 0.032691001892089844\n",
      "iteration 2899, dc_loss: 0.09825468808412552, tv_loss: 0.032703425735235214\n",
      "iteration 2900, dc_loss: 0.09823696315288544, tv_loss: 0.03270453214645386\n",
      "iteration 2901, dc_loss: 0.09823323041200638, tv_loss: 0.03270840272307396\n",
      "iteration 2902, dc_loss: 0.09823630005121231, tv_loss: 0.032689329236745834\n",
      "iteration 2903, dc_loss: 0.09822544455528259, tv_loss: 0.0327053926885128\n",
      "iteration 2904, dc_loss: 0.0982067734003067, tv_loss: 0.03270340710878372\n",
      "iteration 2905, dc_loss: 0.09820391982793808, tv_loss: 0.03271672874689102\n",
      "iteration 2906, dc_loss: 0.09820418059825897, tv_loss: 0.03269250690937042\n",
      "iteration 2907, dc_loss: 0.09819632768630981, tv_loss: 0.03270946815609932\n",
      "iteration 2908, dc_loss: 0.09818117320537567, tv_loss: 0.032707057893276215\n",
      "iteration 2909, dc_loss: 0.09817913174629211, tv_loss: 0.03270702809095383\n",
      "iteration 2910, dc_loss: 0.09817343205213547, tv_loss: 0.032702427357435226\n",
      "iteration 2911, dc_loss: 0.09816181659698486, tv_loss: 0.0327068567276001\n",
      "iteration 2912, dc_loss: 0.09815767407417297, tv_loss: 0.03270009905099869\n",
      "iteration 2913, dc_loss: 0.09815027564764023, tv_loss: 0.03270763158798218\n",
      "iteration 2914, dc_loss: 0.09813902527093887, tv_loss: 0.032701168209314346\n",
      "iteration 2915, dc_loss: 0.09813826531171799, tv_loss: 0.032711245119571686\n",
      "iteration 2916, dc_loss: 0.09812942147254944, tv_loss: 0.0326995924115181\n",
      "iteration 2917, dc_loss: 0.098116934299469, tv_loss: 0.03271770104765892\n",
      "iteration 2918, dc_loss: 0.09811276942491531, tv_loss: 0.03270671144127846\n",
      "iteration 2919, dc_loss: 0.09811054170131683, tv_loss: 0.032704055309295654\n",
      "iteration 2920, dc_loss: 0.09809806942939758, tv_loss: 0.03270949795842171\n",
      "iteration 2921, dc_loss: 0.09809041023254395, tv_loss: 0.032705746591091156\n",
      "iteration 2922, dc_loss: 0.0980844497680664, tv_loss: 0.03270381689071655\n",
      "iteration 2923, dc_loss: 0.09807771444320679, tv_loss: 0.03271234408020973\n",
      "iteration 2924, dc_loss: 0.0980682447552681, tv_loss: 0.0327012874186039\n",
      "iteration 2925, dc_loss: 0.09806519746780396, tv_loss: 0.032714392989873886\n",
      "iteration 2926, dc_loss: 0.09805882722139359, tv_loss: 0.032701220363378525\n",
      "iteration 2927, dc_loss: 0.09804855287075043, tv_loss: 0.03271407634019852\n",
      "iteration 2928, dc_loss: 0.09803922474384308, tv_loss: 0.03270994499325752\n",
      "iteration 2929, dc_loss: 0.09803588688373566, tv_loss: 0.03271033987402916\n",
      "iteration 2930, dc_loss: 0.09802663326263428, tv_loss: 0.03270795941352844\n",
      "iteration 2931, dc_loss: 0.09802249819040298, tv_loss: 0.03270724043250084\n",
      "iteration 2932, dc_loss: 0.09801267087459564, tv_loss: 0.03270906209945679\n",
      "iteration 2933, dc_loss: 0.09800460934638977, tv_loss: 0.0327116921544075\n",
      "iteration 2934, dc_loss: 0.09800136834383011, tv_loss: 0.032698359340429306\n",
      "iteration 2935, dc_loss: 0.09799586981534958, tv_loss: 0.03271503001451492\n",
      "iteration 2936, dc_loss: 0.09798195958137512, tv_loss: 0.032705117017030716\n",
      "iteration 2937, dc_loss: 0.09797637909650803, tv_loss: 0.032721586525440216\n",
      "iteration 2938, dc_loss: 0.0979718267917633, tv_loss: 0.032705627381801605\n",
      "iteration 2939, dc_loss: 0.09796639531850815, tv_loss: 0.03271062672138214\n",
      "iteration 2940, dc_loss: 0.09795436263084412, tv_loss: 0.03271351009607315\n",
      "iteration 2941, dc_loss: 0.09794659912586212, tv_loss: 0.03271448612213135\n",
      "iteration 2942, dc_loss: 0.0979425385594368, tv_loss: 0.03270391374826431\n",
      "iteration 2943, dc_loss: 0.09793640673160553, tv_loss: 0.03271514177322388\n",
      "iteration 2944, dc_loss: 0.09792701154947281, tv_loss: 0.032706767320632935\n",
      "iteration 2945, dc_loss: 0.09792280942201614, tv_loss: 0.03271283209323883\n",
      "iteration 2946, dc_loss: 0.09791342169046402, tv_loss: 0.03270694985985756\n",
      "iteration 2947, dc_loss: 0.09790582954883575, tv_loss: 0.03271894529461861\n",
      "iteration 2948, dc_loss: 0.09789782017469406, tv_loss: 0.032708290964365005\n",
      "iteration 2949, dc_loss: 0.09789233654737473, tv_loss: 0.032717496156692505\n",
      "iteration 2950, dc_loss: 0.09788434952497482, tv_loss: 0.032710615545511246\n",
      "iteration 2951, dc_loss: 0.09787668287754059, tv_loss: 0.032715726643800735\n",
      "iteration 2952, dc_loss: 0.09787153452634811, tv_loss: 0.03270948305726051\n",
      "iteration 2953, dc_loss: 0.09786415100097656, tv_loss: 0.032715462148189545\n",
      "iteration 2954, dc_loss: 0.09785482287406921, tv_loss: 0.03270971402525902\n",
      "iteration 2955, dc_loss: 0.09785176813602448, tv_loss: 0.03271523490548134\n",
      "iteration 2956, dc_loss: 0.09784011542797089, tv_loss: 0.0327138788998127\n",
      "iteration 2957, dc_loss: 0.09783262014389038, tv_loss: 0.032716188579797745\n",
      "iteration 2958, dc_loss: 0.09782875329256058, tv_loss: 0.03270794451236725\n",
      "iteration 2959, dc_loss: 0.09782174974679947, tv_loss: 0.03271551802754402\n",
      "iteration 2960, dc_loss: 0.0978105217218399, tv_loss: 0.032712142914533615\n",
      "iteration 2961, dc_loss: 0.09780722111463547, tv_loss: 0.03271457925438881\n",
      "iteration 2962, dc_loss: 0.09779998660087585, tv_loss: 0.03270938619971275\n",
      "iteration 2963, dc_loss: 0.09779021143913269, tv_loss: 0.03271884098649025\n",
      "iteration 2964, dc_loss: 0.09778527170419693, tv_loss: 0.03270725905895233\n",
      "iteration 2965, dc_loss: 0.09778033196926117, tv_loss: 0.03271685913205147\n",
      "iteration 2966, dc_loss: 0.09776881337165833, tv_loss: 0.03271201625466347\n",
      "iteration 2967, dc_loss: 0.09776397049427032, tv_loss: 0.03271967172622681\n",
      "iteration 2968, dc_loss: 0.09776011109352112, tv_loss: 0.03270910307765007\n",
      "iteration 2969, dc_loss: 0.0977453663945198, tv_loss: 0.03272325545549393\n",
      "iteration 2970, dc_loss: 0.09773855656385422, tv_loss: 0.0327199324965477\n",
      "iteration 2971, dc_loss: 0.09774115681648254, tv_loss: 0.03270990774035454\n",
      "iteration 2972, dc_loss: 0.09772790968418121, tv_loss: 0.03271526098251343\n",
      "iteration 2973, dc_loss: 0.09771662950515747, tv_loss: 0.03272281959652901\n",
      "iteration 2974, dc_loss: 0.09771590679883957, tv_loss: 0.03270946443080902\n",
      "iteration 2975, dc_loss: 0.0977102592587471, tv_loss: 0.032717764377593994\n",
      "iteration 2976, dc_loss: 0.09769661724567413, tv_loss: 0.032717883586883545\n",
      "iteration 2977, dc_loss: 0.09769345074892044, tv_loss: 0.03271796181797981\n",
      "iteration 2978, dc_loss: 0.09768813103437424, tv_loss: 0.032711271196603775\n",
      "iteration 2979, dc_loss: 0.09767492860555649, tv_loss: 0.032724376767873764\n",
      "iteration 2980, dc_loss: 0.09767307341098785, tv_loss: 0.032713383436203\n",
      "iteration 2981, dc_loss: 0.09766656905412674, tv_loss: 0.03271597996354103\n",
      "iteration 2982, dc_loss: 0.09765467047691345, tv_loss: 0.03271438553929329\n",
      "iteration 2983, dc_loss: 0.09764895588159561, tv_loss: 0.03272690623998642\n",
      "iteration 2984, dc_loss: 0.09764531254768372, tv_loss: 0.032708149403333664\n",
      "iteration 2985, dc_loss: 0.09763448685407639, tv_loss: 0.032728828489780426\n",
      "iteration 2986, dc_loss: 0.09762586653232574, tv_loss: 0.03271998465061188\n",
      "iteration 2987, dc_loss: 0.09762343019247055, tv_loss: 0.032718073576688766\n",
      "iteration 2988, dc_loss: 0.09761587530374527, tv_loss: 0.032719727605581284\n",
      "iteration 2989, dc_loss: 0.09760819375514984, tv_loss: 0.03271954879164696\n",
      "iteration 2990, dc_loss: 0.0975959524512291, tv_loss: 0.03271680697798729\n",
      "iteration 2991, dc_loss: 0.09759390354156494, tv_loss: 0.03272276744246483\n",
      "iteration 2992, dc_loss: 0.0975862443447113, tv_loss: 0.03271728754043579\n",
      "iteration 2993, dc_loss: 0.09757760912179947, tv_loss: 0.03272242099046707\n",
      "iteration 2994, dc_loss: 0.0975709930062294, tv_loss: 0.03272038325667381\n",
      "iteration 2995, dc_loss: 0.0975622683763504, tv_loss: 0.032722510397434235\n",
      "iteration 2996, dc_loss: 0.09755925834178925, tv_loss: 0.032719388604164124\n",
      "iteration 2997, dc_loss: 0.09755414724349976, tv_loss: 0.03271391615271568\n",
      "iteration 2998, dc_loss: 0.09754011780023575, tv_loss: 0.03271816298365593\n",
      "iteration 2999, dc_loss: 0.09752976894378662, tv_loss: 0.03273223713040352\n",
      "iteration 3000, dc_loss: 0.09753242135047913, tv_loss: 0.032711051404476166\n",
      "iteration 3001, dc_loss: 0.09752481430768967, tv_loss: 0.03271617740392685\n",
      "iteration 3002, dc_loss: 0.0975121557712555, tv_loss: 0.03272378444671631\n",
      "iteration 3003, dc_loss: 0.09751154482364655, tv_loss: 0.03271472826600075\n",
      "iteration 3004, dc_loss: 0.09749757498502731, tv_loss: 0.032716743648052216\n",
      "iteration 3005, dc_loss: 0.09748893976211548, tv_loss: 0.03272717818617821\n",
      "iteration 3006, dc_loss: 0.09749016165733337, tv_loss: 0.03271493688225746\n",
      "iteration 3007, dc_loss: 0.09747853875160217, tv_loss: 0.032718051224946976\n",
      "iteration 3008, dc_loss: 0.09746745228767395, tv_loss: 0.032723844051361084\n",
      "iteration 3009, dc_loss: 0.09747288376092911, tv_loss: 0.03270989656448364\n",
      "iteration 3010, dc_loss: 0.09745438396930695, tv_loss: 0.03272290155291557\n",
      "iteration 3011, dc_loss: 0.09744992852210999, tv_loss: 0.032716304063797\n",
      "iteration 3012, dc_loss: 0.09744735807180405, tv_loss: 0.03271103650331497\n",
      "iteration 3013, dc_loss: 0.0974331945180893, tv_loss: 0.03271808475255966\n",
      "iteration 3014, dc_loss: 0.09743098169565201, tv_loss: 0.03271522372961044\n",
      "iteration 3015, dc_loss: 0.09742048382759094, tv_loss: 0.032720647752285004\n",
      "iteration 3016, dc_loss: 0.09741663187742233, tv_loss: 0.03271206468343735\n",
      "iteration 3017, dc_loss: 0.09741037338972092, tv_loss: 0.03271118178963661\n",
      "iteration 3018, dc_loss: 0.0973932296037674, tv_loss: 0.032721471041440964\n",
      "iteration 3019, dc_loss: 0.09740085154771805, tv_loss: 0.032708585262298584\n",
      "iteration 3020, dc_loss: 0.09738578647375107, tv_loss: 0.03271997720003128\n",
      "iteration 3021, dc_loss: 0.09738071262836456, tv_loss: 0.032719798386096954\n",
      "iteration 3022, dc_loss: 0.09736857563257217, tv_loss: 0.03272193297743797\n",
      "iteration 3023, dc_loss: 0.09736872464418411, tv_loss: 0.03271462768316269\n",
      "iteration 3024, dc_loss: 0.09735529869794846, tv_loss: 0.03272182494401932\n",
      "iteration 3025, dc_loss: 0.09735377877950668, tv_loss: 0.03271658346056938\n",
      "iteration 3026, dc_loss: 0.09734369069337845, tv_loss: 0.032723307609558105\n",
      "iteration 3027, dc_loss: 0.09733700752258301, tv_loss: 0.03272385522723198\n",
      "iteration 3028, dc_loss: 0.097330741584301, tv_loss: 0.03272240608930588\n",
      "iteration 3029, dc_loss: 0.09732136875391006, tv_loss: 0.0327225886285305\n",
      "iteration 3030, dc_loss: 0.09731471538543701, tv_loss: 0.032721396535634995\n",
      "iteration 3031, dc_loss: 0.0973094180226326, tv_loss: 0.032720983028411865\n",
      "iteration 3032, dc_loss: 0.0973009392619133, tv_loss: 0.03272099047899246\n",
      "iteration 3033, dc_loss: 0.09729406982660294, tv_loss: 0.032717637717723846\n",
      "iteration 3034, dc_loss: 0.09729065746068954, tv_loss: 0.03271540254354477\n",
      "iteration 3035, dc_loss: 0.09728135913610458, tv_loss: 0.032722100615501404\n",
      "iteration 3036, dc_loss: 0.097267284989357, tv_loss: 0.03272922337055206\n",
      "iteration 3037, dc_loss: 0.09727242588996887, tv_loss: 0.032721493393182755\n",
      "iteration 3038, dc_loss: 0.09725556522607803, tv_loss: 0.03272915631532669\n",
      "iteration 3039, dc_loss: 0.09725382179021835, tv_loss: 0.03271821513772011\n",
      "iteration 3040, dc_loss: 0.09724453091621399, tv_loss: 0.03272130712866783\n",
      "iteration 3041, dc_loss: 0.0972433015704155, tv_loss: 0.03271540254354477\n",
      "iteration 3042, dc_loss: 0.09722690284252167, tv_loss: 0.03272674232721329\n",
      "iteration 3043, dc_loss: 0.09723066538572311, tv_loss: 0.03271748125553131\n",
      "iteration 3044, dc_loss: 0.0972113087773323, tv_loss: 0.032727912068367004\n",
      "iteration 3045, dc_loss: 0.09721764922142029, tv_loss: 0.03271281719207764\n",
      "iteration 3046, dc_loss: 0.09719602018594742, tv_loss: 0.03273003548383713\n",
      "iteration 3047, dc_loss: 0.09720063954591751, tv_loss: 0.03272151201963425\n",
      "iteration 3048, dc_loss: 0.0971883237361908, tv_loss: 0.03272976726293564\n",
      "iteration 3049, dc_loss: 0.09718608111143112, tv_loss: 0.03272167220711708\n",
      "iteration 3050, dc_loss: 0.09716866910457611, tv_loss: 0.03272661939263344\n",
      "iteration 3051, dc_loss: 0.09717067331075668, tv_loss: 0.032719481736421585\n",
      "iteration 3052, dc_loss: 0.0971573069691658, tv_loss: 0.032729409635066986\n",
      "iteration 3053, dc_loss: 0.0971597209572792, tv_loss: 0.0327213890850544\n",
      "iteration 3054, dc_loss: 0.09713996946811676, tv_loss: 0.03273611515760422\n",
      "iteration 3055, dc_loss: 0.0971497967839241, tv_loss: 0.03272106498479843\n",
      "iteration 3056, dc_loss: 0.09713374823331833, tv_loss: 0.03272801265120506\n",
      "iteration 3057, dc_loss: 0.09712930768728256, tv_loss: 0.032727163285017014\n",
      "iteration 3058, dc_loss: 0.09711918979883194, tv_loss: 0.032733868807554245\n",
      "iteration 3059, dc_loss: 0.09712005406618118, tv_loss: 0.03272973373532295\n",
      "iteration 3060, dc_loss: 0.09710633009672165, tv_loss: 0.03273409605026245\n",
      "iteration 3061, dc_loss: 0.09711021929979324, tv_loss: 0.03272058814764023\n",
      "iteration 3062, dc_loss: 0.09709171205759048, tv_loss: 0.03273126855492592\n",
      "iteration 3063, dc_loss: 0.09709586203098297, tv_loss: 0.032722704112529755\n",
      "iteration 3064, dc_loss: 0.09707370400428772, tv_loss: 0.03273911774158478\n",
      "iteration 3065, dc_loss: 0.09709128737449646, tv_loss: 0.03271346539258957\n",
      "iteration 3066, dc_loss: 0.09705259650945663, tv_loss: 0.032744526863098145\n",
      "iteration 3067, dc_loss: 0.09707824885845184, tv_loss: 0.03271164745092392\n",
      "iteration 3068, dc_loss: 0.09703641384840012, tv_loss: 0.03274475410580635\n",
      "iteration 3069, dc_loss: 0.09706549346446991, tv_loss: 0.03271103650331497\n",
      "iteration 3070, dc_loss: 0.09702075272798538, tv_loss: 0.0327477864921093\n",
      "iteration 3071, dc_loss: 0.09705426543951035, tv_loss: 0.032708924263715744\n",
      "iteration 3072, dc_loss: 0.09700819104909897, tv_loss: 0.032750338315963745\n",
      "iteration 3073, dc_loss: 0.09703443199396133, tv_loss: 0.032718732953071594\n",
      "iteration 3074, dc_loss: 0.09700259566307068, tv_loss: 0.03274788334965706\n",
      "iteration 3075, dc_loss: 0.0970122218132019, tv_loss: 0.03272750601172447\n",
      "iteration 3076, dc_loss: 0.0969899520277977, tv_loss: 0.03273657336831093\n",
      "iteration 3077, dc_loss: 0.09699124842882156, tv_loss: 0.03272690251469612\n",
      "iteration 3078, dc_loss: 0.09698096662759781, tv_loss: 0.03273386508226395\n",
      "iteration 3079, dc_loss: 0.09697166085243225, tv_loss: 0.03273814544081688\n",
      "iteration 3080, dc_loss: 0.09697240591049194, tv_loss: 0.03272933512926102\n",
      "iteration 3081, dc_loss: 0.09696514904499054, tv_loss: 0.032731376588344574\n",
      "iteration 3082, dc_loss: 0.09696824848651886, tv_loss: 0.03272365406155586\n",
      "iteration 3083, dc_loss: 0.09694653749465942, tv_loss: 0.0327407605946064\n",
      "iteration 3084, dc_loss: 0.09695693850517273, tv_loss: 0.03272788226604462\n",
      "iteration 3085, dc_loss: 0.09693636745214462, tv_loss: 0.03274136781692505\n",
      "iteration 3086, dc_loss: 0.09694662690162659, tv_loss: 0.03272390365600586\n",
      "iteration 3087, dc_loss: 0.0969230905175209, tv_loss: 0.03274067863821983\n",
      "iteration 3088, dc_loss: 0.09693073481321335, tv_loss: 0.032725825905799866\n",
      "iteration 3089, dc_loss: 0.09690552204847336, tv_loss: 0.03274175897240639\n",
      "iteration 3090, dc_loss: 0.09691638499498367, tv_loss: 0.03272177278995514\n",
      "iteration 3091, dc_loss: 0.09688840806484222, tv_loss: 0.032742079347372055\n",
      "iteration 3092, dc_loss: 0.09689163416624069, tv_loss: 0.03272877633571625\n",
      "iteration 3093, dc_loss: 0.09687398374080658, tv_loss: 0.03273952007293701\n",
      "iteration 3094, dc_loss: 0.09687354415655136, tv_loss: 0.03273415192961693\n",
      "iteration 3095, dc_loss: 0.09686446934938431, tv_loss: 0.032738734036684036\n",
      "iteration 3096, dc_loss: 0.09684905409812927, tv_loss: 0.03274238109588623\n",
      "iteration 3097, dc_loss: 0.09685121476650238, tv_loss: 0.03273213654756546\n",
      "iteration 3098, dc_loss: 0.09683962911367416, tv_loss: 0.03273594379425049\n",
      "iteration 3099, dc_loss: 0.09683222323656082, tv_loss: 0.032734621316194534\n",
      "iteration 3100, dc_loss: 0.09682758897542953, tv_loss: 0.03273263946175575\n",
      "iteration 3101, dc_loss: 0.09681730717420578, tv_loss: 0.03273647651076317\n",
      "iteration 3102, dc_loss: 0.09682295471429825, tv_loss: 0.03272583335638046\n",
      "iteration 3103, dc_loss: 0.09680332988500595, tv_loss: 0.03274264559149742\n",
      "iteration 3104, dc_loss: 0.09680168330669403, tv_loss: 0.032738469541072845\n",
      "iteration 3105, dc_loss: 0.09678752720355988, tv_loss: 0.03274386003613472\n",
      "iteration 3106, dc_loss: 0.09679676592350006, tv_loss: 0.032727133482694626\n",
      "iteration 3107, dc_loss: 0.09677740931510925, tv_loss: 0.03274301812052727\n",
      "iteration 3108, dc_loss: 0.09678898751735687, tv_loss: 0.032730016857385635\n",
      "iteration 3109, dc_loss: 0.09676291793584824, tv_loss: 0.03275245055556297\n",
      "iteration 3110, dc_loss: 0.09677907079458237, tv_loss: 0.03272722661495209\n",
      "iteration 3111, dc_loss: 0.096748486161232, tv_loss: 0.03274969011545181\n",
      "iteration 3112, dc_loss: 0.09677569568157196, tv_loss: 0.03271932899951935\n",
      "iteration 3113, dc_loss: 0.09673695266246796, tv_loss: 0.03275128826498985\n",
      "iteration 3114, dc_loss: 0.09676530212163925, tv_loss: 0.03271646052598953\n",
      "iteration 3115, dc_loss: 0.09672118723392487, tv_loss: 0.03275980055332184\n",
      "iteration 3116, dc_loss: 0.09675313532352448, tv_loss: 0.032721251249313354\n",
      "iteration 3117, dc_loss: 0.0967024713754654, tv_loss: 0.03276140242815018\n",
      "iteration 3118, dc_loss: 0.09672760218381882, tv_loss: 0.03272268548607826\n",
      "iteration 3119, dc_loss: 0.0966840609908104, tv_loss: 0.03275537118315697\n",
      "iteration 3120, dc_loss: 0.096703439950943, tv_loss: 0.03272318094968796\n",
      "iteration 3121, dc_loss: 0.09666763246059418, tv_loss: 0.03275119513273239\n",
      "iteration 3122, dc_loss: 0.09668122231960297, tv_loss: 0.03273256868124008\n",
      "iteration 3123, dc_loss: 0.09665164351463318, tv_loss: 0.03275011107325554\n",
      "iteration 3124, dc_loss: 0.09666676819324493, tv_loss: 0.032723959535360336\n",
      "iteration 3125, dc_loss: 0.09664157778024673, tv_loss: 0.032745856791734695\n",
      "iteration 3126, dc_loss: 0.09664937853813171, tv_loss: 0.03273134306073189\n",
      "iteration 3127, dc_loss: 0.09663250297307968, tv_loss: 0.03274136036634445\n",
      "iteration 3128, dc_loss: 0.09663610905408859, tv_loss: 0.0327380895614624\n",
      "iteration 3129, dc_loss: 0.09662459045648575, tv_loss: 0.032749682664871216\n",
      "iteration 3130, dc_loss: 0.09662710130214691, tv_loss: 0.032740551978349686\n",
      "iteration 3131, dc_loss: 0.09661504626274109, tv_loss: 0.032746367156505585\n",
      "iteration 3132, dc_loss: 0.0966176763176918, tv_loss: 0.032741788774728775\n",
      "iteration 3133, dc_loss: 0.0966111496090889, tv_loss: 0.03274160623550415\n",
      "iteration 3134, dc_loss: 0.09661484509706497, tv_loss: 0.0327388197183609\n",
      "iteration 3135, dc_loss: 0.09660756587982178, tv_loss: 0.032746754586696625\n",
      "iteration 3136, dc_loss: 0.0965934544801712, tv_loss: 0.03274917230010033\n",
      "iteration 3137, dc_loss: 0.09658537060022354, tv_loss: 0.03274206444621086\n",
      "iteration 3138, dc_loss: 0.09657610207796097, tv_loss: 0.032743003219366074\n",
      "iteration 3139, dc_loss: 0.09656316787004471, tv_loss: 0.03274641931056976\n",
      "iteration 3140, dc_loss: 0.09655184298753738, tv_loss: 0.03274884074926376\n",
      "iteration 3141, dc_loss: 0.09654049575328827, tv_loss: 0.03275398164987564\n",
      "iteration 3142, dc_loss: 0.09654779732227325, tv_loss: 0.032738521695137024\n",
      "iteration 3143, dc_loss: 0.09653639793395996, tv_loss: 0.03275071457028389\n",
      "iteration 3144, dc_loss: 0.09654145687818527, tv_loss: 0.03274576738476753\n",
      "iteration 3145, dc_loss: 0.09652774780988693, tv_loss: 0.0327584408223629\n",
      "iteration 3146, dc_loss: 0.09651994705200195, tv_loss: 0.03275063633918762\n",
      "iteration 3147, dc_loss: 0.09650515764951706, tv_loss: 0.03274912387132645\n",
      "iteration 3148, dc_loss: 0.09648459404706955, tv_loss: 0.032755639404058456\n",
      "iteration 3149, dc_loss: 0.09649735689163208, tv_loss: 0.03273438289761543\n",
      "iteration 3150, dc_loss: 0.09647016227245331, tv_loss: 0.03276435658335686\n",
      "iteration 3151, dc_loss: 0.09650302678346634, tv_loss: 0.032725851982831955\n",
      "iteration 3152, dc_loss: 0.09646318852901459, tv_loss: 0.03276672214269638\n",
      "iteration 3153, dc_loss: 0.09650194644927979, tv_loss: 0.0327284149825573\n",
      "iteration 3154, dc_loss: 0.09644490480422974, tv_loss: 0.03277452290058136\n",
      "iteration 3155, dc_loss: 0.09648559987545013, tv_loss: 0.032722413539886475\n",
      "iteration 3156, dc_loss: 0.09642763435840607, tv_loss: 0.03277162089943886\n",
      "iteration 3157, dc_loss: 0.09646295756101608, tv_loss: 0.03272517770528793\n",
      "iteration 3158, dc_loss: 0.0964093953371048, tv_loss: 0.03277551755309105\n",
      "iteration 3159, dc_loss: 0.09645749628543854, tv_loss: 0.03272641450166702\n",
      "iteration 3160, dc_loss: 0.09640790522098541, tv_loss: 0.03277077525854111\n",
      "iteration 3161, dc_loss: 0.09644453227519989, tv_loss: 0.032735664397478104\n",
      "iteration 3162, dc_loss: 0.0963997170329094, tv_loss: 0.032768115401268005\n",
      "iteration 3163, dc_loss: 0.09641478955745697, tv_loss: 0.03273984044790268\n",
      "iteration 3164, dc_loss: 0.09638267755508423, tv_loss: 0.03276130557060242\n",
      "iteration 3165, dc_loss: 0.09639469534158707, tv_loss: 0.03273714706301689\n",
      "iteration 3166, dc_loss: 0.09637144953012466, tv_loss: 0.03275703266263008\n",
      "iteration 3167, dc_loss: 0.09637151658535004, tv_loss: 0.03275128826498985\n",
      "iteration 3168, dc_loss: 0.09637192636728287, tv_loss: 0.03274703770875931\n",
      "iteration 3169, dc_loss: 0.09635266661643982, tv_loss: 0.03276214003562927\n",
      "iteration 3170, dc_loss: 0.09635692089796066, tv_loss: 0.03274635970592499\n",
      "iteration 3171, dc_loss: 0.09632546454668045, tv_loss: 0.0327652208507061\n",
      "iteration 3172, dc_loss: 0.09633495658636093, tv_loss: 0.03274538740515709\n",
      "iteration 3173, dc_loss: 0.09630466997623444, tv_loss: 0.03276504948735237\n",
      "iteration 3174, dc_loss: 0.09632531553506851, tv_loss: 0.03273707255721092\n",
      "iteration 3175, dc_loss: 0.0962987169623375, tv_loss: 0.032762475311756134\n",
      "iteration 3176, dc_loss: 0.09630556404590607, tv_loss: 0.032745812088251114\n",
      "iteration 3177, dc_loss: 0.09628620743751526, tv_loss: 0.03276323899626732\n",
      "iteration 3178, dc_loss: 0.09628619998693466, tv_loss: 0.032752808183431625\n",
      "iteration 3179, dc_loss: 0.0962790995836258, tv_loss: 0.032750800251960754\n",
      "iteration 3180, dc_loss: 0.0962606742978096, tv_loss: 0.0327657125890255\n",
      "iteration 3181, dc_loss: 0.09626810997724533, tv_loss: 0.032749369740486145\n",
      "iteration 3182, dc_loss: 0.09624987095594406, tv_loss: 0.03275812789797783\n",
      "iteration 3183, dc_loss: 0.09625746309757233, tv_loss: 0.03274999558925629\n",
      "iteration 3184, dc_loss: 0.09624344855546951, tv_loss: 0.03275642544031143\n",
      "iteration 3185, dc_loss: 0.09624139964580536, tv_loss: 0.03275537118315697\n",
      "iteration 3186, dc_loss: 0.09622931480407715, tv_loss: 0.03276240453124046\n",
      "iteration 3187, dc_loss: 0.0962277203798294, tv_loss: 0.032758284360170364\n",
      "iteration 3188, dc_loss: 0.09622717648744583, tv_loss: 0.032749004662036896\n",
      "iteration 3189, dc_loss: 0.09620819240808487, tv_loss: 0.03276507183909416\n",
      "iteration 3190, dc_loss: 0.09622874855995178, tv_loss: 0.032744523137807846\n",
      "iteration 3191, dc_loss: 0.0961989238858223, tv_loss: 0.032777007669210434\n",
      "iteration 3192, dc_loss: 0.0962362065911293, tv_loss: 0.03273801878094673\n",
      "iteration 3193, dc_loss: 0.09619227051734924, tv_loss: 0.0327882394194603\n",
      "iteration 3194, dc_loss: 0.09626054018735886, tv_loss: 0.03272207826375961\n",
      "iteration 3195, dc_loss: 0.09617935121059418, tv_loss: 0.03280870243906975\n",
      "iteration 3196, dc_loss: 0.09626856446266174, tv_loss: 0.03270561620593071\n",
      "iteration 3197, dc_loss: 0.0961589589715004, tv_loss: 0.032808855175971985\n",
      "iteration 3198, dc_loss: 0.09623219072818756, tv_loss: 0.03271295502781868\n",
      "iteration 3199, dc_loss: 0.09612763673067093, tv_loss: 0.032806411385536194\n",
      "iteration 3200, dc_loss: 0.09619340300559998, tv_loss: 0.03272317349910736\n",
      "iteration 3201, dc_loss: 0.09611210972070694, tv_loss: 0.03278603032231331\n",
      "iteration 3202, dc_loss: 0.09612429887056351, tv_loss: 0.03275222331285477\n",
      "iteration 3203, dc_loss: 0.09612853825092316, tv_loss: 0.03273966908454895\n",
      "iteration 3204, dc_loss: 0.09609673917293549, tv_loss: 0.03277887403964996\n",
      "iteration 3205, dc_loss: 0.09611598402261734, tv_loss: 0.03274545073509216\n",
      "iteration 3206, dc_loss: 0.09610223770141602, tv_loss: 0.03274362161755562\n",
      "iteration 3207, dc_loss: 0.09607702493667603, tv_loss: 0.03277318552136421\n",
      "iteration 3208, dc_loss: 0.09609875828027725, tv_loss: 0.03274470940232277\n",
      "iteration 3209, dc_loss: 0.09607909619808197, tv_loss: 0.03274993225932121\n",
      "iteration 3210, dc_loss: 0.0960586816072464, tv_loss: 0.032766975462436676\n",
      "iteration 3211, dc_loss: 0.09607842564582825, tv_loss: 0.032745685428380966\n",
      "iteration 3212, dc_loss: 0.09605792909860611, tv_loss: 0.03275340422987938\n",
      "iteration 3213, dc_loss: 0.09604912996292114, tv_loss: 0.03275912255048752\n",
      "iteration 3214, dc_loss: 0.09606099873781204, tv_loss: 0.03274444118142128\n",
      "iteration 3215, dc_loss: 0.09604000300168991, tv_loss: 0.03275590389966965\n",
      "iteration 3216, dc_loss: 0.09603049606084824, tv_loss: 0.03275984525680542\n",
      "iteration 3217, dc_loss: 0.09603467583656311, tv_loss: 0.03275107964873314\n",
      "iteration 3218, dc_loss: 0.0960235670208931, tv_loss: 0.03275537118315697\n",
      "iteration 3219, dc_loss: 0.09601530432701111, tv_loss: 0.03275882452726364\n",
      "iteration 3220, dc_loss: 0.09601682424545288, tv_loss: 0.032753825187683105\n",
      "iteration 3221, dc_loss: 0.09600655734539032, tv_loss: 0.03275773301720619\n",
      "iteration 3222, dc_loss: 0.09600052982568741, tv_loss: 0.03275913745164871\n",
      "iteration 3223, dc_loss: 0.0959988459944725, tv_loss: 0.032755594700574875\n",
      "iteration 3224, dc_loss: 0.09598793089389801, tv_loss: 0.032760560512542725\n",
      "iteration 3225, dc_loss: 0.09598636627197266, tv_loss: 0.03275628015398979\n",
      "iteration 3226, dc_loss: 0.09598120301961899, tv_loss: 0.0327545702457428\n",
      "iteration 3227, dc_loss: 0.09597135335206985, tv_loss: 0.03275926783680916\n",
      "iteration 3228, dc_loss: 0.09596596658229828, tv_loss: 0.03275832161307335\n",
      "iteration 3229, dc_loss: 0.09596407413482666, tv_loss: 0.03275563567876816\n",
      "iteration 3230, dc_loss: 0.09595495462417603, tv_loss: 0.032758161425590515\n",
      "iteration 3231, dc_loss: 0.0959525778889656, tv_loss: 0.03275521844625473\n",
      "iteration 3232, dc_loss: 0.09594795107841492, tv_loss: 0.03275243565440178\n",
      "iteration 3233, dc_loss: 0.09593790024518967, tv_loss: 0.0327589251101017\n",
      "iteration 3234, dc_loss: 0.09593173116445541, tv_loss: 0.032762739807367325\n",
      "iteration 3235, dc_loss: 0.0959332138299942, tv_loss: 0.03275464102625847\n",
      "iteration 3236, dc_loss: 0.09591834992170334, tv_loss: 0.03276275843381882\n",
      "iteration 3237, dc_loss: 0.09591861814260483, tv_loss: 0.032754600048065186\n",
      "iteration 3238, dc_loss: 0.0959116742014885, tv_loss: 0.03275803104043007\n",
      "iteration 3239, dc_loss: 0.0959058627486229, tv_loss: 0.032763540744781494\n",
      "iteration 3240, dc_loss: 0.0959019884467125, tv_loss: 0.032764822244644165\n",
      "iteration 3241, dc_loss: 0.09589583426713943, tv_loss: 0.032763343304395676\n",
      "iteration 3242, dc_loss: 0.09588488191366196, tv_loss: 0.03276549652218819\n",
      "iteration 3243, dc_loss: 0.09588762372732162, tv_loss: 0.03275502473115921\n",
      "iteration 3244, dc_loss: 0.09587857127189636, tv_loss: 0.032758865505456924\n",
      "iteration 3245, dc_loss: 0.09586973488330841, tv_loss: 0.032764732837677\n",
      "iteration 3246, dc_loss: 0.09586921334266663, tv_loss: 0.032762277871370316\n",
      "iteration 3247, dc_loss: 0.09586119651794434, tv_loss: 0.03276682272553444\n",
      "iteration 3248, dc_loss: 0.09585340321063995, tv_loss: 0.03276621922850609\n",
      "iteration 3249, dc_loss: 0.0958540290594101, tv_loss: 0.032758474349975586\n",
      "iteration 3250, dc_loss: 0.09584250301122665, tv_loss: 0.03276233375072479\n",
      "iteration 3251, dc_loss: 0.09583792090415955, tv_loss: 0.03276034817099571\n",
      "iteration 3252, dc_loss: 0.09583481401205063, tv_loss: 0.03276076167821884\n",
      "iteration 3253, dc_loss: 0.09583046287298203, tv_loss: 0.032763946801424026\n",
      "iteration 3254, dc_loss: 0.09582027792930603, tv_loss: 0.03276872634887695\n",
      "iteration 3255, dc_loss: 0.0958169475197792, tv_loss: 0.03276482969522476\n",
      "iteration 3256, dc_loss: 0.09581020474433899, tv_loss: 0.03276452049612999\n",
      "iteration 3257, dc_loss: 0.09580783545970917, tv_loss: 0.032759103924036026\n",
      "iteration 3258, dc_loss: 0.09579914063215256, tv_loss: 0.032765038311481476\n",
      "iteration 3259, dc_loss: 0.09579310566186905, tv_loss: 0.032766055315732956\n",
      "iteration 3260, dc_loss: 0.09578977525234222, tv_loss: 0.032762158662080765\n",
      "iteration 3261, dc_loss: 0.09578444063663483, tv_loss: 0.03275866061449051\n",
      "iteration 3262, dc_loss: 0.09577825665473938, tv_loss: 0.0327608659863472\n",
      "iteration 3263, dc_loss: 0.09576990455389023, tv_loss: 0.032766081392765045\n",
      "iteration 3264, dc_loss: 0.09576836973428726, tv_loss: 0.03276108577847481\n",
      "iteration 3265, dc_loss: 0.09575942903757095, tv_loss: 0.03276463598012924\n",
      "iteration 3266, dc_loss: 0.09575783461332321, tv_loss: 0.032761264592409134\n",
      "iteration 3267, dc_loss: 0.0957496240735054, tv_loss: 0.032763514667749405\n",
      "iteration 3268, dc_loss: 0.09574428200721741, tv_loss: 0.03276429697871208\n",
      "iteration 3269, dc_loss: 0.09573777765035629, tv_loss: 0.032765310257673264\n",
      "iteration 3270, dc_loss: 0.09573478251695633, tv_loss: 0.032761428505182266\n",
      "iteration 3271, dc_loss: 0.0957266315817833, tv_loss: 0.032765112817287445\n",
      "iteration 3272, dc_loss: 0.0957237258553505, tv_loss: 0.03276300057768822\n",
      "iteration 3273, dc_loss: 0.09571487456560135, tv_loss: 0.03276735544204712\n",
      "iteration 3274, dc_loss: 0.09571180492639542, tv_loss: 0.03276483342051506\n",
      "iteration 3275, dc_loss: 0.09570272266864777, tv_loss: 0.03276924043893814\n",
      "iteration 3276, dc_loss: 0.0957019031047821, tv_loss: 0.03276442736387253\n",
      "iteration 3277, dc_loss: 0.09569335728883743, tv_loss: 0.03276825696229935\n",
      "iteration 3278, dc_loss: 0.09568764269351959, tv_loss: 0.0327698215842247\n",
      "iteration 3279, dc_loss: 0.09568551927804947, tv_loss: 0.03277100622653961\n",
      "iteration 3280, dc_loss: 0.09567562490701675, tv_loss: 0.03277967497706413\n",
      "iteration 3281, dc_loss: 0.09567304700613022, tv_loss: 0.03276875242590904\n",
      "iteration 3282, dc_loss: 0.09566596895456314, tv_loss: 0.03276580199599266\n",
      "iteration 3283, dc_loss: 0.09565902501344681, tv_loss: 0.03276824951171875\n",
      "iteration 3284, dc_loss: 0.09565587341785431, tv_loss: 0.03276824206113815\n",
      "iteration 3285, dc_loss: 0.09564916789531708, tv_loss: 0.03276984021067619\n",
      "iteration 3286, dc_loss: 0.09564291685819626, tv_loss: 0.03276819735765457\n",
      "iteration 3287, dc_loss: 0.09564173966646194, tv_loss: 0.0327632836997509\n",
      "iteration 3288, dc_loss: 0.09563201665878296, tv_loss: 0.032770540565252304\n",
      "iteration 3289, dc_loss: 0.09562569111585617, tv_loss: 0.03277553990483284\n",
      "iteration 3290, dc_loss: 0.09562082588672638, tv_loss: 0.03277929499745369\n",
      "iteration 3291, dc_loss: 0.09561800211668015, tv_loss: 0.03277169167995453\n",
      "iteration 3292, dc_loss: 0.09561066329479218, tv_loss: 0.032769132405519485\n",
      "iteration 3293, dc_loss: 0.09560682624578476, tv_loss: 0.03276719152927399\n",
      "iteration 3294, dc_loss: 0.09559591114521027, tv_loss: 0.0327761173248291\n",
      "iteration 3295, dc_loss: 0.09559255093336105, tv_loss: 0.032777752727270126\n",
      "iteration 3296, dc_loss: 0.09558956325054169, tv_loss: 0.0327734611928463\n",
      "iteration 3297, dc_loss: 0.09558527916669846, tv_loss: 0.03276778757572174\n",
      "iteration 3298, dc_loss: 0.09557919204235077, tv_loss: 0.032766588032245636\n",
      "iteration 3299, dc_loss: 0.09556925296783447, tv_loss: 0.03277178108692169\n",
      "iteration 3300, dc_loss: 0.09556274861097336, tv_loss: 0.03277324512600899\n",
      "iteration 3301, dc_loss: 0.09556469321250916, tv_loss: 0.032768476754426956\n",
      "iteration 3302, dc_loss: 0.09555132687091827, tv_loss: 0.03277747705578804\n",
      "iteration 3303, dc_loss: 0.09555098414421082, tv_loss: 0.03277292475104332\n",
      "iteration 3304, dc_loss: 0.09554503113031387, tv_loss: 0.03277306631207466\n",
      "iteration 3305, dc_loss: 0.09553655982017517, tv_loss: 0.03277382627129555\n",
      "iteration 3306, dc_loss: 0.09553327411413193, tv_loss: 0.032770510762929916\n",
      "iteration 3307, dc_loss: 0.09552790224552155, tv_loss: 0.03276921808719635\n",
      "iteration 3308, dc_loss: 0.09551927447319031, tv_loss: 0.032775621861219406\n",
      "iteration 3309, dc_loss: 0.09551773220300674, tv_loss: 0.032772231847047806\n",
      "iteration 3310, dc_loss: 0.09550916403532028, tv_loss: 0.032773733139038086\n",
      "iteration 3311, dc_loss: 0.09550344198942184, tv_loss: 0.0327724888920784\n",
      "iteration 3312, dc_loss: 0.09550345689058304, tv_loss: 0.03276651352643967\n",
      "iteration 3313, dc_loss: 0.09549263119697571, tv_loss: 0.032776594161987305\n",
      "iteration 3314, dc_loss: 0.09548705816268921, tv_loss: 0.03277747705578804\n",
      "iteration 3315, dc_loss: 0.09548342227935791, tv_loss: 0.0327720008790493\n",
      "iteration 3316, dc_loss: 0.09547560662031174, tv_loss: 0.03277299180626869\n",
      "iteration 3317, dc_loss: 0.09547194093465805, tv_loss: 0.03277144208550453\n",
      "iteration 3318, dc_loss: 0.09546788036823273, tv_loss: 0.03276890143752098\n",
      "iteration 3319, dc_loss: 0.09545588493347168, tv_loss: 0.032776396721601486\n",
      "iteration 3320, dc_loss: 0.09545700997114182, tv_loss: 0.03277473896741867\n",
      "iteration 3321, dc_loss: 0.09544746577739716, tv_loss: 0.032777152955532074\n",
      "iteration 3322, dc_loss: 0.0954444631934166, tv_loss: 0.032773490995168686\n",
      "iteration 3323, dc_loss: 0.09544017165899277, tv_loss: 0.032771386206150055\n",
      "iteration 3324, dc_loss: 0.09542970359325409, tv_loss: 0.03277428448200226\n",
      "iteration 3325, dc_loss: 0.0954255759716034, tv_loss: 0.03277628496289253\n",
      "iteration 3326, dc_loss: 0.09542398899793625, tv_loss: 0.032778941094875336\n",
      "iteration 3327, dc_loss: 0.09541486948728561, tv_loss: 0.032783277332782745\n",
      "iteration 3328, dc_loss: 0.09541043639183044, tv_loss: 0.03278142958879471\n",
      "iteration 3329, dc_loss: 0.0954042598605156, tv_loss: 0.03277648240327835\n",
      "iteration 3330, dc_loss: 0.0953977033495903, tv_loss: 0.03277549520134926\n",
      "iteration 3331, dc_loss: 0.09539802372455597, tv_loss: 0.03276835009455681\n",
      "iteration 3332, dc_loss: 0.09538476169109344, tv_loss: 0.03277742862701416\n",
      "iteration 3333, dc_loss: 0.09538104385137558, tv_loss: 0.032780978828668594\n",
      "iteration 3334, dc_loss: 0.09537860751152039, tv_loss: 0.032775867730379105\n",
      "iteration 3335, dc_loss: 0.09536788612604141, tv_loss: 0.032780904322862625\n",
      "iteration 3336, dc_loss: 0.09536900371313095, tv_loss: 0.0327715128660202\n",
      "iteration 3337, dc_loss: 0.09536116570234299, tv_loss: 0.03277328237891197\n",
      "iteration 3338, dc_loss: 0.09535219520330429, tv_loss: 0.03277762234210968\n",
      "iteration 3339, dc_loss: 0.09534738212823868, tv_loss: 0.03277698904275894\n",
      "iteration 3340, dc_loss: 0.09534665197134018, tv_loss: 0.0327734500169754\n",
      "iteration 3341, dc_loss: 0.09533421695232391, tv_loss: 0.03278306871652603\n",
      "iteration 3342, dc_loss: 0.09533567726612091, tv_loss: 0.032774269580841064\n",
      "iteration 3343, dc_loss: 0.09532305598258972, tv_loss: 0.03278045356273651\n",
      "iteration 3344, dc_loss: 0.09532555192708969, tv_loss: 0.03277387097477913\n",
      "iteration 3345, dc_loss: 0.09531453251838684, tv_loss: 0.03277956694364548\n",
      "iteration 3346, dc_loss: 0.09530842304229736, tv_loss: 0.03278529644012451\n",
      "iteration 3347, dc_loss: 0.09530596435070038, tv_loss: 0.032782915979623795\n",
      "iteration 3348, dc_loss: 0.09529674798250198, tv_loss: 0.03278697282075882\n",
      "iteration 3349, dc_loss: 0.09529414772987366, tv_loss: 0.03278174623847008\n",
      "iteration 3350, dc_loss: 0.09528864175081253, tv_loss: 0.03277694061398506\n",
      "iteration 3351, dc_loss: 0.09528054296970367, tv_loss: 0.03277696669101715\n",
      "iteration 3352, dc_loss: 0.09528332203626633, tv_loss: 0.032770123332738876\n",
      "iteration 3353, dc_loss: 0.09526476263999939, tv_loss: 0.03278440609574318\n",
      "iteration 3354, dc_loss: 0.0952644795179367, tv_loss: 0.032781898975372314\n",
      "iteration 3355, dc_loss: 0.09526266157627106, tv_loss: 0.03278467431664467\n",
      "iteration 3356, dc_loss: 0.09525199234485626, tv_loss: 0.03279126435518265\n",
      "iteration 3357, dc_loss: 0.09525241702795029, tv_loss: 0.032778915017843246\n",
      "iteration 3358, dc_loss: 0.09524146467447281, tv_loss: 0.03278337046504021\n",
      "iteration 3359, dc_loss: 0.09523265808820724, tv_loss: 0.03278397396206856\n",
      "iteration 3360, dc_loss: 0.09523626416921616, tv_loss: 0.03277542442083359\n",
      "iteration 3361, dc_loss: 0.0952259749174118, tv_loss: 0.032784488052129745\n",
      "iteration 3362, dc_loss: 0.09522350877523422, tv_loss: 0.03278280422091484\n",
      "iteration 3363, dc_loss: 0.09521294385194778, tv_loss: 0.032785020768642426\n",
      "iteration 3364, dc_loss: 0.09521057456731796, tv_loss: 0.03278317302465439\n",
      "iteration 3365, dc_loss: 0.09520338475704193, tv_loss: 0.03278141841292381\n",
      "iteration 3366, dc_loss: 0.09519720077514648, tv_loss: 0.032781876623630524\n",
      "iteration 3367, dc_loss: 0.09519461542367935, tv_loss: 0.03278224915266037\n",
      "iteration 3368, dc_loss: 0.095186248421669, tv_loss: 0.03278438374400139\n",
      "iteration 3369, dc_loss: 0.09518269449472427, tv_loss: 0.03278400003910065\n",
      "iteration 3370, dc_loss: 0.09517888724803925, tv_loss: 0.032778408378362656\n",
      "iteration 3371, dc_loss: 0.09516770392656326, tv_loss: 0.03278360143303871\n",
      "iteration 3372, dc_loss: 0.0951661542057991, tv_loss: 0.032779715955257416\n",
      "iteration 3373, dc_loss: 0.09515716880559921, tv_loss: 0.032783739268779755\n",
      "iteration 3374, dc_loss: 0.0951547771692276, tv_loss: 0.032780971378088\n",
      "iteration 3375, dc_loss: 0.0951460525393486, tv_loss: 0.032788656651973724\n",
      "iteration 3376, dc_loss: 0.09514588117599487, tv_loss: 0.03278283402323723\n",
      "iteration 3377, dc_loss: 0.09513745456933975, tv_loss: 0.03278129920363426\n",
      "iteration 3378, dc_loss: 0.09513109177350998, tv_loss: 0.03278326615691185\n",
      "iteration 3379, dc_loss: 0.09512653946876526, tv_loss: 0.03278505429625511\n",
      "iteration 3380, dc_loss: 0.09511748701334, tv_loss: 0.032788943499326706\n",
      "iteration 3381, dc_loss: 0.09511523693799973, tv_loss: 0.03278403729200363\n",
      "iteration 3382, dc_loss: 0.09510764479637146, tv_loss: 0.03278667852282524\n",
      "iteration 3383, dc_loss: 0.0951070487499237, tv_loss: 0.03278115764260292\n",
      "iteration 3384, dc_loss: 0.09509729593992233, tv_loss: 0.03278735280036926\n",
      "iteration 3385, dc_loss: 0.09509064257144928, tv_loss: 0.032787855714559555\n",
      "iteration 3386, dc_loss: 0.09508838504552841, tv_loss: 0.032784465700387955\n",
      "iteration 3387, dc_loss: 0.09508088231086731, tv_loss: 0.03278718516230583\n",
      "iteration 3388, dc_loss: 0.09508052468299866, tv_loss: 0.0327824130654335\n",
      "iteration 3389, dc_loss: 0.09506527334451675, tv_loss: 0.03279723599553108\n",
      "iteration 3390, dc_loss: 0.0950649082660675, tv_loss: 0.03279218450188637\n",
      "iteration 3391, dc_loss: 0.09506040066480637, tv_loss: 0.03278930485248566\n",
      "iteration 3392, dc_loss: 0.09505396336317062, tv_loss: 0.03278789296746254\n",
      "iteration 3393, dc_loss: 0.095045305788517, tv_loss: 0.03278876096010208\n",
      "iteration 3394, dc_loss: 0.09504301846027374, tv_loss: 0.032784897834062576\n",
      "iteration 3395, dc_loss: 0.09503374993801117, tv_loss: 0.03279230371117592\n",
      "iteration 3396, dc_loss: 0.0950348749756813, tv_loss: 0.03278319165110588\n",
      "iteration 3397, dc_loss: 0.09502673894166946, tv_loss: 0.032788097858428955\n",
      "iteration 3398, dc_loss: 0.09502160549163818, tv_loss: 0.03279315307736397\n",
      "iteration 3399, dc_loss: 0.09501441568136215, tv_loss: 0.03279319033026695\n",
      "iteration 3400, dc_loss: 0.09500613808631897, tv_loss: 0.03279366344213486\n",
      "iteration 3401, dc_loss: 0.09500426054000854, tv_loss: 0.03278703987598419\n",
      "iteration 3402, dc_loss: 0.09499680995941162, tv_loss: 0.032786861062049866\n",
      "iteration 3403, dc_loss: 0.09499378502368927, tv_loss: 0.0327843502163887\n",
      "iteration 3404, dc_loss: 0.09498750418424606, tv_loss: 0.032786641269922256\n",
      "iteration 3405, dc_loss: 0.0949755609035492, tv_loss: 0.03279615938663483\n",
      "iteration 3406, dc_loss: 0.0949811264872551, tv_loss: 0.032787714153528214\n",
      "iteration 3407, dc_loss: 0.09497092664241791, tv_loss: 0.032790981233119965\n",
      "iteration 3408, dc_loss: 0.09496064484119415, tv_loss: 0.03279558941721916\n",
      "iteration 3409, dc_loss: 0.09496050328016281, tv_loss: 0.03279094025492668\n",
      "iteration 3410, dc_loss: 0.09495429694652557, tv_loss: 0.0327942930161953\n",
      "iteration 3411, dc_loss: 0.0949445366859436, tv_loss: 0.03279836103320122\n",
      "iteration 3412, dc_loss: 0.0949457511305809, tv_loss: 0.03278589993715286\n",
      "iteration 3413, dc_loss: 0.09493660926818848, tv_loss: 0.032788462936878204\n",
      "iteration 3414, dc_loss: 0.09492424130439758, tv_loss: 0.0327981673181057\n",
      "iteration 3415, dc_loss: 0.09493107348680496, tv_loss: 0.032783668488264084\n",
      "iteration 3416, dc_loss: 0.09491921961307526, tv_loss: 0.03279272839426994\n",
      "iteration 3417, dc_loss: 0.09491882473230362, tv_loss: 0.032789312303066254\n",
      "iteration 3418, dc_loss: 0.09490356594324112, tv_loss: 0.03279691934585571\n",
      "iteration 3419, dc_loss: 0.09490680694580078, tv_loss: 0.03278648480772972\n",
      "iteration 3420, dc_loss: 0.09489399194717407, tv_loss: 0.0327945239841938\n",
      "iteration 3421, dc_loss: 0.09489171206951141, tv_loss: 0.03279172256588936\n",
      "iteration 3422, dc_loss: 0.09489132463932037, tv_loss: 0.03278796747326851\n",
      "iteration 3423, dc_loss: 0.09487506002187729, tv_loss: 0.03279874846339226\n",
      "iteration 3424, dc_loss: 0.09487948566675186, tv_loss: 0.0327896773815155\n",
      "iteration 3425, dc_loss: 0.09486952424049377, tv_loss: 0.03279493376612663\n",
      "iteration 3426, dc_loss: 0.09486164152622223, tv_loss: 0.0327938087284565\n",
      "iteration 3427, dc_loss: 0.09486088901758194, tv_loss: 0.03279005363583565\n",
      "iteration 3428, dc_loss: 0.09484890848398209, tv_loss: 0.032798219472169876\n",
      "iteration 3429, dc_loss: 0.09485343843698502, tv_loss: 0.03278695419430733\n",
      "iteration 3430, dc_loss: 0.09484048932790756, tv_loss: 0.032797206193208694\n",
      "iteration 3431, dc_loss: 0.09483424574136734, tv_loss: 0.03280406445264816\n",
      "iteration 3432, dc_loss: 0.09483174979686737, tv_loss: 0.03280365467071533\n",
      "iteration 3433, dc_loss: 0.09482257813215256, tv_loss: 0.03279966115951538\n",
      "iteration 3434, dc_loss: 0.09482001513242722, tv_loss: 0.03279345855116844\n",
      "iteration 3435, dc_loss: 0.0948190838098526, tv_loss: 0.03278689831495285\n",
      "iteration 3436, dc_loss: 0.0948035791516304, tv_loss: 0.03279726207256317\n",
      "iteration 3437, dc_loss: 0.09480804204940796, tv_loss: 0.03279067575931549\n",
      "iteration 3438, dc_loss: 0.09479063749313354, tv_loss: 0.03280763700604439\n",
      "iteration 3439, dc_loss: 0.09479999542236328, tv_loss: 0.03279007226228714\n",
      "iteration 3440, dc_loss: 0.09478050470352173, tv_loss: 0.03280215710401535\n",
      "iteration 3441, dc_loss: 0.09478592127561569, tv_loss: 0.03279462084174156\n",
      "iteration 3442, dc_loss: 0.09477414935827255, tv_loss: 0.0328017883002758\n",
      "iteration 3443, dc_loss: 0.09476640075445175, tv_loss: 0.03280019387602806\n",
      "iteration 3444, dc_loss: 0.09476545453071594, tv_loss: 0.032795246690511703\n",
      "iteration 3445, dc_loss: 0.09475605189800262, tv_loss: 0.03280556946992874\n",
      "iteration 3446, dc_loss: 0.094753697514534, tv_loss: 0.03280443698167801\n",
      "iteration 3447, dc_loss: 0.0947466641664505, tv_loss: 0.03280274197459221\n",
      "iteration 3448, dc_loss: 0.09474431723356247, tv_loss: 0.03279637172818184\n",
      "iteration 3449, dc_loss: 0.09473098069429398, tv_loss: 0.03280157595872879\n",
      "iteration 3450, dc_loss: 0.09473418444395065, tv_loss: 0.03279516100883484\n",
      "iteration 3451, dc_loss: 0.09472212940454483, tv_loss: 0.03280170261859894\n",
      "iteration 3452, dc_loss: 0.09472028911113739, tv_loss: 0.03279852867126465\n",
      "iteration 3453, dc_loss: 0.09471271187067032, tv_loss: 0.03280027210712433\n",
      "iteration 3454, dc_loss: 0.09471001476049423, tv_loss: 0.032797910273075104\n",
      "iteration 3455, dc_loss: 0.09470630437135696, tv_loss: 0.03279818594455719\n",
      "iteration 3456, dc_loss: 0.09468823671340942, tv_loss: 0.03281014785170555\n",
      "iteration 3457, dc_loss: 0.09469819068908691, tv_loss: 0.032796140760183334\n",
      "iteration 3458, dc_loss: 0.09468544274568558, tv_loss: 0.03280264884233475\n",
      "iteration 3459, dc_loss: 0.09467875957489014, tv_loss: 0.03279995918273926\n",
      "iteration 3460, dc_loss: 0.09467557072639465, tv_loss: 0.03279552981257439\n",
      "iteration 3461, dc_loss: 0.09466764330863953, tv_loss: 0.03279650956392288\n",
      "iteration 3462, dc_loss: 0.09467007219791412, tv_loss: 0.03279092535376549\n",
      "iteration 3463, dc_loss: 0.0946495309472084, tv_loss: 0.03280854970216751\n",
      "iteration 3464, dc_loss: 0.09466081857681274, tv_loss: 0.032797202467918396\n",
      "iteration 3465, dc_loss: 0.09463698416948318, tv_loss: 0.03281525522470474\n",
      "iteration 3466, dc_loss: 0.09464751183986664, tv_loss: 0.03279529884457588\n",
      "iteration 3467, dc_loss: 0.09463799744844437, tv_loss: 0.03280064836144447\n",
      "iteration 3468, dc_loss: 0.09462660551071167, tv_loss: 0.03280598670244217\n",
      "iteration 3469, dc_loss: 0.09462478011846542, tv_loss: 0.03280413895845413\n",
      "iteration 3470, dc_loss: 0.09461760520935059, tv_loss: 0.032804638147354126\n",
      "iteration 3471, dc_loss: 0.09461934864521027, tv_loss: 0.032796379178762436\n",
      "iteration 3472, dc_loss: 0.09460342675447464, tv_loss: 0.03280721977353096\n",
      "iteration 3473, dc_loss: 0.09460059553384781, tv_loss: 0.03280819207429886\n",
      "iteration 3474, dc_loss: 0.09459832310676575, tv_loss: 0.03280946612358093\n",
      "iteration 3475, dc_loss: 0.0945885181427002, tv_loss: 0.03280918300151825\n",
      "iteration 3476, dc_loss: 0.09458857774734497, tv_loss: 0.03280077129602432\n",
      "iteration 3477, dc_loss: 0.09458056092262268, tv_loss: 0.03280309960246086\n",
      "iteration 3478, dc_loss: 0.0945780947804451, tv_loss: 0.032801706343889236\n",
      "iteration 3479, dc_loss: 0.09457051753997803, tv_loss: 0.03280740603804588\n",
      "iteration 3480, dc_loss: 0.09457020461559296, tv_loss: 0.0328030027449131\n",
      "iteration 3481, dc_loss: 0.09456034749746323, tv_loss: 0.032805219292640686\n",
      "iteration 3482, dc_loss: 0.09456415474414825, tv_loss: 0.03279857710003853\n",
      "iteration 3483, dc_loss: 0.09454870969057083, tv_loss: 0.03281009942293167\n",
      "iteration 3484, dc_loss: 0.09456625580787659, tv_loss: 0.032792460173368454\n",
      "iteration 3485, dc_loss: 0.09454309940338135, tv_loss: 0.032817013561725616\n",
      "iteration 3486, dc_loss: 0.09456213563680649, tv_loss: 0.03279507905244827\n",
      "iteration 3487, dc_loss: 0.09453655034303665, tv_loss: 0.03282106667757034\n",
      "iteration 3488, dc_loss: 0.09454847872257233, tv_loss: 0.032805491238832474\n",
      "iteration 3489, dc_loss: 0.09452879428863525, tv_loss: 0.03281198441982269\n",
      "iteration 3490, dc_loss: 0.0945250391960144, tv_loss: 0.03280005231499672\n",
      "iteration 3491, dc_loss: 0.09450819343328476, tv_loss: 0.032807834446430206\n",
      "iteration 3492, dc_loss: 0.09449844062328339, tv_loss: 0.03281141072511673\n",
      "iteration 3493, dc_loss: 0.09449950605630875, tv_loss: 0.03280575945973396\n",
      "iteration 3494, dc_loss: 0.09448070079088211, tv_loss: 0.03281892091035843\n",
      "iteration 3495, dc_loss: 0.09448607265949249, tv_loss: 0.03280680626630783\n",
      "iteration 3496, dc_loss: 0.09447167813777924, tv_loss: 0.03280922397971153\n",
      "iteration 3497, dc_loss: 0.09447243064641953, tv_loss: 0.03280461207032204\n",
      "iteration 3498, dc_loss: 0.09446866810321808, tv_loss: 0.03281162306666374\n",
      "iteration 3499, dc_loss: 0.09446297585964203, tv_loss: 0.03281733766198158\n",
      "iteration 3500, dc_loss: 0.09445319324731827, tv_loss: 0.03281497582793236\n",
      "iteration 3501, dc_loss: 0.094462089240551, tv_loss: 0.0328054204583168\n",
      "iteration 3502, dc_loss: 0.09443473070859909, tv_loss: 0.032820310443639755\n",
      "iteration 3503, dc_loss: 0.09444545954465866, tv_loss: 0.03280109167098999\n",
      "iteration 3504, dc_loss: 0.09442761540412903, tv_loss: 0.03282123804092407\n",
      "iteration 3505, dc_loss: 0.09443048387765884, tv_loss: 0.03280915319919586\n",
      "iteration 3506, dc_loss: 0.09441865980625153, tv_loss: 0.03281142935156822\n",
      "iteration 3507, dc_loss: 0.0944131538271904, tv_loss: 0.03281186893582344\n",
      "iteration 3508, dc_loss: 0.09440484642982483, tv_loss: 0.032820019870996475\n",
      "iteration 3509, dc_loss: 0.09440252184867859, tv_loss: 0.032813575118780136\n",
      "iteration 3510, dc_loss: 0.09440062195062637, tv_loss: 0.03281246870756149\n",
      "iteration 3511, dc_loss: 0.09438559412956238, tv_loss: 0.03281938284635544\n",
      "iteration 3512, dc_loss: 0.09439268708229065, tv_loss: 0.03280872851610184\n",
      "iteration 3513, dc_loss: 0.09437287598848343, tv_loss: 0.03282179683446884\n",
      "iteration 3514, dc_loss: 0.09437832236289978, tv_loss: 0.032816458493471146\n",
      "iteration 3515, dc_loss: 0.09436869621276855, tv_loss: 0.03281155601143837\n",
      "iteration 3516, dc_loss: 0.0943676009774208, tv_loss: 0.032812584191560745\n",
      "iteration 3517, dc_loss: 0.09435431659221649, tv_loss: 0.03282364085316658\n",
      "iteration 3518, dc_loss: 0.09435459226369858, tv_loss: 0.032812293618917465\n",
      "iteration 3519, dc_loss: 0.09434492141008377, tv_loss: 0.03281547501683235\n",
      "iteration 3520, dc_loss: 0.09434506297111511, tv_loss: 0.03281891345977783\n",
      "iteration 3521, dc_loss: 0.09434302151203156, tv_loss: 0.03281005471944809\n",
      "iteration 3522, dc_loss: 0.09433727711439133, tv_loss: 0.03281146287918091\n",
      "iteration 3523, dc_loss: 0.09431634843349457, tv_loss: 0.03283088281750679\n",
      "iteration 3524, dc_loss: 0.09433919191360474, tv_loss: 0.032808247953653336\n",
      "iteration 3525, dc_loss: 0.09431653469800949, tv_loss: 0.03282255306839943\n",
      "iteration 3526, dc_loss: 0.09433069825172424, tv_loss: 0.03280514106154442\n",
      "iteration 3527, dc_loss: 0.09430738538503647, tv_loss: 0.03282717242836952\n",
      "iteration 3528, dc_loss: 0.0943167582154274, tv_loss: 0.0328158475458622\n",
      "iteration 3529, dc_loss: 0.09429267048835754, tv_loss: 0.03283565118908882\n",
      "iteration 3530, dc_loss: 0.09431995451450348, tv_loss: 0.03279632702469826\n",
      "iteration 3531, dc_loss: 0.09428062289953232, tv_loss: 0.03283294290304184\n",
      "iteration 3532, dc_loss: 0.09430912882089615, tv_loss: 0.03280704841017723\n",
      "iteration 3533, dc_loss: 0.0942763015627861, tv_loss: 0.03283094987273216\n",
      "iteration 3534, dc_loss: 0.0942973867058754, tv_loss: 0.03280067443847656\n",
      "iteration 3535, dc_loss: 0.09425888955593109, tv_loss: 0.032842572778463364\n",
      "iteration 3536, dc_loss: 0.09430031478404999, tv_loss: 0.03279521316289902\n",
      "iteration 3537, dc_loss: 0.09425660222768784, tv_loss: 0.032838113605976105\n",
      "iteration 3538, dc_loss: 0.09428085386753082, tv_loss: 0.032805588096380234\n",
      "iteration 3539, dc_loss: 0.09424109756946564, tv_loss: 0.03283562511205673\n",
      "iteration 3540, dc_loss: 0.09426510334014893, tv_loss: 0.03279551491141319\n",
      "iteration 3541, dc_loss: 0.09422667324542999, tv_loss: 0.03282499313354492\n",
      "iteration 3542, dc_loss: 0.09423358738422394, tv_loss: 0.032814040780067444\n",
      "iteration 3543, dc_loss: 0.09422412514686584, tv_loss: 0.03281741216778755\n",
      "iteration 3544, dc_loss: 0.09421198815107346, tv_loss: 0.03282463550567627\n",
      "iteration 3545, dc_loss: 0.09423694759607315, tv_loss: 0.03279966861009598\n",
      "iteration 3546, dc_loss: 0.09419699758291245, tv_loss: 0.03283784165978432\n",
      "iteration 3547, dc_loss: 0.094220831990242, tv_loss: 0.03279871866106987\n",
      "iteration 3548, dc_loss: 0.09418243169784546, tv_loss: 0.032831016927957535\n",
      "iteration 3549, dc_loss: 0.09419650584459305, tv_loss: 0.03280973806977272\n",
      "iteration 3550, dc_loss: 0.09417158365249634, tv_loss: 0.03282647207379341\n",
      "iteration 3551, dc_loss: 0.0941748172044754, tv_loss: 0.03281824290752411\n",
      "iteration 3552, dc_loss: 0.09416857361793518, tv_loss: 0.03282181918621063\n",
      "iteration 3553, dc_loss: 0.09415794909000397, tv_loss: 0.03282279893755913\n",
      "iteration 3554, dc_loss: 0.09416201710700989, tv_loss: 0.0328105166554451\n",
      "iteration 3555, dc_loss: 0.09414467960596085, tv_loss: 0.03282657638192177\n",
      "iteration 3556, dc_loss: 0.0941452905535698, tv_loss: 0.0328206904232502\n",
      "iteration 3557, dc_loss: 0.09413093328475952, tv_loss: 0.0328272320330143\n",
      "iteration 3558, dc_loss: 0.0941406711935997, tv_loss: 0.032812610268592834\n",
      "iteration 3559, dc_loss: 0.09412304311990738, tv_loss: 0.03282594680786133\n",
      "iteration 3560, dc_loss: 0.09412683546543121, tv_loss: 0.03281885385513306\n",
      "iteration 3561, dc_loss: 0.09411612153053284, tv_loss: 0.032821789383888245\n",
      "iteration 3562, dc_loss: 0.09411151707172394, tv_loss: 0.03281961753964424\n",
      "iteration 3563, dc_loss: 0.09411326795816422, tv_loss: 0.0328141450881958\n",
      "iteration 3564, dc_loss: 0.09409631788730621, tv_loss: 0.03283023461699486\n",
      "iteration 3565, dc_loss: 0.09410227835178375, tv_loss: 0.032816510647535324\n",
      "iteration 3566, dc_loss: 0.09409049898386002, tv_loss: 0.03282257914543152\n",
      "iteration 3567, dc_loss: 0.09409218281507492, tv_loss: 0.03281796723604202\n",
      "iteration 3568, dc_loss: 0.0940755307674408, tv_loss: 0.03282763808965683\n",
      "iteration 3569, dc_loss: 0.09407797455787659, tv_loss: 0.032819557934999466\n",
      "iteration 3570, dc_loss: 0.09406950324773788, tv_loss: 0.032821979373693466\n",
      "iteration 3571, dc_loss: 0.09405577182769775, tv_loss: 0.03283034637570381\n",
      "iteration 3572, dc_loss: 0.09406633675098419, tv_loss: 0.032811544835567474\n",
      "iteration 3573, dc_loss: 0.09404806792736053, tv_loss: 0.03282156586647034\n",
      "iteration 3574, dc_loss: 0.09403891116380692, tv_loss: 0.032828278839588165\n",
      "iteration 3575, dc_loss: 0.09403809905052185, tv_loss: 0.032822202891111374\n",
      "iteration 3576, dc_loss: 0.0940316915512085, tv_loss: 0.03282257914543152\n",
      "iteration 3577, dc_loss: 0.09402506053447723, tv_loss: 0.03282258287072182\n",
      "iteration 3578, dc_loss: 0.09401808679103851, tv_loss: 0.0328219011425972\n",
      "iteration 3579, dc_loss: 0.09401729702949524, tv_loss: 0.032819587737321854\n",
      "iteration 3580, dc_loss: 0.09400191903114319, tv_loss: 0.03282797336578369\n",
      "iteration 3581, dc_loss: 0.09401151537895203, tv_loss: 0.03280958533287048\n",
      "iteration 3582, dc_loss: 0.09399136155843735, tv_loss: 0.032825104892253876\n",
      "iteration 3583, dc_loss: 0.09399071335792542, tv_loss: 0.03282220661640167\n",
      "iteration 3584, dc_loss: 0.09398791193962097, tv_loss: 0.03281812742352486\n",
      "iteration 3585, dc_loss: 0.09398321807384491, tv_loss: 0.03282066807150841\n",
      "iteration 3586, dc_loss: 0.09397348761558533, tv_loss: 0.03282609209418297\n",
      "iteration 3587, dc_loss: 0.09397584199905396, tv_loss: 0.03281836584210396\n",
      "iteration 3588, dc_loss: 0.09396250545978546, tv_loss: 0.03282877057790756\n",
      "iteration 3589, dc_loss: 0.09397298842668533, tv_loss: 0.032825130969285965\n",
      "iteration 3590, dc_loss: 0.09395689517259598, tv_loss: 0.03283995762467384\n",
      "iteration 3591, dc_loss: 0.0939696729183197, tv_loss: 0.03281960263848305\n",
      "iteration 3592, dc_loss: 0.09395042061805725, tv_loss: 0.032830800861120224\n",
      "iteration 3593, dc_loss: 0.09395986050367355, tv_loss: 0.032819412648677826\n",
      "iteration 3594, dc_loss: 0.09394040703773499, tv_loss: 0.032838910818099976\n",
      "iteration 3595, dc_loss: 0.09395022690296173, tv_loss: 0.03282776102423668\n",
      "iteration 3596, dc_loss: 0.09392421692609787, tv_loss: 0.032838042825460434\n",
      "iteration 3597, dc_loss: 0.09393619000911713, tv_loss: 0.03281925618648529\n",
      "iteration 3598, dc_loss: 0.09391139447689056, tv_loss: 0.03284170851111412\n",
      "iteration 3599, dc_loss: 0.0939411148428917, tv_loss: 0.03281707316637039\n",
      "iteration 3600, dc_loss: 0.09390978515148163, tv_loss: 0.03284837305545807\n",
      "iteration 3601, dc_loss: 0.09393053501844406, tv_loss: 0.03281964734196663\n",
      "iteration 3602, dc_loss: 0.0938916876912117, tv_loss: 0.03283338621258736\n",
      "iteration 3603, dc_loss: 0.09388002008199692, tv_loss: 0.03283132612705231\n",
      "iteration 3604, dc_loss: 0.09390894323587418, tv_loss: 0.03281540796160698\n",
      "iteration 3605, dc_loss: 0.09388034790754318, tv_loss: 0.03283679485321045\n",
      "iteration 3606, dc_loss: 0.09386631846427917, tv_loss: 0.03282730653882027\n",
      "iteration 3607, dc_loss: 0.09388524293899536, tv_loss: 0.03281782567501068\n",
      "iteration 3608, dc_loss: 0.09386738389730453, tv_loss: 0.03283395618200302\n",
      "iteration 3609, dc_loss: 0.09385551512241364, tv_loss: 0.03282884880900383\n",
      "iteration 3610, dc_loss: 0.09386537969112396, tv_loss: 0.03281518071889877\n",
      "iteration 3611, dc_loss: 0.093851737678051, tv_loss: 0.032832950353622437\n",
      "iteration 3612, dc_loss: 0.09384259581565857, tv_loss: 0.03282628208398819\n",
      "iteration 3613, dc_loss: 0.09384545683860779, tv_loss: 0.032819900661706924\n",
      "iteration 3614, dc_loss: 0.09383940696716309, tv_loss: 0.03282499313354492\n",
      "iteration 3615, dc_loss: 0.0938335731625557, tv_loss: 0.032825272530317307\n",
      "iteration 3616, dc_loss: 0.09382496029138565, tv_loss: 0.0328267365694046\n",
      "iteration 3617, dc_loss: 0.09381905198097229, tv_loss: 0.032830215990543365\n",
      "iteration 3618, dc_loss: 0.09382058680057526, tv_loss: 0.032819733023643494\n",
      "iteration 3619, dc_loss: 0.09381016343832016, tv_loss: 0.03282707557082176\n",
      "iteration 3620, dc_loss: 0.0938044860959053, tv_loss: 0.032834239304065704\n",
      "iteration 3621, dc_loss: 0.0938098132610321, tv_loss: 0.03282002732157707\n",
      "iteration 3622, dc_loss: 0.09379792213439941, tv_loss: 0.03282307833433151\n",
      "iteration 3623, dc_loss: 0.09378857910633087, tv_loss: 0.032831527292728424\n",
      "iteration 3624, dc_loss: 0.09379049390554428, tv_loss: 0.032827429473400116\n",
      "iteration 3625, dc_loss: 0.09378187358379364, tv_loss: 0.0328284315764904\n",
      "iteration 3626, dc_loss: 0.09377893805503845, tv_loss: 0.032825492322444916\n",
      "iteration 3627, dc_loss: 0.0937778502702713, tv_loss: 0.03282299265265465\n",
      "iteration 3628, dc_loss: 0.09376820176839828, tv_loss: 0.03282870724797249\n",
      "iteration 3629, dc_loss: 0.0937633141875267, tv_loss: 0.03282889351248741\n",
      "iteration 3630, dc_loss: 0.09376418590545654, tv_loss: 0.03281981498003006\n",
      "iteration 3631, dc_loss: 0.09375737607479095, tv_loss: 0.03282308205962181\n",
      "iteration 3632, dc_loss: 0.09374915808439255, tv_loss: 0.03283100202679634\n",
      "iteration 3633, dc_loss: 0.09374900162220001, tv_loss: 0.03282706066966057\n",
      "iteration 3634, dc_loss: 0.09374472498893738, tv_loss: 0.03282114863395691\n",
      "iteration 3635, dc_loss: 0.09373665601015091, tv_loss: 0.03282681480050087\n",
      "iteration 3636, dc_loss: 0.09373275190591812, tv_loss: 0.032826438546180725\n",
      "iteration 3637, dc_loss: 0.09372830390930176, tv_loss: 0.032826218754053116\n",
      "iteration 3638, dc_loss: 0.09372372180223465, tv_loss: 0.03282871097326279\n",
      "iteration 3639, dc_loss: 0.09372258931398392, tv_loss: 0.03282816708087921\n",
      "iteration 3640, dc_loss: 0.09371808171272278, tv_loss: 0.03282586857676506\n",
      "iteration 3641, dc_loss: 0.09371171146631241, tv_loss: 0.03282438963651657\n",
      "iteration 3642, dc_loss: 0.09370500594377518, tv_loss: 0.03282908722758293\n",
      "iteration 3643, dc_loss: 0.09370189905166626, tv_loss: 0.03282634913921356\n",
      "iteration 3644, dc_loss: 0.0936991348862648, tv_loss: 0.03282353654503822\n",
      "iteration 3645, dc_loss: 0.09369318187236786, tv_loss: 0.03282920643687248\n",
      "iteration 3646, dc_loss: 0.09368614852428436, tv_loss: 0.03283027186989784\n",
      "iteration 3647, dc_loss: 0.09368652105331421, tv_loss: 0.03282763436436653\n",
      "iteration 3648, dc_loss: 0.09367969632148743, tv_loss: 0.03282710164785385\n",
      "iteration 3649, dc_loss: 0.09367549419403076, tv_loss: 0.03282710537314415\n",
      "iteration 3650, dc_loss: 0.09367215633392334, tv_loss: 0.032824527472257614\n",
      "iteration 3651, dc_loss: 0.09366527199745178, tv_loss: 0.03282622620463371\n",
      "iteration 3652, dc_loss: 0.09366127103567123, tv_loss: 0.03282470256090164\n",
      "iteration 3653, dc_loss: 0.09365903586149216, tv_loss: 0.03282395005226135\n",
      "iteration 3654, dc_loss: 0.09365247935056686, tv_loss: 0.03282453119754791\n",
      "iteration 3655, dc_loss: 0.09364602714776993, tv_loss: 0.032828494906425476\n",
      "iteration 3656, dc_loss: 0.09364794939756393, tv_loss: 0.032824765890836716\n",
      "iteration 3657, dc_loss: 0.09363815933465958, tv_loss: 0.03283015266060829\n",
      "iteration 3658, dc_loss: 0.09363514930009842, tv_loss: 0.03283097594976425\n",
      "iteration 3659, dc_loss: 0.09363144636154175, tv_loss: 0.032826997339725494\n",
      "iteration 3660, dc_loss: 0.09362458437681198, tv_loss: 0.032829649746418\n",
      "iteration 3661, dc_loss: 0.09362127631902695, tv_loss: 0.03282764554023743\n",
      "iteration 3662, dc_loss: 0.09361673891544342, tv_loss: 0.03282618895173073\n",
      "iteration 3663, dc_loss: 0.09361464530229568, tv_loss: 0.03282454237341881\n",
      "iteration 3664, dc_loss: 0.09360748529434204, tv_loss: 0.03282932564616203\n",
      "iteration 3665, dc_loss: 0.09360328316688538, tv_loss: 0.03283223882317543\n",
      "iteration 3666, dc_loss: 0.09360068291425705, tv_loss: 0.0328323133289814\n",
      "iteration 3667, dc_loss: 0.09359346330165863, tv_loss: 0.03283634036779404\n",
      "iteration 3668, dc_loss: 0.09359052032232285, tv_loss: 0.03283720836043358\n",
      "iteration 3669, dc_loss: 0.09358575195074081, tv_loss: 0.0328337624669075\n",
      "iteration 3670, dc_loss: 0.09357975423336029, tv_loss: 0.0328301265835762\n",
      "iteration 3671, dc_loss: 0.0935765877366066, tv_loss: 0.03282782435417175\n",
      "iteration 3672, dc_loss: 0.093574158847332, tv_loss: 0.03282547742128372\n",
      "iteration 3673, dc_loss: 0.09356766939163208, tv_loss: 0.03283190727233887\n",
      "iteration 3674, dc_loss: 0.09356480836868286, tv_loss: 0.03283308073878288\n",
      "iteration 3675, dc_loss: 0.09355784952640533, tv_loss: 0.032833147794008255\n",
      "iteration 3676, dc_loss: 0.09355345368385315, tv_loss: 0.032828908413648605\n",
      "iteration 3677, dc_loss: 0.09355109930038452, tv_loss: 0.03282756358385086\n",
      "iteration 3678, dc_loss: 0.09354404360055923, tv_loss: 0.03283040225505829\n",
      "iteration 3679, dc_loss: 0.09354373067617416, tv_loss: 0.03282826766371727\n",
      "iteration 3680, dc_loss: 0.093535415828228, tv_loss: 0.03283562883734703\n",
      "iteration 3681, dc_loss: 0.09353035688400269, tv_loss: 0.03283882886171341\n",
      "iteration 3682, dc_loss: 0.09353048354387283, tv_loss: 0.03283802047371864\n",
      "iteration 3683, dc_loss: 0.09352181106805801, tv_loss: 0.03283926099538803\n",
      "iteration 3684, dc_loss: 0.09351889044046402, tv_loss: 0.032832179218530655\n",
      "iteration 3685, dc_loss: 0.09351450949907303, tv_loss: 0.03283194452524185\n",
      "iteration 3686, dc_loss: 0.09350831061601639, tv_loss: 0.03283512219786644\n",
      "iteration 3687, dc_loss: 0.09350739419460297, tv_loss: 0.03283112868666649\n",
      "iteration 3688, dc_loss: 0.09350178390741348, tv_loss: 0.03283052146434784\n",
      "iteration 3689, dc_loss: 0.0934925526380539, tv_loss: 0.032836705446243286\n",
      "iteration 3690, dc_loss: 0.0934939831495285, tv_loss: 0.03283482789993286\n",
      "iteration 3691, dc_loss: 0.0934867113828659, tv_loss: 0.032841380685567856\n",
      "iteration 3692, dc_loss: 0.09348281472921371, tv_loss: 0.03283892199397087\n",
      "iteration 3693, dc_loss: 0.09348027408123016, tv_loss: 0.0328313447535038\n",
      "iteration 3694, dc_loss: 0.0934721976518631, tv_loss: 0.03283467888832092\n",
      "iteration 3695, dc_loss: 0.09346753358840942, tv_loss: 0.03283442556858063\n",
      "iteration 3696, dc_loss: 0.09346648305654526, tv_loss: 0.03283347934484482\n",
      "iteration 3697, dc_loss: 0.09346172958612442, tv_loss: 0.032833293080329895\n",
      "iteration 3698, dc_loss: 0.09345830231904984, tv_loss: 0.032831914722919464\n",
      "iteration 3699, dc_loss: 0.09344935417175293, tv_loss: 0.03283672034740448\n",
      "iteration 3700, dc_loss: 0.09344267100095749, tv_loss: 0.03283658251166344\n",
      "iteration 3701, dc_loss: 0.09344743937253952, tv_loss: 0.03282906115055084\n",
      "iteration 3702, dc_loss: 0.09343423694372177, tv_loss: 0.03283900395035744\n",
      "iteration 3703, dc_loss: 0.09343700110912323, tv_loss: 0.032831985503435135\n",
      "iteration 3704, dc_loss: 0.09342982620000839, tv_loss: 0.03283340483903885\n",
      "iteration 3705, dc_loss: 0.09342442452907562, tv_loss: 0.032835960388183594\n",
      "iteration 3706, dc_loss: 0.09342001378536224, tv_loss: 0.03283761814236641\n",
      "iteration 3707, dc_loss: 0.09341681748628616, tv_loss: 0.032842006534338\n",
      "iteration 3708, dc_loss: 0.09340820461511612, tv_loss: 0.0328463576734066\n",
      "iteration 3709, dc_loss: 0.09340832382440567, tv_loss: 0.032837361097335815\n",
      "iteration 3710, dc_loss: 0.09339988231658936, tv_loss: 0.032837387174367905\n",
      "iteration 3711, dc_loss: 0.0933995470404625, tv_loss: 0.03283097594976425\n",
      "iteration 3712, dc_loss: 0.0933966264128685, tv_loss: 0.03283047676086426\n",
      "iteration 3713, dc_loss: 0.09338680654764175, tv_loss: 0.032836031168699265\n",
      "iteration 3714, dc_loss: 0.09338366240262985, tv_loss: 0.03283529356122017\n",
      "iteration 3715, dc_loss: 0.09337937831878662, tv_loss: 0.032837312668561935\n",
      "iteration 3716, dc_loss: 0.09337354451417923, tv_loss: 0.032839953899383545\n",
      "iteration 3717, dc_loss: 0.09337612241506577, tv_loss: 0.03283332660794258\n",
      "iteration 3718, dc_loss: 0.09336400777101517, tv_loss: 0.032841701060533524\n",
      "iteration 3719, dc_loss: 0.09335929900407791, tv_loss: 0.032847121357917786\n",
      "iteration 3720, dc_loss: 0.09336048364639282, tv_loss: 0.03284238278865814\n",
      "iteration 3721, dc_loss: 0.09335240721702576, tv_loss: 0.032841019332408905\n",
      "iteration 3722, dc_loss: 0.09334725886583328, tv_loss: 0.03283611685037613\n",
      "iteration 3723, dc_loss: 0.09334555268287659, tv_loss: 0.03283426910638809\n",
      "iteration 3724, dc_loss: 0.09333883970975876, tv_loss: 0.032834768295288086\n",
      "iteration 3725, dc_loss: 0.0933370441198349, tv_loss: 0.03283373638987541\n",
      "iteration 3726, dc_loss: 0.09332942217588425, tv_loss: 0.03284080699086189\n",
      "iteration 3727, dc_loss: 0.09332167357206345, tv_loss: 0.03284537047147751\n",
      "iteration 3728, dc_loss: 0.09332817792892456, tv_loss: 0.03283218294382095\n",
      "iteration 3729, dc_loss: 0.09331630915403366, tv_loss: 0.03283734247088432\n",
      "iteration 3730, dc_loss: 0.09330815076828003, tv_loss: 0.032841358333826065\n",
      "iteration 3731, dc_loss: 0.09331143647432327, tv_loss: 0.03283577039837837\n",
      "iteration 3732, dc_loss: 0.09330195188522339, tv_loss: 0.03284536674618721\n",
      "iteration 3733, dc_loss: 0.09330103546380997, tv_loss: 0.03284327685832977\n",
      "iteration 3734, dc_loss: 0.09329403936862946, tv_loss: 0.03284839913249016\n",
      "iteration 3735, dc_loss: 0.09328901022672653, tv_loss: 0.03284522518515587\n",
      "iteration 3736, dc_loss: 0.09328611940145493, tv_loss: 0.03283897414803505\n",
      "iteration 3737, dc_loss: 0.09327922016382217, tv_loss: 0.03284067660570145\n",
      "iteration 3738, dc_loss: 0.09327472001314163, tv_loss: 0.032843440771102905\n",
      "iteration 3739, dc_loss: 0.09327653050422668, tv_loss: 0.0328381210565567\n",
      "iteration 3740, dc_loss: 0.09326742589473724, tv_loss: 0.03284173086285591\n",
      "iteration 3741, dc_loss: 0.09326191246509552, tv_loss: 0.032843999564647675\n",
      "iteration 3742, dc_loss: 0.09325946867465973, tv_loss: 0.03283989056944847\n",
      "iteration 3743, dc_loss: 0.09325245022773743, tv_loss: 0.03284456580877304\n",
      "iteration 3744, dc_loss: 0.09325277805328369, tv_loss: 0.03283855691552162\n",
      "iteration 3745, dc_loss: 0.09324391931295395, tv_loss: 0.03284299373626709\n",
      "iteration 3746, dc_loss: 0.09323599189519882, tv_loss: 0.032844968140125275\n",
      "iteration 3747, dc_loss: 0.0932401642203331, tv_loss: 0.03283529356122017\n",
      "iteration 3748, dc_loss: 0.09323254227638245, tv_loss: 0.03283990919589996\n",
      "iteration 3749, dc_loss: 0.09322566539049149, tv_loss: 0.03284250199794769\n",
      "iteration 3750, dc_loss: 0.09322483092546463, tv_loss: 0.032841842621564865\n",
      "iteration 3751, dc_loss: 0.09321662783622742, tv_loss: 0.03284963220357895\n",
      "iteration 3752, dc_loss: 0.0932122990489006, tv_loss: 0.03284657001495361\n",
      "iteration 3753, dc_loss: 0.09321072697639465, tv_loss: 0.032841358333826065\n",
      "iteration 3754, dc_loss: 0.09320329874753952, tv_loss: 0.03284425288438797\n",
      "iteration 3755, dc_loss: 0.0932001918554306, tv_loss: 0.03284338489174843\n",
      "iteration 3756, dc_loss: 0.09319712221622467, tv_loss: 0.03284271061420441\n",
      "iteration 3757, dc_loss: 0.09319142997264862, tv_loss: 0.032843682914972305\n",
      "iteration 3758, dc_loss: 0.09318606555461884, tv_loss: 0.0328444167971611\n",
      "iteration 3759, dc_loss: 0.09317976236343384, tv_loss: 0.03284739330410957\n",
      "iteration 3760, dc_loss: 0.09317629039287567, tv_loss: 0.032844312489032745\n",
      "iteration 3761, dc_loss: 0.09317784756422043, tv_loss: 0.03283872827887535\n",
      "iteration 3762, dc_loss: 0.09316697716712952, tv_loss: 0.032844413071870804\n",
      "iteration 3763, dc_loss: 0.09316357970237732, tv_loss: 0.03284170478582382\n",
      "iteration 3764, dc_loss: 0.09315884858369827, tv_loss: 0.03284243121743202\n",
      "iteration 3765, dc_loss: 0.09315592050552368, tv_loss: 0.03284194692969322\n",
      "iteration 3766, dc_loss: 0.09314893186092377, tv_loss: 0.0328475721180439\n",
      "iteration 3767, dc_loss: 0.09314745664596558, tv_loss: 0.032843463122844696\n",
      "iteration 3768, dc_loss: 0.09314093738794327, tv_loss: 0.032844074070453644\n",
      "iteration 3769, dc_loss: 0.09313877671957016, tv_loss: 0.03284468501806259\n",
      "iteration 3770, dc_loss: 0.09313207119703293, tv_loss: 0.03284686803817749\n",
      "iteration 3771, dc_loss: 0.09312644600868225, tv_loss: 0.032848868519067764\n",
      "iteration 3772, dc_loss: 0.0931255891919136, tv_loss: 0.03284304216504097\n",
      "iteration 3773, dc_loss: 0.09311671555042267, tv_loss: 0.03284602239727974\n",
      "iteration 3774, dc_loss: 0.09311293065547943, tv_loss: 0.03284575045108795\n",
      "iteration 3775, dc_loss: 0.09311222285032272, tv_loss: 0.0328417643904686\n",
      "iteration 3776, dc_loss: 0.0931040421128273, tv_loss: 0.03284410387277603\n",
      "iteration 3777, dc_loss: 0.09310217201709747, tv_loss: 0.032843612134456635\n",
      "iteration 3778, dc_loss: 0.09309615939855576, tv_loss: 0.032847810536623\n",
      "iteration 3779, dc_loss: 0.09309135377407074, tv_loss: 0.03285155072808266\n",
      "iteration 3780, dc_loss: 0.09308741241693497, tv_loss: 0.03285200521349907\n",
      "iteration 3781, dc_loss: 0.09308363497257233, tv_loss: 0.03285161778330803\n",
      "iteration 3782, dc_loss: 0.09307532012462616, tv_loss: 0.032850876450538635\n",
      "iteration 3783, dc_loss: 0.09307362139225006, tv_loss: 0.03284737095236778\n",
      "iteration 3784, dc_loss: 0.09307079762220383, tv_loss: 0.03284570947289467\n",
      "iteration 3785, dc_loss: 0.09306582063436508, tv_loss: 0.032847434282302856\n",
      "iteration 3786, dc_loss: 0.09305840730667114, tv_loss: 0.032852593809366226\n",
      "iteration 3787, dc_loss: 0.09305757284164429, tv_loss: 0.032849352806806564\n",
      "iteration 3788, dc_loss: 0.0930509939789772, tv_loss: 0.03285154327750206\n",
      "iteration 3789, dc_loss: 0.09304628521203995, tv_loss: 0.0328514389693737\n",
      "iteration 3790, dc_loss: 0.09304079413414001, tv_loss: 0.032851219177246094\n",
      "iteration 3791, dc_loss: 0.09303755313158035, tv_loss: 0.03284955024719238\n",
      "iteration 3792, dc_loss: 0.09303267300128937, tv_loss: 0.032848842442035675\n",
      "iteration 3793, dc_loss: 0.09302982687950134, tv_loss: 0.0328463613986969\n",
      "iteration 3794, dc_loss: 0.09302675724029541, tv_loss: 0.03284540772438049\n",
      "iteration 3795, dc_loss: 0.0930180773139, tv_loss: 0.03284992650151253\n",
      "iteration 3796, dc_loss: 0.09301283210515976, tv_loss: 0.032851431518793106\n",
      "iteration 3797, dc_loss: 0.09301058948040009, tv_loss: 0.03284996375441551\n",
      "iteration 3798, dc_loss: 0.09300308674573898, tv_loss: 0.032852653414011\n",
      "iteration 3799, dc_loss: 0.09300438314676285, tv_loss: 0.03284730017185211\n",
      "iteration 3800, dc_loss: 0.09300016611814499, tv_loss: 0.03284595534205437\n",
      "iteration 3801, dc_loss: 0.092988982796669, tv_loss: 0.03285268321633339\n",
      "iteration 3802, dc_loss: 0.09298889338970184, tv_loss: 0.03285086899995804\n",
      "iteration 3803, dc_loss: 0.09298272430896759, tv_loss: 0.032857682555913925\n",
      "iteration 3804, dc_loss: 0.09298024326562881, tv_loss: 0.03285565227270126\n",
      "iteration 3805, dc_loss: 0.09297267347574234, tv_loss: 0.03285416588187218\n",
      "iteration 3806, dc_loss: 0.0929734855890274, tv_loss: 0.0328487902879715\n",
      "iteration 3807, dc_loss: 0.0929613932967186, tv_loss: 0.03285312280058861\n",
      "iteration 3808, dc_loss: 0.09296402335166931, tv_loss: 0.032846350222826004\n",
      "iteration 3809, dc_loss: 0.09295317530632019, tv_loss: 0.0328526645898819\n",
      "iteration 3810, dc_loss: 0.0929546132683754, tv_loss: 0.032845623791217804\n",
      "iteration 3811, dc_loss: 0.0929451510310173, tv_loss: 0.03285088762640953\n",
      "iteration 3812, dc_loss: 0.09294712543487549, tv_loss: 0.03284521400928497\n",
      "iteration 3813, dc_loss: 0.09293930977582932, tv_loss: 0.032850902527570724\n",
      "iteration 3814, dc_loss: 0.09293337166309357, tv_loss: 0.032853733748197556\n",
      "iteration 3815, dc_loss: 0.09292604774236679, tv_loss: 0.03285926952958107\n",
      "iteration 3816, dc_loss: 0.09292861819267273, tv_loss: 0.03285534679889679\n",
      "iteration 3817, dc_loss: 0.09291885048151016, tv_loss: 0.03285720571875572\n",
      "iteration 3818, dc_loss: 0.09292024374008179, tv_loss: 0.032848432660102844\n",
      "iteration 3819, dc_loss: 0.09290581941604614, tv_loss: 0.03286084160208702\n",
      "iteration 3820, dc_loss: 0.09290783107280731, tv_loss: 0.0328526571393013\n",
      "iteration 3821, dc_loss: 0.09290450811386108, tv_loss: 0.03285070136189461\n",
      "iteration 3822, dc_loss: 0.09289855509996414, tv_loss: 0.03285704180598259\n",
      "iteration 3823, dc_loss: 0.09288866072893143, tv_loss: 0.03285898268222809\n",
      "iteration 3824, dc_loss: 0.09288857132196426, tv_loss: 0.03285369649529457\n",
      "iteration 3825, dc_loss: 0.09288639575242996, tv_loss: 0.032859161496162415\n",
      "iteration 3826, dc_loss: 0.09288039803504944, tv_loss: 0.032853420823812485\n",
      "iteration 3827, dc_loss: 0.09287436306476593, tv_loss: 0.03285844624042511\n",
      "iteration 3828, dc_loss: 0.0928683802485466, tv_loss: 0.03285841643810272\n",
      "iteration 3829, dc_loss: 0.09286563843488693, tv_loss: 0.032860007137060165\n",
      "iteration 3830, dc_loss: 0.09286279231309891, tv_loss: 0.03285858780145645\n",
      "iteration 3831, dc_loss: 0.09285879135131836, tv_loss: 0.03285817429423332\n",
      "iteration 3832, dc_loss: 0.09284967929124832, tv_loss: 0.03285886347293854\n",
      "iteration 3833, dc_loss: 0.09284649789333344, tv_loss: 0.03285762295126915\n",
      "iteration 3834, dc_loss: 0.0928436741232872, tv_loss: 0.032864946871995926\n",
      "iteration 3835, dc_loss: 0.09284184873104095, tv_loss: 0.03285355865955353\n",
      "iteration 3836, dc_loss: 0.09283393621444702, tv_loss: 0.03285811096429825\n",
      "iteration 3837, dc_loss: 0.0928279235959053, tv_loss: 0.03285916894674301\n",
      "iteration 3838, dc_loss: 0.09282305836677551, tv_loss: 0.03286133334040642\n",
      "iteration 3839, dc_loss: 0.09282076358795166, tv_loss: 0.032861385494470596\n",
      "iteration 3840, dc_loss: 0.09281715005636215, tv_loss: 0.032857950776815414\n",
      "iteration 3841, dc_loss: 0.09281305968761444, tv_loss: 0.032857611775398254\n",
      "iteration 3842, dc_loss: 0.09280408918857574, tv_loss: 0.03286215662956238\n",
      "iteration 3843, dc_loss: 0.09280899167060852, tv_loss: 0.0328558050096035\n",
      "iteration 3844, dc_loss: 0.09279654175043106, tv_loss: 0.03286247327923775\n",
      "iteration 3845, dc_loss: 0.09279448539018631, tv_loss: 0.032858703285455704\n",
      "iteration 3846, dc_loss: 0.09278763830661774, tv_loss: 0.03286311402916908\n",
      "iteration 3847, dc_loss: 0.09279342740774155, tv_loss: 0.03285672515630722\n",
      "iteration 3848, dc_loss: 0.09277649968862534, tv_loss: 0.03286614641547203\n",
      "iteration 3849, dc_loss: 0.09278766065835953, tv_loss: 0.03285739943385124\n",
      "iteration 3850, dc_loss: 0.0927719846367836, tv_loss: 0.03286602720618248\n",
      "iteration 3851, dc_loss: 0.0927918329834938, tv_loss: 0.0328463651239872\n",
      "iteration 3852, dc_loss: 0.09276054799556732, tv_loss: 0.032885197550058365\n",
      "iteration 3853, dc_loss: 0.09279059618711472, tv_loss: 0.03284015506505966\n",
      "iteration 3854, dc_loss: 0.09275445342063904, tv_loss: 0.03287702798843384\n",
      "iteration 3855, dc_loss: 0.09276889264583588, tv_loss: 0.0328509546816349\n",
      "iteration 3856, dc_loss: 0.09274007380008698, tv_loss: 0.03286932781338692\n",
      "iteration 3857, dc_loss: 0.09274323284626007, tv_loss: 0.03285845369100571\n",
      "iteration 3858, dc_loss: 0.0927363857626915, tv_loss: 0.03286309912800789\n",
      "iteration 3859, dc_loss: 0.09272920340299606, tv_loss: 0.03286884352564812\n",
      "iteration 3860, dc_loss: 0.09274342656135559, tv_loss: 0.03285227715969086\n",
      "iteration 3861, dc_loss: 0.09271439909934998, tv_loss: 0.03287224844098091\n",
      "iteration 3862, dc_loss: 0.09272300451993942, tv_loss: 0.03285751864314079\n",
      "iteration 3863, dc_loss: 0.09271209686994553, tv_loss: 0.03286319971084595\n",
      "iteration 3864, dc_loss: 0.09270770847797394, tv_loss: 0.032865941524505615\n",
      "iteration 3865, dc_loss: 0.09271444380283356, tv_loss: 0.03285333141684532\n",
      "iteration 3866, dc_loss: 0.09269392490386963, tv_loss: 0.032871063798666\n",
      "iteration 3867, dc_loss: 0.09269557148218155, tv_loss: 0.032865822315216064\n",
      "iteration 3868, dc_loss: 0.09268931299448013, tv_loss: 0.03286329284310341\n",
      "iteration 3869, dc_loss: 0.09268303215503693, tv_loss: 0.03287046402692795\n",
      "iteration 3870, dc_loss: 0.09268607199192047, tv_loss: 0.032861486077308655\n",
      "iteration 3871, dc_loss: 0.09267354756593704, tv_loss: 0.03286894038319588\n",
      "iteration 3872, dc_loss: 0.09268002212047577, tv_loss: 0.03285691514611244\n",
      "iteration 3873, dc_loss: 0.09266359359025955, tv_loss: 0.03286753222346306\n",
      "iteration 3874, dc_loss: 0.09265805035829544, tv_loss: 0.032870132476091385\n",
      "iteration 3875, dc_loss: 0.09266199916601181, tv_loss: 0.03285884112119675\n",
      "iteration 3876, dc_loss: 0.09265130758285522, tv_loss: 0.03286832198500633\n",
      "iteration 3877, dc_loss: 0.09265550225973129, tv_loss: 0.032858747988939285\n",
      "iteration 3878, dc_loss: 0.09264528751373291, tv_loss: 0.03286496177315712\n",
      "iteration 3879, dc_loss: 0.09263607859611511, tv_loss: 0.03286967799067497\n",
      "iteration 3880, dc_loss: 0.0926336720585823, tv_loss: 0.032865267246961594\n",
      "iteration 3881, dc_loss: 0.09263116866350174, tv_loss: 0.0328625924885273\n",
      "iteration 3882, dc_loss: 0.09262704104185104, tv_loss: 0.03286543861031532\n",
      "iteration 3883, dc_loss: 0.09262106567621231, tv_loss: 0.03286338597536087\n",
      "iteration 3884, dc_loss: 0.09261874109506607, tv_loss: 0.03286393731832504\n",
      "iteration 3885, dc_loss: 0.092608742415905, tv_loss: 0.03287094086408615\n",
      "iteration 3886, dc_loss: 0.09260709583759308, tv_loss: 0.03286390379071236\n",
      "iteration 3887, dc_loss: 0.09260547906160355, tv_loss: 0.0328637957572937\n",
      "iteration 3888, dc_loss: 0.09259743988513947, tv_loss: 0.03286620229482651\n",
      "iteration 3889, dc_loss: 0.09259787946939468, tv_loss: 0.0328587181866169\n",
      "iteration 3890, dc_loss: 0.09258563816547394, tv_loss: 0.0328698493540287\n",
      "iteration 3891, dc_loss: 0.09258005023002625, tv_loss: 0.03287070244550705\n",
      "iteration 3892, dc_loss: 0.0925874263048172, tv_loss: 0.03285796195268631\n",
      "iteration 3893, dc_loss: 0.09257226437330246, tv_loss: 0.03286602720618248\n",
      "iteration 3894, dc_loss: 0.09256747364997864, tv_loss: 0.03286820650100708\n",
      "iteration 3895, dc_loss: 0.09257280081510544, tv_loss: 0.03286074474453926\n",
      "iteration 3896, dc_loss: 0.09256190061569214, tv_loss: 0.03286490961909294\n",
      "iteration 3897, dc_loss: 0.0925522893667221, tv_loss: 0.03287138044834137\n",
      "iteration 3898, dc_loss: 0.09255613386631012, tv_loss: 0.03286219388246536\n",
      "iteration 3899, dc_loss: 0.09254739433526993, tv_loss: 0.0328671894967556\n",
      "iteration 3900, dc_loss: 0.09254256635904312, tv_loss: 0.032868608832359314\n",
      "iteration 3901, dc_loss: 0.09253870695829391, tv_loss: 0.032868269830942154\n",
      "iteration 3902, dc_loss: 0.09253550320863724, tv_loss: 0.03286626562476158\n",
      "iteration 3903, dc_loss: 0.09253174066543579, tv_loss: 0.032864611595869064\n",
      "iteration 3904, dc_loss: 0.09252403676509857, tv_loss: 0.03286835923790932\n",
      "iteration 3905, dc_loss: 0.09251751005649567, tv_loss: 0.03286736086010933\n",
      "iteration 3906, dc_loss: 0.09251655638217926, tv_loss: 0.032863520085811615\n",
      "iteration 3907, dc_loss: 0.09251273423433304, tv_loss: 0.032865460962057114\n",
      "iteration 3908, dc_loss: 0.09251187741756439, tv_loss: 0.03286284953355789\n",
      "iteration 3909, dc_loss: 0.09249913692474365, tv_loss: 0.032868657261133194\n",
      "iteration 3910, dc_loss: 0.09250272065401077, tv_loss: 0.03286353498697281\n",
      "iteration 3911, dc_loss: 0.09249404817819595, tv_loss: 0.032867249101400375\n",
      "iteration 3912, dc_loss: 0.09249455481767654, tv_loss: 0.0328606441617012\n",
      "iteration 3913, dc_loss: 0.09248118847608566, tv_loss: 0.03287162631750107\n",
      "iteration 3914, dc_loss: 0.09249258041381836, tv_loss: 0.03285660222172737\n",
      "iteration 3915, dc_loss: 0.0924767553806305, tv_loss: 0.03287053108215332\n",
      "iteration 3916, dc_loss: 0.09248732030391693, tv_loss: 0.032861750572919846\n",
      "iteration 3917, dc_loss: 0.09247255325317383, tv_loss: 0.032872673124074936\n",
      "iteration 3918, dc_loss: 0.0924898236989975, tv_loss: 0.03285925090312958\n",
      "iteration 3919, dc_loss: 0.09246109426021576, tv_loss: 0.032886408269405365\n",
      "iteration 3920, dc_loss: 0.09248161315917969, tv_loss: 0.03285997733473778\n",
      "iteration 3921, dc_loss: 0.09246042370796204, tv_loss: 0.03287847340106964\n",
      "iteration 3922, dc_loss: 0.09246321767568588, tv_loss: 0.03286241739988327\n",
      "iteration 3923, dc_loss: 0.09243850409984589, tv_loss: 0.032877419143915176\n",
      "iteration 3924, dc_loss: 0.09244825690984726, tv_loss: 0.0328572653234005\n",
      "iteration 3925, dc_loss: 0.09242431819438934, tv_loss: 0.032874349504709244\n",
      "iteration 3926, dc_loss: 0.09243177622556686, tv_loss: 0.032862842082977295\n",
      "iteration 3927, dc_loss: 0.09241824597120285, tv_loss: 0.032870061695575714\n",
      "iteration 3928, dc_loss: 0.09241316467523575, tv_loss: 0.03287532553076744\n",
      "iteration 3929, dc_loss: 0.09241730719804764, tv_loss: 0.032869622111320496\n",
      "iteration 3930, dc_loss: 0.09240897744894028, tv_loss: 0.032872334122657776\n",
      "iteration 3931, dc_loss: 0.09240716695785522, tv_loss: 0.03287225216627121\n",
      "iteration 3932, dc_loss: 0.09239954501390457, tv_loss: 0.03287038579583168\n",
      "iteration 3933, dc_loss: 0.092405304312706, tv_loss: 0.03285994753241539\n",
      "iteration 3934, dc_loss: 0.0923868790268898, tv_loss: 0.03287731856107712\n",
      "iteration 3935, dc_loss: 0.09239060431718826, tv_loss: 0.032865606248378754\n",
      "iteration 3936, dc_loss: 0.0923743024468422, tv_loss: 0.03287503123283386\n",
      "iteration 3937, dc_loss: 0.09238512068986893, tv_loss: 0.032858967781066895\n",
      "iteration 3938, dc_loss: 0.09236812591552734, tv_loss: 0.03287206590175629\n",
      "iteration 3939, dc_loss: 0.09237516671419144, tv_loss: 0.03286101296544075\n",
      "iteration 3940, dc_loss: 0.09235702455043793, tv_loss: 0.032880257815122604\n",
      "iteration 3941, dc_loss: 0.09236035495996475, tv_loss: 0.03287016600370407\n",
      "iteration 3942, dc_loss: 0.09235017001628876, tv_loss: 0.03287466987967491\n",
      "iteration 3943, dc_loss: 0.09236212819814682, tv_loss: 0.032858956605196\n",
      "iteration 3944, dc_loss: 0.09233509749174118, tv_loss: 0.03287937492132187\n",
      "iteration 3945, dc_loss: 0.09235106408596039, tv_loss: 0.03285971283912659\n",
      "iteration 3946, dc_loss: 0.09233125299215317, tv_loss: 0.03287797048687935\n",
      "iteration 3947, dc_loss: 0.09234011173248291, tv_loss: 0.032863445580005646\n",
      "iteration 3948, dc_loss: 0.09231968224048615, tv_loss: 0.032879702746868134\n",
      "iteration 3949, dc_loss: 0.09232985973358154, tv_loss: 0.032865677028894424\n",
      "iteration 3950, dc_loss: 0.09231308847665787, tv_loss: 0.03287750482559204\n",
      "iteration 3951, dc_loss: 0.09231606125831604, tv_loss: 0.032868847250938416\n",
      "iteration 3952, dc_loss: 0.09230411797761917, tv_loss: 0.032875459641218185\n",
      "iteration 3953, dc_loss: 0.0923059806227684, tv_loss: 0.0328686349093914\n",
      "iteration 3954, dc_loss: 0.09229334443807602, tv_loss: 0.0328773632645607\n",
      "iteration 3955, dc_loss: 0.09229514747858047, tv_loss: 0.03287384286522865\n",
      "iteration 3956, dc_loss: 0.09228461235761642, tv_loss: 0.032886188477277756\n",
      "iteration 3957, dc_loss: 0.0922832265496254, tv_loss: 0.03288198262453079\n",
      "iteration 3958, dc_loss: 0.09227985888719559, tv_loss: 0.032872505486011505\n",
      "iteration 3959, dc_loss: 0.0922713503241539, tv_loss: 0.03287651389837265\n",
      "iteration 3960, dc_loss: 0.0922766849398613, tv_loss: 0.032868679612874985\n",
      "iteration 3961, dc_loss: 0.0922587662935257, tv_loss: 0.03287995234131813\n",
      "iteration 3962, dc_loss: 0.09226629137992859, tv_loss: 0.03286908194422722\n",
      "iteration 3963, dc_loss: 0.09225034713745117, tv_loss: 0.03287836164236069\n",
      "iteration 3964, dc_loss: 0.09226192533969879, tv_loss: 0.0328625924885273\n",
      "iteration 3965, dc_loss: 0.09224122762680054, tv_loss: 0.03288078308105469\n",
      "iteration 3966, dc_loss: 0.09226041287183762, tv_loss: 0.03286052122712135\n",
      "iteration 3967, dc_loss: 0.09223289787769318, tv_loss: 0.032891228795051575\n",
      "iteration 3968, dc_loss: 0.09226012229919434, tv_loss: 0.032869137823581696\n",
      "iteration 3969, dc_loss: 0.09222753345966339, tv_loss: 0.03289507329463959\n",
      "iteration 3970, dc_loss: 0.09225456416606903, tv_loss: 0.03286181017756462\n",
      "iteration 3971, dc_loss: 0.09222742170095444, tv_loss: 0.03288768231868744\n",
      "iteration 3972, dc_loss: 0.09225475788116455, tv_loss: 0.03286410868167877\n",
      "iteration 3973, dc_loss: 0.09222070127725601, tv_loss: 0.03290044888854027\n",
      "iteration 3974, dc_loss: 0.09225234389305115, tv_loss: 0.032859917730093\n",
      "iteration 3975, dc_loss: 0.09221336245536804, tv_loss: 0.03289355710148811\n",
      "iteration 3976, dc_loss: 0.09222929179668427, tv_loss: 0.03286634758114815\n",
      "iteration 3977, dc_loss: 0.09219193458557129, tv_loss: 0.03289254754781723\n",
      "iteration 3978, dc_loss: 0.09220482409000397, tv_loss: 0.0328693725168705\n",
      "iteration 3979, dc_loss: 0.0921829491853714, tv_loss: 0.03288586810231209\n",
      "iteration 3980, dc_loss: 0.09217958897352219, tv_loss: 0.03288065642118454\n",
      "iteration 3981, dc_loss: 0.09218444675207138, tv_loss: 0.03287555277347565\n",
      "iteration 3982, dc_loss: 0.09216655045747757, tv_loss: 0.03288979455828667\n",
      "iteration 3983, dc_loss: 0.0921865776181221, tv_loss: 0.03287028893828392\n",
      "iteration 3984, dc_loss: 0.09215950965881348, tv_loss: 0.032891545444726944\n",
      "iteration 3985, dc_loss: 0.092173732817173, tv_loss: 0.03287174925208092\n",
      "iteration 3986, dc_loss: 0.09214936941862106, tv_loss: 0.03289322927594185\n",
      "iteration 3987, dc_loss: 0.09215997904539108, tv_loss: 0.03287277743220329\n",
      "iteration 3988, dc_loss: 0.09213948249816895, tv_loss: 0.032892581075429916\n",
      "iteration 3989, dc_loss: 0.0921449288725853, tv_loss: 0.032873306423425674\n",
      "iteration 3990, dc_loss: 0.0921352207660675, tv_loss: 0.032881662249565125\n",
      "iteration 3991, dc_loss: 0.09212527424097061, tv_loss: 0.032889608293771744\n",
      "iteration 3992, dc_loss: 0.09213639795780182, tv_loss: 0.03287411481142044\n",
      "iteration 3993, dc_loss: 0.0921206995844841, tv_loss: 0.0328821986913681\n",
      "iteration 3994, dc_loss: 0.09212096035480499, tv_loss: 0.03287729620933533\n",
      "iteration 3995, dc_loss: 0.09211049973964691, tv_loss: 0.0328894779086113\n",
      "iteration 3996, dc_loss: 0.09211266785860062, tv_loss: 0.032877955585718155\n",
      "iteration 3997, dc_loss: 0.0921010673046112, tv_loss: 0.0328846201300621\n",
      "iteration 3998, dc_loss: 0.09210192412137985, tv_loss: 0.032877519726753235\n",
      "iteration 3999, dc_loss: 0.09208502620458603, tv_loss: 0.03288630023598671\n",
      "iteration 4000, dc_loss: 0.09209437668323517, tv_loss: 0.032879047095775604\n",
      "PSNR Value mt1: 35.77876077524474\n",
      "SSIM Value mt1: 0.7715722252164936\n",
      "tensor([[[6.0409e-04, 7.4484e-05]]]) tensor([[[1.2870, 1.2171]]])\n",
      "torch.Size([1, 1, 2]) torch.Size([1, 1, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| params.keys(): dict_keys(['net.0.linear.weight', 'net.0.linear.bias', 'net.1.linear.weight', 'net.1.linear.bias', 'net.2.linear.weight', 'net.2.linear.bias', 'net.3.linear.weight', 'net.3.linear.bias', 'net.4.linear.weight', 'net.4.linear.bias', 'net.5.weight', 'net.5.bias'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "iteration 1, dc_loss: 2.953230619430542, tv_loss: 0.0003438346029724926\n",
      "iteration 2, dc_loss: 2.824640989303589, tv_loss: 0.003585376078262925\n",
      "iteration 3, dc_loss: 2.749671697616577, tv_loss: 0.006073945667594671\n",
      "iteration 4, dc_loss: 2.7008719444274902, tv_loss: 0.00792218092828989\n",
      "iteration 5, dc_loss: 2.671436071395874, tv_loss: 0.009071550332009792\n",
      "iteration 6, dc_loss: 2.6513469219207764, tv_loss: 0.009749146178364754\n",
      "iteration 7, dc_loss: 2.63533616065979, tv_loss: 0.010172576643526554\n",
      "iteration 8, dc_loss: 2.6196398735046387, tv_loss: 0.010546237230300903\n",
      "iteration 9, dc_loss: 2.6034185886383057, tv_loss: 0.010917861945927143\n",
      "iteration 10, dc_loss: 2.5874154567718506, tv_loss: 0.011237578466534615\n",
      "iteration 11, dc_loss: 2.571352243423462, tv_loss: 0.011504360474646091\n",
      "iteration 12, dc_loss: 2.5555624961853027, tv_loss: 0.011751309037208557\n",
      "iteration 13, dc_loss: 2.540461301803589, tv_loss: 0.012014058418571949\n",
      "iteration 14, dc_loss: 2.5259487628936768, tv_loss: 0.012307981960475445\n",
      "iteration 15, dc_loss: 2.5120108127593994, tv_loss: 0.012598127126693726\n",
      "iteration 16, dc_loss: 2.498236656188965, tv_loss: 0.012823550030589104\n",
      "iteration 17, dc_loss: 2.4842989444732666, tv_loss: 0.012957888655364513\n",
      "iteration 18, dc_loss: 2.470371723175049, tv_loss: 0.01303491648286581\n",
      "iteration 19, dc_loss: 2.456613063812256, tv_loss: 0.013105095364153385\n",
      "iteration 20, dc_loss: 2.443256139755249, tv_loss: 0.013146750628948212\n",
      "iteration 21, dc_loss: 2.430084705352783, tv_loss: 0.013096107169985771\n",
      "iteration 22, dc_loss: 2.416680097579956, tv_loss: 0.01294398307800293\n",
      "iteration 23, dc_loss: 2.4029743671417236, tv_loss: 0.012716146185994148\n",
      "iteration 24, dc_loss: 2.3892459869384766, tv_loss: 0.012405584566295147\n",
      "iteration 25, dc_loss: 2.375737190246582, tv_loss: 0.012006387114524841\n",
      "iteration 26, dc_loss: 2.3624041080474854, tv_loss: 0.011509895324707031\n",
      "iteration 27, dc_loss: 2.349217414855957, tv_loss: 0.010896659456193447\n",
      "iteration 28, dc_loss: 2.335984706878662, tv_loss: 0.010243390686810017\n",
      "iteration 29, dc_loss: 2.322805643081665, tv_loss: 0.00960985291749239\n",
      "iteration 30, dc_loss: 2.310023307800293, tv_loss: 0.008957654237747192\n",
      "iteration 31, dc_loss: 2.297494888305664, tv_loss: 0.008412018418312073\n",
      "iteration 32, dc_loss: 2.2850284576416016, tv_loss: 0.008093656040728092\n",
      "iteration 33, dc_loss: 2.2726898193359375, tv_loss: 0.00791199505329132\n",
      "iteration 34, dc_loss: 2.260397434234619, tv_loss: 0.007859411649405956\n",
      "iteration 35, dc_loss: 2.2481181621551514, tv_loss: 0.007922252640128136\n",
      "iteration 36, dc_loss: 2.2359657287597656, tv_loss: 0.008051950484514236\n",
      "iteration 37, dc_loss: 2.224062204360962, tv_loss: 0.008190590888261795\n",
      "iteration 38, dc_loss: 2.2122702598571777, tv_loss: 0.008287792094051838\n",
      "iteration 39, dc_loss: 2.2004740238189697, tv_loss: 0.00834382139146328\n",
      "iteration 40, dc_loss: 2.1888108253479004, tv_loss: 0.008338943123817444\n",
      "iteration 41, dc_loss: 2.1773428916931152, tv_loss: 0.008296100422739983\n",
      "iteration 42, dc_loss: 2.1660444736480713, tv_loss: 0.00821660552173853\n",
      "iteration 43, dc_loss: 2.154773473739624, tv_loss: 0.00823334138840437\n",
      "iteration 44, dc_loss: 2.1438395977020264, tv_loss: 0.008269736543297768\n",
      "iteration 45, dc_loss: 2.1333577632904053, tv_loss: 0.008472862653434277\n",
      "iteration 46, dc_loss: 2.123004913330078, tv_loss: 0.00848525483161211\n",
      "iteration 47, dc_loss: 2.1118829250335693, tv_loss: 0.008732680231332779\n",
      "iteration 48, dc_loss: 2.0996532440185547, tv_loss: 0.008785668760538101\n",
      "iteration 49, dc_loss: 2.089649200439453, tv_loss: 0.008844796568155289\n",
      "iteration 50, dc_loss: 2.0791471004486084, tv_loss: 0.009050429798662663\n",
      "iteration 51, dc_loss: 2.067382335662842, tv_loss: 0.009116879664361477\n",
      "iteration 52, dc_loss: 2.0577640533447266, tv_loss: 0.009165884926915169\n",
      "iteration 53, dc_loss: 2.046903610229492, tv_loss: 0.00937496405094862\n",
      "iteration 54, dc_loss: 2.035900831222534, tv_loss: 0.009462734684348106\n",
      "iteration 55, dc_loss: 2.0263922214508057, tv_loss: 0.009488099254667759\n",
      "iteration 56, dc_loss: 2.0153050422668457, tv_loss: 0.009684493765234947\n",
      "iteration 57, dc_loss: 2.005227565765381, tv_loss: 0.009802415035665035\n",
      "iteration 58, dc_loss: 1.9953011274337769, tv_loss: 0.009825493209064007\n",
      "iteration 59, dc_loss: 1.9847372770309448, tv_loss: 0.009973762556910515\n",
      "iteration 60, dc_loss: 1.975013017654419, tv_loss: 0.010130436159670353\n",
      "iteration 61, dc_loss: 1.96485435962677, tv_loss: 0.01018416229635477\n",
      "iteration 62, dc_loss: 1.9550451040267944, tv_loss: 0.010281053371727467\n",
      "iteration 63, dc_loss: 1.945249319076538, tv_loss: 0.01045971643179655\n",
      "iteration 64, dc_loss: 1.9352914094924927, tv_loss: 0.010546042583882809\n",
      "iteration 65, dc_loss: 1.9258517026901245, tv_loss: 0.010617777705192566\n",
      "iteration 66, dc_loss: 1.9160584211349487, tv_loss: 0.010795414447784424\n",
      "iteration 67, dc_loss: 1.9064304828643799, tv_loss: 0.010896412655711174\n",
      "iteration 68, dc_loss: 1.8971489667892456, tv_loss: 0.010954891331493855\n",
      "iteration 69, dc_loss: 1.8875272274017334, tv_loss: 0.0111141512170434\n",
      "iteration 70, dc_loss: 1.8781468868255615, tv_loss: 0.01122425775974989\n",
      "iteration 71, dc_loss: 1.8689862489700317, tv_loss: 0.011291143484413624\n",
      "iteration 72, dc_loss: 1.8595956563949585, tv_loss: 0.01144054252654314\n",
      "iteration 73, dc_loss: 1.8504080772399902, tv_loss: 0.011551748029887676\n",
      "iteration 74, dc_loss: 1.841422438621521, tv_loss: 0.011614875867962837\n",
      "iteration 75, dc_loss: 1.8322522640228271, tv_loss: 0.011759860441088676\n",
      "iteration 76, dc_loss: 1.8232160806655884, tv_loss: 0.011862709186971188\n",
      "iteration 77, dc_loss: 1.8143993616104126, tv_loss: 0.011928243562579155\n",
      "iteration 78, dc_loss: 1.8054618835449219, tv_loss: 0.012071052566170692\n",
      "iteration 79, dc_loss: 1.7965902090072632, tv_loss: 0.012150505557656288\n",
      "iteration 80, dc_loss: 1.7878895998001099, tv_loss: 0.01223580352962017\n",
      "iteration 81, dc_loss: 1.7791907787322998, tv_loss: 0.01237936969846487\n",
      "iteration 82, dc_loss: 1.7705156803131104, tv_loss: 0.012450763955712318\n",
      "iteration 83, dc_loss: 1.761912226676941, tv_loss: 0.012548480182886124\n",
      "iteration 84, dc_loss: 1.753427267074585, tv_loss: 0.01268069725483656\n",
      "iteration 85, dc_loss: 1.7451261281967163, tv_loss: 0.012722926214337349\n",
      "iteration 86, dc_loss: 1.7368515729904175, tv_loss: 0.012901117093861103\n",
      "iteration 87, dc_loss: 1.7290171384811401, tv_loss: 0.012989172711968422\n",
      "iteration 88, dc_loss: 1.7211631536483765, tv_loss: 0.013131284154951572\n",
      "iteration 89, dc_loss: 1.7129143476486206, tv_loss: 0.013196885585784912\n",
      "iteration 90, dc_loss: 1.7037264108657837, tv_loss: 0.013228976167738438\n",
      "iteration 91, dc_loss: 1.6953296661376953, tv_loss: 0.013324705883860588\n",
      "iteration 92, dc_loss: 1.6878312826156616, tv_loss: 0.013447046279907227\n",
      "iteration 93, dc_loss: 1.6796656847000122, tv_loss: 0.013571166433393955\n",
      "iteration 94, dc_loss: 1.6711732149124146, tv_loss: 0.013629264198243618\n",
      "iteration 95, dc_loss: 1.663268804550171, tv_loss: 0.013665741309523582\n",
      "iteration 96, dc_loss: 1.6556081771850586, tv_loss: 0.013785448856651783\n",
      "iteration 97, dc_loss: 1.6476471424102783, tv_loss: 0.013949080370366573\n",
      "iteration 98, dc_loss: 1.6395946741104126, tv_loss: 0.013975749723613262\n",
      "iteration 99, dc_loss: 1.6319093704223633, tv_loss: 0.014026706106960773\n",
      "iteration 100, dc_loss: 1.6243021488189697, tv_loss: 0.01409098133444786\n",
      "iteration 101, dc_loss: 1.6164470911026, tv_loss: 0.014209823682904243\n",
      "iteration 102, dc_loss: 1.6087734699249268, tv_loss: 0.014372486621141434\n",
      "iteration 103, dc_loss: 1.601331114768982, tv_loss: 0.014376560226082802\n",
      "iteration 104, dc_loss: 1.5937185287475586, tv_loss: 0.01446474064141512\n",
      "iteration 105, dc_loss: 1.5861021280288696, tv_loss: 0.014516392722725868\n",
      "iteration 106, dc_loss: 1.5786634683609009, tv_loss: 0.014606071636080742\n",
      "iteration 107, dc_loss: 1.5713368654251099, tv_loss: 0.014726251363754272\n",
      "iteration 108, dc_loss: 1.5640164613723755, tv_loss: 0.014834253117442131\n",
      "iteration 109, dc_loss: 1.556636929512024, tv_loss: 0.01490944717079401\n",
      "iteration 110, dc_loss: 1.5493502616882324, tv_loss: 0.01496909186244011\n",
      "iteration 111, dc_loss: 1.5420668125152588, tv_loss: 0.015050527639687061\n",
      "iteration 112, dc_loss: 1.5348386764526367, tv_loss: 0.015135681256651878\n",
      "iteration 113, dc_loss: 1.5277137756347656, tv_loss: 0.01517118327319622\n",
      "iteration 114, dc_loss: 1.5205552577972412, tv_loss: 0.015306341461837292\n",
      "iteration 115, dc_loss: 1.513593077659607, tv_loss: 0.015317876823246479\n",
      "iteration 116, dc_loss: 1.506637454032898, tv_loss: 0.015470207668840885\n",
      "iteration 117, dc_loss: 1.4998717308044434, tv_loss: 0.015478021465241909\n",
      "iteration 118, dc_loss: 1.493077278137207, tv_loss: 0.01563202776014805\n",
      "iteration 119, dc_loss: 1.4861946105957031, tv_loss: 0.01557956263422966\n",
      "iteration 120, dc_loss: 1.4790012836456299, tv_loss: 0.0157486405223608\n",
      "iteration 121, dc_loss: 1.4719288349151611, tv_loss: 0.01571410521864891\n",
      "iteration 122, dc_loss: 1.4649803638458252, tv_loss: 0.01580684818327427\n",
      "iteration 123, dc_loss: 1.4583052396774292, tv_loss: 0.01593363657593727\n",
      "iteration 124, dc_loss: 1.4518953561782837, tv_loss: 0.015937531366944313\n",
      "iteration 125, dc_loss: 1.4452269077301025, tv_loss: 0.016189824789762497\n",
      "iteration 126, dc_loss: 1.4384815692901611, tv_loss: 0.016079211607575417\n",
      "iteration 127, dc_loss: 1.4315686225891113, tv_loss: 0.016246160492300987\n",
      "iteration 128, dc_loss: 1.4250494241714478, tv_loss: 0.016323832795023918\n",
      "iteration 129, dc_loss: 1.418763279914856, tv_loss: 0.016313087195158005\n",
      "iteration 130, dc_loss: 1.412195086479187, tv_loss: 0.016482815146446228\n",
      "iteration 131, dc_loss: 1.405560851097107, tv_loss: 0.016427645459771156\n",
      "iteration 132, dc_loss: 1.3988749980926514, tv_loss: 0.01656241901218891\n",
      "iteration 133, dc_loss: 1.3924970626831055, tv_loss: 0.016649076715111732\n",
      "iteration 134, dc_loss: 1.386269450187683, tv_loss: 0.016636621206998825\n",
      "iteration 135, dc_loss: 1.3798669576644897, tv_loss: 0.016776910051703453\n",
      "iteration 136, dc_loss: 1.3735077381134033, tv_loss: 0.016749704256653786\n",
      "iteration 137, dc_loss: 1.3670798540115356, tv_loss: 0.016846243292093277\n",
      "iteration 138, dc_loss: 1.3607909679412842, tv_loss: 0.01694718562066555\n",
      "iteration 139, dc_loss: 1.3547632694244385, tv_loss: 0.0169923547655344\n",
      "iteration 140, dc_loss: 1.3487187623977661, tv_loss: 0.0171489417552948\n",
      "iteration 141, dc_loss: 1.3426896333694458, tv_loss: 0.017102817073464394\n",
      "iteration 142, dc_loss: 1.3362892866134644, tv_loss: 0.01724427007138729\n",
      "iteration 143, dc_loss: 1.329986810684204, tv_loss: 0.017226267606019974\n",
      "iteration 144, dc_loss: 1.3238184452056885, tv_loss: 0.01729312725365162\n",
      "iteration 145, dc_loss: 1.3178398609161377, tv_loss: 0.01737828738987446\n",
      "iteration 146, dc_loss: 1.3119834661483765, tv_loss: 0.01736351288855076\n",
      "iteration 147, dc_loss: 1.305907964706421, tv_loss: 0.017527902498841286\n",
      "iteration 148, dc_loss: 1.300013542175293, tv_loss: 0.017480256035923958\n",
      "iteration 149, dc_loss: 1.293969988822937, tv_loss: 0.017663253471255302\n",
      "iteration 150, dc_loss: 1.2881416082382202, tv_loss: 0.017678838223218918\n",
      "iteration 151, dc_loss: 1.2823742628097534, tv_loss: 0.017734145745635033\n",
      "iteration 152, dc_loss: 1.2766071557998657, tv_loss: 0.01788170449435711\n",
      "iteration 153, dc_loss: 1.271069884300232, tv_loss: 0.017746469005942345\n",
      "iteration 154, dc_loss: 1.2650636434555054, tv_loss: 0.01804102398455143\n",
      "iteration 155, dc_loss: 1.2593470811843872, tv_loss: 0.01783096045255661\n",
      "iteration 156, dc_loss: 1.2533389329910278, tv_loss: 0.018075836822390556\n",
      "iteration 157, dc_loss: 1.2479653358459473, tv_loss: 0.017996396869421005\n",
      "iteration 158, dc_loss: 1.2426369190216064, tv_loss: 0.018159111961722374\n",
      "iteration 159, dc_loss: 1.2372952699661255, tv_loss: 0.018129397183656693\n",
      "iteration 160, dc_loss: 1.2313799858093262, tv_loss: 0.0182588342577219\n",
      "iteration 161, dc_loss: 1.2254289388656616, tv_loss: 0.018174566328525543\n",
      "iteration 162, dc_loss: 1.2196028232574463, tv_loss: 0.01835872419178486\n",
      "iteration 163, dc_loss: 1.2144092321395874, tv_loss: 0.018362611532211304\n",
      "iteration 164, dc_loss: 1.2089413404464722, tv_loss: 0.018533987924456596\n",
      "iteration 165, dc_loss: 1.2031970024108887, tv_loss: 0.01855074055492878\n",
      "iteration 166, dc_loss: 1.197645664215088, tv_loss: 0.018528642132878304\n",
      "iteration 167, dc_loss: 1.1921976804733276, tv_loss: 0.01864553615450859\n",
      "iteration 168, dc_loss: 1.1869066953659058, tv_loss: 0.01859232224524021\n",
      "iteration 169, dc_loss: 1.181283950805664, tv_loss: 0.01870628260076046\n",
      "iteration 170, dc_loss: 1.1758558750152588, tv_loss: 0.018800940364599228\n",
      "iteration 171, dc_loss: 1.1707608699798584, tv_loss: 0.018825028091669083\n",
      "iteration 172, dc_loss: 1.1654473543167114, tv_loss: 0.019001491367816925\n",
      "iteration 173, dc_loss: 1.1601643562316895, tv_loss: 0.01887143775820732\n",
      "iteration 174, dc_loss: 1.1547083854675293, tv_loss: 0.01911376416683197\n",
      "iteration 175, dc_loss: 1.1499207019805908, tv_loss: 0.01889253966510296\n",
      "iteration 176, dc_loss: 1.1446435451507568, tv_loss: 0.019239110872149467\n",
      "iteration 177, dc_loss: 1.1398001909255981, tv_loss: 0.018932567909359932\n",
      "iteration 178, dc_loss: 1.1342813968658447, tv_loss: 0.019418707117438316\n",
      "iteration 179, dc_loss: 1.129454255104065, tv_loss: 0.01903412491083145\n",
      "iteration 180, dc_loss: 1.1238335371017456, tv_loss: 0.019450930878520012\n",
      "iteration 181, dc_loss: 1.1186896562576294, tv_loss: 0.019311299547553062\n",
      "iteration 182, dc_loss: 1.113705039024353, tv_loss: 0.019360896199941635\n",
      "iteration 183, dc_loss: 1.1086939573287964, tv_loss: 0.019597094506025314\n",
      "iteration 184, dc_loss: 1.104113221168518, tv_loss: 0.01926274597644806\n",
      "iteration 185, dc_loss: 1.098718285560608, tv_loss: 0.019837135449051857\n",
      "iteration 186, dc_loss: 1.0937986373901367, tv_loss: 0.019443631172180176\n",
      "iteration 187, dc_loss: 1.0889545679092407, tv_loss: 0.019626230001449585\n",
      "iteration 188, dc_loss: 1.0842393636703491, tv_loss: 0.019904527813196182\n",
      "iteration 189, dc_loss: 1.0800516605377197, tv_loss: 0.019700713455677032\n",
      "iteration 190, dc_loss: 1.0748624801635742, tv_loss: 0.019872086122632027\n",
      "iteration 191, dc_loss: 1.0696039199829102, tv_loss: 0.019833773374557495\n",
      "iteration 192, dc_loss: 1.0645533800125122, tv_loss: 0.019789399579167366\n",
      "iteration 193, dc_loss: 1.0595760345458984, tv_loss: 0.02007514424622059\n",
      "iteration 194, dc_loss: 1.0553810596466064, tv_loss: 0.01982271485030651\n",
      "iteration 195, dc_loss: 1.0503482818603516, tv_loss: 0.020212575793266296\n",
      "iteration 196, dc_loss: 1.0456211566925049, tv_loss: 0.020006608217954636\n",
      "iteration 197, dc_loss: 1.040575385093689, tv_loss: 0.020230207592248917\n",
      "iteration 198, dc_loss: 1.0359596014022827, tv_loss: 0.020259983837604523\n",
      "iteration 199, dc_loss: 1.0314241647720337, tv_loss: 0.020178675651550293\n",
      "iteration 200, dc_loss: 1.0264737606048584, tv_loss: 0.020352108404040337\n",
      "iteration 201, dc_loss: 1.0219793319702148, tv_loss: 0.02024342492222786\n",
      "iteration 202, dc_loss: 1.0172005891799927, tv_loss: 0.020461678504943848\n",
      "iteration 203, dc_loss: 1.0126901865005493, tv_loss: 0.020475678145885468\n",
      "iteration 204, dc_loss: 1.0080795288085938, tv_loss: 0.020462164655327797\n",
      "iteration 205, dc_loss: 1.003409743309021, tv_loss: 0.020621873438358307\n",
      "iteration 206, dc_loss: 0.9990411400794983, tv_loss: 0.020524829626083374\n",
      "iteration 207, dc_loss: 0.9945785999298096, tv_loss: 0.020524971187114716\n",
      "iteration 208, dc_loss: 0.9899995923042297, tv_loss: 0.020838944241404533\n",
      "iteration 209, dc_loss: 0.9863177537918091, tv_loss: 0.02046825923025608\n",
      "iteration 210, dc_loss: 0.9821609258651733, tv_loss: 0.02117243781685829\n",
      "iteration 211, dc_loss: 0.9783092141151428, tv_loss: 0.020487105473876\n",
      "iteration 212, dc_loss: 0.9726085066795349, tv_loss: 0.020940642803907394\n",
      "iteration 213, dc_loss: 0.9677607417106628, tv_loss: 0.020939989015460014\n",
      "iteration 214, dc_loss: 0.9643115401268005, tv_loss: 0.020659644156694412\n",
      "iteration 215, dc_loss: 0.9597682952880859, tv_loss: 0.021276457235217094\n",
      "iteration 216, dc_loss: 0.955553412437439, tv_loss: 0.020937565714120865\n",
      "iteration 217, dc_loss: 0.9510682225227356, tv_loss: 0.021168598905205727\n",
      "iteration 218, dc_loss: 0.9466021656990051, tv_loss: 0.021216195076704025\n",
      "iteration 219, dc_loss: 0.942500650882721, tv_loss: 0.020964624360203743\n",
      "iteration 220, dc_loss: 0.9379382133483887, tv_loss: 0.021327156573534012\n",
      "iteration 221, dc_loss: 0.9339138269424438, tv_loss: 0.02123751863837242\n",
      "iteration 222, dc_loss: 0.9294331073760986, tv_loss: 0.021382581442594528\n",
      "iteration 223, dc_loss: 0.9250596761703491, tv_loss: 0.02142328768968582\n",
      "iteration 224, dc_loss: 0.9209933876991272, tv_loss: 0.02131270430982113\n",
      "iteration 225, dc_loss: 0.9165456295013428, tv_loss: 0.0214979350566864\n",
      "iteration 226, dc_loss: 0.9125190377235413, tv_loss: 0.021405044943094254\n",
      "iteration 227, dc_loss: 0.9081652760505676, tv_loss: 0.021560225635766983\n",
      "iteration 228, dc_loss: 0.9040466547012329, tv_loss: 0.021610485389828682\n",
      "iteration 229, dc_loss: 0.8999132513999939, tv_loss: 0.02160518802702427\n",
      "iteration 230, dc_loss: 0.8957958817481995, tv_loss: 0.0217154361307621\n",
      "iteration 231, dc_loss: 0.8917657136917114, tv_loss: 0.021707359701395035\n",
      "iteration 232, dc_loss: 0.8877012133598328, tv_loss: 0.021724721416831017\n",
      "iteration 233, dc_loss: 0.8836019039154053, tv_loss: 0.021802889183163643\n",
      "iteration 234, dc_loss: 0.8796404600143433, tv_loss: 0.021829454228281975\n",
      "iteration 235, dc_loss: 0.8760252594947815, tv_loss: 0.0218926090747118\n",
      "iteration 236, dc_loss: 0.8725520968437195, tv_loss: 0.02186562307178974\n",
      "iteration 237, dc_loss: 0.8695732951164246, tv_loss: 0.022059351205825806\n",
      "iteration 238, dc_loss: 0.8657442927360535, tv_loss: 0.02173496037721634\n",
      "iteration 239, dc_loss: 0.8611003756523132, tv_loss: 0.022161392495036125\n",
      "iteration 240, dc_loss: 0.8561803698539734, tv_loss: 0.022017421200871468\n",
      "iteration 241, dc_loss: 0.8524070382118225, tv_loss: 0.0221486184746027\n",
      "iteration 242, dc_loss: 0.8490937352180481, tv_loss: 0.0222761407494545\n",
      "iteration 243, dc_loss: 0.8452295064926147, tv_loss: 0.021954411640763283\n",
      "iteration 244, dc_loss: 0.8405786156654358, tv_loss: 0.022376924753189087\n",
      "iteration 245, dc_loss: 0.8366492390632629, tv_loss: 0.022499145939946175\n",
      "iteration 246, dc_loss: 0.83318030834198, tv_loss: 0.022145751863718033\n",
      "iteration 247, dc_loss: 0.8292444944381714, tv_loss: 0.022473523393273354\n",
      "iteration 248, dc_loss: 0.825433075428009, tv_loss: 0.022617287933826447\n",
      "iteration 249, dc_loss: 0.82146817445755, tv_loss: 0.022384177893400192\n",
      "iteration 250, dc_loss: 0.8176572918891907, tv_loss: 0.022391846403479576\n",
      "iteration 251, dc_loss: 0.81401127576828, tv_loss: 0.022582629695534706\n",
      "iteration 252, dc_loss: 0.8104547262191772, tv_loss: 0.0226883627474308\n",
      "iteration 253, dc_loss: 0.8066760897636414, tv_loss: 0.02254670485854149\n",
      "iteration 254, dc_loss: 0.8026877045631409, tv_loss: 0.022572720423340797\n",
      "iteration 255, dc_loss: 0.7990140318870544, tv_loss: 0.022552968934178352\n",
      "iteration 256, dc_loss: 0.7954142093658447, tv_loss: 0.022724127396941185\n",
      "iteration 257, dc_loss: 0.7920157313346863, tv_loss: 0.022795962169766426\n",
      "iteration 258, dc_loss: 0.7884217500686646, tv_loss: 0.0229549091309309\n",
      "iteration 259, dc_loss: 0.785142719745636, tv_loss: 0.022748567163944244\n",
      "iteration 260, dc_loss: 0.7813911437988281, tv_loss: 0.023045865818858147\n",
      "iteration 261, dc_loss: 0.7783252596855164, tv_loss: 0.022624721750617027\n",
      "iteration 262, dc_loss: 0.7745985984802246, tv_loss: 0.023405082523822784\n",
      "iteration 263, dc_loss: 0.7717475295066833, tv_loss: 0.0226618479937315\n",
      "iteration 264, dc_loss: 0.7675761580467224, tv_loss: 0.023371653631329536\n",
      "iteration 265, dc_loss: 0.763891339302063, tv_loss: 0.022946538403630257\n",
      "iteration 266, dc_loss: 0.7603880167007446, tv_loss: 0.023061243817210197\n",
      "iteration 267, dc_loss: 0.7568688988685608, tv_loss: 0.02339118719100952\n",
      "iteration 268, dc_loss: 0.7543268203735352, tv_loss: 0.022877097129821777\n",
      "iteration 269, dc_loss: 0.7500696182250977, tv_loss: 0.023561954498291016\n",
      "iteration 270, dc_loss: 0.7465542554855347, tv_loss: 0.023129824548959732\n",
      "iteration 271, dc_loss: 0.7432419657707214, tv_loss: 0.023226022720336914\n",
      "iteration 272, dc_loss: 0.7400918006896973, tv_loss: 0.02362397313117981\n",
      "iteration 273, dc_loss: 0.7370187044143677, tv_loss: 0.02328386716544628\n",
      "iteration 274, dc_loss: 0.732950747013092, tv_loss: 0.023493941873311996\n",
      "iteration 275, dc_loss: 0.7293630838394165, tv_loss: 0.023481333628296852\n",
      "iteration 276, dc_loss: 0.7265354990959167, tv_loss: 0.023283587768673897\n",
      "iteration 277, dc_loss: 0.7229023575782776, tv_loss: 0.023887034505605698\n",
      "iteration 278, dc_loss: 0.7197321057319641, tv_loss: 0.02340824156999588\n",
      "iteration 279, dc_loss: 0.7161083221435547, tv_loss: 0.023512903600931168\n",
      "iteration 280, dc_loss: 0.7128522396087646, tv_loss: 0.023769840598106384\n",
      "iteration 281, dc_loss: 0.7101062536239624, tv_loss: 0.023635417222976685\n",
      "iteration 282, dc_loss: 0.7066723108291626, tv_loss: 0.023900866508483887\n",
      "iteration 283, dc_loss: 0.7035946249961853, tv_loss: 0.023616604506969452\n",
      "iteration 284, dc_loss: 0.7003887295722961, tv_loss: 0.02386322245001793\n",
      "iteration 285, dc_loss: 0.6971526741981506, tv_loss: 0.023907482624053955\n",
      "iteration 286, dc_loss: 0.6941274404525757, tv_loss: 0.023767322301864624\n",
      "iteration 287, dc_loss: 0.6906416416168213, tv_loss: 0.02385735884308815\n",
      "iteration 288, dc_loss: 0.6872723698616028, tv_loss: 0.023925146088004112\n",
      "iteration 289, dc_loss: 0.6841309070587158, tv_loss: 0.024036629125475883\n",
      "iteration 290, dc_loss: 0.6810309290885925, tv_loss: 0.02413419634103775\n",
      "iteration 291, dc_loss: 0.6784105896949768, tv_loss: 0.023936143144965172\n",
      "iteration 292, dc_loss: 0.6749100685119629, tv_loss: 0.024214990437030792\n",
      "iteration 293, dc_loss: 0.6718581318855286, tv_loss: 0.024139536544680595\n",
      "iteration 294, dc_loss: 0.6687437891960144, tv_loss: 0.024053702130913734\n",
      "iteration 295, dc_loss: 0.6656522750854492, tv_loss: 0.024343807250261307\n",
      "iteration 296, dc_loss: 0.6635684370994568, tv_loss: 0.023878008127212524\n",
      "iteration 297, dc_loss: 0.6609351634979248, tv_loss: 0.024753907695412636\n",
      "iteration 298, dc_loss: 0.6578125357627869, tv_loss: 0.023903632536530495\n",
      "iteration 299, dc_loss: 0.6534397602081299, tv_loss: 0.024307506158947945\n",
      "iteration 300, dc_loss: 0.650357186794281, tv_loss: 0.024360839277505875\n",
      "iteration 301, dc_loss: 0.6479610204696655, tv_loss: 0.024122487753629684\n",
      "iteration 302, dc_loss: 0.6444768905639648, tv_loss: 0.02476530522108078\n",
      "iteration 303, dc_loss: 0.6415322422981262, tv_loss: 0.024358583614230156\n",
      "iteration 304, dc_loss: 0.6385924220085144, tv_loss: 0.02434227615594864\n",
      "iteration 305, dc_loss: 0.6352585554122925, tv_loss: 0.02452888898551464\n",
      "iteration 306, dc_loss: 0.6323694586753845, tv_loss: 0.0244168508797884\n",
      "iteration 307, dc_loss: 0.6292970776557922, tv_loss: 0.024696385487914085\n",
      "iteration 308, dc_loss: 0.6262969374656677, tv_loss: 0.02460067719221115\n",
      "iteration 309, dc_loss: 0.6232789754867554, tv_loss: 0.024575700983405113\n",
      "iteration 310, dc_loss: 0.6204749345779419, tv_loss: 0.024678286164999008\n",
      "iteration 311, dc_loss: 0.617782473564148, tv_loss: 0.02470284327864647\n",
      "iteration 312, dc_loss: 0.6146829724311829, tv_loss: 0.024867579340934753\n",
      "iteration 313, dc_loss: 0.6119000315666199, tv_loss: 0.024690553545951843\n",
      "iteration 314, dc_loss: 0.6089415550231934, tv_loss: 0.024805130437016487\n",
      "iteration 315, dc_loss: 0.6059867739677429, tv_loss: 0.024940824136137962\n",
      "iteration 316, dc_loss: 0.6033902764320374, tv_loss: 0.02472785860300064\n",
      "iteration 317, dc_loss: 0.6004058122634888, tv_loss: 0.02504967339336872\n",
      "iteration 318, dc_loss: 0.5980126261711121, tv_loss: 0.024776889011263847\n",
      "iteration 319, dc_loss: 0.5950317978858948, tv_loss: 0.025165608152747154\n",
      "iteration 320, dc_loss: 0.5926727056503296, tv_loss: 0.02483178861439228\n",
      "iteration 321, dc_loss: 0.590155839920044, tv_loss: 0.025094086304306984\n",
      "iteration 322, dc_loss: 0.5878551006317139, tv_loss: 0.02493639849126339\n",
      "iteration 323, dc_loss: 0.5853167772293091, tv_loss: 0.025151461362838745\n",
      "iteration 324, dc_loss: 0.5820910334587097, tv_loss: 0.02498895861208439\n",
      "iteration 325, dc_loss: 0.5787456035614014, tv_loss: 0.025099314749240875\n",
      "iteration 326, dc_loss: 0.575839638710022, tv_loss: 0.025086456909775734\n",
      "iteration 327, dc_loss: 0.5734903216362, tv_loss: 0.025299204513430595\n",
      "iteration 328, dc_loss: 0.5714715123176575, tv_loss: 0.025458132848143578\n",
      "iteration 329, dc_loss: 0.5690398812294006, tv_loss: 0.025157136842608452\n",
      "iteration 330, dc_loss: 0.5660445690155029, tv_loss: 0.025529077276587486\n",
      "iteration 331, dc_loss: 0.5632784366607666, tv_loss: 0.02518903650343418\n",
      "iteration 332, dc_loss: 0.5603606700897217, tv_loss: 0.025618575513362885\n",
      "iteration 333, dc_loss: 0.5583820939064026, tv_loss: 0.02530854195356369\n",
      "iteration 334, dc_loss: 0.555670440196991, tv_loss: 0.025604719296097755\n",
      "iteration 335, dc_loss: 0.5531193614006042, tv_loss: 0.025419961661100388\n",
      "iteration 336, dc_loss: 0.5500421524047852, tv_loss: 0.02550705336034298\n",
      "iteration 337, dc_loss: 0.5473206043243408, tv_loss: 0.025678478181362152\n",
      "iteration 338, dc_loss: 0.5451880693435669, tv_loss: 0.025466393679380417\n",
      "iteration 339, dc_loss: 0.5427624583244324, tv_loss: 0.025756875053048134\n",
      "iteration 340, dc_loss: 0.5405128002166748, tv_loss: 0.025582091882824898\n",
      "iteration 341, dc_loss: 0.5378016233444214, tv_loss: 0.025646356865763664\n",
      "iteration 342, dc_loss: 0.5351638793945312, tv_loss: 0.025923388078808784\n",
      "iteration 343, dc_loss: 0.5333424210548401, tv_loss: 0.02550438977777958\n",
      "iteration 344, dc_loss: 0.5310317873954773, tv_loss: 0.02618703991174698\n",
      "iteration 345, dc_loss: 0.5291391015052795, tv_loss: 0.025432202965021133\n",
      "iteration 346, dc_loss: 0.5257944464683533, tv_loss: 0.026176802814006805\n",
      "iteration 347, dc_loss: 0.5230550765991211, tv_loss: 0.025748493149876595\n",
      "iteration 348, dc_loss: 0.5205837488174438, tv_loss: 0.025836734101176262\n",
      "iteration 349, dc_loss: 0.518242597579956, tv_loss: 0.02613675594329834\n",
      "iteration 350, dc_loss: 0.5164346098899841, tv_loss: 0.02564512938261032\n",
      "iteration 351, dc_loss: 0.513381838798523, tv_loss: 0.026199933141469955\n",
      "iteration 352, dc_loss: 0.5111005306243896, tv_loss: 0.02588728815317154\n",
      "iteration 353, dc_loss: 0.508933424949646, tv_loss: 0.02607845887541771\n",
      "iteration 354, dc_loss: 0.5068420171737671, tv_loss: 0.02617744915187359\n",
      "iteration 355, dc_loss: 0.5049360990524292, tv_loss: 0.026020251214504242\n",
      "iteration 356, dc_loss: 0.5022169947624207, tv_loss: 0.026095755398273468\n",
      "iteration 357, dc_loss: 0.49968940019607544, tv_loss: 0.02624444104731083\n",
      "iteration 358, dc_loss: 0.497843474149704, tv_loss: 0.02590211294591427\n",
      "iteration 359, dc_loss: 0.49554550647735596, tv_loss: 0.02659350261092186\n",
      "iteration 360, dc_loss: 0.49354737997055054, tv_loss: 0.025906462222337723\n",
      "iteration 361, dc_loss: 0.49037298560142517, tv_loss: 0.026420313864946365\n",
      "iteration 362, dc_loss: 0.48800310492515564, tv_loss: 0.026213211938738823\n",
      "iteration 363, dc_loss: 0.4860284924507141, tv_loss: 0.026229824870824814\n",
      "iteration 364, dc_loss: 0.4838186800479889, tv_loss: 0.026654433459043503\n",
      "iteration 365, dc_loss: 0.4820181727409363, tv_loss: 0.026187142357230186\n",
      "iteration 366, dc_loss: 0.47939789295196533, tv_loss: 0.02660670131444931\n",
      "iteration 367, dc_loss: 0.47751739621162415, tv_loss: 0.026326294988393784\n",
      "iteration 368, dc_loss: 0.47584471106529236, tv_loss: 0.026557309553027153\n",
      "iteration 369, dc_loss: 0.47373583912849426, tv_loss: 0.026476403698325157\n",
      "iteration 370, dc_loss: 0.47143757343292236, tv_loss: 0.026537282392382622\n",
      "iteration 371, dc_loss: 0.468514621257782, tv_loss: 0.026446327567100525\n",
      "iteration 372, dc_loss: 0.46598467230796814, tv_loss: 0.026627976447343826\n",
      "iteration 373, dc_loss: 0.4643213450908661, tv_loss: 0.026571830734610558\n",
      "iteration 374, dc_loss: 0.46233633160591125, tv_loss: 0.02677781693637371\n",
      "iteration 375, dc_loss: 0.4602959156036377, tv_loss: 0.026612011715769768\n",
      "iteration 376, dc_loss: 0.457795649766922, tv_loss: 0.02660667896270752\n",
      "iteration 377, dc_loss: 0.4554423987865448, tv_loss: 0.02683515101671219\n",
      "iteration 378, dc_loss: 0.4538843333721161, tv_loss: 0.026591649278998375\n",
      "iteration 379, dc_loss: 0.45169201493263245, tv_loss: 0.0270380899310112\n",
      "iteration 380, dc_loss: 0.44982701539993286, tv_loss: 0.026603180915117264\n",
      "iteration 381, dc_loss: 0.4471570551395416, tv_loss: 0.026930565014481544\n",
      "iteration 382, dc_loss: 0.44519472122192383, tv_loss: 0.02678041346371174\n",
      "iteration 383, dc_loss: 0.4432680308818817, tv_loss: 0.02692410908639431\n",
      "iteration 384, dc_loss: 0.4413905143737793, tv_loss: 0.02696795016527176\n",
      "iteration 385, dc_loss: 0.43950873613357544, tv_loss: 0.02691272832453251\n",
      "iteration 386, dc_loss: 0.4372421205043793, tv_loss: 0.026939351111650467\n",
      "iteration 387, dc_loss: 0.4350537359714508, tv_loss: 0.02705416828393936\n",
      "iteration 388, dc_loss: 0.4333685636520386, tv_loss: 0.026903385296463966\n",
      "iteration 389, dc_loss: 0.43144339323043823, tv_loss: 0.027309060096740723\n",
      "iteration 390, dc_loss: 0.4303644597530365, tv_loss: 0.02672724425792694\n",
      "iteration 391, dc_loss: 0.4284264147281647, tv_loss: 0.027478300034999847\n",
      "iteration 392, dc_loss: 0.42719921469688416, tv_loss: 0.02667699009180069\n",
      "iteration 393, dc_loss: 0.4256041347980499, tv_loss: 0.027490846812725067\n",
      "iteration 394, dc_loss: 0.4230402112007141, tv_loss: 0.02716892585158348\n",
      "iteration 395, dc_loss: 0.420369416475296, tv_loss: 0.027192311361432076\n",
      "iteration 396, dc_loss: 0.4177823066711426, tv_loss: 0.027400817722082138\n",
      "iteration 397, dc_loss: 0.4166609048843384, tv_loss: 0.026910893619060516\n",
      "iteration 398, dc_loss: 0.4148487448692322, tv_loss: 0.02748972550034523\n",
      "iteration 399, dc_loss: 0.4128175675868988, tv_loss: 0.02724536508321762\n",
      "iteration 400, dc_loss: 0.41046836972236633, tv_loss: 0.02736085280776024\n",
      "iteration 401, dc_loss: 0.40815725922584534, tv_loss: 0.0275247972458601\n",
      "iteration 402, dc_loss: 0.40696585178375244, tv_loss: 0.027216214686632156\n",
      "iteration 403, dc_loss: 0.40534311532974243, tv_loss: 0.02729775756597519\n",
      "iteration 404, dc_loss: 0.4036804139614105, tv_loss: 0.02758995071053505\n",
      "iteration 405, dc_loss: 0.40239429473876953, tv_loss: 0.027292169630527496\n",
      "iteration 406, dc_loss: 0.4009387493133545, tv_loss: 0.02733500674366951\n",
      "iteration 407, dc_loss: 0.3992673456668854, tv_loss: 0.027633972465991974\n",
      "iteration 408, dc_loss: 0.3978952169418335, tv_loss: 0.027449967339634895\n",
      "iteration 409, dc_loss: 0.3966016173362732, tv_loss: 0.02729862369596958\n",
      "iteration 410, dc_loss: 0.39491215348243713, tv_loss: 0.02754494547843933\n",
      "iteration 411, dc_loss: 0.39355596899986267, tv_loss: 0.027640849351882935\n",
      "iteration 412, dc_loss: 0.39224883913993835, tv_loss: 0.02746874839067459\n",
      "iteration 413, dc_loss: 0.3905852735042572, tv_loss: 0.02756105177104473\n",
      "iteration 414, dc_loss: 0.3892524242401123, tv_loss: 0.027547648176550865\n",
      "iteration 415, dc_loss: 0.38794028759002686, tv_loss: 0.027466826140880585\n",
      "iteration 416, dc_loss: 0.3863501250743866, tv_loss: 0.027583038434386253\n",
      "iteration 417, dc_loss: 0.3849990665912628, tv_loss: 0.02761066146194935\n",
      "iteration 418, dc_loss: 0.3837181329727173, tv_loss: 0.027617715299129486\n",
      "iteration 419, dc_loss: 0.38223376870155334, tv_loss: 0.027814390137791634\n",
      "iteration 420, dc_loss: 0.3807982802391052, tv_loss: 0.02770186774432659\n",
      "iteration 421, dc_loss: 0.3794916868209839, tv_loss: 0.027624381706118584\n",
      "iteration 422, dc_loss: 0.3780770003795624, tv_loss: 0.027679376304149628\n",
      "iteration 423, dc_loss: 0.3766475319862366, tv_loss: 0.02782837301492691\n",
      "iteration 424, dc_loss: 0.3753853142261505, tv_loss: 0.027863606810569763\n",
      "iteration 425, dc_loss: 0.3739962577819824, tv_loss: 0.027765564620494843\n",
      "iteration 426, dc_loss: 0.37257450819015503, tv_loss: 0.02781844325363636\n",
      "iteration 427, dc_loss: 0.3713354766368866, tv_loss: 0.027711639180779457\n",
      "iteration 428, dc_loss: 0.3698936998844147, tv_loss: 0.02781200036406517\n",
      "iteration 429, dc_loss: 0.36856478452682495, tv_loss: 0.027856046333909035\n",
      "iteration 430, dc_loss: 0.3673214912414551, tv_loss: 0.02798089198768139\n",
      "iteration 431, dc_loss: 0.3659154772758484, tv_loss: 0.028019798919558525\n",
      "iteration 432, dc_loss: 0.36467015743255615, tv_loss: 0.027906175702810287\n",
      "iteration 433, dc_loss: 0.3632740080356598, tv_loss: 0.02797224000096321\n",
      "iteration 434, dc_loss: 0.3619650900363922, tv_loss: 0.02805306203663349\n",
      "iteration 435, dc_loss: 0.3606804609298706, tv_loss: 0.028053728863596916\n",
      "iteration 436, dc_loss: 0.3593606948852539, tv_loss: 0.028047936037182808\n",
      "iteration 437, dc_loss: 0.35806387662887573, tv_loss: 0.028058834373950958\n",
      "iteration 438, dc_loss: 0.3567827641963959, tv_loss: 0.02808854728937149\n",
      "iteration 439, dc_loss: 0.3554905951023102, tv_loss: 0.028147317469120026\n",
      "iteration 440, dc_loss: 0.35419753193855286, tv_loss: 0.028124086558818817\n",
      "iteration 441, dc_loss: 0.3529091477394104, tv_loss: 0.028156988322734833\n",
      "iteration 442, dc_loss: 0.35168033838272095, tv_loss: 0.028166618198156357\n",
      "iteration 443, dc_loss: 0.3503774404525757, tv_loss: 0.028234517201781273\n",
      "iteration 444, dc_loss: 0.34918251633644104, tv_loss: 0.028179561719298363\n",
      "iteration 445, dc_loss: 0.3478499948978424, tv_loss: 0.02826840430498123\n",
      "iteration 446, dc_loss: 0.34672608971595764, tv_loss: 0.028216244652867317\n",
      "iteration 447, dc_loss: 0.3453744649887085, tv_loss: 0.028366953134536743\n",
      "iteration 448, dc_loss: 0.3442869484424591, tv_loss: 0.028195010498166084\n",
      "iteration 449, dc_loss: 0.3429199755191803, tv_loss: 0.028385795652866364\n",
      "iteration 450, dc_loss: 0.34188559651374817, tv_loss: 0.028282837942242622\n",
      "iteration 451, dc_loss: 0.34050071239471436, tv_loss: 0.028424793854355812\n",
      "iteration 452, dc_loss: 0.3394726514816284, tv_loss: 0.028259877115488052\n",
      "iteration 453, dc_loss: 0.3380748927593231, tv_loss: 0.028533417731523514\n",
      "iteration 454, dc_loss: 0.33708661794662476, tv_loss: 0.028294580057263374\n",
      "iteration 455, dc_loss: 0.3356361985206604, tv_loss: 0.028541825711727142\n",
      "iteration 456, dc_loss: 0.3346027731895447, tv_loss: 0.028350191190838814\n",
      "iteration 457, dc_loss: 0.33316776156425476, tv_loss: 0.02853855863213539\n",
      "iteration 458, dc_loss: 0.332061231136322, tv_loss: 0.028408732265233994\n",
      "iteration 459, dc_loss: 0.33072707056999207, tv_loss: 0.028553778305649757\n",
      "iteration 460, dc_loss: 0.32957372069358826, tv_loss: 0.028504924848675728\n",
      "iteration 461, dc_loss: 0.3283608555793762, tv_loss: 0.02851482294499874\n",
      "iteration 462, dc_loss: 0.32717567682266235, tv_loss: 0.028568711131811142\n",
      "iteration 463, dc_loss: 0.3260604739189148, tv_loss: 0.02855125069618225\n",
      "iteration 464, dc_loss: 0.3248405158519745, tv_loss: 0.02864426001906395\n",
      "iteration 465, dc_loss: 0.3238762617111206, tv_loss: 0.028537403792142868\n",
      "iteration 466, dc_loss: 0.32264286279678345, tv_loss: 0.028738100081682205\n",
      "iteration 467, dc_loss: 0.3219214677810669, tv_loss: 0.02847183309495449\n",
      "iteration 468, dc_loss: 0.3206959664821625, tv_loss: 0.028883354738354683\n",
      "iteration 469, dc_loss: 0.3200777471065521, tv_loss: 0.028438646346330643\n",
      "iteration 470, dc_loss: 0.3185959756374359, tv_loss: 0.028921548277139664\n",
      "iteration 471, dc_loss: 0.31749603152275085, tv_loss: 0.028502903878688812\n",
      "iteration 472, dc_loss: 0.3157729506492615, tv_loss: 0.02879461459815502\n",
      "iteration 473, dc_loss: 0.314549058675766, tv_loss: 0.028738021850585938\n",
      "iteration 474, dc_loss: 0.31353047490119934, tv_loss: 0.0287492536008358\n",
      "iteration 475, dc_loss: 0.312406986951828, tv_loss: 0.028924282640218735\n",
      "iteration 476, dc_loss: 0.3117309808731079, tv_loss: 0.028592590242624283\n",
      "iteration 477, dc_loss: 0.3103742003440857, tv_loss: 0.028969304636120796\n",
      "iteration 478, dc_loss: 0.3092978298664093, tv_loss: 0.02873683162033558\n",
      "iteration 479, dc_loss: 0.30783286690711975, tv_loss: 0.028936710208654404\n",
      "iteration 480, dc_loss: 0.3066767752170563, tv_loss: 0.028857892379164696\n",
      "iteration 481, dc_loss: 0.3056620657444, tv_loss: 0.028805430978536606\n",
      "iteration 482, dc_loss: 0.3045017719268799, tv_loss: 0.028987698256969452\n",
      "iteration 483, dc_loss: 0.30369046330451965, tv_loss: 0.028837015852332115\n",
      "iteration 484, dc_loss: 0.3026142716407776, tv_loss: 0.02903429977595806\n",
      "iteration 485, dc_loss: 0.3015112578868866, tv_loss: 0.028876863420009613\n",
      "iteration 486, dc_loss: 0.30024945735931396, tv_loss: 0.028976358473300934\n",
      "iteration 487, dc_loss: 0.2991344928741455, tv_loss: 0.02901763655245304\n",
      "iteration 488, dc_loss: 0.2981388568878174, tv_loss: 0.028958195820450783\n",
      "iteration 489, dc_loss: 0.2969886064529419, tv_loss: 0.02911541238427162\n",
      "iteration 490, dc_loss: 0.29623278975486755, tv_loss: 0.028909754008054733\n",
      "iteration 491, dc_loss: 0.2950444519519806, tv_loss: 0.029183894395828247\n",
      "iteration 492, dc_loss: 0.29424774646759033, tv_loss: 0.02892153151333332\n",
      "iteration 493, dc_loss: 0.2932296395301819, tv_loss: 0.029258066788315773\n",
      "iteration 494, dc_loss: 0.2921532690525055, tv_loss: 0.02902296744287014\n",
      "iteration 495, dc_loss: 0.2908811867237091, tv_loss: 0.02920779399573803\n",
      "iteration 496, dc_loss: 0.2898807227611542, tv_loss: 0.029106900095939636\n",
      "iteration 497, dc_loss: 0.2888035476207733, tv_loss: 0.029153453186154366\n",
      "iteration 498, dc_loss: 0.28777703642845154, tv_loss: 0.02922547422349453\n",
      "iteration 499, dc_loss: 0.2867280840873718, tv_loss: 0.02915489859879017\n",
      "iteration 500, dc_loss: 0.2857033610343933, tv_loss: 0.029236191883683205\n",
      "iteration 501, dc_loss: 0.28472477197647095, tv_loss: 0.029157962650060654\n",
      "iteration 502, dc_loss: 0.28351330757141113, tv_loss: 0.02930615097284317\n",
      "iteration 503, dc_loss: 0.2827470302581787, tv_loss: 0.0291510671377182\n",
      "iteration 504, dc_loss: 0.2814774215221405, tv_loss: 0.029389502480626106\n",
      "iteration 505, dc_loss: 0.28064361214637756, tv_loss: 0.02918729931116104\n",
      "iteration 506, dc_loss: 0.2794845402240753, tv_loss: 0.029360756278038025\n",
      "iteration 507, dc_loss: 0.27859994769096375, tv_loss: 0.02920910343527794\n",
      "iteration 508, dc_loss: 0.2774980962276459, tv_loss: 0.029429350048303604\n",
      "iteration 509, dc_loss: 0.27660927176475525, tv_loss: 0.029283177107572556\n",
      "iteration 510, dc_loss: 0.27564725279808044, tv_loss: 0.0293751023709774\n",
      "iteration 511, dc_loss: 0.2748073637485504, tv_loss: 0.02943240851163864\n",
      "iteration 512, dc_loss: 0.27395427227020264, tv_loss: 0.02930598147213459\n",
      "iteration 513, dc_loss: 0.2729732096195221, tv_loss: 0.0295329038053751\n",
      "iteration 514, dc_loss: 0.27213335037231445, tv_loss: 0.029338650405406952\n",
      "iteration 515, dc_loss: 0.2710568904876709, tv_loss: 0.02959972247481346\n",
      "iteration 516, dc_loss: 0.2706594169139862, tv_loss: 0.0292042288929224\n",
      "iteration 517, dc_loss: 0.2697015702724457, tv_loss: 0.029719164595007896\n",
      "iteration 518, dc_loss: 0.26925283670425415, tv_loss: 0.029286926612257957\n",
      "iteration 519, dc_loss: 0.267579585313797, tv_loss: 0.029750769957900047\n",
      "iteration 520, dc_loss: 0.2666594684123993, tv_loss: 0.029446253553032875\n",
      "iteration 521, dc_loss: 0.2658534646034241, tv_loss: 0.02951671928167343\n",
      "iteration 522, dc_loss: 0.2647072672843933, tv_loss: 0.029731061309576035\n",
      "iteration 523, dc_loss: 0.2640921175479889, tv_loss: 0.029412755742669106\n",
      "iteration 524, dc_loss: 0.26280924677848816, tv_loss: 0.02970597892999649\n",
      "iteration 525, dc_loss: 0.26192182302474976, tv_loss: 0.029491176828742027\n",
      "iteration 526, dc_loss: 0.26075270771980286, tv_loss: 0.02959422580897808\n",
      "iteration 527, dc_loss: 0.2599300444126129, tv_loss: 0.029825206845998764\n",
      "iteration 528, dc_loss: 0.25926530361175537, tv_loss: 0.02948177047073841\n",
      "iteration 529, dc_loss: 0.25785067677497864, tv_loss: 0.029784074053168297\n",
      "iteration 530, dc_loss: 0.25707611441612244, tv_loss: 0.029573528096079826\n",
      "iteration 531, dc_loss: 0.25621500611305237, tv_loss: 0.02956380322575569\n",
      "iteration 532, dc_loss: 0.25511032342910767, tv_loss: 0.029954900965094566\n",
      "iteration 533, dc_loss: 0.2544185519218445, tv_loss: 0.029573125764727592\n",
      "iteration 534, dc_loss: 0.25331804156303406, tv_loss: 0.0296761654317379\n",
      "iteration 535, dc_loss: 0.25241684913635254, tv_loss: 0.029732555150985718\n",
      "iteration 536, dc_loss: 0.25159916281700134, tv_loss: 0.029717164114117622\n",
      "iteration 537, dc_loss: 0.25057148933410645, tv_loss: 0.029897453263401985\n",
      "iteration 538, dc_loss: 0.24981410801410675, tv_loss: 0.02975461632013321\n",
      "iteration 539, dc_loss: 0.24892733991146088, tv_loss: 0.02974509820342064\n",
      "iteration 540, dc_loss: 0.2479487508535385, tv_loss: 0.029777556657791138\n",
      "iteration 541, dc_loss: 0.24711431562900543, tv_loss: 0.029783954843878746\n",
      "iteration 542, dc_loss: 0.2463001012802124, tv_loss: 0.029901983216404915\n",
      "iteration 543, dc_loss: 0.24538744986057281, tv_loss: 0.02992507629096508\n",
      "iteration 544, dc_loss: 0.24440324306488037, tv_loss: 0.0299264807254076\n",
      "iteration 545, dc_loss: 0.24373485147953033, tv_loss: 0.02970101311802864\n",
      "iteration 546, dc_loss: 0.24260316789150238, tv_loss: 0.029923902824521065\n",
      "iteration 547, dc_loss: 0.24184632301330566, tv_loss: 0.029805338010191917\n",
      "iteration 548, dc_loss: 0.2409752458333969, tv_loss: 0.029833143576979637\n",
      "iteration 549, dc_loss: 0.2400740385055542, tv_loss: 0.02988447993993759\n",
      "iteration 550, dc_loss: 0.23926006257534027, tv_loss: 0.029851509258151054\n",
      "iteration 551, dc_loss: 0.23837032914161682, tv_loss: 0.029912373051047325\n",
      "iteration 552, dc_loss: 0.23761418461799622, tv_loss: 0.029917558655142784\n",
      "iteration 553, dc_loss: 0.23673896491527557, tv_loss: 0.030203815549612045\n",
      "iteration 554, dc_loss: 0.23598404228687286, tv_loss: 0.029926398769021034\n",
      "iteration 555, dc_loss: 0.23505614697933197, tv_loss: 0.03003092296421528\n",
      "iteration 556, dc_loss: 0.234467014670372, tv_loss: 0.029917532578110695\n",
      "iteration 557, dc_loss: 0.23345385491847992, tv_loss: 0.03034001775085926\n",
      "iteration 558, dc_loss: 0.23282526433467865, tv_loss: 0.029943494126200676\n",
      "iteration 559, dc_loss: 0.23187808692455292, tv_loss: 0.030130280181765556\n",
      "iteration 560, dc_loss: 0.2311476320028305, tv_loss: 0.030338911339640617\n",
      "iteration 561, dc_loss: 0.23024922609329224, tv_loss: 0.030123017728328705\n",
      "iteration 562, dc_loss: 0.2296195924282074, tv_loss: 0.030390862375497818\n",
      "iteration 563, dc_loss: 0.2287183403968811, tv_loss: 0.03005753457546234\n",
      "iteration 564, dc_loss: 0.22791124880313873, tv_loss: 0.030564915388822556\n",
      "iteration 565, dc_loss: 0.22727608680725098, tv_loss: 0.030031578615307808\n",
      "iteration 566, dc_loss: 0.22646473348140717, tv_loss: 0.03066425770521164\n",
      "iteration 567, dc_loss: 0.22603504359722137, tv_loss: 0.030001191422343254\n",
      "iteration 568, dc_loss: 0.22523733973503113, tv_loss: 0.030949551612138748\n",
      "iteration 569, dc_loss: 0.22483029961585999, tv_loss: 0.030148526653647423\n",
      "iteration 570, dc_loss: 0.22376205027103424, tv_loss: 0.030864020809531212\n",
      "iteration 571, dc_loss: 0.2230314165353775, tv_loss: 0.030227700248360634\n",
      "iteration 572, dc_loss: 0.22179895639419556, tv_loss: 0.03072933666408062\n",
      "iteration 573, dc_loss: 0.2208181619644165, tv_loss: 0.03051934391260147\n",
      "iteration 574, dc_loss: 0.22050058841705322, tv_loss: 0.030429884791374207\n",
      "iteration 575, dc_loss: 0.2194470465183258, tv_loss: 0.030671751126646996\n",
      "iteration 576, dc_loss: 0.21916340291500092, tv_loss: 0.030457058921456337\n",
      "iteration 577, dc_loss: 0.21785202622413635, tv_loss: 0.030697716400027275\n",
      "iteration 578, dc_loss: 0.21728532016277313, tv_loss: 0.03050358220934868\n",
      "iteration 579, dc_loss: 0.2163512259721756, tv_loss: 0.03042749874293804\n",
      "iteration 580, dc_loss: 0.2156400978565216, tv_loss: 0.03080417774617672\n",
      "iteration 581, dc_loss: 0.21511414647102356, tv_loss: 0.030373556539416313\n",
      "iteration 582, dc_loss: 0.2142067849636078, tv_loss: 0.030817514285445213\n",
      "iteration 583, dc_loss: 0.2134649008512497, tv_loss: 0.03048388846218586\n",
      "iteration 584, dc_loss: 0.21265412867069244, tv_loss: 0.03067123331129551\n",
      "iteration 585, dc_loss: 0.21176351606845856, tv_loss: 0.030612323433160782\n",
      "iteration 586, dc_loss: 0.2113994061946869, tv_loss: 0.030541762709617615\n",
      "iteration 587, dc_loss: 0.21032589673995972, tv_loss: 0.0306800976395607\n",
      "iteration 588, dc_loss: 0.2098648101091385, tv_loss: 0.03069283440709114\n",
      "iteration 589, dc_loss: 0.20884273946285248, tv_loss: 0.030739232897758484\n",
      "iteration 590, dc_loss: 0.20825417339801788, tv_loss: 0.0306550320237875\n",
      "iteration 591, dc_loss: 0.2074616700410843, tv_loss: 0.03058149293065071\n",
      "iteration 592, dc_loss: 0.20673024654388428, tv_loss: 0.030811866745352745\n",
      "iteration 593, dc_loss: 0.2061329036951065, tv_loss: 0.03052249550819397\n",
      "iteration 594, dc_loss: 0.20533223450183868, tv_loss: 0.030862931162118912\n",
      "iteration 595, dc_loss: 0.20466506481170654, tv_loss: 0.030612029135227203\n",
      "iteration 596, dc_loss: 0.20387326180934906, tv_loss: 0.030845781788229942\n",
      "iteration 597, dc_loss: 0.20314548909664154, tv_loss: 0.030704163014888763\n",
      "iteration 598, dc_loss: 0.20255862176418304, tv_loss: 0.030704660341143608\n",
      "iteration 599, dc_loss: 0.20170170068740845, tv_loss: 0.030707385390996933\n",
      "iteration 600, dc_loss: 0.2011772245168686, tv_loss: 0.030804762616753578\n",
      "iteration 601, dc_loss: 0.2003353089094162, tv_loss: 0.030785979703068733\n",
      "iteration 602, dc_loss: 0.19984330236911774, tv_loss: 0.030767804011702538\n",
      "iteration 603, dc_loss: 0.19893863797187805, tv_loss: 0.030823761597275734\n",
      "iteration 604, dc_loss: 0.1983816921710968, tv_loss: 0.030823780223727226\n",
      "iteration 605, dc_loss: 0.19762517511844635, tv_loss: 0.030753737315535545\n",
      "iteration 606, dc_loss: 0.19701793789863586, tv_loss: 0.030862675979733467\n",
      "iteration 607, dc_loss: 0.19635432958602905, tv_loss: 0.03075379692018032\n",
      "iteration 608, dc_loss: 0.1957143396139145, tv_loss: 0.030981063842773438\n",
      "iteration 609, dc_loss: 0.19532427191734314, tv_loss: 0.03071797825396061\n",
      "iteration 610, dc_loss: 0.19480538368225098, tv_loss: 0.030988862738013268\n",
      "iteration 611, dc_loss: 0.194641575217247, tv_loss: 0.030643686652183533\n",
      "iteration 612, dc_loss: 0.19426703453063965, tv_loss: 0.03118249773979187\n",
      "iteration 613, dc_loss: 0.19394069910049438, tv_loss: 0.030657269060611725\n",
      "iteration 614, dc_loss: 0.19280414283275604, tv_loss: 0.03112691268324852\n",
      "iteration 615, dc_loss: 0.19158892333507538, tv_loss: 0.030816763639450073\n",
      "iteration 616, dc_loss: 0.19036538898944855, tv_loss: 0.030925104394555092\n",
      "iteration 617, dc_loss: 0.18980824947357178, tv_loss: 0.03096308372914791\n",
      "iteration 618, dc_loss: 0.18977628648281097, tv_loss: 0.030730005353689194\n",
      "iteration 619, dc_loss: 0.1888239085674286, tv_loss: 0.031042689457535744\n",
      "iteration 620, dc_loss: 0.187985360622406, tv_loss: 0.030931230634450912\n",
      "iteration 621, dc_loss: 0.18715019524097443, tv_loss: 0.030843617394566536\n",
      "iteration 622, dc_loss: 0.18655861914157867, tv_loss: 0.0310957133769989\n",
      "iteration 623, dc_loss: 0.18626241385936737, tv_loss: 0.03081677295267582\n",
      "iteration 624, dc_loss: 0.18530122935771942, tv_loss: 0.031156882643699646\n",
      "iteration 625, dc_loss: 0.18464308977127075, tv_loss: 0.030886033549904823\n",
      "iteration 626, dc_loss: 0.18402104079723358, tv_loss: 0.03096959926187992\n",
      "iteration 627, dc_loss: 0.18336035311222076, tv_loss: 0.03102271817624569\n",
      "iteration 628, dc_loss: 0.18289010226726532, tv_loss: 0.03102693520486355\n",
      "iteration 629, dc_loss: 0.1821056753396988, tv_loss: 0.030984792858362198\n",
      "iteration 630, dc_loss: 0.1815069019794464, tv_loss: 0.031049642711877823\n",
      "iteration 631, dc_loss: 0.18086273968219757, tv_loss: 0.030954821035265923\n",
      "iteration 632, dc_loss: 0.1802545040845871, tv_loss: 0.031208720058202744\n",
      "iteration 633, dc_loss: 0.17971543967723846, tv_loss: 0.031003667041659355\n",
      "iteration 634, dc_loss: 0.17905429005622864, tv_loss: 0.0311074610799551\n",
      "iteration 635, dc_loss: 0.17835448682308197, tv_loss: 0.031060539186000824\n",
      "iteration 636, dc_loss: 0.17781881988048553, tv_loss: 0.031141657382249832\n",
      "iteration 637, dc_loss: 0.17719334363937378, tv_loss: 0.031093759462237358\n",
      "iteration 638, dc_loss: 0.17671889066696167, tv_loss: 0.031115137040615082\n",
      "iteration 639, dc_loss: 0.17601288855075836, tv_loss: 0.031054364517331123\n",
      "iteration 640, dc_loss: 0.17537276446819305, tv_loss: 0.031253714114427567\n",
      "iteration 641, dc_loss: 0.17481504380702972, tv_loss: 0.03109615668654442\n",
      "iteration 642, dc_loss: 0.17423833906650543, tv_loss: 0.031235357746481895\n",
      "iteration 643, dc_loss: 0.17369391024112701, tv_loss: 0.031112520024180412\n",
      "iteration 644, dc_loss: 0.17304761707782745, tv_loss: 0.031265705823898315\n",
      "iteration 645, dc_loss: 0.1724756807088852, tv_loss: 0.031133078038692474\n",
      "iteration 646, dc_loss: 0.1719054877758026, tv_loss: 0.031229496002197266\n",
      "iteration 647, dc_loss: 0.17129942774772644, tv_loss: 0.031154612079262733\n",
      "iteration 648, dc_loss: 0.17074504494667053, tv_loss: 0.03129000961780548\n",
      "iteration 649, dc_loss: 0.17018063366413116, tv_loss: 0.0311955064535141\n",
      "iteration 650, dc_loss: 0.1696224808692932, tv_loss: 0.031287260353565216\n",
      "iteration 651, dc_loss: 0.16905874013900757, tv_loss: 0.031180480495095253\n",
      "iteration 652, dc_loss: 0.16843032836914062, tv_loss: 0.03135664388537407\n",
      "iteration 653, dc_loss: 0.16793310642242432, tv_loss: 0.031196309253573418\n",
      "iteration 654, dc_loss: 0.1673203855752945, tv_loss: 0.03136077895760536\n",
      "iteration 655, dc_loss: 0.16689836978912354, tv_loss: 0.031161831691861153\n",
      "iteration 656, dc_loss: 0.16620895266532898, tv_loss: 0.0314493328332901\n",
      "iteration 657, dc_loss: 0.1659245491027832, tv_loss: 0.031132344156503677\n",
      "iteration 658, dc_loss: 0.16521038115024567, tv_loss: 0.03154226392507553\n",
      "iteration 659, dc_loss: 0.1652013212442398, tv_loss: 0.031070338562130928\n",
      "iteration 660, dc_loss: 0.16445180773735046, tv_loss: 0.031650617718696594\n",
      "iteration 661, dc_loss: 0.16456599533557892, tv_loss: 0.03101179376244545\n",
      "iteration 662, dc_loss: 0.16356129944324493, tv_loss: 0.03174492344260216\n",
      "iteration 663, dc_loss: 0.16347920894622803, tv_loss: 0.031026706099510193\n",
      "iteration 664, dc_loss: 0.16211768984794617, tv_loss: 0.03156040981411934\n",
      "iteration 665, dc_loss: 0.16146236658096313, tv_loss: 0.03128635883331299\n",
      "iteration 666, dc_loss: 0.1611061841249466, tv_loss: 0.031246816739439964\n",
      "iteration 667, dc_loss: 0.16036716103553772, tv_loss: 0.03153739124536514\n",
      "iteration 668, dc_loss: 0.16033318638801575, tv_loss: 0.03128096088767052\n",
      "iteration 669, dc_loss: 0.159319669008255, tv_loss: 0.031480614095926285\n",
      "iteration 670, dc_loss: 0.15885525941848755, tv_loss: 0.031375952064991\n",
      "iteration 671, dc_loss: 0.1582634299993515, tv_loss: 0.03130090609192848\n",
      "iteration 672, dc_loss: 0.15774011611938477, tv_loss: 0.03164083510637283\n",
      "iteration 673, dc_loss: 0.1575067937374115, tv_loss: 0.031216617673635483\n",
      "iteration 674, dc_loss: 0.15664570033550262, tv_loss: 0.03157796338200569\n",
      "iteration 675, dc_loss: 0.15608760714530945, tv_loss: 0.03139530494809151\n",
      "iteration 676, dc_loss: 0.15566708147525787, tv_loss: 0.03142858296632767\n",
      "iteration 677, dc_loss: 0.1550573855638504, tv_loss: 0.03151778504252434\n",
      "iteration 678, dc_loss: 0.1549062877893448, tv_loss: 0.03127281367778778\n",
      "iteration 679, dc_loss: 0.15398652851581573, tv_loss: 0.03152885660529137\n",
      "iteration 680, dc_loss: 0.15357743203639984, tv_loss: 0.03146540746092796\n",
      "iteration 681, dc_loss: 0.1531270444393158, tv_loss: 0.03134968876838684\n",
      "iteration 682, dc_loss: 0.1525280475616455, tv_loss: 0.03153524547815323\n",
      "iteration 683, dc_loss: 0.15215228497982025, tv_loss: 0.03140638768672943\n",
      "iteration 684, dc_loss: 0.15149597823619843, tv_loss: 0.03154091536998749\n",
      "iteration 685, dc_loss: 0.1511109173297882, tv_loss: 0.03140721097588539\n",
      "iteration 686, dc_loss: 0.15058690309524536, tv_loss: 0.03139915689826012\n",
      "iteration 687, dc_loss: 0.14997288584709167, tv_loss: 0.0315689779818058\n",
      "iteration 688, dc_loss: 0.14973275363445282, tv_loss: 0.031385283917188644\n",
      "iteration 689, dc_loss: 0.14908112585544586, tv_loss: 0.03149263188242912\n",
      "iteration 690, dc_loss: 0.14863255620002747, tv_loss: 0.03148648515343666\n",
      "iteration 691, dc_loss: 0.14811314642429352, tv_loss: 0.0314985029399395\n",
      "iteration 692, dc_loss: 0.14761346578598022, tv_loss: 0.031498268246650696\n",
      "iteration 693, dc_loss: 0.14721299707889557, tv_loss: 0.03143693879246712\n",
      "iteration 694, dc_loss: 0.14664709568023682, tv_loss: 0.031599316745996475\n",
      "iteration 695, dc_loss: 0.14634744822978973, tv_loss: 0.03144926577806473\n",
      "iteration 696, dc_loss: 0.1457223892211914, tv_loss: 0.031601037830114365\n",
      "iteration 697, dc_loss: 0.14545667171478271, tv_loss: 0.03140323609113693\n",
      "iteration 698, dc_loss: 0.1447952687740326, tv_loss: 0.03163895383477211\n",
      "iteration 699, dc_loss: 0.1445244550704956, tv_loss: 0.03146279603242874\n",
      "iteration 700, dc_loss: 0.14399470388889313, tv_loss: 0.03161003813147545\n",
      "iteration 701, dc_loss: 0.14374195039272308, tv_loss: 0.03148294612765312\n",
      "iteration 702, dc_loss: 0.1433044970035553, tv_loss: 0.031622469425201416\n",
      "iteration 703, dc_loss: 0.14299547672271729, tv_loss: 0.03156621381640434\n",
      "iteration 704, dc_loss: 0.1426982581615448, tv_loss: 0.03160080313682556\n",
      "iteration 705, dc_loss: 0.14213892817497253, tv_loss: 0.03161254897713661\n",
      "iteration 706, dc_loss: 0.14156265556812286, tv_loss: 0.0316193625330925\n",
      "iteration 707, dc_loss: 0.14083677530288696, tv_loss: 0.03162417933344841\n",
      "iteration 708, dc_loss: 0.14022484421730042, tv_loss: 0.03159809857606888\n",
      "iteration 709, dc_loss: 0.13968122005462646, tv_loss: 0.03166351467370987\n",
      "iteration 710, dc_loss: 0.13940642774105072, tv_loss: 0.031624626368284225\n",
      "iteration 711, dc_loss: 0.13897517323493958, tv_loss: 0.031725455075502396\n",
      "iteration 712, dc_loss: 0.13870584964752197, tv_loss: 0.03155243396759033\n",
      "iteration 713, dc_loss: 0.13804568350315094, tv_loss: 0.03174474090337753\n",
      "iteration 714, dc_loss: 0.13769733905792236, tv_loss: 0.03156934306025505\n",
      "iteration 715, dc_loss: 0.13702236115932465, tv_loss: 0.03176003694534302\n",
      "iteration 716, dc_loss: 0.13679738342761993, tv_loss: 0.03156702592968941\n",
      "iteration 717, dc_loss: 0.13617615401744843, tv_loss: 0.0318249873816967\n",
      "iteration 718, dc_loss: 0.13597846031188965, tv_loss: 0.03165978938341141\n",
      "iteration 719, dc_loss: 0.13542166352272034, tv_loss: 0.03179190307855606\n",
      "iteration 720, dc_loss: 0.13506855070590973, tv_loss: 0.031669970601797104\n",
      "iteration 721, dc_loss: 0.13447779417037964, tv_loss: 0.03178193420171738\n",
      "iteration 722, dc_loss: 0.134105384349823, tv_loss: 0.031688570976257324\n",
      "iteration 723, dc_loss: 0.1335960030555725, tv_loss: 0.0317949540913105\n",
      "iteration 724, dc_loss: 0.1332656294107437, tv_loss: 0.031760185956954956\n",
      "iteration 725, dc_loss: 0.13276226818561554, tv_loss: 0.031838759779930115\n",
      "iteration 726, dc_loss: 0.13250921666622162, tv_loss: 0.031687505543231964\n",
      "iteration 727, dc_loss: 0.13196560740470886, tv_loss: 0.031868062913417816\n",
      "iteration 728, dc_loss: 0.1318165361881256, tv_loss: 0.03164409473538399\n",
      "iteration 729, dc_loss: 0.13114893436431885, tv_loss: 0.03197912871837616\n",
      "iteration 730, dc_loss: 0.13117265701293945, tv_loss: 0.031617630273103714\n",
      "iteration 731, dc_loss: 0.13041922450065613, tv_loss: 0.032071635127067566\n",
      "iteration 732, dc_loss: 0.13042649626731873, tv_loss: 0.03166117146611214\n",
      "iteration 733, dc_loss: 0.12959611415863037, tv_loss: 0.0320405587553978\n",
      "iteration 734, dc_loss: 0.12946471571922302, tv_loss: 0.03167399391531944\n",
      "iteration 735, dc_loss: 0.1286977380514145, tv_loss: 0.031961336731910706\n",
      "iteration 736, dc_loss: 0.12844093143939972, tv_loss: 0.03175564855337143\n",
      "iteration 737, dc_loss: 0.12789508700370789, tv_loss: 0.031873367726802826\n",
      "iteration 738, dc_loss: 0.12751266360282898, tv_loss: 0.031874220818281174\n",
      "iteration 739, dc_loss: 0.12719495594501495, tv_loss: 0.03187526762485504\n",
      "iteration 740, dc_loss: 0.1266968846321106, tv_loss: 0.032000526785850525\n",
      "iteration 741, dc_loss: 0.12647731602191925, tv_loss: 0.031795598566532135\n",
      "iteration 742, dc_loss: 0.12591299414634705, tv_loss: 0.03197312355041504\n",
      "iteration 743, dc_loss: 0.12566497921943665, tv_loss: 0.03181558847427368\n",
      "iteration 744, dc_loss: 0.12512421607971191, tv_loss: 0.03195486590266228\n",
      "iteration 745, dc_loss: 0.12482580542564392, tv_loss: 0.03186902031302452\n",
      "iteration 746, dc_loss: 0.12434649467468262, tv_loss: 0.03202720731496811\n",
      "iteration 747, dc_loss: 0.12403519451618195, tv_loss: 0.031909260898828506\n",
      "iteration 748, dc_loss: 0.12360609322786331, tv_loss: 0.03196563571691513\n",
      "iteration 749, dc_loss: 0.12322825193405151, tv_loss: 0.031948626041412354\n",
      "iteration 750, dc_loss: 0.12289272248744965, tv_loss: 0.03192827105522156\n",
      "iteration 751, dc_loss: 0.12247174978256226, tv_loss: 0.032055795192718506\n",
      "iteration 752, dc_loss: 0.12219057232141495, tv_loss: 0.03194236382842064\n",
      "iteration 753, dc_loss: 0.12168911099433899, tv_loss: 0.03209073096513748\n",
      "iteration 754, dc_loss: 0.1215544268488884, tv_loss: 0.031852882355451584\n",
      "iteration 755, dc_loss: 0.12095396220684052, tv_loss: 0.03214375302195549\n",
      "iteration 756, dc_loss: 0.12092304229736328, tv_loss: 0.03188570216298103\n",
      "iteration 757, dc_loss: 0.12026488780975342, tv_loss: 0.03220584616065025\n",
      "iteration 758, dc_loss: 0.12025374174118042, tv_loss: 0.031833093613386154\n",
      "iteration 759, dc_loss: 0.1195283979177475, tv_loss: 0.03219497203826904\n",
      "iteration 760, dc_loss: 0.11946303397417068, tv_loss: 0.03189795836806297\n",
      "iteration 761, dc_loss: 0.11879534274339676, tv_loss: 0.03218178078532219\n",
      "iteration 762, dc_loss: 0.11870456486940384, tv_loss: 0.0318852923810482\n",
      "iteration 763, dc_loss: 0.11805690079927444, tv_loss: 0.032176606357097626\n",
      "iteration 764, dc_loss: 0.11796341836452484, tv_loss: 0.031951770186424255\n",
      "iteration 765, dc_loss: 0.11742506921291351, tv_loss: 0.03223804011940956\n",
      "iteration 766, dc_loss: 0.11742161214351654, tv_loss: 0.03199825808405876\n",
      "iteration 767, dc_loss: 0.1170491948723793, tv_loss: 0.03225001320242882\n",
      "iteration 768, dc_loss: 0.11728635430335999, tv_loss: 0.031922560185194016\n",
      "iteration 769, dc_loss: 0.11726615577936172, tv_loss: 0.03233049437403679\n",
      "iteration 770, dc_loss: 0.11756720393896103, tv_loss: 0.03193102777004242\n",
      "iteration 771, dc_loss: 0.11724686622619629, tv_loss: 0.03232662007212639\n",
      "iteration 772, dc_loss: 0.11626545339822769, tv_loss: 0.03207595273852348\n",
      "iteration 773, dc_loss: 0.1150861456990242, tv_loss: 0.032070714980363846\n",
      "iteration 774, dc_loss: 0.1143592819571495, tv_loss: 0.03228708729147911\n",
      "iteration 775, dc_loss: 0.1148613914847374, tv_loss: 0.03191104158759117\n",
      "iteration 776, dc_loss: 0.11429563164710999, tv_loss: 0.03240622580051422\n",
      "iteration 777, dc_loss: 0.11368896067142487, tv_loss: 0.03204192966222763\n",
      "iteration 778, dc_loss: 0.1130252406001091, tv_loss: 0.03209381178021431\n",
      "iteration 779, dc_loss: 0.11281556636095047, tv_loss: 0.032377514988183975\n",
      "iteration 780, dc_loss: 0.11304248869419098, tv_loss: 0.031963374465703964\n",
      "iteration 781, dc_loss: 0.11200925707817078, tv_loss: 0.03234739601612091\n",
      "iteration 782, dc_loss: 0.11162172257900238, tv_loss: 0.03214899078011513\n",
      "iteration 783, dc_loss: 0.11159619688987732, tv_loss: 0.03207791596651077\n",
      "iteration 784, dc_loss: 0.11115449666976929, tv_loss: 0.0323624461889267\n",
      "iteration 785, dc_loss: 0.11086692661046982, tv_loss: 0.03208508342504501\n",
      "iteration 786, dc_loss: 0.11025567352771759, tv_loss: 0.03227722644805908\n",
      "iteration 787, dc_loss: 0.11007221788167953, tv_loss: 0.032300565391778946\n",
      "iteration 788, dc_loss: 0.10993033647537231, tv_loss: 0.032140981405973434\n",
      "iteration 789, dc_loss: 0.10931465774774551, tv_loss: 0.032282717525959015\n",
      "iteration 790, dc_loss: 0.10896904766559601, tv_loss: 0.03221261128783226\n",
      "iteration 791, dc_loss: 0.10877019166946411, tv_loss: 0.032232291996479034\n",
      "iteration 792, dc_loss: 0.10849135369062424, tv_loss: 0.03234323859214783\n",
      "iteration 793, dc_loss: 0.10807295888662338, tv_loss: 0.03229577839374542\n",
      "iteration 794, dc_loss: 0.10773493349552155, tv_loss: 0.03220652416348457\n",
      "iteration 795, dc_loss: 0.10734579712152481, tv_loss: 0.03235553205013275\n",
      "iteration 796, dc_loss: 0.10722513496875763, tv_loss: 0.03227373585104942\n",
      "iteration 797, dc_loss: 0.10675375908613205, tv_loss: 0.03235900402069092\n",
      "iteration 798, dc_loss: 0.10639770328998566, tv_loss: 0.032322175800800323\n",
      "iteration 799, dc_loss: 0.10623316466808319, tv_loss: 0.03223031386733055\n",
      "iteration 800, dc_loss: 0.10577910393476486, tv_loss: 0.03248253092169762\n",
      "iteration 801, dc_loss: 0.10566335171461105, tv_loss: 0.03225289657711983\n",
      "iteration 802, dc_loss: 0.1052112728357315, tv_loss: 0.0323338583111763\n",
      "iteration 803, dc_loss: 0.10496498644351959, tv_loss: 0.03240358456969261\n",
      "iteration 804, dc_loss: 0.10485167056322098, tv_loss: 0.03229454904794693\n",
      "iteration 805, dc_loss: 0.1044970378279686, tv_loss: 0.032343920320272446\n",
      "iteration 806, dc_loss: 0.10424482077360153, tv_loss: 0.03238799795508385\n",
      "iteration 807, dc_loss: 0.10407724231481552, tv_loss: 0.03232526406645775\n",
      "iteration 808, dc_loss: 0.10377759486436844, tv_loss: 0.03237233683466911\n",
      "iteration 809, dc_loss: 0.10351577401161194, tv_loss: 0.032385971397161484\n",
      "iteration 810, dc_loss: 0.10333991050720215, tv_loss: 0.0323319248855114\n",
      "iteration 811, dc_loss: 0.10307080298662186, tv_loss: 0.03239389508962631\n",
      "iteration 812, dc_loss: 0.10280804336071014, tv_loss: 0.03239891305565834\n",
      "iteration 813, dc_loss: 0.1026187315583229, tv_loss: 0.03233795240521431\n",
      "iteration 814, dc_loss: 0.10234974324703217, tv_loss: 0.03239496797323227\n",
      "iteration 815, dc_loss: 0.10211547464132309, tv_loss: 0.032406195998191833\n",
      "iteration 816, dc_loss: 0.10190795361995697, tv_loss: 0.03236839920282364\n",
      "iteration 817, dc_loss: 0.10163220018148422, tv_loss: 0.03239972144365311\n",
      "iteration 818, dc_loss: 0.10142239928245544, tv_loss: 0.032394930720329285\n",
      "iteration 819, dc_loss: 0.10119375586509705, tv_loss: 0.03240806609392166\n",
      "iteration 820, dc_loss: 0.10093700140714645, tv_loss: 0.0324411578476429\n",
      "iteration 821, dc_loss: 0.10074454545974731, tv_loss: 0.03239702805876732\n",
      "iteration 822, dc_loss: 0.10048423707485199, tv_loss: 0.032404109835624695\n",
      "iteration 823, dc_loss: 0.10025597363710403, tv_loss: 0.03241947665810585\n",
      "iteration 824, dc_loss: 0.10005931556224823, tv_loss: 0.03240922465920448\n",
      "iteration 825, dc_loss: 0.09978486597537994, tv_loss: 0.03245134651660919\n",
      "iteration 826, dc_loss: 0.09957822412252426, tv_loss: 0.032442864030599594\n",
      "iteration 827, dc_loss: 0.09936871379613876, tv_loss: 0.032420564442873\n",
      "iteration 828, dc_loss: 0.09911827743053436, tv_loss: 0.03243386000394821\n",
      "iteration 829, dc_loss: 0.09890660643577576, tv_loss: 0.032435983419418335\n",
      "iteration 830, dc_loss: 0.09867307543754578, tv_loss: 0.032450880855321884\n",
      "iteration 831, dc_loss: 0.09845922142267227, tv_loss: 0.032428398728370667\n",
      "iteration 832, dc_loss: 0.09823454916477203, tv_loss: 0.03243651241064072\n",
      "iteration 833, dc_loss: 0.09799773246049881, tv_loss: 0.03246741741895676\n",
      "iteration 834, dc_loss: 0.09780121594667435, tv_loss: 0.032449815422296524\n",
      "iteration 835, dc_loss: 0.09755436331033707, tv_loss: 0.03250455483794212\n",
      "iteration 836, dc_loss: 0.09735103696584702, tv_loss: 0.032532792538404465\n",
      "iteration 837, dc_loss: 0.09712839871644974, tv_loss: 0.03249619901180267\n",
      "iteration 838, dc_loss: 0.09692154824733734, tv_loss: 0.032482340931892395\n",
      "iteration 839, dc_loss: 0.09667149186134338, tv_loss: 0.03256535902619362\n",
      "iteration 840, dc_loss: 0.09647826850414276, tv_loss: 0.03254970163106918\n",
      "iteration 841, dc_loss: 0.09628903865814209, tv_loss: 0.03247062489390373\n",
      "iteration 842, dc_loss: 0.09602788835763931, tv_loss: 0.03255268186330795\n",
      "iteration 843, dc_loss: 0.09583040326833725, tv_loss: 0.03258836269378662\n",
      "iteration 844, dc_loss: 0.09563320130109787, tv_loss: 0.032510675489902496\n",
      "iteration 845, dc_loss: 0.09537798911333084, tv_loss: 0.03257293626666069\n",
      "iteration 846, dc_loss: 0.09520828723907471, tv_loss: 0.032591041177511215\n",
      "iteration 847, dc_loss: 0.09498607367277145, tv_loss: 0.03253290802240372\n",
      "iteration 848, dc_loss: 0.09473542869091034, tv_loss: 0.032582711428403854\n",
      "iteration 849, dc_loss: 0.09458980709314346, tv_loss: 0.03261274844408035\n",
      "iteration 850, dc_loss: 0.09434615075588226, tv_loss: 0.032551463693380356\n",
      "iteration 851, dc_loss: 0.09410224109888077, tv_loss: 0.0326533205807209\n",
      "iteration 852, dc_loss: 0.0939611867070198, tv_loss: 0.03259490802884102\n",
      "iteration 853, dc_loss: 0.09372320771217346, tv_loss: 0.03256365656852722\n",
      "iteration 854, dc_loss: 0.0934886783361435, tv_loss: 0.032662782818078995\n",
      "iteration 855, dc_loss: 0.09332167357206345, tv_loss: 0.03258823603391647\n",
      "iteration 856, dc_loss: 0.0930936262011528, tv_loss: 0.03257828950881958\n",
      "iteration 857, dc_loss: 0.09290754795074463, tv_loss: 0.03258414939045906\n",
      "iteration 858, dc_loss: 0.09268881380558014, tv_loss: 0.03261343017220497\n",
      "iteration 859, dc_loss: 0.09246381372213364, tv_loss: 0.03262943774461746\n",
      "iteration 860, dc_loss: 0.09227899461984634, tv_loss: 0.0325930193066597\n",
      "iteration 861, dc_loss: 0.09208851307630539, tv_loss: 0.03258064016699791\n",
      "iteration 862, dc_loss: 0.09186351299285889, tv_loss: 0.032640933990478516\n",
      "iteration 863, dc_loss: 0.09166873246431351, tv_loss: 0.032604217529296875\n",
      "iteration 864, dc_loss: 0.09145861119031906, tv_loss: 0.032622575759887695\n",
      "iteration 865, dc_loss: 0.09126818925142288, tv_loss: 0.032614272087812424\n",
      "iteration 866, dc_loss: 0.09108081459999084, tv_loss: 0.032637473195791245\n",
      "iteration 867, dc_loss: 0.09085218608379364, tv_loss: 0.03267529234290123\n",
      "iteration 868, dc_loss: 0.09066137671470642, tv_loss: 0.03263961896300316\n",
      "iteration 869, dc_loss: 0.09048601984977722, tv_loss: 0.03262150660157204\n",
      "iteration 870, dc_loss: 0.09025727212429047, tv_loss: 0.03265318274497986\n",
      "iteration 871, dc_loss: 0.0900752916932106, tv_loss: 0.032674502581357956\n",
      "iteration 872, dc_loss: 0.08988849818706512, tv_loss: 0.03266172483563423\n",
      "iteration 873, dc_loss: 0.08966828137636185, tv_loss: 0.03265795484185219\n",
      "iteration 874, dc_loss: 0.08950820565223694, tv_loss: 0.03264337778091431\n",
      "iteration 875, dc_loss: 0.08926990628242493, tv_loss: 0.03269471228122711\n",
      "iteration 876, dc_loss: 0.08911296725273132, tv_loss: 0.03269026428461075\n",
      "iteration 877, dc_loss: 0.08893061429262161, tv_loss: 0.0326719656586647\n",
      "iteration 878, dc_loss: 0.08867884427309036, tv_loss: 0.03271014243364334\n",
      "iteration 879, dc_loss: 0.08855801075696945, tv_loss: 0.03264361247420311\n",
      "iteration 880, dc_loss: 0.08831214159727097, tv_loss: 0.03269471973180771\n",
      "iteration 881, dc_loss: 0.08814632147550583, tv_loss: 0.03266739845275879\n",
      "iteration 882, dc_loss: 0.08795042335987091, tv_loss: 0.03268544748425484\n",
      "iteration 883, dc_loss: 0.08776140213012695, tv_loss: 0.03270556405186653\n",
      "iteration 884, dc_loss: 0.08756571263074875, tv_loss: 0.032718587666749954\n",
      "iteration 885, dc_loss: 0.0873849168419838, tv_loss: 0.03273078799247742\n",
      "iteration 886, dc_loss: 0.08718350529670715, tv_loss: 0.032724712044000626\n",
      "iteration 887, dc_loss: 0.08703207969665527, tv_loss: 0.03270248696208\n",
      "iteration 888, dc_loss: 0.08678539097309113, tv_loss: 0.03275173902511597\n",
      "iteration 889, dc_loss: 0.08667811006307602, tv_loss: 0.032662082463502884\n",
      "iteration 890, dc_loss: 0.08642403781414032, tv_loss: 0.03273922577500343\n",
      "iteration 891, dc_loss: 0.08627267926931381, tv_loss: 0.03273250535130501\n",
      "iteration 892, dc_loss: 0.0860801562666893, tv_loss: 0.032748766243457794\n",
      "iteration 893, dc_loss: 0.08587145060300827, tv_loss: 0.032805897295475006\n",
      "iteration 894, dc_loss: 0.08576249331235886, tv_loss: 0.032713476568460464\n",
      "iteration 895, dc_loss: 0.08549106121063232, tv_loss: 0.0328054204583168\n",
      "iteration 896, dc_loss: 0.08539138734340668, tv_loss: 0.032706327736377716\n",
      "iteration 897, dc_loss: 0.08514145761728287, tv_loss: 0.03277621790766716\n",
      "iteration 898, dc_loss: 0.08500402420759201, tv_loss: 0.03278035670518875\n",
      "iteration 899, dc_loss: 0.08481062948703766, tv_loss: 0.0327824167907238\n",
      "iteration 900, dc_loss: 0.08461973816156387, tv_loss: 0.03280176222324371\n",
      "iteration 901, dc_loss: 0.08449200540781021, tv_loss: 0.03273637965321541\n",
      "iteration 902, dc_loss: 0.0842542052268982, tv_loss: 0.03280409798026085\n",
      "iteration 903, dc_loss: 0.08411537855863571, tv_loss: 0.03275586664676666\n",
      "iteration 904, dc_loss: 0.08391211926937103, tv_loss: 0.03279196098446846\n",
      "iteration 905, dc_loss: 0.08372367173433304, tv_loss: 0.03281712904572487\n",
      "iteration 906, dc_loss: 0.08358345925807953, tv_loss: 0.03279316425323486\n",
      "iteration 907, dc_loss: 0.08336378633975983, tv_loss: 0.03285575285553932\n",
      "iteration 908, dc_loss: 0.08329413086175919, tv_loss: 0.03274688497185707\n",
      "iteration 909, dc_loss: 0.08299309015274048, tv_loss: 0.03288751840591431\n",
      "iteration 910, dc_loss: 0.08293891698122025, tv_loss: 0.032756540924310684\n",
      "iteration 911, dc_loss: 0.08266457915306091, tv_loss: 0.032867688685655594\n",
      "iteration 912, dc_loss: 0.08260799944400787, tv_loss: 0.032759394496679306\n",
      "iteration 913, dc_loss: 0.08233214169740677, tv_loss: 0.032880138605833054\n",
      "iteration 914, dc_loss: 0.08233478665351868, tv_loss: 0.032733745872974396\n",
      "iteration 915, dc_loss: 0.08205065131187439, tv_loss: 0.03288625180721283\n",
      "iteration 916, dc_loss: 0.08204390108585358, tv_loss: 0.03274187073111534\n",
      "iteration 917, dc_loss: 0.08171699196100235, tv_loss: 0.03294149041175842\n",
      "iteration 918, dc_loss: 0.08175051957368851, tv_loss: 0.03282031789422035\n",
      "iteration 919, dc_loss: 0.08133892714977264, tv_loss: 0.03299398720264435\n",
      "iteration 920, dc_loss: 0.08138851821422577, tv_loss: 0.03271762654185295\n",
      "iteration 921, dc_loss: 0.08093643188476562, tv_loss: 0.03302081674337387\n",
      "iteration 922, dc_loss: 0.08094184845685959, tv_loss: 0.03281345218420029\n",
      "iteration 923, dc_loss: 0.08064431697130203, tv_loss: 0.032899241894483566\n",
      "iteration 924, dc_loss: 0.08045397698879242, tv_loss: 0.03290696442127228\n",
      "iteration 925, dc_loss: 0.08046835660934448, tv_loss: 0.032766032963991165\n",
      "iteration 926, dc_loss: 0.08013517409563065, tv_loss: 0.0329829566180706\n",
      "iteration 927, dc_loss: 0.08022391051054001, tv_loss: 0.03276892751455307\n",
      "iteration 928, dc_loss: 0.07980988174676895, tv_loss: 0.03305770084261894\n",
      "iteration 929, dc_loss: 0.07982555776834488, tv_loss: 0.03283414617180824\n",
      "iteration 930, dc_loss: 0.07948676496744156, tv_loss: 0.03294703736901283\n",
      "iteration 931, dc_loss: 0.07935876399278641, tv_loss: 0.03289002180099487\n",
      "iteration 932, dc_loss: 0.07921216636896133, tv_loss: 0.0328839085996151\n",
      "iteration 933, dc_loss: 0.07900815457105637, tv_loss: 0.032977622002363205\n",
      "iteration 934, dc_loss: 0.07894223928451538, tv_loss: 0.03287156671285629\n",
      "iteration 935, dc_loss: 0.07866240292787552, tv_loss: 0.03298858925700188\n",
      "iteration 936, dc_loss: 0.0786573737859726, tv_loss: 0.03284443914890289\n",
      "iteration 937, dc_loss: 0.07836737483739853, tv_loss: 0.03298810496926308\n",
      "iteration 938, dc_loss: 0.0783209279179573, tv_loss: 0.03289218991994858\n",
      "iteration 939, dc_loss: 0.07804669439792633, tv_loss: 0.03299528360366821\n",
      "iteration 940, dc_loss: 0.07792314887046814, tv_loss: 0.032935820519924164\n",
      "iteration 941, dc_loss: 0.07776869088411331, tv_loss: 0.032907817512750626\n",
      "iteration 942, dc_loss: 0.07756223529577255, tv_loss: 0.032969601452350616\n",
      "iteration 943, dc_loss: 0.07751046866178513, tv_loss: 0.032889094203710556\n",
      "iteration 944, dc_loss: 0.07724743336439133, tv_loss: 0.03302927315235138\n",
      "iteration 945, dc_loss: 0.07721401751041412, tv_loss: 0.032924674451351166\n",
      "iteration 946, dc_loss: 0.07694380730390549, tv_loss: 0.03302816301584244\n",
      "iteration 947, dc_loss: 0.07687584310770035, tv_loss: 0.03292388468980789\n",
      "iteration 948, dc_loss: 0.07667645812034607, tv_loss: 0.03297474607825279\n",
      "iteration 949, dc_loss: 0.07655193656682968, tv_loss: 0.0329473614692688\n",
      "iteration 950, dc_loss: 0.07635794579982758, tv_loss: 0.03298594430088997\n",
      "iteration 951, dc_loss: 0.07620532065629959, tv_loss: 0.03298959136009216\n",
      "iteration 952, dc_loss: 0.07608704268932343, tv_loss: 0.032970018684864044\n",
      "iteration 953, dc_loss: 0.07588475942611694, tv_loss: 0.033035751432180405\n",
      "iteration 954, dc_loss: 0.0757964625954628, tv_loss: 0.03295666351914406\n",
      "iteration 955, dc_loss: 0.07555589079856873, tv_loss: 0.033046092838048935\n",
      "iteration 956, dc_loss: 0.07554112374782562, tv_loss: 0.03292056545615196\n",
      "iteration 957, dc_loss: 0.07526929676532745, tv_loss: 0.03306948393583298\n",
      "iteration 958, dc_loss: 0.07530791312456131, tv_loss: 0.032916393131017685\n",
      "iteration 959, dc_loss: 0.07501982897520065, tv_loss: 0.0331101156771183\n",
      "iteration 960, dc_loss: 0.07507658004760742, tv_loss: 0.03294191136956215\n",
      "iteration 961, dc_loss: 0.07474473118782043, tv_loss: 0.033122316002845764\n",
      "iteration 962, dc_loss: 0.07481551170349121, tv_loss: 0.032908614724874496\n",
      "iteration 963, dc_loss: 0.0744892805814743, tv_loss: 0.033151112496852875\n",
      "iteration 964, dc_loss: 0.07457003742456436, tv_loss: 0.03296985477209091\n",
      "iteration 965, dc_loss: 0.07417434453964233, tv_loss: 0.03319617360830307\n",
      "iteration 966, dc_loss: 0.074273481965065, tv_loss: 0.03289001062512398\n",
      "iteration 967, dc_loss: 0.07385196536779404, tv_loss: 0.03317181393504143\n",
      "iteration 968, dc_loss: 0.07390068471431732, tv_loss: 0.03298679366707802\n",
      "iteration 969, dc_loss: 0.07350616902112961, tv_loss: 0.03314009681344032\n",
      "iteration 970, dc_loss: 0.07347535341978073, tv_loss: 0.032999586313962936\n",
      "iteration 971, dc_loss: 0.07327253371477127, tv_loss: 0.033083416521549225\n",
      "iteration 972, dc_loss: 0.07310275733470917, tv_loss: 0.0331411212682724\n",
      "iteration 973, dc_loss: 0.07302885502576828, tv_loss: 0.03303316608071327\n",
      "iteration 974, dc_loss: 0.07284604758024216, tv_loss: 0.03310501575469971\n",
      "iteration 975, dc_loss: 0.07281529158353806, tv_loss: 0.03304773569107056\n",
      "iteration 976, dc_loss: 0.07254403084516525, tv_loss: 0.03317178040742874\n",
      "iteration 977, dc_loss: 0.07252538204193115, tv_loss: 0.0330171175301075\n",
      "iteration 978, dc_loss: 0.07227231562137604, tv_loss: 0.03313607722520828\n",
      "iteration 979, dc_loss: 0.0722341537475586, tv_loss: 0.03302042931318283\n",
      "iteration 980, dc_loss: 0.0719597265124321, tv_loss: 0.03317069634795189\n",
      "iteration 981, dc_loss: 0.07191593199968338, tv_loss: 0.033047016710042953\n",
      "iteration 982, dc_loss: 0.07173261046409607, tv_loss: 0.033092133700847626\n",
      "iteration 983, dc_loss: 0.0715702474117279, tv_loss: 0.033111024647951126\n",
      "iteration 984, dc_loss: 0.07146836072206497, tv_loss: 0.0330953449010849\n",
      "iteration 985, dc_loss: 0.07129395753145218, tv_loss: 0.03313983604311943\n",
      "iteration 986, dc_loss: 0.07123707979917526, tv_loss: 0.033049724996089935\n",
      "iteration 987, dc_loss: 0.07100684940814972, tv_loss: 0.033161766827106476\n",
      "iteration 988, dc_loss: 0.07096412032842636, tv_loss: 0.03308110311627388\n",
      "iteration 989, dc_loss: 0.07074511051177979, tv_loss: 0.033185943961143494\n",
      "iteration 990, dc_loss: 0.07074156403541565, tv_loss: 0.03306801989674568\n",
      "iteration 991, dc_loss: 0.07046181708574295, tv_loss: 0.033208075910806656\n",
      "iteration 992, dc_loss: 0.07052944600582123, tv_loss: 0.033025458455085754\n",
      "iteration 993, dc_loss: 0.07021375000476837, tv_loss: 0.03325396403670311\n",
      "iteration 994, dc_loss: 0.07035783678293228, tv_loss: 0.03298038989305496\n",
      "iteration 995, dc_loss: 0.06996937841176987, tv_loss: 0.03331499546766281\n",
      "iteration 996, dc_loss: 0.07025781273841858, tv_loss: 0.032937001436948776\n",
      "iteration 997, dc_loss: 0.06980728358030319, tv_loss: 0.03338022530078888\n",
      "iteration 998, dc_loss: 0.07014276087284088, tv_loss: 0.03295844793319702\n",
      "iteration 999, dc_loss: 0.06957687437534332, tv_loss: 0.03340598940849304\n",
      "iteration 1000, dc_loss: 0.06988276541233063, tv_loss: 0.03290821984410286\n",
      "iteration 1001, dc_loss: 0.06926200538873672, tv_loss: 0.03333835303783417\n",
      "iteration 1002, dc_loss: 0.069316565990448, tv_loss: 0.033038266003131866\n",
      "iteration 1003, dc_loss: 0.06892246752977371, tv_loss: 0.03325646370649338\n",
      "iteration 1004, dc_loss: 0.06883516162633896, tv_loss: 0.033192820847034454\n",
      "iteration 1005, dc_loss: 0.06878593564033508, tv_loss: 0.03309907391667366\n",
      "iteration 1006, dc_loss: 0.06851570308208466, tv_loss: 0.0332784578204155\n",
      "iteration 1007, dc_loss: 0.06866451352834702, tv_loss: 0.03307709842920303\n",
      "iteration 1008, dc_loss: 0.06829764693975449, tv_loss: 0.03336932882666588\n",
      "iteration 1009, dc_loss: 0.06836112588644028, tv_loss: 0.03308726102113724\n",
      "iteration 1010, dc_loss: 0.06801137328147888, tv_loss: 0.03327235206961632\n",
      "iteration 1011, dc_loss: 0.06796961277723312, tv_loss: 0.03319503739476204\n",
      "iteration 1012, dc_loss: 0.06786098331212997, tv_loss: 0.03317669406533241\n",
      "iteration 1013, dc_loss: 0.06765083223581314, tv_loss: 0.03327373042702675\n",
      "iteration 1014, dc_loss: 0.06774252653121948, tv_loss: 0.03309648111462593\n",
      "iteration 1015, dc_loss: 0.06745246052742004, tv_loss: 0.033338937908411026\n",
      "iteration 1016, dc_loss: 0.06751247495412827, tv_loss: 0.03310738503932953\n",
      "iteration 1017, dc_loss: 0.06724713742733002, tv_loss: 0.03326430544257164\n",
      "iteration 1018, dc_loss: 0.06721017509698868, tv_loss: 0.033204685896635056\n",
      "iteration 1019, dc_loss: 0.06718294322490692, tv_loss: 0.03321961313486099\n",
      "iteration 1020, dc_loss: 0.06704112142324448, tv_loss: 0.033269915729761124\n",
      "iteration 1021, dc_loss: 0.06708357483148575, tv_loss: 0.033150721341371536\n",
      "iteration 1022, dc_loss: 0.06681410223245621, tv_loss: 0.03330092877149582\n",
      "iteration 1023, dc_loss: 0.06681902706623077, tv_loss: 0.03319333493709564\n",
      "iteration 1024, dc_loss: 0.06649305671453476, tv_loss: 0.03329193964600563\n",
      "iteration 1025, dc_loss: 0.06636489182710648, tv_loss: 0.03319219872355461\n",
      "iteration 1026, dc_loss: 0.0661003515124321, tv_loss: 0.03324119374155998\n",
      "iteration 1027, dc_loss: 0.06595614552497864, tv_loss: 0.03324994817376137\n",
      "iteration 1028, dc_loss: 0.06594591587781906, tv_loss: 0.033174920827150345\n",
      "iteration 1029, dc_loss: 0.06573132425546646, tv_loss: 0.03331122547388077\n",
      "iteration 1030, dc_loss: 0.06579989194869995, tv_loss: 0.03316662460565567\n",
      "iteration 1031, dc_loss: 0.06554120033979416, tv_loss: 0.033315807580947876\n",
      "iteration 1032, dc_loss: 0.06549783796072006, tv_loss: 0.03321319445967674\n",
      "iteration 1033, dc_loss: 0.06526897102594376, tv_loss: 0.033283937722444534\n",
      "iteration 1034, dc_loss: 0.06514899432659149, tv_loss: 0.03325456753373146\n",
      "iteration 1035, dc_loss: 0.0650721862912178, tv_loss: 0.033211078494787216\n",
      "iteration 1036, dc_loss: 0.06487647444009781, tv_loss: 0.03329597786068916\n",
      "iteration 1037, dc_loss: 0.06487834453582764, tv_loss: 0.03320113196969032\n",
      "iteration 1038, dc_loss: 0.06465350091457367, tv_loss: 0.03332176432013512\n",
      "iteration 1039, dc_loss: 0.06469275802373886, tv_loss: 0.03316393867135048\n",
      "iteration 1040, dc_loss: 0.06441199779510498, tv_loss: 0.033331841230392456\n",
      "iteration 1041, dc_loss: 0.06441888958215714, tv_loss: 0.03319397196173668\n",
      "iteration 1042, dc_loss: 0.06418991088867188, tv_loss: 0.03331094607710838\n",
      "iteration 1043, dc_loss: 0.06415832042694092, tv_loss: 0.03325521945953369\n",
      "iteration 1044, dc_loss: 0.06396064907312393, tv_loss: 0.03338246792554855\n",
      "iteration 1045, dc_loss: 0.06392843276262283, tv_loss: 0.033268868923187256\n",
      "iteration 1046, dc_loss: 0.0637790635228157, tv_loss: 0.03330378606915474\n",
      "iteration 1047, dc_loss: 0.0637851133942604, tv_loss: 0.033267658203840256\n",
      "iteration 1048, dc_loss: 0.0635690689086914, tv_loss: 0.033394455909729004\n",
      "iteration 1049, dc_loss: 0.06363440304994583, tv_loss: 0.03324102982878685\n",
      "iteration 1050, dc_loss: 0.06343571841716766, tv_loss: 0.03338943421840668\n",
      "iteration 1051, dc_loss: 0.06352570652961731, tv_loss: 0.03326451778411865\n",
      "iteration 1052, dc_loss: 0.06326263397932053, tv_loss: 0.03347083181142807\n",
      "iteration 1053, dc_loss: 0.06347450613975525, tv_loss: 0.03312830254435539\n",
      "iteration 1054, dc_loss: 0.06306424736976624, tv_loss: 0.03349786624312401\n",
      "iteration 1055, dc_loss: 0.06326719373464584, tv_loss: 0.03318812698125839\n",
      "iteration 1056, dc_loss: 0.06275909394025803, tv_loss: 0.03351432457566261\n",
      "iteration 1057, dc_loss: 0.06282716244459152, tv_loss: 0.03316855430603027\n",
      "iteration 1058, dc_loss: 0.062419500201940536, tv_loss: 0.03346306458115578\n",
      "iteration 1059, dc_loss: 0.06238383427262306, tv_loss: 0.03335597366094589\n",
      "iteration 1060, dc_loss: 0.06230512633919716, tv_loss: 0.03330293297767639\n",
      "iteration 1061, dc_loss: 0.06217380240559578, tv_loss: 0.03338757902383804\n",
      "iteration 1062, dc_loss: 0.06224317103624344, tv_loss: 0.033301740884780884\n",
      "iteration 1063, dc_loss: 0.06194009631872177, tv_loss: 0.03347957879304886\n",
      "iteration 1064, dc_loss: 0.061993908137083054, tv_loss: 0.03323184326291084\n",
      "iteration 1065, dc_loss: 0.061688363552093506, tv_loss: 0.033424459397792816\n",
      "iteration 1066, dc_loss: 0.06164802610874176, tv_loss: 0.033332355320453644\n",
      "iteration 1067, dc_loss: 0.06151486933231354, tv_loss: 0.03333240747451782\n",
      "iteration 1068, dc_loss: 0.06138508394360542, tv_loss: 0.033377908170223236\n",
      "iteration 1069, dc_loss: 0.06145111098885536, tv_loss: 0.03324839845299721\n",
      "iteration 1070, dc_loss: 0.061170659959316254, tv_loss: 0.03347664326429367\n",
      "iteration 1071, dc_loss: 0.06124624237418175, tv_loss: 0.033286698162555695\n",
      "iteration 1072, dc_loss: 0.06096258386969566, tv_loss: 0.03342374786734581\n",
      "iteration 1073, dc_loss: 0.06091351807117462, tv_loss: 0.03330594301223755\n",
      "iteration 1074, dc_loss: 0.06077572703361511, tv_loss: 0.03332940861582756\n",
      "iteration 1075, dc_loss: 0.0606267936527729, tv_loss: 0.033396873623132706\n",
      "iteration 1076, dc_loss: 0.060669757425785065, tv_loss: 0.03326625004410744\n",
      "iteration 1077, dc_loss: 0.060418371111154556, tv_loss: 0.03343559056520462\n",
      "iteration 1078, dc_loss: 0.060506757348775864, tv_loss: 0.0332692414522171\n",
      "iteration 1079, dc_loss: 0.06022019311785698, tv_loss: 0.03351004421710968\n",
      "iteration 1080, dc_loss: 0.060265470296144485, tv_loss: 0.033315613865852356\n",
      "iteration 1081, dc_loss: 0.06003226712346077, tv_loss: 0.03340029716491699\n",
      "iteration 1082, dc_loss: 0.059944331645965576, tv_loss: 0.033391404896974564\n",
      "iteration 1083, dc_loss: 0.059900518506765366, tv_loss: 0.033437781035900116\n",
      "iteration 1084, dc_loss: 0.05974050983786583, tv_loss: 0.03342890739440918\n",
      "iteration 1085, dc_loss: 0.059710677713155746, tv_loss: 0.03336448222398758\n",
      "iteration 1086, dc_loss: 0.0595255009829998, tv_loss: 0.0335540845990181\n",
      "iteration 1087, dc_loss: 0.0595780685544014, tv_loss: 0.03332864120602608\n",
      "iteration 1088, dc_loss: 0.059323593974113464, tv_loss: 0.03350396454334259\n",
      "iteration 1089, dc_loss: 0.05939072370529175, tv_loss: 0.03338118642568588\n",
      "iteration 1090, dc_loss: 0.059121135622262955, tv_loss: 0.033493686467409134\n",
      "iteration 1091, dc_loss: 0.059181809425354004, tv_loss: 0.03331032767891884\n",
      "iteration 1092, dc_loss: 0.05893642082810402, tv_loss: 0.03348749503493309\n",
      "iteration 1093, dc_loss: 0.05899205058813095, tv_loss: 0.03337499871850014\n",
      "iteration 1094, dc_loss: 0.05874602496623993, tv_loss: 0.03349510207772255\n",
      "iteration 1095, dc_loss: 0.05882776901125908, tv_loss: 0.03332170471549034\n",
      "iteration 1096, dc_loss: 0.05857878178358078, tv_loss: 0.03352921083569527\n",
      "iteration 1097, dc_loss: 0.058700453490018845, tv_loss: 0.03335007652640343\n",
      "iteration 1098, dc_loss: 0.058460939675569534, tv_loss: 0.033514175564050674\n",
      "iteration 1099, dc_loss: 0.05859607458114624, tv_loss: 0.03331555798649788\n",
      "iteration 1100, dc_loss: 0.05833230912685394, tv_loss: 0.03350362926721573\n",
      "iteration 1101, dc_loss: 0.05847655236721039, tv_loss: 0.0333365760743618\n",
      "iteration 1102, dc_loss: 0.05821878835558891, tv_loss: 0.03352680429816246\n",
      "iteration 1103, dc_loss: 0.05828189477324486, tv_loss: 0.033369213342666626\n",
      "iteration 1104, dc_loss: 0.057980529963970184, tv_loss: 0.033497121185064316\n",
      "iteration 1105, dc_loss: 0.05793217942118645, tv_loss: 0.03338919207453728\n",
      "iteration 1106, dc_loss: 0.05774208903312683, tv_loss: 0.03342194855213165\n",
      "iteration 1107, dc_loss: 0.057617079466581345, tv_loss: 0.03343674913048744\n",
      "iteration 1108, dc_loss: 0.05761498957872391, tv_loss: 0.033356793224811554\n",
      "iteration 1109, dc_loss: 0.05739181488752365, tv_loss: 0.033531807363033295\n",
      "iteration 1110, dc_loss: 0.05760088935494423, tv_loss: 0.033272482454776764\n",
      "iteration 1111, dc_loss: 0.05720604583621025, tv_loss: 0.03365854173898697\n",
      "iteration 1112, dc_loss: 0.057508233934640884, tv_loss: 0.03326946869492531\n",
      "iteration 1113, dc_loss: 0.05702199786901474, tv_loss: 0.03368687257170677\n",
      "iteration 1114, dc_loss: 0.057354409247636795, tv_loss: 0.03323793411254883\n",
      "iteration 1115, dc_loss: 0.05689186230301857, tv_loss: 0.0336671806871891\n",
      "iteration 1116, dc_loss: 0.057155873626470566, tv_loss: 0.03329949826002121\n",
      "iteration 1117, dc_loss: 0.05677534639835358, tv_loss: 0.033598627895116806\n",
      "iteration 1118, dc_loss: 0.056851547211408615, tv_loss: 0.0333419032394886\n",
      "iteration 1119, dc_loss: 0.05653651803731918, tv_loss: 0.03349759429693222\n",
      "iteration 1120, dc_loss: 0.0564652644097805, tv_loss: 0.03342169150710106\n",
      "iteration 1121, dc_loss: 0.0563596747815609, tv_loss: 0.03342406079173088\n",
      "iteration 1122, dc_loss: 0.05622168257832527, tv_loss: 0.03349790349602699\n",
      "iteration 1123, dc_loss: 0.05623892322182655, tv_loss: 0.03344142064452171\n",
      "iteration 1124, dc_loss: 0.056014932692050934, tv_loss: 0.03357608616352081\n",
      "iteration 1125, dc_loss: 0.05603192374110222, tv_loss: 0.03340645879507065\n",
      "iteration 1126, dc_loss: 0.05580364540219307, tv_loss: 0.03354353830218315\n",
      "iteration 1127, dc_loss: 0.055889587849378586, tv_loss: 0.03337594121694565\n",
      "iteration 1128, dc_loss: 0.05570947378873825, tv_loss: 0.033509355038404465\n",
      "iteration 1129, dc_loss: 0.055666252970695496, tv_loss: 0.03346360847353935\n",
      "iteration 1130, dc_loss: 0.05552426725625992, tv_loss: 0.03352336958050728\n",
      "iteration 1131, dc_loss: 0.05539508908987045, tv_loss: 0.033537209033966064\n",
      "iteration 1132, dc_loss: 0.05530693382024765, tv_loss: 0.03347167745232582\n",
      "iteration 1133, dc_loss: 0.05522121116518974, tv_loss: 0.03350203111767769\n",
      "iteration 1134, dc_loss: 0.055174898356199265, tv_loss: 0.033523574471473694\n",
      "iteration 1135, dc_loss: 0.055053867399692535, tv_loss: 0.0335485078394413\n",
      "iteration 1136, dc_loss: 0.055012255907058716, tv_loss: 0.033467601984739304\n",
      "iteration 1137, dc_loss: 0.054876524955034256, tv_loss: 0.03352345898747444\n",
      "iteration 1138, dc_loss: 0.05485404655337334, tv_loss: 0.03352309763431549\n",
      "iteration 1139, dc_loss: 0.05472217872738838, tv_loss: 0.0335674062371254\n",
      "iteration 1140, dc_loss: 0.054678864777088165, tv_loss: 0.03347219526767731\n",
      "iteration 1141, dc_loss: 0.05454600974917412, tv_loss: 0.03353209048509598\n",
      "iteration 1142, dc_loss: 0.05449021980166435, tv_loss: 0.033554013818502426\n",
      "iteration 1143, dc_loss: 0.0543583482503891, tv_loss: 0.03357117995619774\n",
      "iteration 1144, dc_loss: 0.054337792098522186, tv_loss: 0.03347529098391533\n",
      "iteration 1145, dc_loss: 0.054193202406167984, tv_loss: 0.03361751511693001\n",
      "iteration 1146, dc_loss: 0.05418094992637634, tv_loss: 0.033510979264974594\n",
      "iteration 1147, dc_loss: 0.054015278816223145, tv_loss: 0.03355809673666954\n",
      "iteration 1148, dc_loss: 0.0540178120136261, tv_loss: 0.03353114798665047\n",
      "iteration 1149, dc_loss: 0.053843408823013306, tv_loss: 0.033632535487413406\n",
      "iteration 1150, dc_loss: 0.05386738106608391, tv_loss: 0.033471930772066116\n",
      "iteration 1151, dc_loss: 0.053675971925258636, tv_loss: 0.033639855682849884\n",
      "iteration 1152, dc_loss: 0.053755003958940506, tv_loss: 0.03349125385284424\n",
      "iteration 1153, dc_loss: 0.05352040007710457, tv_loss: 0.03361213952302933\n",
      "iteration 1154, dc_loss: 0.05362469330430031, tv_loss: 0.03344922140240669\n",
      "iteration 1155, dc_loss: 0.0533689484000206, tv_loss: 0.033689066767692566\n",
      "iteration 1156, dc_loss: 0.053576577454805374, tv_loss: 0.03341660276055336\n",
      "iteration 1157, dc_loss: 0.053245916962623596, tv_loss: 0.03370768576860428\n",
      "iteration 1158, dc_loss: 0.05357108637690544, tv_loss: 0.033348169177770615\n",
      "iteration 1159, dc_loss: 0.05319218337535858, tv_loss: 0.03378833830356598\n",
      "iteration 1160, dc_loss: 0.053524743765592575, tv_loss: 0.03335261344909668\n",
      "iteration 1161, dc_loss: 0.05304788053035736, tv_loss: 0.03374733030796051\n",
      "iteration 1162, dc_loss: 0.053273044526576996, tv_loss: 0.033382561057806015\n",
      "iteration 1163, dc_loss: 0.05283142626285553, tv_loss: 0.03371455892920494\n",
      "iteration 1164, dc_loss: 0.05289427191019058, tv_loss: 0.033456236124038696\n",
      "iteration 1165, dc_loss: 0.05260356143116951, tv_loss: 0.033586159348487854\n",
      "iteration 1166, dc_loss: 0.05250699073076248, tv_loss: 0.033603306859731674\n",
      "iteration 1167, dc_loss: 0.05259453132748604, tv_loss: 0.03345442935824394\n",
      "iteration 1168, dc_loss: 0.05235229432582855, tv_loss: 0.03367142379283905\n",
      "iteration 1169, dc_loss: 0.05258068069815636, tv_loss: 0.03342992067337036\n",
      "iteration 1170, dc_loss: 0.0522037073969841, tv_loss: 0.033764924854040146\n",
      "iteration 1171, dc_loss: 0.05238231644034386, tv_loss: 0.03344660997390747\n",
      "iteration 1172, dc_loss: 0.05201635882258415, tv_loss: 0.03368552774190903\n",
      "iteration 1173, dc_loss: 0.052067723125219345, tv_loss: 0.03353247418999672\n",
      "iteration 1174, dc_loss: 0.05192260816693306, tv_loss: 0.03358420357108116\n",
      "iteration 1175, dc_loss: 0.05183003470301628, tv_loss: 0.033603742718696594\n",
      "iteration 1176, dc_loss: 0.05188542231917381, tv_loss: 0.033490341156721115\n",
      "iteration 1177, dc_loss: 0.05164574459195137, tv_loss: 0.033694230020046234\n",
      "iteration 1178, dc_loss: 0.05185087025165558, tv_loss: 0.03342580795288086\n",
      "iteration 1179, dc_loss: 0.05150514096021652, tv_loss: 0.033717866986989975\n",
      "iteration 1180, dc_loss: 0.05158499628305435, tv_loss: 0.03353288769721985\n",
      "iteration 1181, dc_loss: 0.05135811120271683, tv_loss: 0.03363416716456413\n",
      "iteration 1182, dc_loss: 0.051339469850063324, tv_loss: 0.033558908849954605\n",
      "iteration 1183, dc_loss: 0.05129392072558403, tv_loss: 0.033534225076436996\n",
      "iteration 1184, dc_loss: 0.051118701696395874, tv_loss: 0.03368039429187775\n",
      "iteration 1185, dc_loss: 0.05125407129526138, tv_loss: 0.03352537378668785\n",
      "iteration 1186, dc_loss: 0.050964392721652985, tv_loss: 0.03373510390520096\n",
      "iteration 1187, dc_loss: 0.05108194798231125, tv_loss: 0.033510126173496246\n",
      "iteration 1188, dc_loss: 0.05084402114152908, tv_loss: 0.033681273460388184\n",
      "iteration 1189, dc_loss: 0.05093337967991829, tv_loss: 0.03357132896780968\n",
      "iteration 1190, dc_loss: 0.05069679021835327, tv_loss: 0.03369800001382828\n",
      "iteration 1191, dc_loss: 0.050778765231370926, tv_loss: 0.033535588532686234\n",
      "iteration 1192, dc_loss: 0.050576843321323395, tv_loss: 0.03370925411581993\n",
      "iteration 1193, dc_loss: 0.05063464865088463, tv_loss: 0.03360670059919357\n",
      "iteration 1194, dc_loss: 0.0504952110350132, tv_loss: 0.03363654762506485\n",
      "iteration 1195, dc_loss: 0.050509754568338394, tv_loss: 0.03360830619931221\n",
      "iteration 1196, dc_loss: 0.0504993200302124, tv_loss: 0.033648379147052765\n",
      "iteration 1197, dc_loss: 0.05054987594485283, tv_loss: 0.03366855904459953\n",
      "iteration 1198, dc_loss: 0.05044778436422348, tv_loss: 0.03365430608391762\n",
      "iteration 1199, dc_loss: 0.050597090274095535, tv_loss: 0.03356771171092987\n",
      "iteration 1200, dc_loss: 0.05041326582431793, tv_loss: 0.033741727471351624\n",
      "iteration 1201, dc_loss: 0.05044388398528099, tv_loss: 0.03356555476784706\n",
      "iteration 1202, dc_loss: 0.04997159168124199, tv_loss: 0.03364759311079979\n",
      "iteration 1203, dc_loss: 0.04998848959803581, tv_loss: 0.033648427575826645\n",
      "iteration 1204, dc_loss: 0.05010438710451126, tv_loss: 0.0336153544485569\n",
      "iteration 1205, dc_loss: 0.049827974289655685, tv_loss: 0.03366415947675705\n",
      "iteration 1206, dc_loss: 0.049785420298576355, tv_loss: 0.033623747527599335\n",
      "iteration 1207, dc_loss: 0.049855202436447144, tv_loss: 0.03360716626048088\n",
      "iteration 1208, dc_loss: 0.04961330443620682, tv_loss: 0.033671002835035324\n",
      "iteration 1209, dc_loss: 0.04961177706718445, tv_loss: 0.03362840414047241\n",
      "iteration 1210, dc_loss: 0.0495893619954586, tv_loss: 0.03363470360636711\n",
      "iteration 1211, dc_loss: 0.04947387054562569, tv_loss: 0.033639486879110336\n",
      "iteration 1212, dc_loss: 0.049423184245824814, tv_loss: 0.03364994004368782\n",
      "iteration 1213, dc_loss: 0.04939187690615654, tv_loss: 0.03364055976271629\n",
      "iteration 1214, dc_loss: 0.04927739128470421, tv_loss: 0.03365088626742363\n",
      "iteration 1215, dc_loss: 0.04925668612122536, tv_loss: 0.03364889323711395\n",
      "iteration 1216, dc_loss: 0.04917334020137787, tv_loss: 0.03365902975201607\n",
      "iteration 1217, dc_loss: 0.04914219677448273, tv_loss: 0.03361854329705238\n",
      "iteration 1218, dc_loss: 0.04905690625309944, tv_loss: 0.0336507223546505\n",
      "iteration 1219, dc_loss: 0.048997167497873306, tv_loss: 0.03367603197693825\n",
      "iteration 1220, dc_loss: 0.0489548034965992, tv_loss: 0.0336180254817009\n",
      "iteration 1221, dc_loss: 0.04889557510614395, tv_loss: 0.03365299478173256\n",
      "iteration 1222, dc_loss: 0.04882710427045822, tv_loss: 0.033674515783786774\n",
      "iteration 1223, dc_loss: 0.048807453364133835, tv_loss: 0.033614035695791245\n",
      "iteration 1224, dc_loss: 0.04869258776307106, tv_loss: 0.033666402101516724\n",
      "iteration 1225, dc_loss: 0.04866831749677658, tv_loss: 0.033664822578430176\n",
      "iteration 1226, dc_loss: 0.04863978177309036, tv_loss: 0.03363160789012909\n",
      "iteration 1227, dc_loss: 0.04853260517120361, tv_loss: 0.03366604819893837\n",
      "iteration 1228, dc_loss: 0.04852043464779854, tv_loss: 0.03361815959215164\n",
      "iteration 1229, dc_loss: 0.04847756773233414, tv_loss: 0.033615730702877045\n",
      "iteration 1230, dc_loss: 0.048370588570833206, tv_loss: 0.0336616113781929\n",
      "iteration 1231, dc_loss: 0.048367343842983246, tv_loss: 0.033613719046115875\n",
      "iteration 1232, dc_loss: 0.048283204436302185, tv_loss: 0.0336395762860775\n",
      "iteration 1233, dc_loss: 0.04822227358818054, tv_loss: 0.033678412437438965\n",
      "iteration 1234, dc_loss: 0.048219889402389526, tv_loss: 0.033635590225458145\n",
      "iteration 1235, dc_loss: 0.04812485724687576, tv_loss: 0.03365197032690048\n",
      "iteration 1236, dc_loss: 0.04806869477033615, tv_loss: 0.03366340696811676\n",
      "iteration 1237, dc_loss: 0.048060450702905655, tv_loss: 0.03363211452960968\n",
      "iteration 1238, dc_loss: 0.04794997349381447, tv_loss: 0.03368665650486946\n",
      "iteration 1239, dc_loss: 0.04792390391230583, tv_loss: 0.033672209829092026\n",
      "iteration 1240, dc_loss: 0.04791034013032913, tv_loss: 0.0336248055100441\n",
      "iteration 1241, dc_loss: 0.04779955372214317, tv_loss: 0.033693328499794006\n",
      "iteration 1242, dc_loss: 0.047782767564058304, tv_loss: 0.03365205600857735\n",
      "iteration 1243, dc_loss: 0.04773614928126335, tv_loss: 0.03363708779215813\n",
      "iteration 1244, dc_loss: 0.04765209183096886, tv_loss: 0.03369538113474846\n",
      "iteration 1245, dc_loss: 0.047644276171922684, tv_loss: 0.033656202256679535\n",
      "iteration 1246, dc_loss: 0.04757238179445267, tv_loss: 0.03365913778543472\n",
      "iteration 1247, dc_loss: 0.0475173182785511, tv_loss: 0.03367418795824051\n",
      "iteration 1248, dc_loss: 0.04747197404503822, tv_loss: 0.033652786165475845\n",
      "iteration 1249, dc_loss: 0.04741370677947998, tv_loss: 0.03365885093808174\n",
      "iteration 1250, dc_loss: 0.04738122224807739, tv_loss: 0.03366102650761604\n",
      "iteration 1251, dc_loss: 0.04731203243136406, tv_loss: 0.03369053453207016\n",
      "iteration 1252, dc_loss: 0.04726957529783249, tv_loss: 0.03369596600532532\n",
      "iteration 1253, dc_loss: 0.04722026363015175, tv_loss: 0.03367609530687332\n",
      "iteration 1254, dc_loss: 0.047171708196401596, tv_loss: 0.03366293013095856\n",
      "iteration 1255, dc_loss: 0.04711911454796791, tv_loss: 0.03367219492793083\n",
      "iteration 1256, dc_loss: 0.04706380516290665, tv_loss: 0.033689070492982864\n",
      "iteration 1257, dc_loss: 0.04702339321374893, tv_loss: 0.033694930374622345\n",
      "iteration 1258, dc_loss: 0.04696991667151451, tv_loss: 0.03368915989995003\n",
      "iteration 1259, dc_loss: 0.046928443014621735, tv_loss: 0.033669814467430115\n",
      "iteration 1260, dc_loss: 0.04686484485864639, tv_loss: 0.0336759090423584\n",
      "iteration 1261, dc_loss: 0.04683494195342064, tv_loss: 0.033667419105768204\n",
      "iteration 1262, dc_loss: 0.04678136110305786, tv_loss: 0.03368755802512169\n",
      "iteration 1263, dc_loss: 0.046707022935152054, tv_loss: 0.03373486548662186\n",
      "iteration 1264, dc_loss: 0.04669485241174698, tv_loss: 0.033685632050037384\n",
      "iteration 1265, dc_loss: 0.046632520854473114, tv_loss: 0.03367232903838158\n",
      "iteration 1266, dc_loss: 0.04657409340143204, tv_loss: 0.033682990819215775\n",
      "iteration 1267, dc_loss: 0.04653018340468407, tv_loss: 0.03368242830038071\n",
      "iteration 1268, dc_loss: 0.046489909291267395, tv_loss: 0.03369145840406418\n",
      "iteration 1269, dc_loss: 0.046443693339824677, tv_loss: 0.033720288425683975\n",
      "iteration 1270, dc_loss: 0.04638121277093887, tv_loss: 0.033714037388563156\n",
      "iteration 1271, dc_loss: 0.046353794634342194, tv_loss: 0.03367602825164795\n",
      "iteration 1272, dc_loss: 0.0462832897901535, tv_loss: 0.033693429082632065\n",
      "iteration 1273, dc_loss: 0.046245500445365906, tv_loss: 0.033699240535497665\n",
      "iteration 1274, dc_loss: 0.04621522128582001, tv_loss: 0.03370283171534538\n",
      "iteration 1275, dc_loss: 0.046134598553180695, tv_loss: 0.03374049440026283\n",
      "iteration 1276, dc_loss: 0.04612020403146744, tv_loss: 0.033692725002765656\n",
      "iteration 1277, dc_loss: 0.046049565076828, tv_loss: 0.03369138762354851\n",
      "iteration 1278, dc_loss: 0.04600849002599716, tv_loss: 0.03369307145476341\n",
      "iteration 1279, dc_loss: 0.04598356783390045, tv_loss: 0.03367239236831665\n",
      "iteration 1280, dc_loss: 0.045897673815488815, tv_loss: 0.033730052411556244\n",
      "iteration 1281, dc_loss: 0.045873433351516724, tv_loss: 0.03373658284544945\n",
      "iteration 1282, dc_loss: 0.04581693932414055, tv_loss: 0.03373177722096443\n",
      "iteration 1283, dc_loss: 0.0457855649292469, tv_loss: 0.03369172662496567\n",
      "iteration 1284, dc_loss: 0.04572587087750435, tv_loss: 0.03370824083685875\n",
      "iteration 1285, dc_loss: 0.045676011592149734, tv_loss: 0.03373321518301964\n",
      "iteration 1286, dc_loss: 0.04564765468239784, tv_loss: 0.033734869211912155\n",
      "iteration 1287, dc_loss: 0.045570697635412216, tv_loss: 0.03374788165092468\n",
      "iteration 1288, dc_loss: 0.04556296765804291, tv_loss: 0.033684514462947845\n",
      "iteration 1289, dc_loss: 0.045501090586185455, tv_loss: 0.03370662406086922\n",
      "iteration 1290, dc_loss: 0.04544215276837349, tv_loss: 0.03374234959483147\n",
      "iteration 1291, dc_loss: 0.04542132467031479, tv_loss: 0.03373541682958603\n",
      "iteration 1292, dc_loss: 0.04534175992012024, tv_loss: 0.033747199922800064\n",
      "iteration 1293, dc_loss: 0.045331504195928574, tv_loss: 0.03369603678584099\n",
      "iteration 1294, dc_loss: 0.04525228589773178, tv_loss: 0.033734578639268875\n",
      "iteration 1295, dc_loss: 0.045224275439977646, tv_loss: 0.03371834754943848\n",
      "iteration 1296, dc_loss: 0.04519921541213989, tv_loss: 0.0337224081158638\n",
      "iteration 1297, dc_loss: 0.04511788859963417, tv_loss: 0.03377065062522888\n",
      "iteration 1298, dc_loss: 0.04510451853275299, tv_loss: 0.03371860086917877\n",
      "iteration 1299, dc_loss: 0.045021697878837585, tv_loss: 0.03373519703745842\n",
      "iteration 1300, dc_loss: 0.04500490427017212, tv_loss: 0.03371580317616463\n",
      "iteration 1301, dc_loss: 0.044959280639886856, tv_loss: 0.033711593598127365\n",
      "iteration 1302, dc_loss: 0.044900793582201004, tv_loss: 0.03372805938124657\n",
      "iteration 1303, dc_loss: 0.04486791044473648, tv_loss: 0.033717162907123566\n",
      "iteration 1304, dc_loss: 0.044819075614213943, tv_loss: 0.03373384848237038\n",
      "iteration 1305, dc_loss: 0.044758688658475876, tv_loss: 0.03377402573823929\n",
      "iteration 1306, dc_loss: 0.04474615305662155, tv_loss: 0.03373412787914276\n",
      "iteration 1307, dc_loss: 0.04466888681054115, tv_loss: 0.033742792904376984\n",
      "iteration 1308, dc_loss: 0.044661492109298706, tv_loss: 0.0337132029235363\n",
      "iteration 1309, dc_loss: 0.04458164796233177, tv_loss: 0.03374718502163887\n",
      "iteration 1310, dc_loss: 0.044553905725479126, tv_loss: 0.03373951464891434\n",
      "iteration 1311, dc_loss: 0.04452751576900482, tv_loss: 0.033751342445611954\n",
      "iteration 1312, dc_loss: 0.04444284364581108, tv_loss: 0.033790260553359985\n",
      "iteration 1313, dc_loss: 0.044439367949962616, tv_loss: 0.033714745193719864\n",
      "iteration 1314, dc_loss: 0.044370513409376144, tv_loss: 0.03373928368091583\n",
      "iteration 1315, dc_loss: 0.044334422796964645, tv_loss: 0.033749014139175415\n",
      "iteration 1316, dc_loss: 0.04429367929697037, tv_loss: 0.033761195838451385\n",
      "iteration 1317, dc_loss: 0.04423018544912338, tv_loss: 0.03377913311123848\n",
      "iteration 1318, dc_loss: 0.04423856362700462, tv_loss: 0.033710677176713943\n",
      "iteration 1319, dc_loss: 0.044133421033620834, tv_loss: 0.03376644104719162\n",
      "iteration 1320, dc_loss: 0.044130194932222366, tv_loss: 0.03372197598218918\n",
      "iteration 1321, dc_loss: 0.04407532513141632, tv_loss: 0.03374861553311348\n",
      "iteration 1322, dc_loss: 0.04401378333568573, tv_loss: 0.03377186134457588\n",
      "iteration 1323, dc_loss: 0.043998897075653076, tv_loss: 0.03375597298145294\n",
      "iteration 1324, dc_loss: 0.04393208771944046, tv_loss: 0.033759620040655136\n",
      "iteration 1325, dc_loss: 0.04390330985188484, tv_loss: 0.03373977914452553\n",
      "iteration 1326, dc_loss: 0.0438615083694458, tv_loss: 0.03374296426773071\n",
      "iteration 1327, dc_loss: 0.043809086084365845, tv_loss: 0.03375430032610893\n",
      "iteration 1328, dc_loss: 0.04377589002251625, tv_loss: 0.03375886753201485\n",
      "iteration 1329, dc_loss: 0.04373488947749138, tv_loss: 0.03378579765558243\n",
      "iteration 1330, dc_loss: 0.04367204010486603, tv_loss: 0.03378082439303398\n",
      "iteration 1331, dc_loss: 0.04366501048207283, tv_loss: 0.033730264753103256\n",
      "iteration 1332, dc_loss: 0.04358606040477753, tv_loss: 0.03379395231604576\n",
      "iteration 1333, dc_loss: 0.043576423078775406, tv_loss: 0.03377757593989372\n",
      "iteration 1334, dc_loss: 0.0435221791267395, tv_loss: 0.03377222642302513\n",
      "iteration 1335, dc_loss: 0.04345456138253212, tv_loss: 0.03378352150321007\n",
      "iteration 1336, dc_loss: 0.04346606507897377, tv_loss: 0.033728692680597305\n",
      "iteration 1337, dc_loss: 0.043362341821193695, tv_loss: 0.03378988057374954\n",
      "iteration 1338, dc_loss: 0.04337938129901886, tv_loss: 0.033734045922756195\n",
      "iteration 1339, dc_loss: 0.043312136083841324, tv_loss: 0.03377356007695198\n",
      "iteration 1340, dc_loss: 0.043258700519800186, tv_loss: 0.033793505281209946\n",
      "iteration 1341, dc_loss: 0.043250687420368195, tv_loss: 0.03375987708568573\n",
      "iteration 1342, dc_loss: 0.04315552860498428, tv_loss: 0.033804234117269516\n",
      "iteration 1343, dc_loss: 0.04317786544561386, tv_loss: 0.033725325018167496\n",
      "iteration 1344, dc_loss: 0.04309173673391342, tv_loss: 0.03377287834882736\n",
      "iteration 1345, dc_loss: 0.043061934411525726, tv_loss: 0.03377839922904968\n",
      "iteration 1346, dc_loss: 0.043031465262174606, tv_loss: 0.033775489777326584\n",
      "iteration 1347, dc_loss: 0.042967576533555984, tv_loss: 0.033799219876527786\n",
      "iteration 1348, dc_loss: 0.042984168976545334, tv_loss: 0.03374123200774193\n",
      "iteration 1349, dc_loss: 0.04286037012934685, tv_loss: 0.033808521926403046\n",
      "iteration 1350, dc_loss: 0.04289180040359497, tv_loss: 0.03374365717172623\n",
      "iteration 1351, dc_loss: 0.04281112179160118, tv_loss: 0.03377983346581459\n",
      "iteration 1352, dc_loss: 0.042758263647556305, tv_loss: 0.033792149275541306\n",
      "iteration 1353, dc_loss: 0.04275602847337723, tv_loss: 0.033752135932445526\n",
      "iteration 1354, dc_loss: 0.042685624212026596, tv_loss: 0.03380211815237999\n",
      "iteration 1355, dc_loss: 0.042678795754909515, tv_loss: 0.03377453237771988\n",
      "iteration 1356, dc_loss: 0.042596329003572464, tv_loss: 0.03380192816257477\n",
      "iteration 1357, dc_loss: 0.04257553443312645, tv_loss: 0.033783819526433945\n",
      "iteration 1358, dc_loss: 0.042546406388282776, tv_loss: 0.033752817660570145\n",
      "iteration 1359, dc_loss: 0.042472001165151596, tv_loss: 0.03380462899804115\n",
      "iteration 1360, dc_loss: 0.04247565567493439, tv_loss: 0.03377695381641388\n",
      "iteration 1361, dc_loss: 0.042413413524627686, tv_loss: 0.0338163860142231\n",
      "iteration 1362, dc_loss: 0.04237142577767372, tv_loss: 0.033803537487983704\n",
      "iteration 1363, dc_loss: 0.042344123125076294, tv_loss: 0.033775415271520615\n",
      "iteration 1364, dc_loss: 0.042283132672309875, tv_loss: 0.03379559516906738\n",
      "iteration 1365, dc_loss: 0.042289771139621735, tv_loss: 0.03374931961297989\n",
      "iteration 1366, dc_loss: 0.04218463599681854, tv_loss: 0.03381132706999779\n",
      "iteration 1367, dc_loss: 0.04220924153923988, tv_loss: 0.033750299364328384\n",
      "iteration 1368, dc_loss: 0.04212921857833862, tv_loss: 0.0338003896176815\n",
      "iteration 1369, dc_loss: 0.042112983763217926, tv_loss: 0.03381636366248131\n",
      "iteration 1370, dc_loss: 0.042055144906044006, tv_loss: 0.033829815685749054\n",
      "iteration 1371, dc_loss: 0.04201526567339897, tv_loss: 0.033791329711675644\n",
      "iteration 1372, dc_loss: 0.04199102520942688, tv_loss: 0.03378310799598694\n",
      "iteration 1373, dc_loss: 0.04192528501152992, tv_loss: 0.033830758184194565\n",
      "iteration 1374, dc_loss: 0.04191288352012634, tv_loss: 0.03381037712097168\n",
      "iteration 1375, dc_loss: 0.04185732454061508, tv_loss: 0.033824119716882706\n",
      "iteration 1376, dc_loss: 0.04185270518064499, tv_loss: 0.03376572206616402\n",
      "iteration 1377, dc_loss: 0.041773173958063126, tv_loss: 0.03380690515041351\n",
      "iteration 1378, dc_loss: 0.04176333174109459, tv_loss: 0.0337817445397377\n",
      "iteration 1379, dc_loss: 0.04171673581004143, tv_loss: 0.03380129486322403\n",
      "iteration 1380, dc_loss: 0.04165640473365784, tv_loss: 0.03384057804942131\n",
      "iteration 1381, dc_loss: 0.04166579619050026, tv_loss: 0.03380177170038223\n",
      "iteration 1382, dc_loss: 0.041566696017980576, tv_loss: 0.03385018929839134\n",
      "iteration 1383, dc_loss: 0.041629865765571594, tv_loss: 0.03373711183667183\n",
      "iteration 1384, dc_loss: 0.04147075116634369, tv_loss: 0.03386669233441353\n",
      "iteration 1385, dc_loss: 0.04157283529639244, tv_loss: 0.03372545167803764\n",
      "iteration 1386, dc_loss: 0.04139818251132965, tv_loss: 0.03385152667760849\n",
      "iteration 1387, dc_loss: 0.041494350880384445, tv_loss: 0.033729568123817444\n",
      "iteration 1388, dc_loss: 0.04133455455303192, tv_loss: 0.0338544026017189\n",
      "iteration 1389, dc_loss: 0.04142781347036362, tv_loss: 0.033731311559677124\n",
      "iteration 1390, dc_loss: 0.041282474994659424, tv_loss: 0.0338791124522686\n",
      "iteration 1391, dc_loss: 0.041377101093530655, tv_loss: 0.033782847225666046\n",
      "iteration 1392, dc_loss: 0.04122287780046463, tv_loss: 0.03390763700008392\n",
      "iteration 1393, dc_loss: 0.04133657366037369, tv_loss: 0.033726807683706284\n",
      "iteration 1394, dc_loss: 0.041122592985630035, tv_loss: 0.033906083554029465\n",
      "iteration 1395, dc_loss: 0.04126719757914543, tv_loss: 0.03370482474565506\n",
      "iteration 1396, dc_loss: 0.04105363413691521, tv_loss: 0.03389497846364975\n",
      "iteration 1397, dc_loss: 0.04117511212825775, tv_loss: 0.033741191029548645\n",
      "iteration 1398, dc_loss: 0.04096490144729614, tv_loss: 0.03392355144023895\n",
      "iteration 1399, dc_loss: 0.041047077625989914, tv_loss: 0.03376682475209236\n",
      "iteration 1400, dc_loss: 0.04090261831879616, tv_loss: 0.033846415579319\n",
      "iteration 1401, dc_loss: 0.04090161621570587, tv_loss: 0.03379356116056442\n",
      "iteration 1402, dc_loss: 0.040850888937711716, tv_loss: 0.033815737813711166\n",
      "iteration 1403, dc_loss: 0.04080682247877121, tv_loss: 0.03382941707968712\n",
      "iteration 1404, dc_loss: 0.04080040007829666, tv_loss: 0.03380177542567253\n",
      "iteration 1405, dc_loss: 0.04071449115872383, tv_loss: 0.03386359661817551\n",
      "iteration 1406, dc_loss: 0.04076600447297096, tv_loss: 0.0337856225669384\n",
      "iteration 1407, dc_loss: 0.04062535986304283, tv_loss: 0.03388933464884758\n",
      "iteration 1408, dc_loss: 0.04069560393691063, tv_loss: 0.03378069028258324\n",
      "iteration 1409, dc_loss: 0.040553852915763855, tv_loss: 0.033879250288009644\n",
      "iteration 1410, dc_loss: 0.04062960669398308, tv_loss: 0.03376433625817299\n",
      "iteration 1411, dc_loss: 0.040485307574272156, tv_loss: 0.03387832269072533\n",
      "iteration 1412, dc_loss: 0.04056818410754204, tv_loss: 0.03375016525387764\n",
      "iteration 1413, dc_loss: 0.04041587933897972, tv_loss: 0.03387148305773735\n",
      "iteration 1414, dc_loss: 0.04048903286457062, tv_loss: 0.03377290815114975\n",
      "iteration 1415, dc_loss: 0.04034990072250366, tv_loss: 0.03387612849473953\n",
      "iteration 1416, dc_loss: 0.04040931537747383, tv_loss: 0.033804845064878464\n",
      "iteration 1417, dc_loss: 0.04028687998652458, tv_loss: 0.03390593081712723\n",
      "iteration 1418, dc_loss: 0.040362436324357986, tv_loss: 0.033783599734306335\n",
      "iteration 1419, dc_loss: 0.04019981250166893, tv_loss: 0.03389803692698479\n",
      "iteration 1420, dc_loss: 0.040284059941768646, tv_loss: 0.0337749719619751\n",
      "iteration 1421, dc_loss: 0.040147535502910614, tv_loss: 0.03387885168194771\n",
      "iteration 1422, dc_loss: 0.04020580276846886, tv_loss: 0.03377337008714676\n",
      "iteration 1423, dc_loss: 0.04006825014948845, tv_loss: 0.03387720510363579\n",
      "iteration 1424, dc_loss: 0.04014185070991516, tv_loss: 0.03378986194729805\n",
      "iteration 1425, dc_loss: 0.03998256474733353, tv_loss: 0.03392072021961212\n",
      "iteration 1426, dc_loss: 0.04007937014102936, tv_loss: 0.03377874568104744\n",
      "iteration 1427, dc_loss: 0.039931073784828186, tv_loss: 0.033899519592523575\n",
      "iteration 1428, dc_loss: 0.040011536329984665, tv_loss: 0.03376854956150055\n",
      "iteration 1429, dc_loss: 0.039851993322372437, tv_loss: 0.033899035304784775\n",
      "iteration 1430, dc_loss: 0.039960600435733795, tv_loss: 0.033766862004995346\n",
      "iteration 1431, dc_loss: 0.039782531559467316, tv_loss: 0.03391512483358383\n",
      "iteration 1432, dc_loss: 0.03991621360182762, tv_loss: 0.033747412264347076\n",
      "iteration 1433, dc_loss: 0.03972664102911949, tv_loss: 0.033919110894203186\n",
      "iteration 1434, dc_loss: 0.03987830877304077, tv_loss: 0.03374943509697914\n",
      "iteration 1435, dc_loss: 0.03966004028916359, tv_loss: 0.0339771993458271\n",
      "iteration 1436, dc_loss: 0.03981827199459076, tv_loss: 0.033767230808734894\n",
      "iteration 1437, dc_loss: 0.039586614817380905, tv_loss: 0.03394828736782074\n",
      "iteration 1438, dc_loss: 0.03975106030702591, tv_loss: 0.033725183457136154\n",
      "iteration 1439, dc_loss: 0.03951110318303108, tv_loss: 0.03392814099788666\n",
      "iteration 1440, dc_loss: 0.03963826596736908, tv_loss: 0.03374793380498886\n",
      "iteration 1441, dc_loss: 0.03945353627204895, tv_loss: 0.03389831632375717\n",
      "iteration 1442, dc_loss: 0.03954347223043442, tv_loss: 0.033790118992328644\n",
      "iteration 1443, dc_loss: 0.039371270686388016, tv_loss: 0.03393927589058876\n",
      "iteration 1444, dc_loss: 0.03942300006747246, tv_loss: 0.03383565694093704\n",
      "iteration 1445, dc_loss: 0.039346903562545776, tv_loss: 0.03385640308260918\n",
      "iteration 1446, dc_loss: 0.03929678350687027, tv_loss: 0.03386535495519638\n",
      "iteration 1447, dc_loss: 0.03932610899209976, tv_loss: 0.03381022438406944\n",
      "iteration 1448, dc_loss: 0.0392390675842762, tv_loss: 0.0338633731007576\n",
      "iteration 1449, dc_loss: 0.03926708921790123, tv_loss: 0.03380601853132248\n",
      "iteration 1450, dc_loss: 0.03915303200483322, tv_loss: 0.03390192613005638\n",
      "iteration 1451, dc_loss: 0.03924008458852768, tv_loss: 0.033826254308223724\n",
      "iteration 1452, dc_loss: 0.039083417505025864, tv_loss: 0.03395487740635872\n",
      "iteration 1453, dc_loss: 0.03919551894068718, tv_loss: 0.033793047070503235\n",
      "iteration 1454, dc_loss: 0.039020437747240067, tv_loss: 0.03392511233687401\n",
      "iteration 1455, dc_loss: 0.039136290550231934, tv_loss: 0.033795785158872604\n",
      "iteration 1456, dc_loss: 0.03896300122141838, tv_loss: 0.03395076096057892\n",
      "iteration 1457, dc_loss: 0.03908366709947586, tv_loss: 0.03379199281334877\n",
      "iteration 1458, dc_loss: 0.038900766521692276, tv_loss: 0.03390897065401077\n",
      "iteration 1459, dc_loss: 0.03895711153745651, tv_loss: 0.033817026764154434\n",
      "iteration 1460, dc_loss: 0.03885035216808319, tv_loss: 0.0338703878223896\n",
      "iteration 1461, dc_loss: 0.03884925693273544, tv_loss: 0.033849623054265976\n",
      "iteration 1462, dc_loss: 0.038811229169368744, tv_loss: 0.03387652337551117\n",
      "iteration 1463, dc_loss: 0.038766779005527496, tv_loss: 0.033897314220666885\n",
      "iteration 1464, dc_loss: 0.03877608850598335, tv_loss: 0.03385656699538231\n",
      "iteration 1465, dc_loss: 0.038683827966451645, tv_loss: 0.03389732539653778\n",
      "iteration 1466, dc_loss: 0.03874872624874115, tv_loss: 0.03381224721670151\n",
      "iteration 1467, dc_loss: 0.03861381858587265, tv_loss: 0.03393298014998436\n",
      "iteration 1468, dc_loss: 0.038758955895900726, tv_loss: 0.03378654271364212\n",
      "iteration 1469, dc_loss: 0.038556020706892014, tv_loss: 0.0339965894818306\n",
      "iteration 1470, dc_loss: 0.038787174969911575, tv_loss: 0.033759478479623795\n",
      "iteration 1471, dc_loss: 0.03851131722331047, tv_loss: 0.03402402624487877\n",
      "iteration 1472, dc_loss: 0.03880290314555168, tv_loss: 0.0337115079164505\n",
      "iteration 1473, dc_loss: 0.038449324667453766, tv_loss: 0.034065477550029755\n",
      "iteration 1474, dc_loss: 0.03876776993274689, tv_loss: 0.03370114788413048\n",
      "iteration 1475, dc_loss: 0.03836079686880112, tv_loss: 0.03406171128153801\n",
      "iteration 1476, dc_loss: 0.03858138248324394, tv_loss: 0.033742114901542664\n",
      "iteration 1477, dc_loss: 0.038261644542217255, tv_loss: 0.033962808549404144\n",
      "iteration 1478, dc_loss: 0.03834294155240059, tv_loss: 0.033802103251218796\n",
      "iteration 1479, dc_loss: 0.0382327102124691, tv_loss: 0.0338732935488224\n",
      "iteration 1480, dc_loss: 0.03819321468472481, tv_loss: 0.03388978913426399\n",
      "iteration 1481, dc_loss: 0.038257941603660583, tv_loss: 0.03380260244011879\n",
      "iteration 1482, dc_loss: 0.03810754045844078, tv_loss: 0.033936306834220886\n",
      "iteration 1483, dc_loss: 0.03826215863227844, tv_loss: 0.033770449459552765\n",
      "iteration 1484, dc_loss: 0.03804726153612137, tv_loss: 0.03400256484746933\n",
      "iteration 1485, dc_loss: 0.03816315531730652, tv_loss: 0.033817898482084274\n",
      "iteration 1486, dc_loss: 0.03801017999649048, tv_loss: 0.03391800448298454\n",
      "iteration 1487, dc_loss: 0.03800755366683006, tv_loss: 0.0338832288980484\n",
      "iteration 1488, dc_loss: 0.03803609311580658, tv_loss: 0.033834103494882584\n",
      "iteration 1489, dc_loss: 0.03789925575256348, tv_loss: 0.033997200429439545\n",
      "iteration 1490, dc_loss: 0.0380110926926136, tv_loss: 0.03381911292672157\n",
      "iteration 1491, dc_loss: 0.03782396763563156, tv_loss: 0.03395804017782211\n",
      "iteration 1492, dc_loss: 0.037953075021505356, tv_loss: 0.03382232040166855\n",
      "iteration 1493, dc_loss: 0.03775655850768089, tv_loss: 0.03398645296692848\n",
      "iteration 1494, dc_loss: 0.03786598891019821, tv_loss: 0.033849071711301804\n",
      "iteration 1495, dc_loss: 0.03772738575935364, tv_loss: 0.03390752524137497\n",
      "iteration 1496, dc_loss: 0.03775233030319214, tv_loss: 0.033870287239551544\n",
      "iteration 1497, dc_loss: 0.03768272325396538, tv_loss: 0.033917661756277084\n",
      "iteration 1498, dc_loss: 0.037646979093551636, tv_loss: 0.03391861170530319\n",
      "iteration 1499, dc_loss: 0.037662237882614136, tv_loss: 0.033858705312013626\n",
      "iteration 1500, dc_loss: 0.03755418583750725, tv_loss: 0.0339297391474247\n",
      "iteration 1501, dc_loss: 0.03767695277929306, tv_loss: 0.03382068872451782\n",
      "iteration 1502, dc_loss: 0.03748767450451851, tv_loss: 0.03399171680212021\n",
      "iteration 1503, dc_loss: 0.037610143423080444, tv_loss: 0.033816538751125336\n",
      "iteration 1504, dc_loss: 0.03742663189768791, tv_loss: 0.033963482826948166\n",
      "iteration 1505, dc_loss: 0.0375196635723114, tv_loss: 0.033832404762506485\n",
      "iteration 1506, dc_loss: 0.03737899288535118, tv_loss: 0.033944737166166306\n",
      "iteration 1507, dc_loss: 0.037467118352651596, tv_loss: 0.03382977098226547\n",
      "iteration 1508, dc_loss: 0.03733896464109421, tv_loss: 0.03393974155187607\n",
      "iteration 1509, dc_loss: 0.03738997504115105, tv_loss: 0.033880677074193954\n",
      "iteration 1510, dc_loss: 0.03731519356369972, tv_loss: 0.03393985331058502\n",
      "iteration 1511, dc_loss: 0.03738198056817055, tv_loss: 0.03384438902139664\n",
      "iteration 1512, dc_loss: 0.037272389978170395, tv_loss: 0.03394676372408867\n",
      "iteration 1513, dc_loss: 0.037366826087236404, tv_loss: 0.033851396292448044\n",
      "iteration 1514, dc_loss: 0.03731765225529671, tv_loss: 0.033930376172065735\n",
      "iteration 1515, dc_loss: 0.03739986568689346, tv_loss: 0.0338437519967556\n",
      "iteration 1516, dc_loss: 0.037402473390102386, tv_loss: 0.03392685204744339\n",
      "iteration 1517, dc_loss: 0.03743593767285347, tv_loss: 0.03389477729797363\n",
      "iteration 1518, dc_loss: 0.037416841834783554, tv_loss: 0.0339360237121582\n",
      "iteration 1519, dc_loss: 0.03729711472988129, tv_loss: 0.03392704948782921\n",
      "iteration 1520, dc_loss: 0.03725460171699524, tv_loss: 0.0338381789624691\n",
      "iteration 1521, dc_loss: 0.03699909523129463, tv_loss: 0.03395844250917435\n",
      "iteration 1522, dc_loss: 0.0370502844452858, tv_loss: 0.033826690167188644\n",
      "iteration 1523, dc_loss: 0.03690338507294655, tv_loss: 0.03396529704332352\n",
      "iteration 1524, dc_loss: 0.037066828459501266, tv_loss: 0.033852189779281616\n",
      "iteration 1525, dc_loss: 0.03695622459053993, tv_loss: 0.033974841237068176\n",
      "iteration 1526, dc_loss: 0.037032611668109894, tv_loss: 0.03384649381041527\n",
      "iteration 1527, dc_loss: 0.03687093034386635, tv_loss: 0.0339425690472126\n",
      "iteration 1528, dc_loss: 0.03680599853396416, tv_loss: 0.03391370549798012\n",
      "iteration 1529, dc_loss: 0.03678521141409874, tv_loss: 0.033923838287591934\n",
      "iteration 1530, dc_loss: 0.036723602563142776, tv_loss: 0.03394332155585289\n",
      "iteration 1531, dc_loss: 0.03678569570183754, tv_loss: 0.03388496860861778\n",
      "iteration 1532, dc_loss: 0.03673464059829712, tv_loss: 0.033937618136405945\n",
      "iteration 1533, dc_loss: 0.03676847368478775, tv_loss: 0.03384890779852867\n",
      "iteration 1534, dc_loss: 0.0365951769053936, tv_loss: 0.0339861735701561\n",
      "iteration 1535, dc_loss: 0.036662351340055466, tv_loss: 0.033849187195301056\n",
      "iteration 1536, dc_loss: 0.03653787821531296, tv_loss: 0.03394578769803047\n",
      "iteration 1537, dc_loss: 0.036621227860450745, tv_loss: 0.03386237472295761\n",
      "iteration 1538, dc_loss: 0.03649262711405754, tv_loss: 0.0339813306927681\n",
      "iteration 1539, dc_loss: 0.03659908473491669, tv_loss: 0.03385613113641739\n",
      "iteration 1540, dc_loss: 0.03644867613911629, tv_loss: 0.03394876793026924\n",
      "iteration 1541, dc_loss: 0.036495186388492584, tv_loss: 0.033851757645606995\n",
      "iteration 1542, dc_loss: 0.03635840490460396, tv_loss: 0.03395265340805054\n",
      "iteration 1543, dc_loss: 0.036422234028577805, tv_loss: 0.03386398404836655\n",
      "iteration 1544, dc_loss: 0.03630455583333969, tv_loss: 0.03393929824233055\n",
      "iteration 1545, dc_loss: 0.036393243819475174, tv_loss: 0.03383402153849602\n",
      "iteration 1546, dc_loss: 0.03626958653330803, tv_loss: 0.03395288065075874\n",
      "iteration 1547, dc_loss: 0.03633567690849304, tv_loss: 0.03386200964450836\n",
      "iteration 1548, dc_loss: 0.036220844835042953, tv_loss: 0.0339394211769104\n",
      "iteration 1549, dc_loss: 0.03624469041824341, tv_loss: 0.033868446946144104\n",
      "iteration 1550, dc_loss: 0.036140356212854385, tv_loss: 0.03395643085241318\n",
      "iteration 1551, dc_loss: 0.036183152347803116, tv_loss: 0.03391927108168602\n",
      "iteration 1552, dc_loss: 0.03609200939536095, tv_loss: 0.033988628536462784\n",
      "iteration 1553, dc_loss: 0.036140430718660355, tv_loss: 0.03387707471847534\n",
      "iteration 1554, dc_loss: 0.0360458567738533, tv_loss: 0.033962927758693695\n",
      "iteration 1555, dc_loss: 0.036093030124902725, tv_loss: 0.03391672298312187\n",
      "iteration 1556, dc_loss: 0.035990893840789795, tv_loss: 0.03396853059530258\n",
      "iteration 1557, dc_loss: 0.03601427376270294, tv_loss: 0.033899128437042236\n",
      "iteration 1558, dc_loss: 0.03596033900976181, tv_loss: 0.03392336517572403\n",
      "iteration 1559, dc_loss: 0.03592999652028084, tv_loss: 0.033948998898267746\n",
      "iteration 1560, dc_loss: 0.03588784486055374, tv_loss: 0.033953066915273666\n",
      "iteration 1561, dc_loss: 0.035897232592105865, tv_loss: 0.0339043103158474\n",
      "iteration 1562, dc_loss: 0.035839200019836426, tv_loss: 0.033935945481061935\n",
      "iteration 1563, dc_loss: 0.035831499844789505, tv_loss: 0.033921707421541214\n",
      "iteration 1564, dc_loss: 0.03578701242804527, tv_loss: 0.0339658297598362\n",
      "iteration 1565, dc_loss: 0.035782843828201294, tv_loss: 0.033930618315935135\n",
      "iteration 1566, dc_loss: 0.03574822098016739, tv_loss: 0.03393053635954857\n",
      "iteration 1567, dc_loss: 0.035712432116270065, tv_loss: 0.03393978625535965\n",
      "iteration 1568, dc_loss: 0.035732876509428024, tv_loss: 0.03388246148824692\n",
      "iteration 1569, dc_loss: 0.0356447696685791, tv_loss: 0.03395682945847511\n",
      "iteration 1570, dc_loss: 0.03566093370318413, tv_loss: 0.033910997211933136\n",
      "iteration 1571, dc_loss: 0.035606492310762405, tv_loss: 0.03393293917179108\n",
      "iteration 1572, dc_loss: 0.03562868759036064, tv_loss: 0.03388930857181549\n",
      "iteration 1573, dc_loss: 0.03553120046854019, tv_loss: 0.03396078199148178\n",
      "iteration 1574, dc_loss: 0.03559943288564682, tv_loss: 0.0338718481361866\n",
      "iteration 1575, dc_loss: 0.035456765443086624, tv_loss: 0.03399602696299553\n",
      "iteration 1576, dc_loss: 0.03562343493103981, tv_loss: 0.0338253453373909\n",
      "iteration 1577, dc_loss: 0.03539272025227547, tv_loss: 0.03405488654971123\n",
      "iteration 1578, dc_loss: 0.035665590316057205, tv_loss: 0.03380561247467995\n",
      "iteration 1579, dc_loss: 0.03539034351706505, tv_loss: 0.034120600670576096\n",
      "iteration 1580, dc_loss: 0.03590890020132065, tv_loss: 0.03368310257792473\n",
      "iteration 1581, dc_loss: 0.03552393242716789, tv_loss: 0.03426896408200264\n",
      "iteration 1582, dc_loss: 0.03629976511001587, tv_loss: 0.03361235558986664\n",
      "iteration 1583, dc_loss: 0.035696182399988174, tv_loss: 0.034349240362644196\n",
      "iteration 1584, dc_loss: 0.03611329570412636, tv_loss: 0.033638738095760345\n",
      "iteration 1585, dc_loss: 0.03532439470291138, tv_loss: 0.0341494083404541\n",
      "iteration 1586, dc_loss: 0.03536786139011383, tv_loss: 0.033868297934532166\n",
      "iteration 1587, dc_loss: 0.035371001809835434, tv_loss: 0.033861663192510605\n",
      "iteration 1588, dc_loss: 0.035196702927351, tv_loss: 0.03418031707406044\n",
      "iteration 1589, dc_loss: 0.03571111708879471, tv_loss: 0.03366776928305626\n",
      "iteration 1590, dc_loss: 0.03510918468236923, tv_loss: 0.03414022549986839\n",
      "iteration 1591, dc_loss: 0.035215701907873154, tv_loss: 0.03384793922305107\n",
      "iteration 1592, dc_loss: 0.03520618751645088, tv_loss: 0.03383587673306465\n",
      "iteration 1593, dc_loss: 0.03502015024423599, tv_loss: 0.03409450873732567\n",
      "iteration 1594, dc_loss: 0.03535942733287811, tv_loss: 0.033769313246011734\n",
      "iteration 1595, dc_loss: 0.03498636558651924, tv_loss: 0.03406615927815437\n",
      "iteration 1596, dc_loss: 0.03501860797405243, tv_loss: 0.03393698111176491\n",
      "iteration 1597, dc_loss: 0.03507646545767784, tv_loss: 0.033865537494421005\n",
      "iteration 1598, dc_loss: 0.03491395339369774, tv_loss: 0.03404923900961876\n",
      "iteration 1599, dc_loss: 0.035124458372592926, tv_loss: 0.03381909802556038\n",
      "iteration 1600, dc_loss: 0.03486030176281929, tv_loss: 0.034017834812402725\n",
      "iteration 1601, dc_loss: 0.03487824276089668, tv_loss: 0.03393780067563057\n",
      "iteration 1602, dc_loss: 0.03492221236228943, tv_loss: 0.033857639878988266\n",
      "iteration 1603, dc_loss: 0.03479325398802757, tv_loss: 0.033958550542593\n",
      "iteration 1604, dc_loss: 0.03479135036468506, tv_loss: 0.033934012055397034\n",
      "iteration 1605, dc_loss: 0.03485892713069916, tv_loss: 0.03386141359806061\n",
      "iteration 1606, dc_loss: 0.03474578261375427, tv_loss: 0.03395720198750496\n",
      "iteration 1607, dc_loss: 0.03472120314836502, tv_loss: 0.03395861014723778\n",
      "iteration 1608, dc_loss: 0.034776490181684494, tv_loss: 0.033879354596138\n",
      "iteration 1609, dc_loss: 0.03468727320432663, tv_loss: 0.03392772376537323\n",
      "iteration 1610, dc_loss: 0.03467424586415291, tv_loss: 0.033936794847249985\n",
      "iteration 1611, dc_loss: 0.03470822423696518, tv_loss: 0.03390219435095787\n",
      "iteration 1612, dc_loss: 0.0346301905810833, tv_loss: 0.033950258046388626\n",
      "iteration 1613, dc_loss: 0.03460939601063728, tv_loss: 0.03393461927771568\n",
      "iteration 1614, dc_loss: 0.03464072570204735, tv_loss: 0.03388708829879761\n",
      "iteration 1615, dc_loss: 0.03456674888730049, tv_loss: 0.03393464908003807\n",
      "iteration 1616, dc_loss: 0.034546028822660446, tv_loss: 0.033955641090869904\n",
      "iteration 1617, dc_loss: 0.03457861766219139, tv_loss: 0.0338924303650856\n",
      "iteration 1618, dc_loss: 0.034513458609580994, tv_loss: 0.033928170800209045\n",
      "iteration 1619, dc_loss: 0.034487854689359665, tv_loss: 0.033932801336050034\n",
      "iteration 1620, dc_loss: 0.03451862931251526, tv_loss: 0.03390539810061455\n",
      "iteration 1621, dc_loss: 0.034458525478839874, tv_loss: 0.033975474536418915\n",
      "iteration 1622, dc_loss: 0.0344189815223217, tv_loss: 0.0339578241109848\n",
      "iteration 1623, dc_loss: 0.03446156904101372, tv_loss: 0.03388771414756775\n",
      "iteration 1624, dc_loss: 0.0344092883169651, tv_loss: 0.03394872695207596\n",
      "iteration 1625, dc_loss: 0.03436225652694702, tv_loss: 0.0339934267103672\n",
      "iteration 1626, dc_loss: 0.0343899168074131, tv_loss: 0.03390879929065704\n",
      "iteration 1627, dc_loss: 0.03434975817799568, tv_loss: 0.03392389416694641\n",
      "iteration 1628, dc_loss: 0.03431443125009537, tv_loss: 0.03398258984088898\n",
      "iteration 1629, dc_loss: 0.03433595970273018, tv_loss: 0.03391989693045616\n",
      "iteration 1630, dc_loss: 0.034289490431547165, tv_loss: 0.03393681347370148\n",
      "iteration 1631, dc_loss: 0.03425023704767227, tv_loss: 0.03397742286324501\n",
      "iteration 1632, dc_loss: 0.03428063169121742, tv_loss: 0.033955059945583344\n",
      "iteration 1633, dc_loss: 0.034239139407873154, tv_loss: 0.03392687812447548\n",
      "iteration 1634, dc_loss: 0.034201741218566895, tv_loss: 0.0339721255004406\n",
      "iteration 1635, dc_loss: 0.03420776128768921, tv_loss: 0.03396178409457207\n",
      "iteration 1636, dc_loss: 0.0341753326356411, tv_loss: 0.03394254297018051\n",
      "iteration 1637, dc_loss: 0.0341629683971405, tv_loss: 0.03392808884382248\n",
      "iteration 1638, dc_loss: 0.03415227308869362, tv_loss: 0.033952925354242325\n",
      "iteration 1639, dc_loss: 0.03411800041794777, tv_loss: 0.03396494686603546\n",
      "iteration 1640, dc_loss: 0.03410862758755684, tv_loss: 0.0339285209774971\n",
      "iteration 1641, dc_loss: 0.03408823534846306, tv_loss: 0.03392889350652695\n",
      "iteration 1642, dc_loss: 0.034055136144161224, tv_loss: 0.033949512988328934\n",
      "iteration 1643, dc_loss: 0.03406216949224472, tv_loss: 0.033935464918613434\n",
      "iteration 1644, dc_loss: 0.034031543880701065, tv_loss: 0.03393622115254402\n",
      "iteration 1645, dc_loss: 0.03399654105305672, tv_loss: 0.03394756093621254\n",
      "iteration 1646, dc_loss: 0.0340031236410141, tv_loss: 0.03391105309128761\n",
      "iteration 1647, dc_loss: 0.03398117423057556, tv_loss: 0.03391682729125023\n",
      "iteration 1648, dc_loss: 0.03395138308405876, tv_loss: 0.0339244045317173\n",
      "iteration 1649, dc_loss: 0.03394197300076485, tv_loss: 0.033915240317583084\n",
      "iteration 1650, dc_loss: 0.03390752524137497, tv_loss: 0.03393854573369026\n",
      "iteration 1651, dc_loss: 0.033903270959854126, tv_loss: 0.03393518179655075\n",
      "iteration 1652, dc_loss: 0.03390133008360863, tv_loss: 0.03394964709877968\n",
      "iteration 1653, dc_loss: 0.03386242687702179, tv_loss: 0.03395485505461693\n",
      "iteration 1654, dc_loss: 0.03383844718337059, tv_loss: 0.03393179178237915\n",
      "iteration 1655, dc_loss: 0.03383160009980202, tv_loss: 0.03392485901713371\n",
      "iteration 1656, dc_loss: 0.03380249813199043, tv_loss: 0.03396880254149437\n",
      "iteration 1657, dc_loss: 0.03379477933049202, tv_loss: 0.03396070748567581\n",
      "iteration 1658, dc_loss: 0.03378012403845787, tv_loss: 0.03392583131790161\n",
      "iteration 1659, dc_loss: 0.03374306857585907, tv_loss: 0.03395211324095726\n",
      "iteration 1660, dc_loss: 0.03374619781970978, tv_loss: 0.03397824615240097\n",
      "iteration 1661, dc_loss: 0.03372740373015404, tv_loss: 0.033943407237529755\n",
      "iteration 1662, dc_loss: 0.03369010612368584, tv_loss: 0.03394994139671326\n",
      "iteration 1663, dc_loss: 0.03368952125310898, tv_loss: 0.033966317772865295\n",
      "iteration 1664, dc_loss: 0.033676330000162125, tv_loss: 0.03396865352988243\n",
      "iteration 1665, dc_loss: 0.03363218158483505, tv_loss: 0.03395968675613403\n",
      "iteration 1666, dc_loss: 0.03362785652279854, tv_loss: 0.0339568629860878\n",
      "iteration 1667, dc_loss: 0.03361748903989792, tv_loss: 0.033963318914175034\n",
      "iteration 1668, dc_loss: 0.033589884638786316, tv_loss: 0.03395909070968628\n",
      "iteration 1669, dc_loss: 0.033590737730264664, tv_loss: 0.03392132371664047\n",
      "iteration 1670, dc_loss: 0.033553700894117355, tv_loss: 0.033945757895708084\n",
      "iteration 1671, dc_loss: 0.0335342176258564, tv_loss: 0.03395852819085121\n",
      "iteration 1672, dc_loss: 0.033532701432704926, tv_loss: 0.033944979310035706\n",
      "iteration 1673, dc_loss: 0.03349608927965164, tv_loss: 0.03395308554172516\n",
      "iteration 1674, dc_loss: 0.03349423408508301, tv_loss: 0.03392721340060234\n",
      "iteration 1675, dc_loss: 0.0334751233458519, tv_loss: 0.03392687067389488\n",
      "iteration 1676, dc_loss: 0.03343893960118294, tv_loss: 0.03394433110952377\n",
      "iteration 1677, dc_loss: 0.033440880477428436, tv_loss: 0.03392387554049492\n",
      "iteration 1678, dc_loss: 0.03341468423604965, tv_loss: 0.0339428186416626\n",
      "iteration 1679, dc_loss: 0.03339263051748276, tv_loss: 0.033954937011003494\n",
      "iteration 1680, dc_loss: 0.03339054808020592, tv_loss: 0.03395356237888336\n",
      "iteration 1681, dc_loss: 0.03336102142930031, tv_loss: 0.033955879509449005\n",
      "iteration 1682, dc_loss: 0.03333891183137894, tv_loss: 0.03394068405032158\n",
      "iteration 1683, dc_loss: 0.033344827592372894, tv_loss: 0.0339142270386219\n",
      "iteration 1684, dc_loss: 0.033304885029792786, tv_loss: 0.03393600508570671\n",
      "iteration 1685, dc_loss: 0.03328520432114601, tv_loss: 0.03394157066941261\n",
      "iteration 1686, dc_loss: 0.03328433260321617, tv_loss: 0.03393490985035896\n",
      "iteration 1687, dc_loss: 0.033252399414777756, tv_loss: 0.033955495804548264\n",
      "iteration 1688, dc_loss: 0.03323816508054733, tv_loss: 0.03396286815404892\n",
      "iteration 1689, dc_loss: 0.03322487324476242, tv_loss: 0.03394921496510506\n",
      "iteration 1690, dc_loss: 0.0332135409116745, tv_loss: 0.033931516110897064\n",
      "iteration 1691, dc_loss: 0.03319500759243965, tv_loss: 0.03392768278717995\n",
      "iteration 1692, dc_loss: 0.0331580713391304, tv_loss: 0.03394278138875961\n",
      "iteration 1693, dc_loss: 0.03315562754869461, tv_loss: 0.03392946720123291\n",
      "iteration 1694, dc_loss: 0.0331440344452858, tv_loss: 0.03394080325961113\n",
      "iteration 1695, dc_loss: 0.033113349229097366, tv_loss: 0.033973999321460724\n",
      "iteration 1696, dc_loss: 0.033106569200754166, tv_loss: 0.03396372124552727\n",
      "iteration 1697, dc_loss: 0.0330955870449543, tv_loss: 0.033940330147743225\n",
      "iteration 1698, dc_loss: 0.03306182473897934, tv_loss: 0.03394860774278641\n",
      "iteration 1699, dc_loss: 0.033048126846551895, tv_loss: 0.03394189104437828\n",
      "iteration 1700, dc_loss: 0.03303544968366623, tv_loss: 0.033933475613594055\n",
      "iteration 1701, dc_loss: 0.033023372292518616, tv_loss: 0.03392921760678291\n",
      "iteration 1702, dc_loss: 0.032995834946632385, tv_loss: 0.033941060304641724\n",
      "iteration 1703, dc_loss: 0.032986175268888474, tv_loss: 0.03393806144595146\n",
      "iteration 1704, dc_loss: 0.03298025205731392, tv_loss: 0.03393229842185974\n",
      "iteration 1705, dc_loss: 0.03294404596090317, tv_loss: 0.03396190330386162\n",
      "iteration 1706, dc_loss: 0.032929837703704834, tv_loss: 0.03397856652736664\n",
      "iteration 1707, dc_loss: 0.032919738441705704, tv_loss: 0.03395187854766846\n",
      "iteration 1708, dc_loss: 0.032888997346162796, tv_loss: 0.0339488722383976\n",
      "iteration 1709, dc_loss: 0.03288600221276283, tv_loss: 0.03394211083650589\n",
      "iteration 1710, dc_loss: 0.03287901356816292, tv_loss: 0.03392690792679787\n",
      "iteration 1711, dc_loss: 0.03283601626753807, tv_loss: 0.03395511955022812\n",
      "iteration 1712, dc_loss: 0.03283454105257988, tv_loss: 0.033938754349946976\n",
      "iteration 1713, dc_loss: 0.03282095119357109, tv_loss: 0.033937059342861176\n",
      "iteration 1714, dc_loss: 0.032784100621938705, tv_loss: 0.0339634045958519\n",
      "iteration 1715, dc_loss: 0.03279271349310875, tv_loss: 0.033929403871297836\n",
      "iteration 1716, dc_loss: 0.03277824446558952, tv_loss: 0.03393172100186348\n",
      "iteration 1717, dc_loss: 0.03273747116327286, tv_loss: 0.033969391137361526\n",
      "iteration 1718, dc_loss: 0.03273659199476242, tv_loss: 0.033940356224775314\n",
      "iteration 1719, dc_loss: 0.03271600231528282, tv_loss: 0.03394309803843498\n",
      "iteration 1720, dc_loss: 0.03269001096487045, tv_loss: 0.03397122025489807\n",
      "iteration 1721, dc_loss: 0.03269514441490173, tv_loss: 0.03392944112420082\n",
      "iteration 1722, dc_loss: 0.03266299515962601, tv_loss: 0.03394995629787445\n",
      "iteration 1723, dc_loss: 0.03264384716749191, tv_loss: 0.0339571051299572\n",
      "iteration 1724, dc_loss: 0.03263982757925987, tv_loss: 0.033939674496650696\n",
      "iteration 1725, dc_loss: 0.032606784254312515, tv_loss: 0.033949945122003555\n",
      "iteration 1726, dc_loss: 0.032614875584840775, tv_loss: 0.03392137959599495\n",
      "iteration 1727, dc_loss: 0.03258325904607773, tv_loss: 0.0339527353644371\n",
      "iteration 1728, dc_loss: 0.0325598381459713, tv_loss: 0.03396066278219223\n",
      "iteration 1729, dc_loss: 0.03256441280245781, tv_loss: 0.03394996002316475\n",
      "iteration 1730, dc_loss: 0.032529450953006744, tv_loss: 0.0339837372303009\n",
      "iteration 1731, dc_loss: 0.032514527440071106, tv_loss: 0.033980436623096466\n",
      "iteration 1732, dc_loss: 0.03250691294670105, tv_loss: 0.03396126627922058\n",
      "iteration 1733, dc_loss: 0.03249124437570572, tv_loss: 0.03394021838903427\n",
      "iteration 1734, dc_loss: 0.03246438875794411, tv_loss: 0.03395327553153038\n",
      "iteration 1735, dc_loss: 0.03245692327618599, tv_loss: 0.033944759517908096\n",
      "iteration 1736, dc_loss: 0.032444629818201065, tv_loss: 0.03394052013754845\n",
      "iteration 1737, dc_loss: 0.03241300955414772, tv_loss: 0.033949270844459534\n",
      "iteration 1738, dc_loss: 0.0324113667011261, tv_loss: 0.03394260257482529\n",
      "iteration 1739, dc_loss: 0.0323970690369606, tv_loss: 0.033958159387111664\n",
      "iteration 1740, dc_loss: 0.032364435493946075, tv_loss: 0.03400047868490219\n",
      "iteration 1741, dc_loss: 0.03235618397593498, tv_loss: 0.03396698459982872\n",
      "iteration 1742, dc_loss: 0.03235192596912384, tv_loss: 0.033938758075237274\n",
      "iteration 1743, dc_loss: 0.03231431916356087, tv_loss: 0.03396592661738396\n",
      "iteration 1744, dc_loss: 0.03230650722980499, tv_loss: 0.03398291394114494\n",
      "iteration 1745, dc_loss: 0.0322989821434021, tv_loss: 0.0339730829000473\n",
      "iteration 1746, dc_loss: 0.03226893022656441, tv_loss: 0.03396596759557724\n",
      "iteration 1747, dc_loss: 0.03227660432457924, tv_loss: 0.033944666385650635\n",
      "iteration 1748, dc_loss: 0.03223626688122749, tv_loss: 0.033974528312683105\n",
      "iteration 1749, dc_loss: 0.032222338020801544, tv_loss: 0.03397506847977638\n",
      "iteration 1750, dc_loss: 0.03224094584584236, tv_loss: 0.03393257409334183\n",
      "iteration 1751, dc_loss: 0.032175417989492416, tv_loss: 0.033975593745708466\n",
      "iteration 1752, dc_loss: 0.03219256177544594, tv_loss: 0.03393912315368652\n",
      "iteration 1753, dc_loss: 0.03217554837465286, tv_loss: 0.033951010555028915\n",
      "iteration 1754, dc_loss: 0.03213982284069061, tv_loss: 0.03397899121046066\n",
      "iteration 1755, dc_loss: 0.032139889895915985, tv_loss: 0.03398605436086655\n",
      "iteration 1756, dc_loss: 0.03213341161608696, tv_loss: 0.033963222056627274\n",
      "iteration 1757, dc_loss: 0.03211260214447975, tv_loss: 0.03395070880651474\n",
      "iteration 1758, dc_loss: 0.03209161013364792, tv_loss: 0.03397038206458092\n",
      "iteration 1759, dc_loss: 0.03207777813076973, tv_loss: 0.03398760035634041\n",
      "iteration 1760, dc_loss: 0.03208597004413605, tv_loss: 0.033970702439546585\n",
      "iteration 1761, dc_loss: 0.03202883526682854, tv_loss: 0.03398764505982399\n",
      "iteration 1762, dc_loss: 0.032096017152071, tv_loss: 0.03390571475028992\n",
      "iteration 1763, dc_loss: 0.03200098127126694, tv_loss: 0.03402900695800781\n",
      "iteration 1764, dc_loss: 0.03204333782196045, tv_loss: 0.03396959602832794\n",
      "iteration 1765, dc_loss: 0.03201446682214737, tv_loss: 0.03397030755877495\n",
      "iteration 1766, dc_loss: 0.032009582966566086, tv_loss: 0.033953532576560974\n",
      "iteration 1767, dc_loss: 0.03197775036096573, tv_loss: 0.0339956097304821\n",
      "iteration 1768, dc_loss: 0.03199046850204468, tv_loss: 0.033963773399591446\n",
      "iteration 1769, dc_loss: 0.031910866498947144, tv_loss: 0.0339965894818306\n",
      "iteration 1770, dc_loss: 0.03194088116288185, tv_loss: 0.033940352499485016\n",
      "iteration 1771, dc_loss: 0.03187350183725357, tv_loss: 0.033980924636125565\n",
      "iteration 1772, dc_loss: 0.031897127628326416, tv_loss: 0.033966582268476486\n",
      "iteration 1773, dc_loss: 0.03185253217816353, tv_loss: 0.03398866206407547\n",
      "iteration 1774, dc_loss: 0.03183089941740036, tv_loss: 0.03397784009575844\n",
      "iteration 1775, dc_loss: 0.031847305595874786, tv_loss: 0.033940043300390244\n",
      "iteration 1776, dc_loss: 0.03179401531815529, tv_loss: 0.03400317206978798\n",
      "iteration 1777, dc_loss: 0.031827885657548904, tv_loss: 0.03396881744265556\n",
      "iteration 1778, dc_loss: 0.031788356602191925, tv_loss: 0.03398073464632034\n",
      "iteration 1779, dc_loss: 0.03177359327673912, tv_loss: 0.03396258130669594\n",
      "iteration 1780, dc_loss: 0.03176901862025261, tv_loss: 0.03397122770547867\n",
      "iteration 1781, dc_loss: 0.031752314418554306, tv_loss: 0.033967290073633194\n",
      "iteration 1782, dc_loss: 0.03171134740114212, tv_loss: 0.03398832306265831\n",
      "iteration 1783, dc_loss: 0.03173491731286049, tv_loss: 0.03394745662808418\n",
      "iteration 1784, dc_loss: 0.03168246150016785, tv_loss: 0.03397868573665619\n",
      "iteration 1785, dc_loss: 0.03170568868517876, tv_loss: 0.033932607620954514\n",
      "iteration 1786, dc_loss: 0.031652338802814484, tv_loss: 0.03397664055228233\n",
      "iteration 1787, dc_loss: 0.03166407719254494, tv_loss: 0.0339474081993103\n",
      "iteration 1788, dc_loss: 0.03162188082933426, tv_loss: 0.033978529274463654\n",
      "iteration 1789, dc_loss: 0.0316183976829052, tv_loss: 0.03396531939506531\n",
      "iteration 1790, dc_loss: 0.031615376472473145, tv_loss: 0.03395925834774971\n",
      "iteration 1791, dc_loss: 0.03158134967088699, tv_loss: 0.03397662565112114\n",
      "iteration 1792, dc_loss: 0.0315847210586071, tv_loss: 0.03394992649555206\n",
      "iteration 1793, dc_loss: 0.03155973181128502, tv_loss: 0.03395768627524376\n",
      "iteration 1794, dc_loss: 0.031525880098342896, tv_loss: 0.03397432342171669\n",
      "iteration 1795, dc_loss: 0.031553950160741806, tv_loss: 0.03393637016415596\n",
      "iteration 1796, dc_loss: 0.031503815203905106, tv_loss: 0.03396809101104736\n",
      "iteration 1797, dc_loss: 0.03153035044670105, tv_loss: 0.033953871577978134\n",
      "iteration 1798, dc_loss: 0.03148375824093819, tv_loss: 0.03401372209191322\n",
      "iteration 1799, dc_loss: 0.03150423616170883, tv_loss: 0.033970318734645844\n",
      "iteration 1800, dc_loss: 0.031484510749578476, tv_loss: 0.03396448865532875\n",
      "iteration 1801, dc_loss: 0.03151027858257294, tv_loss: 0.03394301235675812\n",
      "iteration 1802, dc_loss: 0.03145111724734306, tv_loss: 0.0340215228497982\n",
      "iteration 1803, dc_loss: 0.03150191530585289, tv_loss: 0.03395269438624382\n",
      "iteration 1804, dc_loss: 0.031420715153217316, tv_loss: 0.034005846828222275\n",
      "iteration 1805, dc_loss: 0.03147997707128525, tv_loss: 0.033923614770174026\n",
      "iteration 1806, dc_loss: 0.03137362003326416, tv_loss: 0.03402116149663925\n",
      "iteration 1807, dc_loss: 0.03142821788787842, tv_loss: 0.033948689699172974\n",
      "iteration 1808, dc_loss: 0.03133231773972511, tv_loss: 0.03400794789195061\n",
      "iteration 1809, dc_loss: 0.03133784979581833, tv_loss: 0.03396696597337723\n",
      "iteration 1810, dc_loss: 0.03133297711610794, tv_loss: 0.03395170718431473\n",
      "iteration 1811, dc_loss: 0.0312935896217823, tv_loss: 0.03398292884230614\n",
      "iteration 1812, dc_loss: 0.0312795527279377, tv_loss: 0.03398105874657631\n",
      "iteration 1813, dc_loss: 0.03125464543700218, tv_loss: 0.03397362679243088\n",
      "iteration 1814, dc_loss: 0.03127538412809372, tv_loss: 0.033943772315979004\n",
      "iteration 1815, dc_loss: 0.031238991767168045, tv_loss: 0.033967722207307816\n",
      "iteration 1816, dc_loss: 0.031239645555615425, tv_loss: 0.033967018127441406\n",
      "iteration 1817, dc_loss: 0.03123326040804386, tv_loss: 0.03396780788898468\n",
      "iteration 1818, dc_loss: 0.031232045963406563, tv_loss: 0.033971358090639114\n",
      "iteration 1819, dc_loss: 0.031173111870884895, tv_loss: 0.03400377556681633\n",
      "iteration 1820, dc_loss: 0.03121156059205532, tv_loss: 0.033939287066459656\n",
      "iteration 1821, dc_loss: 0.031130382791161537, tv_loss: 0.03400634601712227\n",
      "iteration 1822, dc_loss: 0.031195372343063354, tv_loss: 0.033943839371204376\n",
      "iteration 1823, dc_loss: 0.031110422685742378, tv_loss: 0.03403281792998314\n",
      "iteration 1824, dc_loss: 0.03112146258354187, tv_loss: 0.033979080617427826\n",
      "iteration 1825, dc_loss: 0.031109577044844627, tv_loss: 0.03396092727780342\n",
      "iteration 1826, dc_loss: 0.031082576140761375, tv_loss: 0.03399118408560753\n",
      "iteration 1827, dc_loss: 0.031074484810233116, tv_loss: 0.03400762379169464\n",
      "iteration 1828, dc_loss: 0.031052537262439728, tv_loss: 0.033973559737205505\n",
      "iteration 1829, dc_loss: 0.031032970175147057, tv_loss: 0.03398653864860535\n",
      "iteration 1830, dc_loss: 0.031040513888001442, tv_loss: 0.03396116942167282\n",
      "iteration 1831, dc_loss: 0.030999621376395226, tv_loss: 0.03399856016039848\n",
      "iteration 1832, dc_loss: 0.031010808423161507, tv_loss: 0.033963121473789215\n",
      "iteration 1833, dc_loss: 0.03097894974052906, tv_loss: 0.033971626311540604\n",
      "iteration 1834, dc_loss: 0.030969412997364998, tv_loss: 0.03396761417388916\n",
      "iteration 1835, dc_loss: 0.030939210206270218, tv_loss: 0.03398657590150833\n",
      "iteration 1836, dc_loss: 0.03097034990787506, tv_loss: 0.0339558869600296\n",
      "iteration 1837, dc_loss: 0.03091607801616192, tv_loss: 0.033989522606134415\n",
      "iteration 1838, dc_loss: 0.030927104875445366, tv_loss: 0.03395460173487663\n",
      "iteration 1839, dc_loss: 0.03088628500699997, tv_loss: 0.033992841839790344\n",
      "iteration 1840, dc_loss: 0.030926022678613663, tv_loss: 0.03394119068980217\n",
      "iteration 1841, dc_loss: 0.03087771125137806, tv_loss: 0.03398524597287178\n",
      "iteration 1842, dc_loss: 0.030920643359422684, tv_loss: 0.03394109010696411\n",
      "iteration 1843, dc_loss: 0.03084968402981758, tv_loss: 0.0340166911482811\n",
      "iteration 1844, dc_loss: 0.030956046655774117, tv_loss: 0.03391089290380478\n",
      "iteration 1845, dc_loss: 0.030857563018798828, tv_loss: 0.03402016684412956\n",
      "iteration 1846, dc_loss: 0.030947351828217506, tv_loss: 0.033919237554073334\n",
      "iteration 1847, dc_loss: 0.030891356989741325, tv_loss: 0.03399622440338135\n",
      "iteration 1848, dc_loss: 0.03092288039624691, tv_loss: 0.033931028097867966\n",
      "iteration 1849, dc_loss: 0.030818814411759377, tv_loss: 0.034004077315330505\n",
      "iteration 1850, dc_loss: 0.030825994908809662, tv_loss: 0.03395477309823036\n",
      "iteration 1851, dc_loss: 0.03075307235121727, tv_loss: 0.034009963274002075\n",
      "iteration 1852, dc_loss: 0.03073149174451828, tv_loss: 0.03398435190320015\n",
      "iteration 1853, dc_loss: 0.030710598453879356, tv_loss: 0.03396118804812431\n",
      "iteration 1854, dc_loss: 0.030717900022864342, tv_loss: 0.03396817669272423\n",
      "iteration 1855, dc_loss: 0.03070475161075592, tv_loss: 0.0339849554002285\n",
      "iteration 1856, dc_loss: 0.03069455362856388, tv_loss: 0.03397957235574722\n",
      "iteration 1857, dc_loss: 0.03070329688489437, tv_loss: 0.03396343067288399\n",
      "iteration 1858, dc_loss: 0.03065807744860649, tv_loss: 0.03398854285478592\n",
      "iteration 1859, dc_loss: 0.030693767592310905, tv_loss: 0.03392888978123665\n",
      "iteration 1860, dc_loss: 0.030606551095843315, tv_loss: 0.03401435166597366\n",
      "iteration 1861, dc_loss: 0.030641252174973488, tv_loss: 0.03395844250917435\n",
      "iteration 1862, dc_loss: 0.030575724318623543, tv_loss: 0.033990100026130676\n",
      "iteration 1863, dc_loss: 0.030569536611437798, tv_loss: 0.033970918506383896\n",
      "iteration 1864, dc_loss: 0.030567007139325142, tv_loss: 0.03395908698439598\n",
      "iteration 1865, dc_loss: 0.030534453690052032, tv_loss: 0.03397922217845917\n",
      "iteration 1866, dc_loss: 0.030559301376342773, tv_loss: 0.033942557871341705\n",
      "iteration 1867, dc_loss: 0.030517293140292168, tv_loss: 0.033978916704654694\n",
      "iteration 1868, dc_loss: 0.030531292781233788, tv_loss: 0.03396766632795334\n",
      "iteration 1869, dc_loss: 0.030496930703520775, tv_loss: 0.03400479257106781\n",
      "iteration 1870, dc_loss: 0.030517810955643654, tv_loss: 0.03395767882466316\n",
      "iteration 1871, dc_loss: 0.03048323094844818, tv_loss: 0.03397858887910843\n",
      "iteration 1872, dc_loss: 0.030486060306429863, tv_loss: 0.03397034853696823\n",
      "iteration 1873, dc_loss: 0.03044632077217102, tv_loss: 0.03399147093296051\n",
      "iteration 1874, dc_loss: 0.030458718538284302, tv_loss: 0.033966802060604095\n",
      "iteration 1875, dc_loss: 0.030422218143939972, tv_loss: 0.03397781401872635\n",
      "iteration 1876, dc_loss: 0.03042243793606758, tv_loss: 0.03396080806851387\n",
      "iteration 1877, dc_loss: 0.030393825843930244, tv_loss: 0.03398360311985016\n",
      "iteration 1878, dc_loss: 0.030390523374080658, tv_loss: 0.03396524488925934\n",
      "iteration 1879, dc_loss: 0.0303538478910923, tv_loss: 0.03398769721388817\n",
      "iteration 1880, dc_loss: 0.03037770465016365, tv_loss: 0.03394898399710655\n",
      "iteration 1881, dc_loss: 0.03033054620027542, tv_loss: 0.03399210050702095\n",
      "iteration 1882, dc_loss: 0.030323699116706848, tv_loss: 0.03399106487631798\n",
      "iteration 1883, dc_loss: 0.03031122125685215, tv_loss: 0.03397997468709946\n",
      "iteration 1884, dc_loss: 0.03031306341290474, tv_loss: 0.03396701440215111\n",
      "iteration 1885, dc_loss: 0.03027038648724556, tv_loss: 0.03398709371685982\n",
      "iteration 1886, dc_loss: 0.030304070562124252, tv_loss: 0.033939529210329056\n",
      "iteration 1887, dc_loss: 0.03025001659989357, tv_loss: 0.03399350866675377\n",
      "iteration 1888, dc_loss: 0.03026755340397358, tv_loss: 0.03396230563521385\n",
      "iteration 1889, dc_loss: 0.030240628868341446, tv_loss: 0.03398929536342621\n",
      "iteration 1890, dc_loss: 0.03025582805275917, tv_loss: 0.03397226706147194\n",
      "iteration 1891, dc_loss: 0.03021821193397045, tv_loss: 0.033992454409599304\n",
      "iteration 1892, dc_loss: 0.030262304469943047, tv_loss: 0.03394138440489769\n",
      "iteration 1893, dc_loss: 0.030205657705664635, tv_loss: 0.03400398790836334\n",
      "iteration 1894, dc_loss: 0.030263952910900116, tv_loss: 0.033937569707632065\n",
      "iteration 1895, dc_loss: 0.03019639477133751, tv_loss: 0.03402518853545189\n",
      "iteration 1896, dc_loss: 0.030267680063843727, tv_loss: 0.03395523875951767\n",
      "iteration 1897, dc_loss: 0.030155684798955917, tv_loss: 0.03404249995946884\n",
      "iteration 1898, dc_loss: 0.0302262045443058, tv_loss: 0.0339302234351635\n",
      "iteration 1899, dc_loss: 0.030120152980089188, tv_loss: 0.03401314094662666\n",
      "iteration 1900, dc_loss: 0.03015749529004097, tv_loss: 0.0339445099234581\n",
      "iteration 1901, dc_loss: 0.030067402869462967, tv_loss: 0.0340123325586319\n",
      "iteration 1902, dc_loss: 0.03013160452246666, tv_loss: 0.03391963616013527\n",
      "iteration 1903, dc_loss: 0.030027665197849274, tv_loss: 0.0340108685195446\n",
      "iteration 1904, dc_loss: 0.03005867637693882, tv_loss: 0.03396586701273918\n",
      "iteration 1905, dc_loss: 0.03002588078379631, tv_loss: 0.03399554640054703\n",
      "iteration 1906, dc_loss: 0.030008045956492424, tv_loss: 0.03399590030312538\n",
      "iteration 1907, dc_loss: 0.030036959797143936, tv_loss: 0.03395836427807808\n",
      "iteration 1908, dc_loss: 0.02997090294957161, tv_loss: 0.034013569355010986\n",
      "iteration 1909, dc_loss: 0.03003404475748539, tv_loss: 0.03394469991326332\n",
      "iteration 1910, dc_loss: 0.029933152720332146, tv_loss: 0.034039389342069626\n",
      "iteration 1911, dc_loss: 0.03002830222249031, tv_loss: 0.03392739221453667\n",
      "iteration 1912, dc_loss: 0.02992059849202633, tv_loss: 0.03401658311486244\n",
      "iteration 1913, dc_loss: 0.030000686645507812, tv_loss: 0.03392759710550308\n",
      "iteration 1914, dc_loss: 0.02988513745367527, tv_loss: 0.034019093960523605\n",
      "iteration 1915, dc_loss: 0.029956938698887825, tv_loss: 0.03394583240151405\n",
      "iteration 1916, dc_loss: 0.02987946756184101, tv_loss: 0.03401578217744827\n",
      "iteration 1917, dc_loss: 0.02991853840649128, tv_loss: 0.03397674486041069\n",
      "iteration 1918, dc_loss: 0.029866496101021767, tv_loss: 0.03400455042719841\n",
      "iteration 1919, dc_loss: 0.029889734461903572, tv_loss: 0.03396115079522133\n",
      "iteration 1920, dc_loss: 0.02986406721174717, tv_loss: 0.03397931903600693\n",
      "iteration 1921, dc_loss: 0.02985367551445961, tv_loss: 0.033990249037742615\n",
      "iteration 1922, dc_loss: 0.029856260865926743, tv_loss: 0.033974286168813705\n",
      "iteration 1923, dc_loss: 0.029856234788894653, tv_loss: 0.0339968204498291\n",
      "iteration 1924, dc_loss: 0.029878728091716766, tv_loss: 0.03396908566355705\n",
      "iteration 1925, dc_loss: 0.02984343096613884, tv_loss: 0.033998988568782806\n",
      "iteration 1926, dc_loss: 0.029875794425606728, tv_loss: 0.033970899879932404\n",
      "iteration 1927, dc_loss: 0.029842287302017212, tv_loss: 0.03398188576102257\n",
      "iteration 1928, dc_loss: 0.029837794601917267, tv_loss: 0.03396908566355705\n",
      "iteration 1929, dc_loss: 0.02979537472128868, tv_loss: 0.03397512063384056\n",
      "iteration 1930, dc_loss: 0.029757006093859673, tv_loss: 0.033980678766965866\n",
      "iteration 1931, dc_loss: 0.029749689623713493, tv_loss: 0.03395618498325348\n",
      "iteration 1932, dc_loss: 0.029704168438911438, tv_loss: 0.03398217260837555\n",
      "iteration 1933, dc_loss: 0.0297397393733263, tv_loss: 0.033923566341400146\n",
      "iteration 1934, dc_loss: 0.02962610125541687, tv_loss: 0.03402789309620857\n",
      "iteration 1935, dc_loss: 0.029744809493422508, tv_loss: 0.03391974791884422\n",
      "iteration 1936, dc_loss: 0.02961607463657856, tv_loss: 0.03407201170921326\n",
      "iteration 1937, dc_loss: 0.02969065122306347, tv_loss: 0.03394902125000954\n",
      "iteration 1938, dc_loss: 0.02961563691496849, tv_loss: 0.033994078636169434\n",
      "iteration 1939, dc_loss: 0.029628852382302284, tv_loss: 0.03397052362561226\n",
      "iteration 1940, dc_loss: 0.02960112877190113, tv_loss: 0.034048523753881454\n",
      "iteration 1941, dc_loss: 0.029590995982289314, tv_loss: 0.03399974852800369\n",
      "iteration 1942, dc_loss: 0.029580315575003624, tv_loss: 0.033982258290052414\n",
      "iteration 1943, dc_loss: 0.0295573677867651, tv_loss: 0.034047115594148636\n",
      "iteration 1944, dc_loss: 0.029589546844363213, tv_loss: 0.033978790044784546\n",
      "iteration 1945, dc_loss: 0.02951148711144924, tv_loss: 0.03402816876769066\n",
      "iteration 1946, dc_loss: 0.029584486037492752, tv_loss: 0.033961985260248184\n",
      "iteration 1947, dc_loss: 0.029489638283848763, tv_loss: 0.03408004343509674\n",
      "iteration 1948, dc_loss: 0.029578926041722298, tv_loss: 0.03394782915711403\n",
      "iteration 1949, dc_loss: 0.029474297538399696, tv_loss: 0.034045543521642685\n",
      "iteration 1950, dc_loss: 0.029555793851614, tv_loss: 0.0339789018034935\n",
      "iteration 1951, dc_loss: 0.029475810006260872, tv_loss: 0.03405241668224335\n",
      "iteration 1952, dc_loss: 0.029565569013357162, tv_loss: 0.033934030681848526\n",
      "iteration 1953, dc_loss: 0.029437586665153503, tv_loss: 0.03407182916998863\n",
      "iteration 1954, dc_loss: 0.02954867109656334, tv_loss: 0.03394721448421478\n",
      "iteration 1955, dc_loss: 0.02945314534008503, tv_loss: 0.034033648669719696\n",
      "iteration 1956, dc_loss: 0.029510969296097755, tv_loss: 0.03396216779947281\n",
      "iteration 1957, dc_loss: 0.029465623199939728, tv_loss: 0.03400414064526558\n",
      "iteration 1958, dc_loss: 0.02950136736035347, tv_loss: 0.03396125137805939\n",
      "iteration 1959, dc_loss: 0.02944597415626049, tv_loss: 0.0340210422873497\n",
      "iteration 1960, dc_loss: 0.029455700889229774, tv_loss: 0.0339621901512146\n",
      "iteration 1961, dc_loss: 0.02938821353018284, tv_loss: 0.03399532288312912\n",
      "iteration 1962, dc_loss: 0.029385706409811974, tv_loss: 0.03398274630308151\n",
      "iteration 1963, dc_loss: 0.02933747135102749, tv_loss: 0.03401865065097809\n",
      "iteration 1964, dc_loss: 0.029348917305469513, tv_loss: 0.03396528214216232\n",
      "iteration 1965, dc_loss: 0.029313353821635246, tv_loss: 0.03397959843277931\n",
      "iteration 1966, dc_loss: 0.029279178008437157, tv_loss: 0.03400129824876785\n",
      "iteration 1967, dc_loss: 0.02931193634867668, tv_loss: 0.033964671194553375\n",
      "iteration 1968, dc_loss: 0.029268445447087288, tv_loss: 0.03401673585176468\n",
      "iteration 1969, dc_loss: 0.029288865625858307, tv_loss: 0.03398275747895241\n",
      "iteration 1970, dc_loss: 0.029262401163578033, tv_loss: 0.033989161252975464\n",
      "iteration 1971, dc_loss: 0.029292715713381767, tv_loss: 0.03395572677254677\n",
      "iteration 1972, dc_loss: 0.029235484078526497, tv_loss: 0.03399905934929848\n",
      "iteration 1973, dc_loss: 0.02926737815141678, tv_loss: 0.03395162150263786\n",
      "iteration 1974, dc_loss: 0.029220862314105034, tv_loss: 0.034003496170043945\n",
      "iteration 1975, dc_loss: 0.029233720153570175, tv_loss: 0.033983029425144196\n",
      "iteration 1976, dc_loss: 0.029207026585936546, tv_loss: 0.0340004563331604\n",
      "iteration 1977, dc_loss: 0.029214443638920784, tv_loss: 0.03395414352416992\n",
      "iteration 1978, dc_loss: 0.029145920649170876, tv_loss: 0.03399857133626938\n",
      "iteration 1979, dc_loss: 0.02918112277984619, tv_loss: 0.03395110368728638\n",
      "iteration 1980, dc_loss: 0.029116932302713394, tv_loss: 0.03401397168636322\n",
      "iteration 1981, dc_loss: 0.02915787324309349, tv_loss: 0.03395398333668709\n",
      "iteration 1982, dc_loss: 0.02909880504012108, tv_loss: 0.03401517495512962\n",
      "iteration 1983, dc_loss: 0.029128799214959145, tv_loss: 0.03397076576948166\n",
      "iteration 1984, dc_loss: 0.029067721217870712, tv_loss: 0.03400677442550659\n",
      "iteration 1985, dc_loss: 0.029115203768014908, tv_loss: 0.03394369035959244\n",
      "iteration 1986, dc_loss: 0.02905014529824257, tv_loss: 0.03400015830993652\n",
      "iteration 1987, dc_loss: 0.02905840054154396, tv_loss: 0.0339929535984993\n",
      "iteration 1988, dc_loss: 0.02904982678592205, tv_loss: 0.03400515019893646\n",
      "iteration 1989, dc_loss: 0.029026757925748825, tv_loss: 0.03401903808116913\n",
      "iteration 1990, dc_loss: 0.029044996947050095, tv_loss: 0.033966485410928726\n",
      "iteration 1991, dc_loss: 0.028983842581510544, tv_loss: 0.03401016816496849\n",
      "iteration 1992, dc_loss: 0.029061153531074524, tv_loss: 0.03392602875828743\n",
      "iteration 1993, dc_loss: 0.028954967856407166, tv_loss: 0.034032877534627914\n",
      "iteration 1994, dc_loss: 0.029065517708659172, tv_loss: 0.03392734378576279\n",
      "iteration 1995, dc_loss: 0.028942670673131943, tv_loss: 0.034053076058626175\n",
      "iteration 1996, dc_loss: 0.029061755165457726, tv_loss: 0.03395332396030426\n",
      "iteration 1997, dc_loss: 0.028933104127645493, tv_loss: 0.034059736877679825\n",
      "iteration 1998, dc_loss: 0.02911008708178997, tv_loss: 0.033882901072502136\n",
      "iteration 1999, dc_loss: 0.02892417274415493, tv_loss: 0.03411991149187088\n",
      "iteration 2000, dc_loss: 0.029156483709812164, tv_loss: 0.0338950976729393\n",
      "iteration 2001, dc_loss: 0.028958184644579887, tv_loss: 0.034115131944417953\n",
      "iteration 2002, dc_loss: 0.029016269370913506, tv_loss: 0.0339062474668026\n",
      "iteration 2003, dc_loss: 0.02894335612654686, tv_loss: 0.033939287066459656\n",
      "iteration 2004, dc_loss: 0.028894955292344093, tv_loss: 0.03409770503640175\n",
      "iteration 2005, dc_loss: 0.029015546664595604, tv_loss: 0.03390839323401451\n",
      "iteration 2006, dc_loss: 0.028878282755613327, tv_loss: 0.03396115079522133\n",
      "iteration 2007, dc_loss: 0.02882939763367176, tv_loss: 0.0340600349009037\n",
      "iteration 2008, dc_loss: 0.02898912876844406, tv_loss: 0.03389845788478851\n",
      "iteration 2009, dc_loss: 0.02884196676313877, tv_loss: 0.033980272710323334\n",
      "iteration 2010, dc_loss: 0.028792453929781914, tv_loss: 0.03403197601437569\n",
      "iteration 2011, dc_loss: 0.028930749744176865, tv_loss: 0.03389876335859299\n",
      "iteration 2012, dc_loss: 0.028817068785429, tv_loss: 0.033973898738622665\n",
      "iteration 2013, dc_loss: 0.028769802302122116, tv_loss: 0.03401455655694008\n",
      "iteration 2014, dc_loss: 0.028877882286906242, tv_loss: 0.033919818699359894\n",
      "iteration 2015, dc_loss: 0.02877514623105526, tv_loss: 0.03400128707289696\n",
      "iteration 2016, dc_loss: 0.028759002685546875, tv_loss: 0.034000370651483536\n",
      "iteration 2017, dc_loss: 0.02884099818766117, tv_loss: 0.033906273543834686\n",
      "iteration 2018, dc_loss: 0.028737200424075127, tv_loss: 0.033989857882261276\n",
      "iteration 2019, dc_loss: 0.028747521340847015, tv_loss: 0.03397098183631897\n",
      "iteration 2020, dc_loss: 0.028794821351766586, tv_loss: 0.033914972096681595\n",
      "iteration 2021, dc_loss: 0.02870243601500988, tv_loss: 0.03398941084742546\n",
      "iteration 2022, dc_loss: 0.028728093951940536, tv_loss: 0.03395196422934532\n",
      "iteration 2023, dc_loss: 0.028749966993927956, tv_loss: 0.03393019735813141\n",
      "iteration 2024, dc_loss: 0.02867419645190239, tv_loss: 0.03398876264691353\n",
      "iteration 2025, dc_loss: 0.028704781085252762, tv_loss: 0.03394694626331329\n",
      "iteration 2026, dc_loss: 0.028717923909425735, tv_loss: 0.03395144268870354\n",
      "iteration 2027, dc_loss: 0.028648875653743744, tv_loss: 0.034031253308057785\n",
      "iteration 2028, dc_loss: 0.028667235746979713, tv_loss: 0.03397712484002113\n",
      "iteration 2029, dc_loss: 0.028693614527583122, tv_loss: 0.0339268296957016\n",
      "iteration 2030, dc_loss: 0.028629308566451073, tv_loss: 0.034000709652900696\n",
      "iteration 2031, dc_loss: 0.028648123145103455, tv_loss: 0.03399619460105896\n",
      "iteration 2032, dc_loss: 0.028653258457779884, tv_loss: 0.03396686911582947\n",
      "iteration 2033, dc_loss: 0.028604429215192795, tv_loss: 0.033987127244472504\n",
      "iteration 2034, dc_loss: 0.028630560263991356, tv_loss: 0.03396567702293396\n",
      "iteration 2035, dc_loss: 0.028615424409508705, tv_loss: 0.03399648144841194\n",
      "iteration 2036, dc_loss: 0.028575856238603592, tv_loss: 0.034006260335445404\n",
      "iteration 2037, dc_loss: 0.028615877032279968, tv_loss: 0.03394657000899315\n",
      "iteration 2038, dc_loss: 0.028580626472830772, tv_loss: 0.03399209305644035\n",
      "iteration 2039, dc_loss: 0.02855643257498741, tv_loss: 0.03401121497154236\n",
      "iteration 2040, dc_loss: 0.02859754115343094, tv_loss: 0.03394646570086479\n",
      "iteration 2041, dc_loss: 0.02854120172560215, tv_loss: 0.03399796411395073\n",
      "iteration 2042, dc_loss: 0.028539160266518593, tv_loss: 0.033997174352407455\n",
      "iteration 2043, dc_loss: 0.028574081137776375, tv_loss: 0.03394962102174759\n",
      "iteration 2044, dc_loss: 0.028504876419901848, tv_loss: 0.034002166241407394\n",
      "iteration 2045, dc_loss: 0.028521284461021423, tv_loss: 0.03396618738770485\n",
      "iteration 2046, dc_loss: 0.028544658794999123, tv_loss: 0.03394383564591408\n",
      "iteration 2047, dc_loss: 0.028479935601353645, tv_loss: 0.03400729224085808\n",
      "iteration 2048, dc_loss: 0.028506388887763023, tv_loss: 0.03395894914865494\n",
      "iteration 2049, dc_loss: 0.028514966368675232, tv_loss: 0.033943574875593185\n",
      "iteration 2050, dc_loss: 0.028466109186410904, tv_loss: 0.03397683426737785\n",
      "iteration 2051, dc_loss: 0.028477724641561508, tv_loss: 0.03396281972527504\n",
      "iteration 2052, dc_loss: 0.028467971831560135, tv_loss: 0.03398686274886131\n",
      "iteration 2053, dc_loss: 0.028442412614822388, tv_loss: 0.03400116041302681\n",
      "iteration 2054, dc_loss: 0.02845284342765808, tv_loss: 0.03396236523985863\n",
      "iteration 2055, dc_loss: 0.028442762792110443, tv_loss: 0.03395361453294754\n",
      "iteration 2056, dc_loss: 0.02842661365866661, tv_loss: 0.03399238735437393\n",
      "iteration 2057, dc_loss: 0.028432762250304222, tv_loss: 0.033986080437898636\n",
      "iteration 2058, dc_loss: 0.028419241309165955, tv_loss: 0.03397462144494057\n",
      "iteration 2059, dc_loss: 0.02838922291994095, tv_loss: 0.03397553786635399\n",
      "iteration 2060, dc_loss: 0.028409205377101898, tv_loss: 0.0339675098657608\n",
      "iteration 2061, dc_loss: 0.028389502316713333, tv_loss: 0.0340108685195446\n",
      "iteration 2062, dc_loss: 0.028364524245262146, tv_loss: 0.0339968167245388\n",
      "iteration 2063, dc_loss: 0.028387436643242836, tv_loss: 0.033952903002500534\n",
      "iteration 2064, dc_loss: 0.02836846187710762, tv_loss: 0.0339946374297142\n",
      "iteration 2065, dc_loss: 0.028350478038191795, tv_loss: 0.0340048223733902\n",
      "iteration 2066, dc_loss: 0.02836218848824501, tv_loss: 0.033964142203330994\n",
      "iteration 2067, dc_loss: 0.028326578438282013, tv_loss: 0.03399418666958809\n",
      "iteration 2068, dc_loss: 0.028332848101854324, tv_loss: 0.03400062024593353\n",
      "iteration 2069, dc_loss: 0.028347330167889595, tv_loss: 0.03396068885922432\n",
      "iteration 2070, dc_loss: 0.02829144336283207, tv_loss: 0.03400209918618202\n",
      "iteration 2071, dc_loss: 0.028304506093263626, tv_loss: 0.0339914932847023\n",
      "iteration 2072, dc_loss: 0.02832144871354103, tv_loss: 0.03395833075046539\n",
      "iteration 2073, dc_loss: 0.028278527781367302, tv_loss: 0.03398389741778374\n",
      "iteration 2074, dc_loss: 0.028292082250118256, tv_loss: 0.033962611109018326\n",
      "iteration 2075, dc_loss: 0.028277752920985222, tv_loss: 0.03396247327327728\n",
      "iteration 2076, dc_loss: 0.028261259198188782, tv_loss: 0.03397798538208008\n",
      "iteration 2077, dc_loss: 0.028275415301322937, tv_loss: 0.03395552933216095\n",
      "iteration 2078, dc_loss: 0.028241584077477455, tv_loss: 0.03398154675960541\n",
      "iteration 2079, dc_loss: 0.0282451044768095, tv_loss: 0.03396233916282654\n",
      "iteration 2080, dc_loss: 0.02824598364531994, tv_loss: 0.03396573290228844\n",
      "iteration 2081, dc_loss: 0.028218885883688927, tv_loss: 0.03398418053984642\n",
      "iteration 2082, dc_loss: 0.028221795335412025, tv_loss: 0.03397199511528015\n",
      "iteration 2083, dc_loss: 0.02821013703942299, tv_loss: 0.033966463059186935\n",
      "iteration 2084, dc_loss: 0.02819177880883217, tv_loss: 0.03397887945175171\n",
      "iteration 2085, dc_loss: 0.02820499613881111, tv_loss: 0.03395941108465195\n",
      "iteration 2086, dc_loss: 0.028187289834022522, tv_loss: 0.03396591544151306\n",
      "iteration 2087, dc_loss: 0.02818305790424347, tv_loss: 0.03397772088646889\n",
      "iteration 2088, dc_loss: 0.028171472251415253, tv_loss: 0.033983100205659866\n",
      "iteration 2089, dc_loss: 0.028157370164990425, tv_loss: 0.03398403897881508\n",
      "iteration 2090, dc_loss: 0.028152137994766235, tv_loss: 0.03396683186292648\n",
      "iteration 2091, dc_loss: 0.028153318911790848, tv_loss: 0.033964868634939194\n",
      "iteration 2092, dc_loss: 0.028131792321801186, tv_loss: 0.03398899361491203\n",
      "iteration 2093, dc_loss: 0.028134852647781372, tv_loss: 0.033985961228609085\n",
      "iteration 2094, dc_loss: 0.0281341802328825, tv_loss: 0.03396812453866005\n",
      "iteration 2095, dc_loss: 0.02809562161564827, tv_loss: 0.03398475795984268\n",
      "iteration 2096, dc_loss: 0.028106635436415672, tv_loss: 0.033966027200222015\n",
      "iteration 2097, dc_loss: 0.028115520253777504, tv_loss: 0.033959634602069855\n",
      "iteration 2098, dc_loss: 0.028080876916646957, tv_loss: 0.03399306535720825\n",
      "iteration 2099, dc_loss: 0.028087681159377098, tv_loss: 0.03398045524954796\n",
      "iteration 2100, dc_loss: 0.028072822839021683, tv_loss: 0.03397255018353462\n",
      "iteration 2101, dc_loss: 0.028077073395252228, tv_loss: 0.03396083414554596\n",
      "iteration 2102, dc_loss: 0.028066622093319893, tv_loss: 0.033958643674850464\n",
      "iteration 2103, dc_loss: 0.02803729474544525, tv_loss: 0.033988241106271744\n",
      "iteration 2104, dc_loss: 0.028044171631336212, tv_loss: 0.03397800773382187\n",
      "iteration 2105, dc_loss: 0.02803611010313034, tv_loss: 0.0339835099875927\n",
      "iteration 2106, dc_loss: 0.028036968782544136, tv_loss: 0.03397035226225853\n",
      "iteration 2107, dc_loss: 0.02801741473376751, tv_loss: 0.033973369747400284\n",
      "iteration 2108, dc_loss: 0.028008757159113884, tv_loss: 0.03397529572248459\n",
      "iteration 2109, dc_loss: 0.028020380064845085, tv_loss: 0.033944983035326004\n",
      "iteration 2110, dc_loss: 0.027985457330942154, tv_loss: 0.03398248925805092\n",
      "iteration 2111, dc_loss: 0.027982326224446297, tv_loss: 0.03398587554693222\n",
      "iteration 2112, dc_loss: 0.027998510748147964, tv_loss: 0.03395695239305496\n",
      "iteration 2113, dc_loss: 0.027975374832749367, tv_loss: 0.03397275134921074\n",
      "iteration 2114, dc_loss: 0.02795279026031494, tv_loss: 0.03397635743021965\n",
      "iteration 2115, dc_loss: 0.02797098271548748, tv_loss: 0.033955659717321396\n",
      "iteration 2116, dc_loss: 0.027950948104262352, tv_loss: 0.03396633267402649\n",
      "iteration 2117, dc_loss: 0.027933601289987564, tv_loss: 0.03397844359278679\n",
      "iteration 2118, dc_loss: 0.02794540673494339, tv_loss: 0.0339595191180706\n",
      "iteration 2119, dc_loss: 0.02793286368250847, tv_loss: 0.0339619517326355\n",
      "iteration 2120, dc_loss: 0.027919113636016846, tv_loss: 0.03398137167096138\n",
      "iteration 2121, dc_loss: 0.027913816273212433, tv_loss: 0.03397464379668236\n",
      "iteration 2122, dc_loss: 0.027892425656318665, tv_loss: 0.033980078995227814\n",
      "iteration 2123, dc_loss: 0.027898544445633888, tv_loss: 0.03396562114357948\n",
      "iteration 2124, dc_loss: 0.027897536754608154, tv_loss: 0.033950138837099075\n",
      "iteration 2125, dc_loss: 0.027876010164618492, tv_loss: 0.03397578373551369\n",
      "iteration 2126, dc_loss: 0.027892202138900757, tv_loss: 0.03394897282123566\n",
      "iteration 2127, dc_loss: 0.027857711538672447, tv_loss: 0.03398837149143219\n",
      "iteration 2128, dc_loss: 0.0278629120439291, tv_loss: 0.03398226201534271\n",
      "iteration 2129, dc_loss: 0.027851685881614685, tv_loss: 0.033978935331106186\n",
      "iteration 2130, dc_loss: 0.02785409614443779, tv_loss: 0.03395915776491165\n",
      "iteration 2131, dc_loss: 0.027831215411424637, tv_loss: 0.03397827222943306\n",
      "iteration 2132, dc_loss: 0.027844179421663284, tv_loss: 0.03396667167544365\n",
      "iteration 2133, dc_loss: 0.02783135138452053, tv_loss: 0.03400115296244621\n",
      "iteration 2134, dc_loss: 0.027817536145448685, tv_loss: 0.033990297466516495\n",
      "iteration 2135, dc_loss: 0.027822220697999, tv_loss: 0.03396698832511902\n",
      "iteration 2136, dc_loss: 0.027819540351629257, tv_loss: 0.03396616503596306\n",
      "iteration 2137, dc_loss: 0.027788154780864716, tv_loss: 0.03401970490813255\n",
      "iteration 2138, dc_loss: 0.02780858986079693, tv_loss: 0.033970680087804794\n",
      "iteration 2139, dc_loss: 0.02777109295129776, tv_loss: 0.033985916525125504\n",
      "iteration 2140, dc_loss: 0.0277810487896204, tv_loss: 0.033973656594753265\n",
      "iteration 2141, dc_loss: 0.027769843116402626, tv_loss: 0.033990755677223206\n",
      "iteration 2142, dc_loss: 0.027747072279453278, tv_loss: 0.03399099409580231\n",
      "iteration 2143, dc_loss: 0.027750862762331963, tv_loss: 0.03396710753440857\n",
      "iteration 2144, dc_loss: 0.027728969231247902, tv_loss: 0.033979400992393494\n",
      "iteration 2145, dc_loss: 0.027738139033317566, tv_loss: 0.03396892175078392\n",
      "iteration 2146, dc_loss: 0.027728814631700516, tv_loss: 0.03397941216826439\n",
      "iteration 2147, dc_loss: 0.02770623378455639, tv_loss: 0.03399480879306793\n",
      "iteration 2148, dc_loss: 0.027728235349059105, tv_loss: 0.03395513445138931\n",
      "iteration 2149, dc_loss: 0.027692006900906563, tv_loss: 0.03397474065423012\n",
      "iteration 2150, dc_loss: 0.02768980711698532, tv_loss: 0.03396810591220856\n",
      "iteration 2151, dc_loss: 0.02770458534359932, tv_loss: 0.03394616022706032\n",
      "iteration 2152, dc_loss: 0.027665717527270317, tv_loss: 0.03397916629910469\n",
      "iteration 2153, dc_loss: 0.027694648131728172, tv_loss: 0.033951375633478165\n",
      "iteration 2154, dc_loss: 0.027672838419675827, tv_loss: 0.03399603068828583\n",
      "iteration 2155, dc_loss: 0.02765343338251114, tv_loss: 0.033998217433691025\n",
      "iteration 2156, dc_loss: 0.02767253667116165, tv_loss: 0.03395535796880722\n",
      "iteration 2157, dc_loss: 0.027642356231808662, tv_loss: 0.033972762525081635\n",
      "iteration 2158, dc_loss: 0.02763727866113186, tv_loss: 0.03397445008158684\n",
      "iteration 2159, dc_loss: 0.027643969282507896, tv_loss: 0.03396331146359444\n",
      "iteration 2160, dc_loss: 0.027609998360276222, tv_loss: 0.033997710794210434\n",
      "iteration 2161, dc_loss: 0.027633320540189743, tv_loss: 0.03395743668079376\n",
      "iteration 2162, dc_loss: 0.027599945664405823, tv_loss: 0.03398197889328003\n",
      "iteration 2163, dc_loss: 0.02761719562113285, tv_loss: 0.033946141600608826\n",
      "iteration 2164, dc_loss: 0.02758382074534893, tv_loss: 0.03397165983915329\n",
      "iteration 2165, dc_loss: 0.02757871337234974, tv_loss: 0.03397655114531517\n",
      "iteration 2166, dc_loss: 0.02758663333952427, tv_loss: 0.03397990018129349\n",
      "iteration 2167, dc_loss: 0.02755354531109333, tv_loss: 0.03400595486164093\n",
      "iteration 2168, dc_loss: 0.027577612549066544, tv_loss: 0.03395361080765724\n",
      "iteration 2169, dc_loss: 0.027551164850592613, tv_loss: 0.033973053097724915\n",
      "iteration 2170, dc_loss: 0.02754085324704647, tv_loss: 0.03397916257381439\n",
      "iteration 2171, dc_loss: 0.027556506916880608, tv_loss: 0.03396080061793327\n",
      "iteration 2172, dc_loss: 0.027508504688739777, tv_loss: 0.03400972858071327\n",
      "iteration 2173, dc_loss: 0.027537377551198006, tv_loss: 0.033964768052101135\n",
      "iteration 2174, dc_loss: 0.027520284056663513, tv_loss: 0.03396981209516525\n",
      "iteration 2175, dc_loss: 0.02751498855650425, tv_loss: 0.03396298736333847\n",
      "iteration 2176, dc_loss: 0.027507102116942406, tv_loss: 0.033966854214668274\n",
      "iteration 2177, dc_loss: 0.02750057727098465, tv_loss: 0.03396497294306755\n",
      "iteration 2178, dc_loss: 0.027496084570884705, tv_loss: 0.03395889326930046\n",
      "iteration 2179, dc_loss: 0.027491994202136993, tv_loss: 0.03396112099289894\n",
      "iteration 2180, dc_loss: 0.027497880160808563, tv_loss: 0.03395838662981987\n",
      "iteration 2181, dc_loss: 0.027497028931975365, tv_loss: 0.033955901861190796\n",
      "iteration 2182, dc_loss: 0.02747369557619095, tv_loss: 0.03398674353957176\n",
      "iteration 2183, dc_loss: 0.02748272381722927, tv_loss: 0.03396456316113472\n",
      "iteration 2184, dc_loss: 0.02745693549513817, tv_loss: 0.034000858664512634\n",
      "iteration 2185, dc_loss: 0.027472082525491714, tv_loss: 0.03396899625658989\n",
      "iteration 2186, dc_loss: 0.027428291738033295, tv_loss: 0.033986806869506836\n",
      "iteration 2187, dc_loss: 0.027423085644841194, tv_loss: 0.03397015109658241\n",
      "iteration 2188, dc_loss: 0.027424288913607597, tv_loss: 0.03396432474255562\n",
      "iteration 2189, dc_loss: 0.0274024847894907, tv_loss: 0.03397107496857643\n",
      "iteration 2190, dc_loss: 0.02740182727575302, tv_loss: 0.033965494483709335\n",
      "iteration 2191, dc_loss: 0.027400998398661613, tv_loss: 0.03396403789520264\n",
      "iteration 2192, dc_loss: 0.02738557942211628, tv_loss: 0.033977728337049484\n",
      "iteration 2193, dc_loss: 0.02737702988088131, tv_loss: 0.03398090600967407\n",
      "iteration 2194, dc_loss: 0.027385246008634567, tv_loss: 0.03396504744887352\n",
      "iteration 2195, dc_loss: 0.02735995687544346, tv_loss: 0.03397464007139206\n",
      "iteration 2196, dc_loss: 0.0273740291595459, tv_loss: 0.03395625948905945\n",
      "iteration 2197, dc_loss: 0.027356499806046486, tv_loss: 0.033966779708862305\n",
      "iteration 2198, dc_loss: 0.027352482080459595, tv_loss: 0.03396574780344963\n",
      "iteration 2199, dc_loss: 0.02735240012407303, tv_loss: 0.0339568667113781\n",
      "iteration 2200, dc_loss: 0.02733001299202442, tv_loss: 0.0339745432138443\n",
      "iteration 2201, dc_loss: 0.027336835861206055, tv_loss: 0.0339699350297451\n",
      "iteration 2202, dc_loss: 0.027334263548254967, tv_loss: 0.033975303173065186\n",
      "iteration 2203, dc_loss: 0.02729923650622368, tv_loss: 0.03400540351867676\n",
      "iteration 2204, dc_loss: 0.027319246903061867, tv_loss: 0.03396333381533623\n",
      "iteration 2205, dc_loss: 0.027293726801872253, tv_loss: 0.033968765288591385\n",
      "iteration 2206, dc_loss: 0.027291107922792435, tv_loss: 0.03396415337920189\n",
      "iteration 2207, dc_loss: 0.02728363871574402, tv_loss: 0.033969275653362274\n",
      "iteration 2208, dc_loss: 0.027271848171949387, tv_loss: 0.03398760035634041\n",
      "iteration 2209, dc_loss: 0.027273377403616905, tv_loss: 0.03397171199321747\n",
      "iteration 2210, dc_loss: 0.027257468551397324, tv_loss: 0.03397565707564354\n",
      "iteration 2211, dc_loss: 0.02725830115377903, tv_loss: 0.033956099301576614\n",
      "iteration 2212, dc_loss: 0.027249112725257874, tv_loss: 0.0339619442820549\n",
      "iteration 2213, dc_loss: 0.027239952236413956, tv_loss: 0.033966176211833954\n",
      "iteration 2214, dc_loss: 0.027238676324486732, tv_loss: 0.03399120643734932\n",
      "iteration 2215, dc_loss: 0.0272175632417202, tv_loss: 0.03400392830371857\n",
      "iteration 2216, dc_loss: 0.02724360302090645, tv_loss: 0.03394828736782074\n",
      "iteration 2217, dc_loss: 0.02719619870185852, tv_loss: 0.033990781754255295\n",
      "iteration 2218, dc_loss: 0.027229860424995422, tv_loss: 0.0339733250439167\n",
      "iteration 2219, dc_loss: 0.027210760861635208, tv_loss: 0.03399117290973663\n",
      "iteration 2220, dc_loss: 0.027195634320378304, tv_loss: 0.03399083763360977\n",
      "iteration 2221, dc_loss: 0.027228020131587982, tv_loss: 0.03395719826221466\n",
      "iteration 2222, dc_loss: 0.027200687676668167, tv_loss: 0.033989522606134415\n",
      "iteration 2223, dc_loss: 0.027202889323234558, tv_loss: 0.03399809077382088\n",
      "iteration 2224, dc_loss: 0.02718527242541313, tv_loss: 0.03397513926029205\n",
      "iteration 2225, dc_loss: 0.027164818719029427, tv_loss: 0.034000612795352936\n",
      "iteration 2226, dc_loss: 0.027203531935811043, tv_loss: 0.03394032269716263\n",
      "iteration 2227, dc_loss: 0.02714865654706955, tv_loss: 0.033996421843767166\n",
      "iteration 2228, dc_loss: 0.027161207050085068, tv_loss: 0.033963143825531006\n",
      "iteration 2229, dc_loss: 0.027127806097269058, tv_loss: 0.033980049192905426\n",
      "iteration 2230, dc_loss: 0.02711823210120201, tv_loss: 0.033975716680288315\n",
      "iteration 2231, dc_loss: 0.0271331537514925, tv_loss: 0.03395993262529373\n",
      "iteration 2232, dc_loss: 0.027104685083031654, tv_loss: 0.03398503363132477\n",
      "iteration 2233, dc_loss: 0.027108611539006233, tv_loss: 0.0339667834341526\n",
      "iteration 2234, dc_loss: 0.02709290012717247, tv_loss: 0.03397412598133087\n",
      "iteration 2235, dc_loss: 0.027085578069090843, tv_loss: 0.03397531434893608\n",
      "iteration 2236, dc_loss: 0.027097104117274284, tv_loss: 0.0339595302939415\n",
      "iteration 2237, dc_loss: 0.02706894464790821, tv_loss: 0.03397878631949425\n",
      "iteration 2238, dc_loss: 0.027080165222287178, tv_loss: 0.03395964205265045\n",
      "iteration 2239, dc_loss: 0.027047626674175262, tv_loss: 0.033983927220106125\n",
      "iteration 2240, dc_loss: 0.027061572298407555, tv_loss: 0.03396998345851898\n",
      "iteration 2241, dc_loss: 0.027059532701969147, tv_loss: 0.03396337106823921\n",
      "iteration 2242, dc_loss: 0.027042552828788757, tv_loss: 0.033976517617702484\n",
      "iteration 2243, dc_loss: 0.027048729360103607, tv_loss: 0.03395900875329971\n",
      "iteration 2244, dc_loss: 0.02703736536204815, tv_loss: 0.03396866098046303\n",
      "iteration 2245, dc_loss: 0.02704235538840294, tv_loss: 0.03395969420671463\n",
      "iteration 2246, dc_loss: 0.0270213782787323, tv_loss: 0.03397132828831673\n",
      "iteration 2247, dc_loss: 0.027041617780923843, tv_loss: 0.03395281359553337\n",
      "iteration 2248, dc_loss: 0.027021894231438637, tv_loss: 0.033987656235694885\n",
      "iteration 2249, dc_loss: 0.0270308218896389, tv_loss: 0.033983346074819565\n",
      "iteration 2250, dc_loss: 0.02699773758649826, tv_loss: 0.03398893401026726\n",
      "iteration 2251, dc_loss: 0.02700611762702465, tv_loss: 0.033960603177547455\n",
      "iteration 2252, dc_loss: 0.026981830596923828, tv_loss: 0.03398306295275688\n",
      "iteration 2253, dc_loss: 0.02700427919626236, tv_loss: 0.03395348787307739\n",
      "iteration 2254, dc_loss: 0.02695777826011181, tv_loss: 0.034008342772722244\n",
      "iteration 2255, dc_loss: 0.026964932680130005, tv_loss: 0.03397892788052559\n",
      "iteration 2256, dc_loss: 0.02696235477924347, tv_loss: 0.033963412046432495\n",
      "iteration 2257, dc_loss: 0.026935841888189316, tv_loss: 0.03398260101675987\n",
      "iteration 2258, dc_loss: 0.02694958634674549, tv_loss: 0.033962029963731766\n",
      "iteration 2259, dc_loss: 0.026921799406409264, tv_loss: 0.03398067131638527\n",
      "iteration 2260, dc_loss: 0.02692583203315735, tv_loss: 0.033962178975343704\n",
      "iteration 2261, dc_loss: 0.02691570296883583, tv_loss: 0.033962398767471313\n",
      "iteration 2262, dc_loss: 0.026907121762633324, tv_loss: 0.033961206674575806\n",
      "iteration 2263, dc_loss: 0.026917926967144012, tv_loss: 0.033947259187698364\n",
      "iteration 2264, dc_loss: 0.026878762990236282, tv_loss: 0.03399069607257843\n",
      "iteration 2265, dc_loss: 0.026905372738838196, tv_loss: 0.03398032486438751\n",
      "iteration 2266, dc_loss: 0.0268714502453804, tv_loss: 0.03398789465427399\n",
      "iteration 2267, dc_loss: 0.026876889169216156, tv_loss: 0.033962398767471313\n",
      "iteration 2268, dc_loss: 0.02687525376677513, tv_loss: 0.03395957499742508\n",
      "iteration 2269, dc_loss: 0.02684868313372135, tv_loss: 0.03400017321109772\n",
      "iteration 2270, dc_loss: 0.02688283659517765, tv_loss: 0.03395119309425354\n",
      "iteration 2271, dc_loss: 0.026831820607185364, tv_loss: 0.033986952155828476\n",
      "iteration 2272, dc_loss: 0.026831066235899925, tv_loss: 0.03397206589579582\n",
      "iteration 2273, dc_loss: 0.0268549881875515, tv_loss: 0.033955350518226624\n",
      "iteration 2274, dc_loss: 0.02682466246187687, tv_loss: 0.03400178253650665\n",
      "iteration 2275, dc_loss: 0.026823945343494415, tv_loss: 0.0339764840900898\n",
      "iteration 2276, dc_loss: 0.026808297261595726, tv_loss: 0.03396790474653244\n",
      "iteration 2277, dc_loss: 0.026813972741365433, tv_loss: 0.033971529453992844\n",
      "iteration 2278, dc_loss: 0.026809467002749443, tv_loss: 0.03398938477039337\n",
      "iteration 2279, dc_loss: 0.02679695375263691, tv_loss: 0.033982258290052414\n",
      "iteration 2280, dc_loss: 0.026780087500810623, tv_loss: 0.03397773206233978\n",
      "iteration 2281, dc_loss: 0.02678561396896839, tv_loss: 0.03397331014275551\n",
      "iteration 2282, dc_loss: 0.026787757873535156, tv_loss: 0.03397686034440994\n",
      "iteration 2283, dc_loss: 0.026798363775014877, tv_loss: 0.03397361934185028\n",
      "iteration 2284, dc_loss: 0.02678738720715046, tv_loss: 0.033985160291194916\n",
      "iteration 2285, dc_loss: 0.026824327185750008, tv_loss: 0.03395976126194\n",
      "iteration 2286, dc_loss: 0.026856038719415665, tv_loss: 0.03398165479302406\n",
      "iteration 2287, dc_loss: 0.026874052360653877, tv_loss: 0.03394995257258415\n",
      "iteration 2288, dc_loss: 0.0268313717097044, tv_loss: 0.03399350494146347\n",
      "iteration 2289, dc_loss: 0.02684011496603489, tv_loss: 0.03394582122564316\n",
      "iteration 2290, dc_loss: 0.026822833344340324, tv_loss: 0.03395356610417366\n",
      "iteration 2291, dc_loss: 0.026758883148431778, tv_loss: 0.033993061631917953\n",
      "iteration 2292, dc_loss: 0.026737431064248085, tv_loss: 0.033966317772865295\n",
      "iteration 2293, dc_loss: 0.026740530505776405, tv_loss: 0.03394018113613129\n",
      "iteration 2294, dc_loss: 0.026680734008550644, tv_loss: 0.03399958461523056\n",
      "iteration 2295, dc_loss: 0.026720097288489342, tv_loss: 0.03395376726984978\n",
      "iteration 2296, dc_loss: 0.026712000370025635, tv_loss: 0.033970095217227936\n",
      "iteration 2297, dc_loss: 0.02672933228313923, tv_loss: 0.033974822610616684\n",
      "iteration 2298, dc_loss: 0.026737842708826065, tv_loss: 0.0339469388127327\n",
      "iteration 2299, dc_loss: 0.026681454852223396, tv_loss: 0.03398561477661133\n",
      "iteration 2300, dc_loss: 0.02670300379395485, tv_loss: 0.03395281732082367\n",
      "iteration 2301, dc_loss: 0.026658302173018456, tv_loss: 0.033974211663007736\n",
      "iteration 2302, dc_loss: 0.02665364369750023, tv_loss: 0.03395979478955269\n",
      "iteration 2303, dc_loss: 0.02665560320019722, tv_loss: 0.03396464139223099\n",
      "iteration 2304, dc_loss: 0.026638075709342957, tv_loss: 0.03397607430815697\n",
      "iteration 2305, dc_loss: 0.026643339544534683, tv_loss: 0.03396925702691078\n",
      "iteration 2306, dc_loss: 0.02665337175130844, tv_loss: 0.03397037461400032\n",
      "iteration 2307, dc_loss: 0.026642978191375732, tv_loss: 0.0339512936770916\n",
      "iteration 2308, dc_loss: 0.026612896472215652, tv_loss: 0.03397168964147568\n",
      "iteration 2309, dc_loss: 0.026635343208909035, tv_loss: 0.03394978120923042\n",
      "iteration 2310, dc_loss: 0.026587801054120064, tv_loss: 0.03399388864636421\n",
      "iteration 2311, dc_loss: 0.026600118726491928, tv_loss: 0.03398096188902855\n",
      "iteration 2312, dc_loss: 0.026587598025798798, tv_loss: 0.033979907631874084\n",
      "iteration 2313, dc_loss: 0.02659006603062153, tv_loss: 0.03396229073405266\n",
      "iteration 2314, dc_loss: 0.026570826768875122, tv_loss: 0.033981285989284515\n",
      "iteration 2315, dc_loss: 0.026570798829197884, tv_loss: 0.03397273272275925\n",
      "iteration 2316, dc_loss: 0.026576634496450424, tv_loss: 0.033961303532123566\n",
      "iteration 2317, dc_loss: 0.026552913710474968, tv_loss: 0.0339822843670845\n",
      "iteration 2318, dc_loss: 0.026571298018097878, tv_loss: 0.03394891321659088\n",
      "iteration 2319, dc_loss: 0.026531817391514778, tv_loss: 0.03398152440786362\n",
      "iteration 2320, dc_loss: 0.026558494195342064, tv_loss: 0.03394576907157898\n",
      "iteration 2321, dc_loss: 0.026498515158891678, tv_loss: 0.03399994969367981\n",
      "iteration 2322, dc_loss: 0.026540393009781837, tv_loss: 0.03394756466150284\n",
      "iteration 2323, dc_loss: 0.026518482714891434, tv_loss: 0.033977434039115906\n",
      "iteration 2324, dc_loss: 0.026501251384615898, tv_loss: 0.033989377319812775\n",
      "iteration 2325, dc_loss: 0.026536639779806137, tv_loss: 0.03394702821969986\n",
      "iteration 2326, dc_loss: 0.02647976391017437, tv_loss: 0.033997345715761185\n",
      "iteration 2327, dc_loss: 0.02651391364634037, tv_loss: 0.033943794667720795\n",
      "iteration 2328, dc_loss: 0.026468273252248764, tv_loss: 0.03398246690630913\n",
      "iteration 2329, dc_loss: 0.026508865877985954, tv_loss: 0.03393775224685669\n",
      "iteration 2330, dc_loss: 0.026468880474567413, tv_loss: 0.03397279605269432\n",
      "iteration 2331, dc_loss: 0.026498524472117424, tv_loss: 0.03393888473510742\n",
      "iteration 2332, dc_loss: 0.02645469829440117, tv_loss: 0.03399841487407684\n",
      "iteration 2333, dc_loss: 0.026473330333828926, tv_loss: 0.03398742526769638\n",
      "iteration 2334, dc_loss: 0.026455948129296303, tv_loss: 0.033979739993810654\n",
      "iteration 2335, dc_loss: 0.026457644999027252, tv_loss: 0.033960066735744476\n",
      "iteration 2336, dc_loss: 0.02644485794007778, tv_loss: 0.03397209197282791\n",
      "iteration 2337, dc_loss: 0.026448054239153862, tv_loss: 0.03398596867918968\n",
      "iteration 2338, dc_loss: 0.02643493562936783, tv_loss: 0.0339980274438858\n",
      "iteration 2339, dc_loss: 0.026435833424329758, tv_loss: 0.03396332263946533\n",
      "iteration 2340, dc_loss: 0.02642260491847992, tv_loss: 0.03398408368229866\n",
      "iteration 2341, dc_loss: 0.026416882872581482, tv_loss: 0.03399054333567619\n",
      "iteration 2342, dc_loss: 0.02642008289694786, tv_loss: 0.03397616744041443\n",
      "iteration 2343, dc_loss: 0.026420047506690025, tv_loss: 0.033947475254535675\n",
      "iteration 2344, dc_loss: 0.026379000395536423, tv_loss: 0.03399532660841942\n",
      "iteration 2345, dc_loss: 0.026398681104183197, tv_loss: 0.03396881744265556\n",
      "iteration 2346, dc_loss: 0.02638295106589794, tv_loss: 0.033985435962677\n",
      "iteration 2347, dc_loss: 0.026390835642814636, tv_loss: 0.033956002444028854\n",
      "iteration 2348, dc_loss: 0.026348743587732315, tv_loss: 0.033986080437898636\n",
      "iteration 2349, dc_loss: 0.026373902335762978, tv_loss: 0.03396068513393402\n",
      "iteration 2350, dc_loss: 0.026359690353274345, tv_loss: 0.033974383026361465\n",
      "iteration 2351, dc_loss: 0.02636021561920643, tv_loss: 0.03396137058734894\n",
      "iteration 2352, dc_loss: 0.026346800848841667, tv_loss: 0.033969007432460785\n",
      "iteration 2353, dc_loss: 0.026340415701270103, tv_loss: 0.0339578241109848\n",
      "iteration 2354, dc_loss: 0.026315324008464813, tv_loss: 0.03397039696574211\n",
      "iteration 2355, dc_loss: 0.02633402682840824, tv_loss: 0.03396036475896835\n",
      "iteration 2356, dc_loss: 0.02631860226392746, tv_loss: 0.03397847339510918\n",
      "iteration 2357, dc_loss: 0.02631586790084839, tv_loss: 0.03397379443049431\n",
      "iteration 2358, dc_loss: 0.02631043642759323, tv_loss: 0.03396366164088249\n",
      "iteration 2359, dc_loss: 0.026307720690965652, tv_loss: 0.033958569169044495\n",
      "iteration 2360, dc_loss: 0.026295432820916176, tv_loss: 0.03396313637495041\n",
      "iteration 2361, dc_loss: 0.026289530098438263, tv_loss: 0.0339692123234272\n",
      "iteration 2362, dc_loss: 0.02629392221570015, tv_loss: 0.03398073837161064\n",
      "iteration 2363, dc_loss: 0.02629457227885723, tv_loss: 0.03397882729768753\n",
      "iteration 2364, dc_loss: 0.02626795321702957, tv_loss: 0.033992670476436615\n",
      "iteration 2365, dc_loss: 0.026302142068743706, tv_loss: 0.03394347056746483\n",
      "iteration 2366, dc_loss: 0.02626798115670681, tv_loss: 0.03398704156279564\n",
      "iteration 2367, dc_loss: 0.02630312740802765, tv_loss: 0.03396507725119591\n",
      "iteration 2368, dc_loss: 0.026264218613505363, tv_loss: 0.033999715000391006\n",
      "iteration 2369, dc_loss: 0.026304196566343307, tv_loss: 0.0339399054646492\n",
      "iteration 2370, dc_loss: 0.02625753916800022, tv_loss: 0.03399059548974037\n",
      "iteration 2371, dc_loss: 0.026286372914910316, tv_loss: 0.033951520919799805\n",
      "iteration 2372, dc_loss: 0.026242347434163094, tv_loss: 0.034004390239715576\n",
      "iteration 2373, dc_loss: 0.026267169043421745, tv_loss: 0.03395118564367294\n",
      "iteration 2374, dc_loss: 0.026212425902485847, tv_loss: 0.033986400812864304\n",
      "iteration 2375, dc_loss: 0.026221483945846558, tv_loss: 0.03395451605319977\n",
      "iteration 2376, dc_loss: 0.02620570920407772, tv_loss: 0.033965256065130234\n",
      "iteration 2377, dc_loss: 0.026194361969828606, tv_loss: 0.033974550664424896\n",
      "iteration 2378, dc_loss: 0.026179993525147438, tv_loss: 0.03398635610938072\n",
      "iteration 2379, dc_loss: 0.02616804838180542, tv_loss: 0.0339764766395092\n",
      "iteration 2380, dc_loss: 0.026189405471086502, tv_loss: 0.03394802659749985\n",
      "iteration 2381, dc_loss: 0.026163354516029358, tv_loss: 0.033976148813962936\n",
      "iteration 2382, dc_loss: 0.02617953158915043, tv_loss: 0.033953212201595306\n",
      "iteration 2383, dc_loss: 0.026149185374379158, tv_loss: 0.0339723564684391\n",
      "iteration 2384, dc_loss: 0.026168975979089737, tv_loss: 0.033943794667720795\n",
      "iteration 2385, dc_loss: 0.02612561173737049, tv_loss: 0.0339910164475441\n",
      "iteration 2386, dc_loss: 0.026183392852544785, tv_loss: 0.03392902389168739\n",
      "iteration 2387, dc_loss: 0.026124441996216774, tv_loss: 0.033985886722803116\n",
      "iteration 2388, dc_loss: 0.02615043707191944, tv_loss: 0.03396272286772728\n",
      "iteration 2389, dc_loss: 0.02612048014998436, tv_loss: 0.03397944197058678\n",
      "iteration 2390, dc_loss: 0.026137778535485268, tv_loss: 0.0339561328291893\n",
      "iteration 2391, dc_loss: 0.026113707572221756, tv_loss: 0.03396749123930931\n",
      "iteration 2392, dc_loss: 0.026128074154257774, tv_loss: 0.03394690901041031\n",
      "iteration 2393, dc_loss: 0.02609087899327278, tv_loss: 0.033976130187511444\n",
      "iteration 2394, dc_loss: 0.026134664192795753, tv_loss: 0.033930324018001556\n",
      "iteration 2395, dc_loss: 0.026086999103426933, tv_loss: 0.0339733324944973\n",
      "iteration 2396, dc_loss: 0.02610110491514206, tv_loss: 0.03397119790315628\n",
      "iteration 2397, dc_loss: 0.02609020657837391, tv_loss: 0.03397916629910469\n",
      "iteration 2398, dc_loss: 0.026079382747411728, tv_loss: 0.03397858142852783\n",
      "iteration 2399, dc_loss: 0.02609182894229889, tv_loss: 0.033948589116334915\n",
      "iteration 2400, dc_loss: 0.02605479396879673, tv_loss: 0.03397946432232857\n",
      "iteration 2401, dc_loss: 0.02607344277203083, tv_loss: 0.03396615758538246\n",
      "iteration 2402, dc_loss: 0.026039259508252144, tv_loss: 0.033971257507801056\n",
      "iteration 2403, dc_loss: 0.02603721246123314, tv_loss: 0.03395130857825279\n",
      "iteration 2404, dc_loss: 0.026053443551063538, tv_loss: 0.03394939377903938\n",
      "iteration 2405, dc_loss: 0.026025986298918724, tv_loss: 0.03396518528461456\n",
      "iteration 2406, dc_loss: 0.026020705699920654, tv_loss: 0.033959709107875824\n",
      "iteration 2407, dc_loss: 0.02603358030319214, tv_loss: 0.03393358737230301\n",
      "iteration 2408, dc_loss: 0.026014944538474083, tv_loss: 0.03396555036306381\n",
      "iteration 2409, dc_loss: 0.02600322663784027, tv_loss: 0.03397016227245331\n",
      "iteration 2410, dc_loss: 0.02601620741188526, tv_loss: 0.03393898531794548\n",
      "iteration 2411, dc_loss: 0.025995822623372078, tv_loss: 0.0339561365544796\n",
      "iteration 2412, dc_loss: 0.026001818478107452, tv_loss: 0.03394521027803421\n",
      "iteration 2413, dc_loss: 0.025998029857873917, tv_loss: 0.03394561633467674\n",
      "iteration 2414, dc_loss: 0.025978079065680504, tv_loss: 0.033959269523620605\n",
      "iteration 2415, dc_loss: 0.02599479816854, tv_loss: 0.033940572291612625\n",
      "iteration 2416, dc_loss: 0.02597956173121929, tv_loss: 0.0339391864836216\n",
      "iteration 2417, dc_loss: 0.025956952944397926, tv_loss: 0.03395779803395271\n",
      "iteration 2418, dc_loss: 0.025977831333875656, tv_loss: 0.03393780067563057\n",
      "iteration 2419, dc_loss: 0.02596150152385235, tv_loss: 0.03393867611885071\n",
      "iteration 2420, dc_loss: 0.025948062539100647, tv_loss: 0.03395228460431099\n",
      "iteration 2421, dc_loss: 0.025964220985770226, tv_loss: 0.033930275589227676\n",
      "iteration 2422, dc_loss: 0.025949254631996155, tv_loss: 0.033939190208911896\n",
      "iteration 2423, dc_loss: 0.025942625477910042, tv_loss: 0.0339432917535305\n",
      "iteration 2424, dc_loss: 0.025941917672753334, tv_loss: 0.03394218906760216\n",
      "iteration 2425, dc_loss: 0.025929586961865425, tv_loss: 0.033956289291381836\n",
      "iteration 2426, dc_loss: 0.0259215347468853, tv_loss: 0.03396926075220108\n",
      "iteration 2427, dc_loss: 0.025937844067811966, tv_loss: 0.03393765911459923\n",
      "iteration 2428, dc_loss: 0.025915440171957016, tv_loss: 0.03395313024520874\n",
      "iteration 2429, dc_loss: 0.02591850236058235, tv_loss: 0.03394239395856857\n",
      "iteration 2430, dc_loss: 0.025914093479514122, tv_loss: 0.033949315547943115\n",
      "iteration 2431, dc_loss: 0.025896945968270302, tv_loss: 0.03397267311811447\n",
      "iteration 2432, dc_loss: 0.02591070905327797, tv_loss: 0.03394068032503128\n",
      "iteration 2433, dc_loss: 0.02589372731745243, tv_loss: 0.03394949063658714\n",
      "iteration 2434, dc_loss: 0.025882909074425697, tv_loss: 0.03395312651991844\n",
      "iteration 2435, dc_loss: 0.025903278961777687, tv_loss: 0.03392792493104935\n",
      "iteration 2436, dc_loss: 0.025881541892886162, tv_loss: 0.0339580662548542\n",
      "iteration 2437, dc_loss: 0.025874201208353043, tv_loss: 0.033956680446863174\n",
      "iteration 2438, dc_loss: 0.025881262496113777, tv_loss: 0.033941250294446945\n",
      "iteration 2439, dc_loss: 0.025867534801363945, tv_loss: 0.03394734114408493\n",
      "iteration 2440, dc_loss: 0.025864161550998688, tv_loss: 0.03395906090736389\n",
      "iteration 2441, dc_loss: 0.025871464982628822, tv_loss: 0.033940162509679794\n",
      "iteration 2442, dc_loss: 0.025855902582406998, tv_loss: 0.03395839035511017\n",
      "iteration 2443, dc_loss: 0.025843802839517593, tv_loss: 0.03395012393593788\n",
      "iteration 2444, dc_loss: 0.02585119567811489, tv_loss: 0.03394445776939392\n",
      "iteration 2445, dc_loss: 0.025846272706985474, tv_loss: 0.033960312604904175\n",
      "iteration 2446, dc_loss: 0.02583392523229122, tv_loss: 0.033962052315473557\n",
      "iteration 2447, dc_loss: 0.02583906799554825, tv_loss: 0.03393922001123428\n",
      "iteration 2448, dc_loss: 0.025829678401350975, tv_loss: 0.03394300490617752\n",
      "iteration 2449, dc_loss: 0.025825221091508865, tv_loss: 0.033956050872802734\n",
      "iteration 2450, dc_loss: 0.025827279314398766, tv_loss: 0.03395157679915428\n",
      "iteration 2451, dc_loss: 0.02581307291984558, tv_loss: 0.033952172845602036\n",
      "iteration 2452, dc_loss: 0.025813085958361626, tv_loss: 0.03393819183111191\n",
      "iteration 2453, dc_loss: 0.02580675669014454, tv_loss: 0.03394163399934769\n",
      "iteration 2454, dc_loss: 0.0258009135723114, tv_loss: 0.033956337720155716\n",
      "iteration 2455, dc_loss: 0.02580195665359497, tv_loss: 0.03395843878388405\n",
      "iteration 2456, dc_loss: 0.02578672021627426, tv_loss: 0.0339558906853199\n",
      "iteration 2457, dc_loss: 0.025803271681070328, tv_loss: 0.033925674855709076\n",
      "iteration 2458, dc_loss: 0.0257814209908247, tv_loss: 0.03394215926527977\n",
      "iteration 2459, dc_loss: 0.025771938264369965, tv_loss: 0.03396543860435486\n",
      "iteration 2460, dc_loss: 0.025787780061364174, tv_loss: 0.03395991772413254\n",
      "iteration 2461, dc_loss: 0.025767024606466293, tv_loss: 0.033953264355659485\n",
      "iteration 2462, dc_loss: 0.025768477469682693, tv_loss: 0.03394113481044769\n",
      "iteration 2463, dc_loss: 0.025769807398319244, tv_loss: 0.033947017043828964\n",
      "iteration 2464, dc_loss: 0.02574916183948517, tv_loss: 0.03398435562849045\n",
      "iteration 2465, dc_loss: 0.025760255753993988, tv_loss: 0.03394410386681557\n",
      "iteration 2466, dc_loss: 0.025747613981366158, tv_loss: 0.03394841030240059\n",
      "iteration 2467, dc_loss: 0.02574213407933712, tv_loss: 0.033969447016716\n",
      "iteration 2468, dc_loss: 0.02575351670384407, tv_loss: 0.03395523875951767\n",
      "iteration 2469, dc_loss: 0.02572772093117237, tv_loss: 0.033957552164793015\n",
      "iteration 2470, dc_loss: 0.025732889771461487, tv_loss: 0.03395039588212967\n",
      "iteration 2471, dc_loss: 0.02573234587907791, tv_loss: 0.033953968435525894\n",
      "iteration 2472, dc_loss: 0.02571381814777851, tv_loss: 0.03395966812968254\n",
      "iteration 2473, dc_loss: 0.02572510577738285, tv_loss: 0.03393884748220444\n",
      "iteration 2474, dc_loss: 0.025723623111844063, tv_loss: 0.033933695405721664\n",
      "iteration 2475, dc_loss: 0.025704586878418922, tv_loss: 0.03395170718431473\n",
      "iteration 2476, dc_loss: 0.0257108174264431, tv_loss: 0.03394858166575432\n",
      "iteration 2477, dc_loss: 0.025695495307445526, tv_loss: 0.03395697847008705\n",
      "iteration 2478, dc_loss: 0.025702867656946182, tv_loss: 0.03393750265240669\n",
      "iteration 2479, dc_loss: 0.025696635246276855, tv_loss: 0.03393162786960602\n",
      "iteration 2480, dc_loss: 0.025679979473352432, tv_loss: 0.03394497185945511\n",
      "iteration 2481, dc_loss: 0.025688812136650085, tv_loss: 0.033930469304323196\n",
      "iteration 2482, dc_loss: 0.025677360594272614, tv_loss: 0.033934637904167175\n",
      "iteration 2483, dc_loss: 0.025672610849142075, tv_loss: 0.0339360311627388\n",
      "iteration 2484, dc_loss: 0.025680815801024437, tv_loss: 0.03391926735639572\n",
      "iteration 2485, dc_loss: 0.02566390670835972, tv_loss: 0.03393477573990822\n",
      "iteration 2486, dc_loss: 0.02566271275281906, tv_loss: 0.03393261134624481\n",
      "iteration 2487, dc_loss: 0.025661703199148178, tv_loss: 0.03393689915537834\n",
      "iteration 2488, dc_loss: 0.025645816698670387, tv_loss: 0.033961158245801926\n",
      "iteration 2489, dc_loss: 0.025654109194874763, tv_loss: 0.03395452722907066\n",
      "iteration 2490, dc_loss: 0.025655405595898628, tv_loss: 0.03393075242638588\n",
      "iteration 2491, dc_loss: 0.02562200278043747, tv_loss: 0.03395380824804306\n",
      "iteration 2492, dc_loss: 0.025640049949288368, tv_loss: 0.03393549472093582\n",
      "iteration 2493, dc_loss: 0.025645701214671135, tv_loss: 0.03392621502280235\n",
      "iteration 2494, dc_loss: 0.02561243064701557, tv_loss: 0.03395920991897583\n",
      "iteration 2495, dc_loss: 0.025626586750149727, tv_loss: 0.03393901512026787\n",
      "iteration 2496, dc_loss: 0.025628073140978813, tv_loss: 0.0339401476085186\n",
      "iteration 2497, dc_loss: 0.02561534009873867, tv_loss: 0.03393947705626488\n",
      "iteration 2498, dc_loss: 0.025612596422433853, tv_loss: 0.033939845860004425\n",
      "iteration 2499, dc_loss: 0.025607125833630562, tv_loss: 0.03393975645303726\n",
      "iteration 2500, dc_loss: 0.02559649758040905, tv_loss: 0.03394079953432083\n",
      "iteration 2501, dc_loss: 0.02560056746006012, tv_loss: 0.033930882811546326\n",
      "iteration 2502, dc_loss: 0.025588586926460266, tv_loss: 0.03394453227519989\n",
      "iteration 2503, dc_loss: 0.025589032098650932, tv_loss: 0.03393473103642464\n",
      "iteration 2504, dc_loss: 0.025592772290110588, tv_loss: 0.0339275561273098\n",
      "iteration 2505, dc_loss: 0.02557326853275299, tv_loss: 0.03394244983792305\n",
      "iteration 2506, dc_loss: 0.025574877858161926, tv_loss: 0.033943451941013336\n",
      "iteration 2507, dc_loss: 0.02557765319943428, tv_loss: 0.033926934003829956\n",
      "iteration 2508, dc_loss: 0.025561895221471786, tv_loss: 0.03393742814660072\n",
      "iteration 2509, dc_loss: 0.025564854964613914, tv_loss: 0.033959951251745224\n",
      "iteration 2510, dc_loss: 0.0255690086632967, tv_loss: 0.03395604342222214\n",
      "iteration 2511, dc_loss: 0.025549253448843956, tv_loss: 0.03393976390361786\n",
      "iteration 2512, dc_loss: 0.02554580383002758, tv_loss: 0.03395025432109833\n",
      "iteration 2513, dc_loss: 0.025550104677677155, tv_loss: 0.0339612178504467\n",
      "iteration 2514, dc_loss: 0.025539560243487358, tv_loss: 0.033961813896894455\n",
      "iteration 2515, dc_loss: 0.025539444759488106, tv_loss: 0.033934999257326126\n",
      "iteration 2516, dc_loss: 0.02553202584385872, tv_loss: 0.033939797431230545\n",
      "iteration 2517, dc_loss: 0.025536159053444862, tv_loss: 0.03394172713160515\n",
      "iteration 2518, dc_loss: 0.025527428835630417, tv_loss: 0.03395385295152664\n",
      "iteration 2519, dc_loss: 0.025512948632240295, tv_loss: 0.033951401710510254\n",
      "iteration 2520, dc_loss: 0.025522859767079353, tv_loss: 0.03393152356147766\n",
      "iteration 2521, dc_loss: 0.025509532541036606, tv_loss: 0.03393950313329697\n",
      "iteration 2522, dc_loss: 0.025507545098662376, tv_loss: 0.03395074978470802\n",
      "iteration 2523, dc_loss: 0.02551012672483921, tv_loss: 0.033943239599466324\n",
      "iteration 2524, dc_loss: 0.025491535663604736, tv_loss: 0.033956706523895264\n",
      "iteration 2525, dc_loss: 0.025496268644928932, tv_loss: 0.033938124775886536\n",
      "iteration 2526, dc_loss: 0.025496352463960648, tv_loss: 0.033931881189346313\n",
      "iteration 2527, dc_loss: 0.02548040822148323, tv_loss: 0.0339520126581192\n",
      "iteration 2528, dc_loss: 0.025483451783657074, tv_loss: 0.03394441679120064\n",
      "iteration 2529, dc_loss: 0.025476176291704178, tv_loss: 0.0339389443397522\n",
      "iteration 2530, dc_loss: 0.025479381904006004, tv_loss: 0.03393048048019409\n",
      "iteration 2531, dc_loss: 0.025471292436122894, tv_loss: 0.033931635320186615\n",
      "iteration 2532, dc_loss: 0.02546345442533493, tv_loss: 0.03393792733550072\n",
      "iteration 2533, dc_loss: 0.02546110562980175, tv_loss: 0.033937033265829086\n",
      "iteration 2534, dc_loss: 0.02545526623725891, tv_loss: 0.03394823521375656\n",
      "iteration 2535, dc_loss: 0.02546059526503086, tv_loss: 0.03393646329641342\n",
      "iteration 2536, dc_loss: 0.02544689178466797, tv_loss: 0.03393897786736488\n",
      "iteration 2537, dc_loss: 0.02543318085372448, tv_loss: 0.033944811671972275\n",
      "iteration 2538, dc_loss: 0.025449352338910103, tv_loss: 0.03392573073506355\n",
      "iteration 2539, dc_loss: 0.02543279156088829, tv_loss: 0.033931050449609756\n",
      "iteration 2540, dc_loss: 0.02542625553905964, tv_loss: 0.03393379971385002\n",
      "iteration 2541, dc_loss: 0.025433994829654694, tv_loss: 0.033926934003829956\n",
      "iteration 2542, dc_loss: 0.025421123951673508, tv_loss: 0.033937834203243256\n",
      "iteration 2543, dc_loss: 0.02541872300207615, tv_loss: 0.03394662216305733\n",
      "iteration 2544, dc_loss: 0.025411168113350868, tv_loss: 0.03394607454538345\n",
      "iteration 2545, dc_loss: 0.02541278302669525, tv_loss: 0.03393641486763954\n",
      "iteration 2546, dc_loss: 0.025408394634723663, tv_loss: 0.033928461372852325\n",
      "iteration 2547, dc_loss: 0.02540021762251854, tv_loss: 0.03393169119954109\n",
      "iteration 2548, dc_loss: 0.02539466693997383, tv_loss: 0.03394196555018425\n",
      "iteration 2549, dc_loss: 0.025390025228261948, tv_loss: 0.0339435413479805\n",
      "iteration 2550, dc_loss: 0.025399064645171165, tv_loss: 0.03393922373652458\n",
      "iteration 2551, dc_loss: 0.025385435670614243, tv_loss: 0.033935196697711945\n",
      "iteration 2552, dc_loss: 0.02537379413843155, tv_loss: 0.03394373878836632\n",
      "iteration 2553, dc_loss: 0.02537543512880802, tv_loss: 0.03393534943461418\n",
      "iteration 2554, dc_loss: 0.02537708543241024, tv_loss: 0.03393125534057617\n",
      "iteration 2555, dc_loss: 0.025362590327858925, tv_loss: 0.033957045525312424\n",
      "iteration 2556, dc_loss: 0.025359442457556725, tv_loss: 0.03395240753889084\n",
      "iteration 2557, dc_loss: 0.02537437342107296, tv_loss: 0.033928532153367996\n",
      "iteration 2558, dc_loss: 0.02535087987780571, tv_loss: 0.03393600508570671\n",
      "iteration 2559, dc_loss: 0.025350123643875122, tv_loss: 0.03394710645079613\n",
      "iteration 2560, dc_loss: 0.02535853162407875, tv_loss: 0.033926595002412796\n",
      "iteration 2561, dc_loss: 0.025343695655465126, tv_loss: 0.03394417464733124\n",
      "iteration 2562, dc_loss: 0.025342291221022606, tv_loss: 0.033945973962545395\n",
      "iteration 2563, dc_loss: 0.025354310870170593, tv_loss: 0.033929940313100815\n",
      "iteration 2564, dc_loss: 0.025360573083162308, tv_loss: 0.03394342586398125\n",
      "iteration 2565, dc_loss: 0.02537251077592373, tv_loss: 0.03391378000378609\n",
      "iteration 2566, dc_loss: 0.02533602900803089, tv_loss: 0.033951349556446075\n",
      "iteration 2567, dc_loss: 0.025354579091072083, tv_loss: 0.03391239419579506\n",
      "iteration 2568, dc_loss: 0.025326360017061234, tv_loss: 0.03393034264445305\n",
      "iteration 2569, dc_loss: 0.025318220257759094, tv_loss: 0.033925723284482956\n",
      "iteration 2570, dc_loss: 0.025307856500148773, tv_loss: 0.03393131494522095\n",
      "iteration 2571, dc_loss: 0.02530638873577118, tv_loss: 0.033945366740226746\n",
      "iteration 2572, dc_loss: 0.02529698796570301, tv_loss: 0.03395603969693184\n",
      "iteration 2573, dc_loss: 0.02529394067823887, tv_loss: 0.03393542394042015\n",
      "iteration 2574, dc_loss: 0.025291506201028824, tv_loss: 0.033929355442523956\n",
      "iteration 2575, dc_loss: 0.025296755135059357, tv_loss: 0.033935125917196274\n",
      "iteration 2576, dc_loss: 0.025280948728322983, tv_loss: 0.03394567221403122\n",
      "iteration 2577, dc_loss: 0.025272734463214874, tv_loss: 0.03395959362387657\n",
      "iteration 2578, dc_loss: 0.02529909834265709, tv_loss: 0.03390916809439659\n",
      "iteration 2579, dc_loss: 0.025253787636756897, tv_loss: 0.0339590460062027\n",
      "iteration 2580, dc_loss: 0.02526521496474743, tv_loss: 0.03393172845244408\n",
      "iteration 2581, dc_loss: 0.025273289531469345, tv_loss: 0.03392709046602249\n",
      "iteration 2582, dc_loss: 0.025243373587727547, tv_loss: 0.03394577279686928\n",
      "iteration 2583, dc_loss: 0.02525952272117138, tv_loss: 0.033925190567970276\n",
      "iteration 2584, dc_loss: 0.025252701714634895, tv_loss: 0.03392971307039261\n",
      "iteration 2585, dc_loss: 0.02524653449654579, tv_loss: 0.03393186628818512\n",
      "iteration 2586, dc_loss: 0.02523980289697647, tv_loss: 0.03393154218792915\n",
      "iteration 2587, dc_loss: 0.02523892931640148, tv_loss: 0.03392648696899414\n",
      "iteration 2588, dc_loss: 0.025237342342734337, tv_loss: 0.03392678126692772\n",
      "iteration 2589, dc_loss: 0.02522958815097809, tv_loss: 0.03392770141363144\n",
      "iteration 2590, dc_loss: 0.025225704535841942, tv_loss: 0.033928267657756805\n",
      "iteration 2591, dc_loss: 0.02522129938006401, tv_loss: 0.03393010422587395\n",
      "iteration 2592, dc_loss: 0.025206923484802246, tv_loss: 0.033942777663469315\n",
      "iteration 2593, dc_loss: 0.025215335190296173, tv_loss: 0.03393416479229927\n",
      "iteration 2594, dc_loss: 0.025206640362739563, tv_loss: 0.033940110355615616\n",
      "iteration 2595, dc_loss: 0.02519999071955681, tv_loss: 0.03393542766571045\n",
      "iteration 2596, dc_loss: 0.02520734816789627, tv_loss: 0.03392407298088074\n",
      "iteration 2597, dc_loss: 0.02519400604069233, tv_loss: 0.03392712399363518\n",
      "iteration 2598, dc_loss: 0.025195855647325516, tv_loss: 0.033924397081136703\n",
      "iteration 2599, dc_loss: 0.02518131397664547, tv_loss: 0.03393670544028282\n",
      "iteration 2600, dc_loss: 0.025188889354467392, tv_loss: 0.03392361104488373\n",
      "iteration 2601, dc_loss: 0.02518036589026451, tv_loss: 0.03393924608826637\n",
      "iteration 2602, dc_loss: 0.02518085576593876, tv_loss: 0.03394054248929024\n",
      "iteration 2603, dc_loss: 0.025178730487823486, tv_loss: 0.03393899276852608\n",
      "iteration 2604, dc_loss: 0.02516734041273594, tv_loss: 0.033931296318769455\n",
      "iteration 2605, dc_loss: 0.02516932040452957, tv_loss: 0.0339348129928112\n",
      "iteration 2606, dc_loss: 0.02516903541982174, tv_loss: 0.03393363952636719\n",
      "iteration 2607, dc_loss: 0.025149645283818245, tv_loss: 0.03395594283938408\n",
      "iteration 2608, dc_loss: 0.02517680823802948, tv_loss: 0.03392135724425316\n",
      "iteration 2609, dc_loss: 0.025146743282675743, tv_loss: 0.033946260809898376\n",
      "iteration 2610, dc_loss: 0.025156427174806595, tv_loss: 0.03392430394887924\n",
      "iteration 2611, dc_loss: 0.025148935616016388, tv_loss: 0.03392942249774933\n",
      "iteration 2612, dc_loss: 0.025145666673779488, tv_loss: 0.03393338620662689\n",
      "iteration 2613, dc_loss: 0.02514147199690342, tv_loss: 0.033939942717552185\n",
      "iteration 2614, dc_loss: 0.02514210157096386, tv_loss: 0.033939097076654434\n",
      "iteration 2615, dc_loss: 0.02513159066438675, tv_loss: 0.03393477201461792\n",
      "iteration 2616, dc_loss: 0.025116246193647385, tv_loss: 0.03393452987074852\n",
      "iteration 2617, dc_loss: 0.025110524147748947, tv_loss: 0.03394053131341934\n",
      "iteration 2618, dc_loss: 0.025126708671450615, tv_loss: 0.03391699492931366\n",
      "iteration 2619, dc_loss: 0.025097981095314026, tv_loss: 0.0339399054646492\n",
      "iteration 2620, dc_loss: 0.02511335350573063, tv_loss: 0.03392154723405838\n",
      "iteration 2621, dc_loss: 0.025102250277996063, tv_loss: 0.03392850607633591\n",
      "iteration 2622, dc_loss: 0.025080256164073944, tv_loss: 0.03394266217947006\n",
      "iteration 2623, dc_loss: 0.025093084201216698, tv_loss: 0.03392874822020531\n",
      "iteration 2624, dc_loss: 0.025100674480199814, tv_loss: 0.03391434997320175\n",
      "iteration 2625, dc_loss: 0.025078514590859413, tv_loss: 0.03393518924713135\n",
      "iteration 2626, dc_loss: 0.025077514350414276, tv_loss: 0.03393169492483139\n",
      "iteration 2627, dc_loss: 0.02508087083697319, tv_loss: 0.03392844647169113\n",
      "iteration 2628, dc_loss: 0.025070244446396828, tv_loss: 0.033945340663194656\n",
      "iteration 2629, dc_loss: 0.025059910491108894, tv_loss: 0.03394568711519241\n",
      "iteration 2630, dc_loss: 0.025067362934350967, tv_loss: 0.033929165452718735\n",
      "iteration 2631, dc_loss: 0.02506575547158718, tv_loss: 0.03391919657588005\n",
      "iteration 2632, dc_loss: 0.025055967271327972, tv_loss: 0.03393281251192093\n",
      "iteration 2633, dc_loss: 0.025051360949873924, tv_loss: 0.033930838108062744\n",
      "iteration 2634, dc_loss: 0.025044770911335945, tv_loss: 0.03394373506307602\n",
      "iteration 2635, dc_loss: 0.025060685351490974, tv_loss: 0.033932097256183624\n",
      "iteration 2636, dc_loss: 0.02503245882689953, tv_loss: 0.033954888582229614\n",
      "iteration 2637, dc_loss: 0.025052400305867195, tv_loss: 0.03391973301768303\n",
      "iteration 2638, dc_loss: 0.025042416527867317, tv_loss: 0.03392866998910904\n",
      "iteration 2639, dc_loss: 0.02505333535373211, tv_loss: 0.03392437472939491\n",
      "iteration 2640, dc_loss: 0.02502880059182644, tv_loss: 0.03395203873515129\n",
      "iteration 2641, dc_loss: 0.02503957599401474, tv_loss: 0.033945489674806595\n",
      "iteration 2642, dc_loss: 0.025036334991455078, tv_loss: 0.03393145650625229\n",
      "iteration 2643, dc_loss: 0.025030875578522682, tv_loss: 0.03392384573817253\n",
      "iteration 2644, dc_loss: 0.025007734075188637, tv_loss: 0.033935435116291046\n",
      "iteration 2645, dc_loss: 0.025014063343405724, tv_loss: 0.03393228352069855\n",
      "iteration 2646, dc_loss: 0.02499617263674736, tv_loss: 0.03394266590476036\n",
      "iteration 2647, dc_loss: 0.025008168071508408, tv_loss: 0.03392169997096062\n",
      "iteration 2648, dc_loss: 0.024980813264846802, tv_loss: 0.033936094492673874\n",
      "iteration 2649, dc_loss: 0.024996323511004448, tv_loss: 0.03391505032777786\n",
      "iteration 2650, dc_loss: 0.024985868483781815, tv_loss: 0.033936113119125366\n",
      "iteration 2651, dc_loss: 0.024974413216114044, tv_loss: 0.03395741805434227\n",
      "iteration 2652, dc_loss: 0.02499201148748398, tv_loss: 0.033924348652362823\n",
      "iteration 2653, dc_loss: 0.024960488080978394, tv_loss: 0.033938173204660416\n",
      "iteration 2654, dc_loss: 0.024984028190374374, tv_loss: 0.033911872655153275\n",
      "iteration 2655, dc_loss: 0.02495861053466797, tv_loss: 0.033938050270080566\n",
      "iteration 2656, dc_loss: 0.024972666054964066, tv_loss: 0.03392370417714119\n",
      "iteration 2657, dc_loss: 0.024963146075606346, tv_loss: 0.033930957317352295\n",
      "iteration 2658, dc_loss: 0.024946942925453186, tv_loss: 0.033950064331293106\n",
      "iteration 2659, dc_loss: 0.02495737001299858, tv_loss: 0.03392805904150009\n",
      "iteration 2660, dc_loss: 0.02494785748422146, tv_loss: 0.033928271383047104\n",
      "iteration 2661, dc_loss: 0.0249403715133667, tv_loss: 0.03392137587070465\n",
      "iteration 2662, dc_loss: 0.024947814643383026, tv_loss: 0.03391488641500473\n",
      "iteration 2663, dc_loss: 0.024922553449869156, tv_loss: 0.033936597406864166\n",
      "iteration 2664, dc_loss: 0.024950820952653885, tv_loss: 0.033918749541044235\n",
      "iteration 2665, dc_loss: 0.02491680160164833, tv_loss: 0.033959656953811646\n",
      "iteration 2666, dc_loss: 0.024927055463194847, tv_loss: 0.03393106535077095\n",
      "iteration 2667, dc_loss: 0.02491649053990841, tv_loss: 0.033927008509635925\n",
      "iteration 2668, dc_loss: 0.024918198585510254, tv_loss: 0.033929117023944855\n",
      "iteration 2669, dc_loss: 0.024905329570174217, tv_loss: 0.033946529030799866\n",
      "iteration 2670, dc_loss: 0.024913934990763664, tv_loss: 0.033928945660591125\n",
      "iteration 2671, dc_loss: 0.024917474016547203, tv_loss: 0.03392168506979942\n",
      "iteration 2672, dc_loss: 0.024896269664168358, tv_loss: 0.03393018990755081\n",
      "iteration 2673, dc_loss: 0.02489943616092205, tv_loss: 0.03392055258154869\n",
      "iteration 2674, dc_loss: 0.024909362196922302, tv_loss: 0.033911868929862976\n",
      "iteration 2675, dc_loss: 0.024884480983018875, tv_loss: 0.033942773938179016\n",
      "iteration 2676, dc_loss: 0.024909313768148422, tv_loss: 0.033923011273145676\n",
      "iteration 2677, dc_loss: 0.02487264573574066, tv_loss: 0.0339687243103981\n",
      "iteration 2678, dc_loss: 0.024912917986512184, tv_loss: 0.03390851616859436\n",
      "iteration 2679, dc_loss: 0.02487891912460327, tv_loss: 0.03393406420946121\n",
      "iteration 2680, dc_loss: 0.02489473484456539, tv_loss: 0.033925872296094894\n",
      "iteration 2681, dc_loss: 0.02488439530134201, tv_loss: 0.033946581184864044\n",
      "iteration 2682, dc_loss: 0.024889666587114334, tv_loss: 0.03392593190073967\n",
      "iteration 2683, dc_loss: 0.02487710863351822, tv_loss: 0.03392219915986061\n",
      "iteration 2684, dc_loss: 0.024848874658346176, tv_loss: 0.03394125774502754\n",
      "iteration 2685, dc_loss: 0.024856703355908394, tv_loss: 0.03393349424004555\n",
      "iteration 2686, dc_loss: 0.024857496842741966, tv_loss: 0.03393174335360527\n",
      "iteration 2687, dc_loss: 0.024823665618896484, tv_loss: 0.03395324945449829\n",
      "iteration 2688, dc_loss: 0.024855367839336395, tv_loss: 0.03391111642122269\n",
      "iteration 2689, dc_loss: 0.02482895739376545, tv_loss: 0.03393438830971718\n",
      "iteration 2690, dc_loss: 0.024828819558024406, tv_loss: 0.03393244370818138\n",
      "iteration 2691, dc_loss: 0.024834509938955307, tv_loss: 0.033929869532585144\n",
      "iteration 2692, dc_loss: 0.024817058816552162, tv_loss: 0.03393658623099327\n",
      "iteration 2693, dc_loss: 0.02483300492167473, tv_loss: 0.03392092511057854\n",
      "iteration 2694, dc_loss: 0.02480417862534523, tv_loss: 0.033940184861421585\n",
      "iteration 2695, dc_loss: 0.02483217418193817, tv_loss: 0.03390545770525932\n",
      "iteration 2696, dc_loss: 0.024799851700663567, tv_loss: 0.033934637904167175\n",
      "iteration 2697, dc_loss: 0.024821169674396515, tv_loss: 0.033911362290382385\n",
      "iteration 2698, dc_loss: 0.02480241283774376, tv_loss: 0.03392539545893669\n",
      "iteration 2699, dc_loss: 0.024794483557343483, tv_loss: 0.03393452614545822\n",
      "iteration 2700, dc_loss: 0.024798698723316193, tv_loss: 0.033923227339982986\n",
      "iteration 2701, dc_loss: 0.024794336408376694, tv_loss: 0.03392118960618973\n",
      "iteration 2702, dc_loss: 0.024781784042716026, tv_loss: 0.03392722085118294\n",
      "iteration 2703, dc_loss: 0.024799762293696404, tv_loss: 0.033902693539857864\n",
      "iteration 2704, dc_loss: 0.024762390181422234, tv_loss: 0.03393546864390373\n",
      "iteration 2705, dc_loss: 0.024780884385108948, tv_loss: 0.03391733393073082\n",
      "iteration 2706, dc_loss: 0.02477346919476986, tv_loss: 0.03391499072313309\n",
      "iteration 2707, dc_loss: 0.024764101952314377, tv_loss: 0.033922914415597916\n",
      "iteration 2708, dc_loss: 0.024766409769654274, tv_loss: 0.033934690058231354\n",
      "iteration 2709, dc_loss: 0.024764232337474823, tv_loss: 0.033937156200408936\n",
      "iteration 2710, dc_loss: 0.024753868579864502, tv_loss: 0.03393161669373512\n",
      "iteration 2711, dc_loss: 0.024738552048802376, tv_loss: 0.033934418112039566\n",
      "iteration 2712, dc_loss: 0.024757785722613335, tv_loss: 0.03392479941248894\n",
      "iteration 2713, dc_loss: 0.024741068482398987, tv_loss: 0.03395732119679451\n",
      "iteration 2714, dc_loss: 0.024734223261475563, tv_loss: 0.0339331179857254\n",
      "iteration 2715, dc_loss: 0.024748440831899643, tv_loss: 0.033920057117938995\n",
      "iteration 2716, dc_loss: 0.024724382907152176, tv_loss: 0.033942755311727524\n",
      "iteration 2717, dc_loss: 0.024740802124142647, tv_loss: 0.03393447399139404\n",
      "iteration 2718, dc_loss: 0.024718983098864555, tv_loss: 0.03393701836466789\n",
      "iteration 2719, dc_loss: 0.024724680930376053, tv_loss: 0.03392456844449043\n",
      "iteration 2720, dc_loss: 0.024725183844566345, tv_loss: 0.03394417092204094\n",
      "iteration 2721, dc_loss: 0.02473430708050728, tv_loss: 0.03392791002988815\n",
      "iteration 2722, dc_loss: 0.024720607325434685, tv_loss: 0.03393583372235298\n",
      "iteration 2723, dc_loss: 0.024727838113904, tv_loss: 0.03392405062913895\n",
      "iteration 2724, dc_loss: 0.024714890867471695, tv_loss: 0.033955201506614685\n",
      "iteration 2725, dc_loss: 0.024752382189035416, tv_loss: 0.03391214832663536\n",
      "iteration 2726, dc_loss: 0.02471817657351494, tv_loss: 0.03394222632050514\n",
      "iteration 2727, dc_loss: 0.02473970130085945, tv_loss: 0.033903446048498154\n",
      "iteration 2728, dc_loss: 0.02470388077199459, tv_loss: 0.03394628316164017\n",
      "iteration 2729, dc_loss: 0.02472812682390213, tv_loss: 0.0339135117828846\n",
      "iteration 2730, dc_loss: 0.024684973061084747, tv_loss: 0.033955950289964676\n",
      "iteration 2731, dc_loss: 0.024705788120627403, tv_loss: 0.03391822800040245\n",
      "iteration 2732, dc_loss: 0.024680940434336662, tv_loss: 0.03392152860760689\n",
      "iteration 2733, dc_loss: 0.02468564547598362, tv_loss: 0.03390521556138992\n",
      "iteration 2734, dc_loss: 0.024661177769303322, tv_loss: 0.03392229601740837\n",
      "iteration 2735, dc_loss: 0.02466687001287937, tv_loss: 0.03390946984291077\n",
      "iteration 2736, dc_loss: 0.024650542065501213, tv_loss: 0.03392712026834488\n",
      "iteration 2737, dc_loss: 0.02465873211622238, tv_loss: 0.03391476720571518\n",
      "iteration 2738, dc_loss: 0.024661023169755936, tv_loss: 0.03390836343169212\n",
      "iteration 2739, dc_loss: 0.02464483119547367, tv_loss: 0.03392896056175232\n",
      "iteration 2740, dc_loss: 0.02466582879424095, tv_loss: 0.03392941132187843\n",
      "iteration 2741, dc_loss: 0.02463875524699688, tv_loss: 0.03396357595920563\n",
      "iteration 2742, dc_loss: 0.024657653644680977, tv_loss: 0.03390393406152725\n",
      "iteration 2743, dc_loss: 0.02462315931916237, tv_loss: 0.03394083306193352\n",
      "iteration 2744, dc_loss: 0.024645933881402016, tv_loss: 0.03393649682402611\n",
      "iteration 2745, dc_loss: 0.0246224794536829, tv_loss: 0.03395108878612518\n",
      "iteration 2746, dc_loss: 0.02461496740579605, tv_loss: 0.03393106907606125\n",
      "iteration 2747, dc_loss: 0.024624967947602272, tv_loss: 0.033932045102119446\n",
      "iteration 2748, dc_loss: 0.024608442559838295, tv_loss: 0.033961303532123566\n",
      "iteration 2749, dc_loss: 0.024617867544293404, tv_loss: 0.033921368420124054\n",
      "iteration 2750, dc_loss: 0.02459910325706005, tv_loss: 0.033947721123695374\n",
      "iteration 2751, dc_loss: 0.02460256777703762, tv_loss: 0.03394704684615135\n",
      "iteration 2752, dc_loss: 0.024603765457868576, tv_loss: 0.033935993909835815\n",
      "iteration 2753, dc_loss: 0.024608971551060677, tv_loss: 0.03391655907034874\n",
      "iteration 2754, dc_loss: 0.02459115907549858, tv_loss: 0.03393689915537834\n",
      "iteration 2755, dc_loss: 0.024601943790912628, tv_loss: 0.03393246978521347\n",
      "iteration 2756, dc_loss: 0.024589188396930695, tv_loss: 0.03392253816127777\n",
      "iteration 2757, dc_loss: 0.024593571200966835, tv_loss: 0.03391357511281967\n",
      "iteration 2758, dc_loss: 0.024573685601353645, tv_loss: 0.033933449536561966\n",
      "iteration 2759, dc_loss: 0.024585895240306854, tv_loss: 0.03392534703016281\n",
      "iteration 2760, dc_loss: 0.024566268548369408, tv_loss: 0.03393475338816643\n",
      "iteration 2761, dc_loss: 0.024578245356678963, tv_loss: 0.03391791507601738\n",
      "iteration 2762, dc_loss: 0.024557696655392647, tv_loss: 0.0339253768324852\n",
      "iteration 2763, dc_loss: 0.024570032954216003, tv_loss: 0.03390857204794884\n",
      "iteration 2764, dc_loss: 0.024551553651690483, tv_loss: 0.033917274326086044\n",
      "iteration 2765, dc_loss: 0.024557679891586304, tv_loss: 0.03390520066022873\n",
      "iteration 2766, dc_loss: 0.0245415847748518, tv_loss: 0.03392515704035759\n",
      "iteration 2767, dc_loss: 0.024560365825891495, tv_loss: 0.03390807658433914\n",
      "iteration 2768, dc_loss: 0.024536868557333946, tv_loss: 0.03393539413809776\n",
      "iteration 2769, dc_loss: 0.024540606886148453, tv_loss: 0.033942900598049164\n",
      "iteration 2770, dc_loss: 0.02452775090932846, tv_loss: 0.03393276035785675\n",
      "iteration 2771, dc_loss: 0.024552274495363235, tv_loss: 0.033907100558280945\n",
      "iteration 2772, dc_loss: 0.02451319992542267, tv_loss: 0.033941224217414856\n",
      "iteration 2773, dc_loss: 0.02455548569560051, tv_loss: 0.0338987372815609\n",
      "iteration 2774, dc_loss: 0.0245148204267025, tv_loss: 0.033941514790058136\n",
      "iteration 2775, dc_loss: 0.024546118453145027, tv_loss: 0.033908236771821976\n",
      "iteration 2776, dc_loss: 0.024532416835427284, tv_loss: 0.033920321613550186\n",
      "iteration 2777, dc_loss: 0.02453620173037052, tv_loss: 0.03391238674521446\n",
      "iteration 2778, dc_loss: 0.02452387660741806, tv_loss: 0.03393193334341049\n",
      "iteration 2779, dc_loss: 0.024562252685427666, tv_loss: 0.03388582542538643\n",
      "iteration 2780, dc_loss: 0.024518851190805435, tv_loss: 0.0339370034635067\n",
      "iteration 2781, dc_loss: 0.024556439369916916, tv_loss: 0.03389713168144226\n",
      "iteration 2782, dc_loss: 0.024519996717572212, tv_loss: 0.033940501511096954\n",
      "iteration 2783, dc_loss: 0.024531474336981773, tv_loss: 0.03391648828983307\n",
      "iteration 2784, dc_loss: 0.02448723278939724, tv_loss: 0.03394416719675064\n",
      "iteration 2785, dc_loss: 0.024523386731743813, tv_loss: 0.03388703614473343\n",
      "iteration 2786, dc_loss: 0.024463150650262833, tv_loss: 0.033936887979507446\n",
      "iteration 2787, dc_loss: 0.024479670450091362, tv_loss: 0.03391928970813751\n",
      "iteration 2788, dc_loss: 0.02447178028523922, tv_loss: 0.03392656147480011\n",
      "iteration 2789, dc_loss: 0.024460013955831528, tv_loss: 0.03392871841788292\n",
      "iteration 2790, dc_loss: 0.024488529190421104, tv_loss: 0.03389989957213402\n",
      "iteration 2791, dc_loss: 0.024449585005640984, tv_loss: 0.03393444046378136\n",
      "iteration 2792, dc_loss: 0.024480115622282028, tv_loss: 0.03390448912978172\n",
      "iteration 2793, dc_loss: 0.0244548711925745, tv_loss: 0.03392826393246651\n",
      "iteration 2794, dc_loss: 0.024489745497703552, tv_loss: 0.03388446569442749\n",
      "iteration 2795, dc_loss: 0.024432234466075897, tv_loss: 0.03394191339612007\n",
      "iteration 2796, dc_loss: 0.024464517831802368, tv_loss: 0.033901240676641464\n",
      "iteration 2797, dc_loss: 0.024438904598355293, tv_loss: 0.033927544951438904\n",
      "iteration 2798, dc_loss: 0.024425184354186058, tv_loss: 0.033935677260160446\n",
      "iteration 2799, dc_loss: 0.02443716488778591, tv_loss: 0.033928778022527695\n",
      "iteration 2800, dc_loss: 0.0244267787784338, tv_loss: 0.03392036259174347\n",
      "iteration 2801, dc_loss: 0.02443082630634308, tv_loss: 0.03390895947813988\n",
      "iteration 2802, dc_loss: 0.024421263486146927, tv_loss: 0.03391079977154732\n",
      "iteration 2803, dc_loss: 0.02441442757844925, tv_loss: 0.03391266241669655\n",
      "iteration 2804, dc_loss: 0.024413343518972397, tv_loss: 0.03391081094741821\n",
      "iteration 2805, dc_loss: 0.02441113069653511, tv_loss: 0.03390514850616455\n",
      "iteration 2806, dc_loss: 0.024409718811511993, tv_loss: 0.03390603885054588\n",
      "iteration 2807, dc_loss: 0.02440432645380497, tv_loss: 0.03390829637646675\n",
      "iteration 2808, dc_loss: 0.024405140429735184, tv_loss: 0.03390497714281082\n",
      "iteration 2809, dc_loss: 0.024408182129263878, tv_loss: 0.03390171006321907\n",
      "iteration 2810, dc_loss: 0.02439900115132332, tv_loss: 0.03390520438551903\n",
      "iteration 2811, dc_loss: 0.02438599430024624, tv_loss: 0.033910442143678665\n",
      "iteration 2812, dc_loss: 0.024392979219555855, tv_loss: 0.0339014008641243\n",
      "iteration 2813, dc_loss: 0.02439028024673462, tv_loss: 0.03389883041381836\n",
      "iteration 2814, dc_loss: 0.02438483014702797, tv_loss: 0.03390243276953697\n",
      "iteration 2815, dc_loss: 0.02438688464462757, tv_loss: 0.03390214219689369\n",
      "iteration 2816, dc_loss: 0.024387871846556664, tv_loss: 0.033905480057001114\n",
      "iteration 2817, dc_loss: 0.02437383309006691, tv_loss: 0.03391636908054352\n",
      "iteration 2818, dc_loss: 0.024369677528738976, tv_loss: 0.03391269966959953\n",
      "iteration 2819, dc_loss: 0.02437187172472477, tv_loss: 0.0338987372815609\n",
      "iteration 2820, dc_loss: 0.024371927604079247, tv_loss: 0.03389691188931465\n",
      "iteration 2821, dc_loss: 0.024362793192267418, tv_loss: 0.033901579678058624\n",
      "iteration 2822, dc_loss: 0.02436606027185917, tv_loss: 0.03390377387404442\n",
      "iteration 2823, dc_loss: 0.024371955543756485, tv_loss: 0.03390004113316536\n",
      "iteration 2824, dc_loss: 0.02435430884361267, tv_loss: 0.03391782566905022\n",
      "iteration 2825, dc_loss: 0.024351296946406364, tv_loss: 0.03391052037477493\n",
      "iteration 2826, dc_loss: 0.02435203827917576, tv_loss: 0.03390751779079437\n",
      "iteration 2827, dc_loss: 0.024344105273485184, tv_loss: 0.033907201141119\n",
      "iteration 2828, dc_loss: 0.02434379979968071, tv_loss: 0.03390693664550781\n",
      "iteration 2829, dc_loss: 0.02435527741909027, tv_loss: 0.03389007970690727\n",
      "iteration 2830, dc_loss: 0.024339713156223297, tv_loss: 0.03390080854296684\n",
      "iteration 2831, dc_loss: 0.024328531697392464, tv_loss: 0.033909354358911514\n",
      "iteration 2832, dc_loss: 0.024345511570572853, tv_loss: 0.033887773752212524\n",
      "iteration 2833, dc_loss: 0.024334480985999107, tv_loss: 0.03389669209718704\n",
      "iteration 2834, dc_loss: 0.024322740733623505, tv_loss: 0.033903393894433975\n",
      "iteration 2835, dc_loss: 0.0243370421230793, tv_loss: 0.03388577327132225\n",
      "iteration 2836, dc_loss: 0.024321923032402992, tv_loss: 0.03390522673726082\n",
      "iteration 2837, dc_loss: 0.024314166978001595, tv_loss: 0.033910658210515976\n",
      "iteration 2838, dc_loss: 0.024325644597411156, tv_loss: 0.03390701860189438\n",
      "iteration 2839, dc_loss: 0.024316133931279182, tv_loss: 0.033913370221853256\n",
      "iteration 2840, dc_loss: 0.024314887821674347, tv_loss: 0.03390376642346382\n",
      "iteration 2841, dc_loss: 0.024312149733304977, tv_loss: 0.03390369936823845\n",
      "iteration 2842, dc_loss: 0.024304216727614403, tv_loss: 0.03390156105160713\n",
      "iteration 2843, dc_loss: 0.024305099621415138, tv_loss: 0.03390061482787132\n",
      "iteration 2844, dc_loss: 0.02430087700486183, tv_loss: 0.03390422835946083\n",
      "iteration 2845, dc_loss: 0.024300679564476013, tv_loss: 0.0339023657143116\n",
      "iteration 2846, dc_loss: 0.024297697469592094, tv_loss: 0.03390369191765785\n",
      "iteration 2847, dc_loss: 0.024294255301356316, tv_loss: 0.03390644118189812\n",
      "iteration 2848, dc_loss: 0.024287482723593712, tv_loss: 0.03391003981232643\n",
      "iteration 2849, dc_loss: 0.024294594302773476, tv_loss: 0.03389719873666763\n",
      "iteration 2850, dc_loss: 0.024286337196826935, tv_loss: 0.03390209376811981\n",
      "iteration 2851, dc_loss: 0.024279417470097542, tv_loss: 0.033906545490026474\n",
      "iteration 2852, dc_loss: 0.024286095052957535, tv_loss: 0.03389313071966171\n",
      "iteration 2853, dc_loss: 0.02427738718688488, tv_loss: 0.033897433429956436\n",
      "iteration 2854, dc_loss: 0.024269474670290947, tv_loss: 0.03390621766448021\n",
      "iteration 2855, dc_loss: 0.024277113378047943, tv_loss: 0.033891353756189346\n",
      "iteration 2856, dc_loss: 0.024270862340927124, tv_loss: 0.03390612453222275\n",
      "iteration 2857, dc_loss: 0.024261729791760445, tv_loss: 0.03391192480921745\n",
      "iteration 2858, dc_loss: 0.024271788075566292, tv_loss: 0.03390617296099663\n",
      "iteration 2859, dc_loss: 0.024263251572847366, tv_loss: 0.03389967232942581\n",
      "iteration 2860, dc_loss: 0.024252433329820633, tv_loss: 0.03390481323003769\n",
      "iteration 2861, dc_loss: 0.02425595000386238, tv_loss: 0.033907610923051834\n",
      "iteration 2862, dc_loss: 0.024255219846963882, tv_loss: 0.03390752151608467\n",
      "iteration 2863, dc_loss: 0.02425427734851837, tv_loss: 0.03390640765428543\n",
      "iteration 2864, dc_loss: 0.024246791377663612, tv_loss: 0.03390005975961685\n",
      "iteration 2865, dc_loss: 0.024245746433734894, tv_loss: 0.033908095210790634\n",
      "iteration 2866, dc_loss: 0.02424362488090992, tv_loss: 0.03390660881996155\n",
      "iteration 2867, dc_loss: 0.024234533309936523, tv_loss: 0.03391613811254501\n",
      "iteration 2868, dc_loss: 0.0242376159876585, tv_loss: 0.033899467438459396\n",
      "iteration 2869, dc_loss: 0.024240784347057343, tv_loss: 0.03389916568994522\n",
      "iteration 2870, dc_loss: 0.02422875352203846, tv_loss: 0.0339057594537735\n",
      "iteration 2871, dc_loss: 0.02423080988228321, tv_loss: 0.03390234708786011\n",
      "iteration 2872, dc_loss: 0.024230318143963814, tv_loss: 0.033897534012794495\n",
      "iteration 2873, dc_loss: 0.024220474064350128, tv_loss: 0.033901579678058624\n",
      "iteration 2874, dc_loss: 0.024224117398262024, tv_loss: 0.033892907202243805\n",
      "iteration 2875, dc_loss: 0.024218173697590828, tv_loss: 0.03390747308731079\n",
      "iteration 2876, dc_loss: 0.024212172254920006, tv_loss: 0.03392034024000168\n",
      "iteration 2877, dc_loss: 0.024216562509536743, tv_loss: 0.03390628099441528\n",
      "iteration 2878, dc_loss: 0.024218926206231117, tv_loss: 0.03388916328549385\n",
      "iteration 2879, dc_loss: 0.0241991113871336, tv_loss: 0.03390691056847572\n",
      "iteration 2880, dc_loss: 0.024199455976486206, tv_loss: 0.03389979153871536\n",
      "iteration 2881, dc_loss: 0.02420983649790287, tv_loss: 0.03388988599181175\n",
      "iteration 2882, dc_loss: 0.02420174703001976, tv_loss: 0.033895932137966156\n",
      "iteration 2883, dc_loss: 0.024197328835725784, tv_loss: 0.033908139914274216\n",
      "iteration 2884, dc_loss: 0.024193745106458664, tv_loss: 0.03391999006271362\n",
      "iteration 2885, dc_loss: 0.024194883182644844, tv_loss: 0.03390663117170334\n",
      "iteration 2886, dc_loss: 0.02418520115315914, tv_loss: 0.033900484442710876\n",
      "iteration 2887, dc_loss: 0.024191459640860558, tv_loss: 0.03389666602015495\n",
      "iteration 2888, dc_loss: 0.024182185530662537, tv_loss: 0.03391950950026512\n",
      "iteration 2889, dc_loss: 0.02417709492146969, tv_loss: 0.0339224711060524\n",
      "iteration 2890, dc_loss: 0.02418495900928974, tv_loss: 0.03389229252934456\n",
      "iteration 2891, dc_loss: 0.024173468351364136, tv_loss: 0.03390992060303688\n",
      "iteration 2892, dc_loss: 0.024165945127606392, tv_loss: 0.03392519801855087\n",
      "iteration 2893, dc_loss: 0.024173596873879433, tv_loss: 0.03391052782535553\n",
      "iteration 2894, dc_loss: 0.024168819189071655, tv_loss: 0.03390124812722206\n",
      "iteration 2895, dc_loss: 0.02417086809873581, tv_loss: 0.033910080790519714\n",
      "iteration 2896, dc_loss: 0.02416262775659561, tv_loss: 0.03391256555914879\n",
      "iteration 2897, dc_loss: 0.024155011400580406, tv_loss: 0.03390764445066452\n",
      "iteration 2898, dc_loss: 0.02416260913014412, tv_loss: 0.033891912549734116\n",
      "iteration 2899, dc_loss: 0.024155063554644585, tv_loss: 0.0339062437415123\n",
      "iteration 2900, dc_loss: 0.024142980575561523, tv_loss: 0.0339260958135128\n",
      "iteration 2901, dc_loss: 0.024154549464583397, tv_loss: 0.0339014045894146\n",
      "iteration 2902, dc_loss: 0.024151407182216644, tv_loss: 0.033893242478370667\n",
      "iteration 2903, dc_loss: 0.024138685315847397, tv_loss: 0.03390895947813988\n",
      "iteration 2904, dc_loss: 0.024145258590579033, tv_loss: 0.033908192068338394\n",
      "iteration 2905, dc_loss: 0.02414257824420929, tv_loss: 0.03390885144472122\n",
      "iteration 2906, dc_loss: 0.024135800078511238, tv_loss: 0.03390299156308174\n",
      "iteration 2907, dc_loss: 0.024129532277584076, tv_loss: 0.033899012953042984\n",
      "iteration 2908, dc_loss: 0.024130886420607567, tv_loss: 0.033904969692230225\n",
      "iteration 2909, dc_loss: 0.024131694808602333, tv_loss: 0.03390999883413315\n",
      "iteration 2910, dc_loss: 0.024119384586811066, tv_loss: 0.03391402214765549\n",
      "iteration 2911, dc_loss: 0.02412542700767517, tv_loss: 0.03389344736933708\n",
      "iteration 2912, dc_loss: 0.024124475196003914, tv_loss: 0.03389480337500572\n",
      "iteration 2913, dc_loss: 0.024119123816490173, tv_loss: 0.03390250727534294\n",
      "iteration 2914, dc_loss: 0.024111535400152206, tv_loss: 0.03390294313430786\n",
      "iteration 2915, dc_loss: 0.02411104366183281, tv_loss: 0.033904120326042175\n",
      "iteration 2916, dc_loss: 0.0241137333214283, tv_loss: 0.03388959541916847\n",
      "iteration 2917, dc_loss: 0.024103008210659027, tv_loss: 0.033898141235113144\n",
      "iteration 2918, dc_loss: 0.024105241522192955, tv_loss: 0.03389299660921097\n",
      "iteration 2919, dc_loss: 0.02410753257572651, tv_loss: 0.03389173001050949\n",
      "iteration 2920, dc_loss: 0.02409336529672146, tv_loss: 0.03390797600150108\n",
      "iteration 2921, dc_loss: 0.02409283071756363, tv_loss: 0.03390182927250862\n",
      "iteration 2922, dc_loss: 0.02409977838397026, tv_loss: 0.03389487788081169\n",
      "iteration 2923, dc_loss: 0.02408880554139614, tv_loss: 0.0339076891541481\n",
      "iteration 2924, dc_loss: 0.024090884253382683, tv_loss: 0.033901311457157135\n",
      "iteration 2925, dc_loss: 0.024087147787213326, tv_loss: 0.03389809653162956\n",
      "iteration 2926, dc_loss: 0.02407967671751976, tv_loss: 0.03390289098024368\n",
      "iteration 2927, dc_loss: 0.02408301644027233, tv_loss: 0.033899810165166855\n",
      "iteration 2928, dc_loss: 0.024075057357549667, tv_loss: 0.03391777351498604\n",
      "iteration 2929, dc_loss: 0.02407342754304409, tv_loss: 0.03391285985708237\n",
      "iteration 2930, dc_loss: 0.02407398261129856, tv_loss: 0.033896930515766144\n",
      "iteration 2931, dc_loss: 0.024076346307992935, tv_loss: 0.03389383479952812\n",
      "iteration 2932, dc_loss: 0.02406413108110428, tv_loss: 0.03390933573246002\n",
      "iteration 2933, dc_loss: 0.02405988983809948, tv_loss: 0.03391445800662041\n",
      "iteration 2934, dc_loss: 0.024074245244264603, tv_loss: 0.03389183431863785\n",
      "iteration 2935, dc_loss: 0.0240572951734066, tv_loss: 0.03389615938067436\n",
      "iteration 2936, dc_loss: 0.02405238337814808, tv_loss: 0.033900726586580276\n",
      "iteration 2937, dc_loss: 0.02406247705221176, tv_loss: 0.03388926014304161\n",
      "iteration 2938, dc_loss: 0.024048535153269768, tv_loss: 0.03390081971883774\n",
      "iteration 2939, dc_loss: 0.024045860394835472, tv_loss: 0.03389783203601837\n",
      "iteration 2940, dc_loss: 0.024043584242463112, tv_loss: 0.0338977575302124\n",
      "iteration 2941, dc_loss: 0.024046285077929497, tv_loss: 0.03389374539256096\n",
      "iteration 2942, dc_loss: 0.02404433861374855, tv_loss: 0.03389917314052582\n",
      "iteration 2943, dc_loss: 0.024040628224611282, tv_loss: 0.033903028815984726\n",
      "iteration 2944, dc_loss: 0.024033404886722565, tv_loss: 0.03390493988990784\n",
      "iteration 2945, dc_loss: 0.024041645228862762, tv_loss: 0.033886805176734924\n",
      "iteration 2946, dc_loss: 0.02402365766465664, tv_loss: 0.03390040993690491\n",
      "iteration 2947, dc_loss: 0.0240285936743021, tv_loss: 0.033892955631017685\n",
      "iteration 2948, dc_loss: 0.024032339453697205, tv_loss: 0.033884864300489426\n",
      "iteration 2949, dc_loss: 0.024026334285736084, tv_loss: 0.03389057517051697\n",
      "iteration 2950, dc_loss: 0.0240198764950037, tv_loss: 0.03390752524137497\n",
      "iteration 2951, dc_loss: 0.024022536352276802, tv_loss: 0.03391295671463013\n",
      "iteration 2952, dc_loss: 0.024012336507439613, tv_loss: 0.03390035033226013\n",
      "iteration 2953, dc_loss: 0.024011291563510895, tv_loss: 0.03389469161629677\n",
      "iteration 2954, dc_loss: 0.024010827764868736, tv_loss: 0.03390613570809364\n",
      "iteration 2955, dc_loss: 0.0240032821893692, tv_loss: 0.03391794487833977\n",
      "iteration 2956, dc_loss: 0.02400621771812439, tv_loss: 0.03390263393521309\n",
      "iteration 2957, dc_loss: 0.024014780297875404, tv_loss: 0.03388385847210884\n",
      "iteration 2958, dc_loss: 0.023998867720365524, tv_loss: 0.0339021235704422\n",
      "iteration 2959, dc_loss: 0.023986928164958954, tv_loss: 0.03391044959425926\n",
      "iteration 2960, dc_loss: 0.0240029189735651, tv_loss: 0.03389083966612816\n",
      "iteration 2961, dc_loss: 0.02398955263197422, tv_loss: 0.033898863941431046\n",
      "iteration 2962, dc_loss: 0.02398410253226757, tv_loss: 0.03390713036060333\n",
      "iteration 2963, dc_loss: 0.02399805746972561, tv_loss: 0.03388768061995506\n",
      "iteration 2964, dc_loss: 0.023992450907826424, tv_loss: 0.03389346972107887\n",
      "iteration 2965, dc_loss: 0.023972539231181145, tv_loss: 0.03390580415725708\n",
      "iteration 2966, dc_loss: 0.02398517355322838, tv_loss: 0.03388577327132225\n",
      "iteration 2967, dc_loss: 0.023981375619769096, tv_loss: 0.033893365412950516\n",
      "iteration 2968, dc_loss: 0.023974765092134476, tv_loss: 0.03389338403940201\n",
      "iteration 2969, dc_loss: 0.023970460519194603, tv_loss: 0.033890075981616974\n",
      "iteration 2970, dc_loss: 0.023980820551514626, tv_loss: 0.03388657048344612\n",
      "iteration 2971, dc_loss: 0.023966191336512566, tv_loss: 0.03390805423259735\n",
      "iteration 2972, dc_loss: 0.02396474778652191, tv_loss: 0.03391064703464508\n",
      "iteration 2973, dc_loss: 0.02396477200090885, tv_loss: 0.033899493515491486\n",
      "iteration 2974, dc_loss: 0.02396811917424202, tv_loss: 0.03388514369726181\n",
      "iteration 2975, dc_loss: 0.023960139602422714, tv_loss: 0.03389519453048706\n",
      "iteration 2976, dc_loss: 0.023965518921613693, tv_loss: 0.03390033543109894\n",
      "iteration 2977, dc_loss: 0.023945089429616928, tv_loss: 0.03392769396305084\n",
      "iteration 2978, dc_loss: 0.02396491728723049, tv_loss: 0.03388325870037079\n",
      "iteration 2979, dc_loss: 0.023944422602653503, tv_loss: 0.03390384092926979\n",
      "iteration 2980, dc_loss: 0.023947345092892647, tv_loss: 0.03390560671687126\n",
      "iteration 2981, dc_loss: 0.023947643116116524, tv_loss: 0.033911868929862976\n",
      "iteration 2982, dc_loss: 0.023940758779644966, tv_loss: 0.033897314220666885\n",
      "iteration 2983, dc_loss: 0.023935547098517418, tv_loss: 0.03389780595898628\n",
      "iteration 2984, dc_loss: 0.023939529433846474, tv_loss: 0.03391306474804878\n",
      "iteration 2985, dc_loss: 0.023925891146063805, tv_loss: 0.03392006456851959\n",
      "iteration 2986, dc_loss: 0.023935332894325256, tv_loss: 0.03389546647667885\n",
      "iteration 2987, dc_loss: 0.023923292756080627, tv_loss: 0.033904798328876495\n",
      "iteration 2988, dc_loss: 0.023922579362988472, tv_loss: 0.03391756862401962\n",
      "iteration 2989, dc_loss: 0.023936159908771515, tv_loss: 0.033885519951581955\n",
      "iteration 2990, dc_loss: 0.023904455825686455, tv_loss: 0.03391768038272858\n",
      "iteration 2991, dc_loss: 0.023915240541100502, tv_loss: 0.0339055061340332\n",
      "iteration 2992, dc_loss: 0.023932307958602905, tv_loss: 0.03388434648513794\n",
      "iteration 2993, dc_loss: 0.02390815131366253, tv_loss: 0.03390670195221901\n",
      "iteration 2994, dc_loss: 0.02390691265463829, tv_loss: 0.03389832004904747\n",
      "iteration 2995, dc_loss: 0.023921698331832886, tv_loss: 0.03388466686010361\n",
      "iteration 2996, dc_loss: 0.023898009210824966, tv_loss: 0.03390764817595482\n",
      "iteration 2997, dc_loss: 0.02390497550368309, tv_loss: 0.03389878571033478\n",
      "iteration 2998, dc_loss: 0.023899994790554047, tv_loss: 0.03389474004507065\n",
      "iteration 2999, dc_loss: 0.02389904484152794, tv_loss: 0.03388868644833565\n",
      "iteration 3000, dc_loss: 0.023892581462860107, tv_loss: 0.03389674052596092\n",
      "iteration 3001, dc_loss: 0.023892490193247795, tv_loss: 0.03389623761177063\n",
      "iteration 3002, dc_loss: 0.023896493017673492, tv_loss: 0.033895932137966156\n",
      "iteration 3003, dc_loss: 0.02388245426118374, tv_loss: 0.03390282019972801\n",
      "iteration 3004, dc_loss: 0.023882528766989708, tv_loss: 0.03389567881822586\n",
      "iteration 3005, dc_loss: 0.023885536938905716, tv_loss: 0.03388531133532524\n",
      "iteration 3006, dc_loss: 0.02387598343193531, tv_loss: 0.0338968001306057\n",
      "iteration 3007, dc_loss: 0.023878024891018867, tv_loss: 0.03389151394367218\n",
      "iteration 3008, dc_loss: 0.023878194391727448, tv_loss: 0.03390110284090042\n",
      "iteration 3009, dc_loss: 0.023870034143328667, tv_loss: 0.03390629217028618\n",
      "iteration 3010, dc_loss: 0.023874210193753242, tv_loss: 0.033890318125486374\n",
      "iteration 3011, dc_loss: 0.02386591024696827, tv_loss: 0.03389723598957062\n",
      "iteration 3012, dc_loss: 0.02386646345257759, tv_loss: 0.03389163315296173\n",
      "iteration 3013, dc_loss: 0.023862095549702644, tv_loss: 0.033898357301950455\n",
      "iteration 3014, dc_loss: 0.023858778178691864, tv_loss: 0.03389361873269081\n",
      "iteration 3015, dc_loss: 0.023854296654462814, tv_loss: 0.03389497101306915\n",
      "iteration 3016, dc_loss: 0.02387164533138275, tv_loss: 0.03387855365872383\n",
      "iteration 3017, dc_loss: 0.0238491240888834, tv_loss: 0.03389890491962433\n",
      "iteration 3018, dc_loss: 0.02386135421693325, tv_loss: 0.03388800844550133\n",
      "iteration 3019, dc_loss: 0.023856578394770622, tv_loss: 0.0338854044675827\n",
      "iteration 3020, dc_loss: 0.023863136768341064, tv_loss: 0.033880408853292465\n",
      "iteration 3021, dc_loss: 0.023847194388508797, tv_loss: 0.03390268608927727\n",
      "iteration 3022, dc_loss: 0.023864680901169777, tv_loss: 0.03387109190225601\n",
      "iteration 3023, dc_loss: 0.023831214755773544, tv_loss: 0.03390178456902504\n",
      "iteration 3024, dc_loss: 0.023854363709688187, tv_loss: 0.03388434648513794\n",
      "iteration 3025, dc_loss: 0.02383718453347683, tv_loss: 0.033890336751937866\n",
      "iteration 3026, dc_loss: 0.023822683840990067, tv_loss: 0.033898379653692245\n",
      "iteration 3027, dc_loss: 0.02383006364107132, tv_loss: 0.03390161693096161\n",
      "iteration 3028, dc_loss: 0.02383880503475666, tv_loss: 0.033885739743709564\n",
      "iteration 3029, dc_loss: 0.02381896786391735, tv_loss: 0.033902984112501144\n",
      "iteration 3030, dc_loss: 0.02381197363138199, tv_loss: 0.03390224277973175\n",
      "iteration 3031, dc_loss: 0.023840321227908134, tv_loss: 0.033887095749378204\n",
      "iteration 3032, dc_loss: 0.02381070889532566, tv_loss: 0.03390859439969063\n",
      "iteration 3033, dc_loss: 0.023817602545022964, tv_loss: 0.033891577273607254\n",
      "iteration 3034, dc_loss: 0.023812850937247276, tv_loss: 0.033894751220941544\n",
      "iteration 3035, dc_loss: 0.02381088212132454, tv_loss: 0.033907450735569\n",
      "iteration 3036, dc_loss: 0.023805854842066765, tv_loss: 0.033902429044246674\n",
      "iteration 3037, dc_loss: 0.02380165085196495, tv_loss: 0.03389634191989899\n",
      "iteration 3038, dc_loss: 0.02380233444273472, tv_loss: 0.03389778733253479\n",
      "iteration 3039, dc_loss: 0.023803113028407097, tv_loss: 0.033899303525686264\n",
      "iteration 3040, dc_loss: 0.023801729083061218, tv_loss: 0.033890724182128906\n",
      "iteration 3041, dc_loss: 0.02378867194056511, tv_loss: 0.0338997021317482\n",
      "iteration 3042, dc_loss: 0.023792603984475136, tv_loss: 0.033892832696437836\n",
      "iteration 3043, dc_loss: 0.023792404681444168, tv_loss: 0.033897943794727325\n",
      "iteration 3044, dc_loss: 0.02378794178366661, tv_loss: 0.03389286622405052\n",
      "iteration 3045, dc_loss: 0.023775191977620125, tv_loss: 0.033901285380125046\n",
      "iteration 3046, dc_loss: 0.02378985844552517, tv_loss: 0.03387819603085518\n",
      "iteration 3047, dc_loss: 0.02379157394170761, tv_loss: 0.03388102352619171\n",
      "iteration 3048, dc_loss: 0.023767314851284027, tv_loss: 0.03390217199921608\n",
      "iteration 3049, dc_loss: 0.02377774752676487, tv_loss: 0.0338955894112587\n",
      "iteration 3050, dc_loss: 0.02377045713365078, tv_loss: 0.0338907316327095\n",
      "iteration 3051, dc_loss: 0.02377050369977951, tv_loss: 0.03388690948486328\n",
      "iteration 3052, dc_loss: 0.023763548582792282, tv_loss: 0.03389008343219757\n",
      "iteration 3053, dc_loss: 0.023770589381456375, tv_loss: 0.03388424217700958\n",
      "iteration 3054, dc_loss: 0.023763883858919144, tv_loss: 0.033895764499902725\n",
      "iteration 3055, dc_loss: 0.02375919558107853, tv_loss: 0.03389964625239372\n",
      "iteration 3056, dc_loss: 0.02375464327633381, tv_loss: 0.033895812928676605\n",
      "iteration 3057, dc_loss: 0.02374250628054142, tv_loss: 0.03389731049537659\n",
      "iteration 3058, dc_loss: 0.023763447999954224, tv_loss: 0.03388799726963043\n",
      "iteration 3059, dc_loss: 0.023758213967084885, tv_loss: 0.03388861194252968\n",
      "iteration 3060, dc_loss: 0.02373793162405491, tv_loss: 0.03390306234359741\n",
      "iteration 3061, dc_loss: 0.023745477199554443, tv_loss: 0.03388966619968414\n",
      "iteration 3062, dc_loss: 0.023753060027956963, tv_loss: 0.03388628736138344\n",
      "iteration 3063, dc_loss: 0.023731650784611702, tv_loss: 0.033920370042324066\n",
      "iteration 3064, dc_loss: 0.023743484169244766, tv_loss: 0.03389403969049454\n",
      "iteration 3065, dc_loss: 0.02372860535979271, tv_loss: 0.03390177711844444\n",
      "iteration 3066, dc_loss: 0.023741809651255608, tv_loss: 0.03389648348093033\n",
      "iteration 3067, dc_loss: 0.023734761402010918, tv_loss: 0.03389975056052208\n",
      "iteration 3068, dc_loss: 0.02373022586107254, tv_loss: 0.03389500454068184\n",
      "iteration 3069, dc_loss: 0.02372516505420208, tv_loss: 0.033900439739227295\n",
      "iteration 3070, dc_loss: 0.023739803582429886, tv_loss: 0.033881548792123795\n",
      "iteration 3071, dc_loss: 0.023718245327472687, tv_loss: 0.03391023725271225\n",
      "iteration 3072, dc_loss: 0.023738255724310875, tv_loss: 0.03387756273150444\n",
      "iteration 3073, dc_loss: 0.023711688816547394, tv_loss: 0.03390360251069069\n",
      "iteration 3074, dc_loss: 0.02373684011399746, tv_loss: 0.033878393471241\n",
      "iteration 3075, dc_loss: 0.023720860481262207, tv_loss: 0.0338936410844326\n",
      "iteration 3076, dc_loss: 0.02371765673160553, tv_loss: 0.03388962522149086\n",
      "iteration 3077, dc_loss: 0.0237042848020792, tv_loss: 0.033896129578351974\n",
      "iteration 3078, dc_loss: 0.023721812292933464, tv_loss: 0.03387245535850525\n",
      "iteration 3079, dc_loss: 0.02369806170463562, tv_loss: 0.03388843312859535\n",
      "iteration 3080, dc_loss: 0.023705150932073593, tv_loss: 0.03387787193059921\n",
      "iteration 3081, dc_loss: 0.023701520636677742, tv_loss: 0.03387753665447235\n",
      "iteration 3082, dc_loss: 0.023695625364780426, tv_loss: 0.03388350084424019\n",
      "iteration 3083, dc_loss: 0.023691099137067795, tv_loss: 0.0338822677731514\n",
      "iteration 3084, dc_loss: 0.02368227019906044, tv_loss: 0.03388824313879013\n",
      "iteration 3085, dc_loss: 0.02369585447013378, tv_loss: 0.03387627378106117\n",
      "iteration 3086, dc_loss: 0.023687943816184998, tv_loss: 0.033885546028614044\n",
      "iteration 3087, dc_loss: 0.02368389442563057, tv_loss: 0.03388963267207146\n",
      "iteration 3088, dc_loss: 0.023678697645664215, tv_loss: 0.03390135616064072\n",
      "iteration 3089, dc_loss: 0.023685473948717117, tv_loss: 0.03388029709458351\n",
      "iteration 3090, dc_loss: 0.023675663396716118, tv_loss: 0.03388708084821701\n",
      "iteration 3091, dc_loss: 0.02366425283253193, tv_loss: 0.03389232978224754\n",
      "iteration 3092, dc_loss: 0.02367374859750271, tv_loss: 0.03387565538287163\n",
      "iteration 3093, dc_loss: 0.02367074228823185, tv_loss: 0.0338907428085804\n",
      "iteration 3094, dc_loss: 0.02365971729159355, tv_loss: 0.033909644931554794\n",
      "iteration 3095, dc_loss: 0.023673800751566887, tv_loss: 0.033885758370161057\n",
      "iteration 3096, dc_loss: 0.023657670244574547, tv_loss: 0.03388429060578346\n",
      "iteration 3097, dc_loss: 0.023656105622649193, tv_loss: 0.03389253094792366\n",
      "iteration 3098, dc_loss: 0.023660769686102867, tv_loss: 0.03389163315296173\n",
      "iteration 3099, dc_loss: 0.023646336048841476, tv_loss: 0.033908963203430176\n",
      "iteration 3100, dc_loss: 0.023658843711018562, tv_loss: 0.03388034924864769\n",
      "iteration 3101, dc_loss: 0.023648716509342194, tv_loss: 0.03388972952961922\n",
      "iteration 3102, dc_loss: 0.02365179918706417, tv_loss: 0.03388415649533272\n",
      "iteration 3103, dc_loss: 0.023647764697670937, tv_loss: 0.03388845920562744\n",
      "iteration 3104, dc_loss: 0.02363474667072296, tv_loss: 0.03389579802751541\n",
      "iteration 3105, dc_loss: 0.02364567667245865, tv_loss: 0.033877260982990265\n",
      "iteration 3106, dc_loss: 0.023643100634217262, tv_loss: 0.0338759608566761\n",
      "iteration 3107, dc_loss: 0.023626253008842468, tv_loss: 0.03388965129852295\n",
      "iteration 3108, dc_loss: 0.02364281937479973, tv_loss: 0.033871423453092575\n",
      "iteration 3109, dc_loss: 0.02363504283130169, tv_loss: 0.03387562930583954\n",
      "iteration 3110, dc_loss: 0.02361758053302765, tv_loss: 0.03388776257634163\n",
      "iteration 3111, dc_loss: 0.023633534088730812, tv_loss: 0.03387343883514404\n",
      "iteration 3112, dc_loss: 0.023637795820832253, tv_loss: 0.033873509615659714\n",
      "iteration 3113, dc_loss: 0.023605827242136, tv_loss: 0.03391101956367493\n",
      "iteration 3114, dc_loss: 0.023634687066078186, tv_loss: 0.033884406089782715\n",
      "iteration 3115, dc_loss: 0.02361695095896721, tv_loss: 0.033893883228302\n",
      "iteration 3116, dc_loss: 0.023640519008040428, tv_loss: 0.03386908397078514\n",
      "iteration 3117, dc_loss: 0.023615116253495216, tv_loss: 0.03389649838209152\n",
      "iteration 3118, dc_loss: 0.023644262924790382, tv_loss: 0.03386591374874115\n",
      "iteration 3119, dc_loss: 0.023628318682312965, tv_loss: 0.033887118101119995\n",
      "iteration 3120, dc_loss: 0.023644765838980675, tv_loss: 0.03386101871728897\n",
      "iteration 3121, dc_loss: 0.02361016720533371, tv_loss: 0.033897772431373596\n",
      "iteration 3122, dc_loss: 0.023644540458917618, tv_loss: 0.033851493149995804\n",
      "iteration 3123, dc_loss: 0.023589711636304855, tv_loss: 0.0338968001306057\n",
      "iteration 3124, dc_loss: 0.023614119738340378, tv_loss: 0.033872656524181366\n",
      "iteration 3125, dc_loss: 0.02359350398182869, tv_loss: 0.03389598801732063\n",
      "iteration 3126, dc_loss: 0.023582477122545242, tv_loss: 0.033893950283527374\n",
      "iteration 3127, dc_loss: 0.023597249761223793, tv_loss: 0.03388107940554619\n",
      "iteration 3128, dc_loss: 0.023594461381435394, tv_loss: 0.03388392925262451\n",
      "iteration 3129, dc_loss: 0.023591401055455208, tv_loss: 0.033889833837747574\n",
      "iteration 3130, dc_loss: 0.02358032763004303, tv_loss: 0.03390063717961311\n",
      "iteration 3131, dc_loss: 0.023606808856129646, tv_loss: 0.03386155888438225\n",
      "iteration 3132, dc_loss: 0.02356071211397648, tv_loss: 0.03389947861433029\n",
      "iteration 3133, dc_loss: 0.023582177236676216, tv_loss: 0.033884111791849136\n",
      "iteration 3134, dc_loss: 0.02358376234769821, tv_loss: 0.03386738896369934\n",
      "iteration 3135, dc_loss: 0.023559506982564926, tv_loss: 0.03388803079724312\n",
      "iteration 3136, dc_loss: 0.02358076348900795, tv_loss: 0.03387126326560974\n",
      "iteration 3137, dc_loss: 0.023572487756609917, tv_loss: 0.03387563303112984\n",
      "iteration 3138, dc_loss: 0.023554159328341484, tv_loss: 0.03388513624668121\n",
      "iteration 3139, dc_loss: 0.023565623909235, tv_loss: 0.03388121351599693\n",
      "iteration 3140, dc_loss: 0.023572852835059166, tv_loss: 0.03388356789946556\n",
      "iteration 3141, dc_loss: 0.023553093895316124, tv_loss: 0.033899612724781036\n",
      "iteration 3142, dc_loss: 0.02355623058974743, tv_loss: 0.03388383612036705\n",
      "iteration 3143, dc_loss: 0.023555465042591095, tv_loss: 0.033877547830343246\n",
      "iteration 3144, dc_loss: 0.023552710190415382, tv_loss: 0.033875878900289536\n",
      "iteration 3145, dc_loss: 0.02354941889643669, tv_loss: 0.03388288617134094\n",
      "iteration 3146, dc_loss: 0.023545844480395317, tv_loss: 0.03388853371143341\n",
      "iteration 3147, dc_loss: 0.023539863526821136, tv_loss: 0.0338829904794693\n",
      "iteration 3148, dc_loss: 0.023541226983070374, tv_loss: 0.033883653581142426\n",
      "iteration 3149, dc_loss: 0.02354808710515499, tv_loss: 0.03386964648962021\n",
      "iteration 3150, dc_loss: 0.02352876402437687, tv_loss: 0.03388695418834686\n",
      "iteration 3151, dc_loss: 0.02354329079389572, tv_loss: 0.033874284476041794\n",
      "iteration 3152, dc_loss: 0.02352750673890114, tv_loss: 0.03389442712068558\n",
      "iteration 3153, dc_loss: 0.02353249303996563, tv_loss: 0.03388805687427521\n",
      "iteration 3154, dc_loss: 0.023525524884462357, tv_loss: 0.0338822603225708\n",
      "iteration 3155, dc_loss: 0.023530947044491768, tv_loss: 0.03387192264199257\n",
      "iteration 3156, dc_loss: 0.023518048226833344, tv_loss: 0.03387864679098129\n",
      "iteration 3157, dc_loss: 0.02352265641093254, tv_loss: 0.03387559577822685\n",
      "iteration 3158, dc_loss: 0.02352205477654934, tv_loss: 0.033879995346069336\n",
      "iteration 3159, dc_loss: 0.02350488305091858, tv_loss: 0.0338919572532177\n",
      "iteration 3160, dc_loss: 0.023518657311797142, tv_loss: 0.03388344496488571\n",
      "iteration 3161, dc_loss: 0.02351592853665352, tv_loss: 0.03387751802802086\n",
      "iteration 3162, dc_loss: 0.023511512205004692, tv_loss: 0.03387828171253204\n",
      "iteration 3163, dc_loss: 0.02349843643605709, tv_loss: 0.03389031067490578\n",
      "iteration 3164, dc_loss: 0.02350779063999653, tv_loss: 0.03387463092803955\n",
      "iteration 3165, dc_loss: 0.023505013436079025, tv_loss: 0.033873673528432846\n",
      "iteration 3166, dc_loss: 0.023504698649048805, tv_loss: 0.03387110307812691\n",
      "iteration 3167, dc_loss: 0.0234988983720541, tv_loss: 0.033879153430461884\n",
      "iteration 3168, dc_loss: 0.023488705977797508, tv_loss: 0.03388061001896858\n",
      "iteration 3169, dc_loss: 0.023495495319366455, tv_loss: 0.03387312591075897\n",
      "iteration 3170, dc_loss: 0.023494984954595566, tv_loss: 0.03387853130698204\n",
      "iteration 3171, dc_loss: 0.023484013974666595, tv_loss: 0.033888302743434906\n",
      "iteration 3172, dc_loss: 0.023497266694903374, tv_loss: 0.03386861830949783\n",
      "iteration 3173, dc_loss: 0.02348080649971962, tv_loss: 0.03388537839055061\n",
      "iteration 3174, dc_loss: 0.023494713008403778, tv_loss: 0.033868275582790375\n",
      "iteration 3175, dc_loss: 0.02348105050623417, tv_loss: 0.03388004004955292\n",
      "iteration 3176, dc_loss: 0.023489635437726974, tv_loss: 0.033879250288009644\n",
      "iteration 3177, dc_loss: 0.02348269894719124, tv_loss: 0.033906806260347366\n",
      "iteration 3178, dc_loss: 0.023513682186603546, tv_loss: 0.03386400267481804\n",
      "iteration 3179, dc_loss: 0.023468313738703728, tv_loss: 0.033913467079401016\n",
      "iteration 3180, dc_loss: 0.023517729714512825, tv_loss: 0.03386543318629265\n",
      "iteration 3181, dc_loss: 0.023493312299251556, tv_loss: 0.033899612724781036\n",
      "iteration 3182, dc_loss: 0.023502914234995842, tv_loss: 0.03388189896941185\n",
      "iteration 3183, dc_loss: 0.023467712104320526, tv_loss: 0.03389953449368477\n",
      "iteration 3184, dc_loss: 0.023507010191679, tv_loss: 0.03385329991579056\n",
      "iteration 3185, dc_loss: 0.023447595536708832, tv_loss: 0.033906858414411545\n",
      "iteration 3186, dc_loss: 0.023462239652872086, tv_loss: 0.03387795761227608\n",
      "iteration 3187, dc_loss: 0.02345726080238819, tv_loss: 0.03388114273548126\n",
      "iteration 3188, dc_loss: 0.023450041189789772, tv_loss: 0.03388005122542381\n",
      "iteration 3189, dc_loss: 0.023457441478967667, tv_loss: 0.033883631229400635\n",
      "iteration 3190, dc_loss: 0.023449277505278587, tv_loss: 0.033895064145326614\n",
      "iteration 3191, dc_loss: 0.02347618155181408, tv_loss: 0.03386067971587181\n",
      "iteration 3192, dc_loss: 0.02342366799712181, tv_loss: 0.03390173986554146\n",
      "iteration 3193, dc_loss: 0.02346160262823105, tv_loss: 0.03385842591524124\n",
      "iteration 3194, dc_loss: 0.023440897464752197, tv_loss: 0.0338875986635685\n",
      "iteration 3195, dc_loss: 0.023426013067364693, tv_loss: 0.033901508897542953\n",
      "iteration 3196, dc_loss: 0.023450275883078575, tv_loss: 0.0338568240404129\n",
      "iteration 3197, dc_loss: 0.023425716906785965, tv_loss: 0.033879127353429794\n",
      "iteration 3198, dc_loss: 0.023441849276423454, tv_loss: 0.03386395424604416\n",
      "iteration 3199, dc_loss: 0.023420026525855064, tv_loss: 0.033888865262269974\n",
      "iteration 3200, dc_loss: 0.02343936450779438, tv_loss: 0.033879827708005905\n",
      "iteration 3201, dc_loss: 0.02342921867966652, tv_loss: 0.03388381376862526\n",
      "iteration 3202, dc_loss: 0.0234150979667902, tv_loss: 0.033875249326229095\n",
      "iteration 3203, dc_loss: 0.02341822162270546, tv_loss: 0.03386811539530754\n",
      "iteration 3204, dc_loss: 0.023421749472618103, tv_loss: 0.03387322649359703\n",
      "iteration 3205, dc_loss: 0.023413296788930893, tv_loss: 0.03387269750237465\n",
      "iteration 3206, dc_loss: 0.02340894751250744, tv_loss: 0.03387359902262688\n",
      "iteration 3207, dc_loss: 0.023416420444846153, tv_loss: 0.03386562690138817\n",
      "iteration 3208, dc_loss: 0.023413721472024918, tv_loss: 0.033863432705402374\n",
      "iteration 3209, dc_loss: 0.02340560220181942, tv_loss: 0.03387050703167915\n",
      "iteration 3210, dc_loss: 0.02340320497751236, tv_loss: 0.0338679701089859\n",
      "iteration 3211, dc_loss: 0.02340473048388958, tv_loss: 0.0338631235063076\n",
      "iteration 3212, dc_loss: 0.02340279333293438, tv_loss: 0.03386170044541359\n",
      "iteration 3213, dc_loss: 0.023402037099003792, tv_loss: 0.033861540257930756\n",
      "iteration 3214, dc_loss: 0.023394513875246048, tv_loss: 0.033867936581373215\n",
      "iteration 3215, dc_loss: 0.023395469412207603, tv_loss: 0.03386269882321358\n",
      "iteration 3216, dc_loss: 0.023402975872159004, tv_loss: 0.033857643604278564\n",
      "iteration 3217, dc_loss: 0.023391902446746826, tv_loss: 0.03386843577027321\n",
      "iteration 3218, dc_loss: 0.023385072126984596, tv_loss: 0.033873558044433594\n",
      "iteration 3219, dc_loss: 0.023393956944346428, tv_loss: 0.033859703689813614\n",
      "iteration 3220, dc_loss: 0.023389985784888268, tv_loss: 0.0338582769036293\n",
      "iteration 3221, dc_loss: 0.0233776792883873, tv_loss: 0.03387066349387169\n",
      "iteration 3222, dc_loss: 0.023385601118206978, tv_loss: 0.03385714069008827\n",
      "iteration 3223, dc_loss: 0.02338438294827938, tv_loss: 0.033860109746456146\n",
      "iteration 3224, dc_loss: 0.02337663248181343, tv_loss: 0.03387215733528137\n",
      "iteration 3225, dc_loss: 0.02338210493326187, tv_loss: 0.03386707231402397\n",
      "iteration 3226, dc_loss: 0.023377617821097374, tv_loss: 0.033866703510284424\n",
      "iteration 3227, dc_loss: 0.02337379939854145, tv_loss: 0.03386153653264046\n",
      "iteration 3228, dc_loss: 0.023376481607556343, tv_loss: 0.033857088536024094\n",
      "iteration 3229, dc_loss: 0.023368123918771744, tv_loss: 0.03386062756180763\n",
      "iteration 3230, dc_loss: 0.023365765810012817, tv_loss: 0.03385988995432854\n",
      "iteration 3231, dc_loss: 0.023375101387500763, tv_loss: 0.033853061497211456\n",
      "iteration 3232, dc_loss: 0.023365052416920662, tv_loss: 0.03386397659778595\n",
      "iteration 3233, dc_loss: 0.023359855636954308, tv_loss: 0.03387732803821564\n",
      "iteration 3234, dc_loss: 0.02336849644780159, tv_loss: 0.033868949860334396\n",
      "iteration 3235, dc_loss: 0.02335783652961254, tv_loss: 0.033865638077259064\n",
      "iteration 3236, dc_loss: 0.023358510807156563, tv_loss: 0.03385896235704422\n",
      "iteration 3237, dc_loss: 0.023362435400485992, tv_loss: 0.033854350447654724\n",
      "iteration 3238, dc_loss: 0.023350797593593597, tv_loss: 0.03386790305376053\n",
      "iteration 3239, dc_loss: 0.023351997137069702, tv_loss: 0.03387037292122841\n",
      "iteration 3240, dc_loss: 0.02335241250693798, tv_loss: 0.03386834263801575\n",
      "iteration 3241, dc_loss: 0.023349178954958916, tv_loss: 0.033864062279462814\n",
      "iteration 3242, dc_loss: 0.023347901180386543, tv_loss: 0.033863186836242676\n",
      "iteration 3243, dc_loss: 0.023354507982730865, tv_loss: 0.03384974226355553\n",
      "iteration 3244, dc_loss: 0.023343872278928757, tv_loss: 0.03386012092232704\n",
      "iteration 3245, dc_loss: 0.02333744987845421, tv_loss: 0.03387414291501045\n",
      "iteration 3246, dc_loss: 0.023347796872258186, tv_loss: 0.03387025371193886\n",
      "iteration 3247, dc_loss: 0.02333860471844673, tv_loss: 0.03386682644486427\n",
      "iteration 3248, dc_loss: 0.023337677121162415, tv_loss: 0.033857010304927826\n",
      "iteration 3249, dc_loss: 0.02333512343466282, tv_loss: 0.03386113420128822\n",
      "iteration 3250, dc_loss: 0.023329414427280426, tv_loss: 0.033865850418806076\n",
      "iteration 3251, dc_loss: 0.023337170481681824, tv_loss: 0.03385698422789574\n",
      "iteration 3252, dc_loss: 0.02333408035337925, tv_loss: 0.03386033698916435\n",
      "iteration 3253, dc_loss: 0.023325592279434204, tv_loss: 0.03386957198381424\n",
      "iteration 3254, dc_loss: 0.023326314985752106, tv_loss: 0.03386153653264046\n",
      "iteration 3255, dc_loss: 0.023330148309469223, tv_loss: 0.03385559469461441\n",
      "iteration 3256, dc_loss: 0.023322319611907005, tv_loss: 0.03385871276259422\n",
      "iteration 3257, dc_loss: 0.023322654888033867, tv_loss: 0.033855944871902466\n",
      "iteration 3258, dc_loss: 0.02332419902086258, tv_loss: 0.03385945409536362\n",
      "iteration 3259, dc_loss: 0.023310868069529533, tv_loss: 0.03388643264770508\n",
      "iteration 3260, dc_loss: 0.02331804670393467, tv_loss: 0.033869244158267975\n",
      "iteration 3261, dc_loss: 0.02332218736410141, tv_loss: 0.03385055065155029\n",
      "iteration 3262, dc_loss: 0.023310665041208267, tv_loss: 0.03387299180030823\n",
      "iteration 3263, dc_loss: 0.02330840565264225, tv_loss: 0.033887725323438644\n",
      "iteration 3264, dc_loss: 0.023311179131269455, tv_loss: 0.03386484831571579\n",
      "iteration 3265, dc_loss: 0.02331107296049595, tv_loss: 0.03386347368359566\n",
      "iteration 3266, dc_loss: 0.023305458948016167, tv_loss: 0.03388230875134468\n",
      "iteration 3267, dc_loss: 0.023301320150494576, tv_loss: 0.033875320106744766\n",
      "iteration 3268, dc_loss: 0.023305175825953484, tv_loss: 0.03386625275015831\n",
      "iteration 3269, dc_loss: 0.0233052559196949, tv_loss: 0.03387356549501419\n",
      "iteration 3270, dc_loss: 0.02329958975315094, tv_loss: 0.033880703151226044\n",
      "iteration 3271, dc_loss: 0.02329869754612446, tv_loss: 0.033864494413137436\n",
      "iteration 3272, dc_loss: 0.023295065388083458, tv_loss: 0.033884257078170776\n",
      "iteration 3273, dc_loss: 0.02328861691057682, tv_loss: 0.03387448191642761\n",
      "iteration 3274, dc_loss: 0.02329503558576107, tv_loss: 0.03386491537094116\n",
      "iteration 3275, dc_loss: 0.023297930136322975, tv_loss: 0.033871185034513474\n",
      "iteration 3276, dc_loss: 0.02328716404736042, tv_loss: 0.033872779458761215\n",
      "iteration 3277, dc_loss: 0.023287838324904442, tv_loss: 0.03386479243636131\n",
      "iteration 3278, dc_loss: 0.023288613185286522, tv_loss: 0.033867739140987396\n",
      "iteration 3279, dc_loss: 0.0232793428003788, tv_loss: 0.03387679159641266\n",
      "iteration 3280, dc_loss: 0.023284079506993294, tv_loss: 0.03386390581727028\n",
      "iteration 3281, dc_loss: 0.023288164287805557, tv_loss: 0.03385898843407631\n",
      "iteration 3282, dc_loss: 0.023270709440112114, tv_loss: 0.033877551555633545\n",
      "iteration 3283, dc_loss: 0.023277640342712402, tv_loss: 0.0338665209710598\n",
      "iteration 3284, dc_loss: 0.023286616429686546, tv_loss: 0.033848512917757034\n",
      "iteration 3285, dc_loss: 0.023272376507520676, tv_loss: 0.03386436775326729\n",
      "iteration 3286, dc_loss: 0.023270370438694954, tv_loss: 0.03387510031461716\n",
      "iteration 3287, dc_loss: 0.023275820538401604, tv_loss: 0.033862680196762085\n",
      "iteration 3288, dc_loss: 0.023273158818483353, tv_loss: 0.033854540437459946\n",
      "iteration 3289, dc_loss: 0.023258334025740623, tv_loss: 0.03386906534433365\n",
      "iteration 3290, dc_loss: 0.023261187598109245, tv_loss: 0.03387890383601189\n",
      "iteration 3291, dc_loss: 0.02327071875333786, tv_loss: 0.03386245295405388\n",
      "iteration 3292, dc_loss: 0.02325940690934658, tv_loss: 0.0338604599237442\n",
      "iteration 3293, dc_loss: 0.02326034940779209, tv_loss: 0.033861663192510605\n",
      "iteration 3294, dc_loss: 0.023268332704901695, tv_loss: 0.0338614359498024\n",
      "iteration 3295, dc_loss: 0.023253846913576126, tv_loss: 0.03386891633272171\n",
      "iteration 3296, dc_loss: 0.023257020860910416, tv_loss: 0.03385606408119202\n",
      "iteration 3297, dc_loss: 0.023255012929439545, tv_loss: 0.03385331481695175\n",
      "iteration 3298, dc_loss: 0.023246649652719498, tv_loss: 0.03386485576629639\n",
      "iteration 3299, dc_loss: 0.023251932114362717, tv_loss: 0.033870797604322433\n",
      "iteration 3300, dc_loss: 0.023250849917531013, tv_loss: 0.03386393561959267\n",
      "iteration 3301, dc_loss: 0.023245662450790405, tv_loss: 0.033854372799396515\n",
      "iteration 3302, dc_loss: 0.023245321586728096, tv_loss: 0.0338573195040226\n",
      "iteration 3303, dc_loss: 0.023245662450790405, tv_loss: 0.03386634215712547\n",
      "iteration 3304, dc_loss: 0.02324148267507553, tv_loss: 0.03386618196964264\n",
      "iteration 3305, dc_loss: 0.02323731780052185, tv_loss: 0.03385848179459572\n",
      "iteration 3306, dc_loss: 0.02324162982404232, tv_loss: 0.03384985402226448\n",
      "iteration 3307, dc_loss: 0.023232240229845047, tv_loss: 0.03385908901691437\n",
      "iteration 3308, dc_loss: 0.023233648389577866, tv_loss: 0.033863600343465805\n",
      "iteration 3309, dc_loss: 0.02324039489030838, tv_loss: 0.03386212885379791\n",
      "iteration 3310, dc_loss: 0.023232173174619675, tv_loss: 0.03385759890079498\n",
      "iteration 3311, dc_loss: 0.023227330297231674, tv_loss: 0.033855415880680084\n",
      "iteration 3312, dc_loss: 0.023227816447615623, tv_loss: 0.03386497497558594\n",
      "iteration 3313, dc_loss: 0.023225875571370125, tv_loss: 0.033873360604047775\n",
      "iteration 3314, dc_loss: 0.02322554402053356, tv_loss: 0.033862434327602386\n",
      "iteration 3315, dc_loss: 0.02322239801287651, tv_loss: 0.0338539257645607\n",
      "iteration 3316, dc_loss: 0.02322530932724476, tv_loss: 0.03386252000927925\n",
      "iteration 3317, dc_loss: 0.023220930248498917, tv_loss: 0.03386858105659485\n",
      "iteration 3318, dc_loss: 0.02321229688823223, tv_loss: 0.03386586531996727\n",
      "iteration 3319, dc_loss: 0.023219088092446327, tv_loss: 0.03385266289114952\n",
      "iteration 3320, dc_loss: 0.02321738563477993, tv_loss: 0.0338640995323658\n",
      "iteration 3321, dc_loss: 0.023210227489471436, tv_loss: 0.033871784806251526\n",
      "iteration 3322, dc_loss: 0.02321120910346508, tv_loss: 0.033858008682727814\n",
      "iteration 3323, dc_loss: 0.023211749270558357, tv_loss: 0.033853672444820404\n",
      "iteration 3324, dc_loss: 0.02320956438779831, tv_loss: 0.033872928470373154\n",
      "iteration 3325, dc_loss: 0.023204931989312172, tv_loss: 0.03386586159467697\n",
      "iteration 3326, dc_loss: 0.023199377581477165, tv_loss: 0.03385878726840019\n",
      "iteration 3327, dc_loss: 0.02320392243564129, tv_loss: 0.03386176750063896\n",
      "iteration 3328, dc_loss: 0.023208798840641975, tv_loss: 0.033865295350551605\n",
      "iteration 3329, dc_loss: 0.023193297907710075, tv_loss: 0.03386707603931427\n",
      "iteration 3330, dc_loss: 0.023195216432213783, tv_loss: 0.033856261521577835\n",
      "iteration 3331, dc_loss: 0.02320248633623123, tv_loss: 0.03385593742132187\n",
      "iteration 3332, dc_loss: 0.023192575201392174, tv_loss: 0.03387190401554108\n",
      "iteration 3333, dc_loss: 0.0231906957924366, tv_loss: 0.033861901611089706\n",
      "iteration 3334, dc_loss: 0.02319491282105446, tv_loss: 0.03384774550795555\n",
      "iteration 3335, dc_loss: 0.02319408766925335, tv_loss: 0.03385664150118828\n",
      "iteration 3336, dc_loss: 0.02318437024950981, tv_loss: 0.03386610373854637\n",
      "iteration 3337, dc_loss: 0.02318032644689083, tv_loss: 0.033863671123981476\n",
      "iteration 3338, dc_loss: 0.023190295323729515, tv_loss: 0.0338456891477108\n",
      "iteration 3339, dc_loss: 0.023179946467280388, tv_loss: 0.03385718911886215\n",
      "iteration 3340, dc_loss: 0.023179154843091965, tv_loss: 0.03386528417468071\n",
      "iteration 3341, dc_loss: 0.02318784035742283, tv_loss: 0.03385324031114578\n",
      "iteration 3342, dc_loss: 0.023173779249191284, tv_loss: 0.03385951370000839\n",
      "iteration 3343, dc_loss: 0.02317233569920063, tv_loss: 0.033855728805065155\n",
      "iteration 3344, dc_loss: 0.023180630058050156, tv_loss: 0.03384888917207718\n",
      "iteration 3345, dc_loss: 0.023172788321971893, tv_loss: 0.033857326954603195\n",
      "iteration 3346, dc_loss: 0.02317116968333721, tv_loss: 0.03386089205741882\n",
      "iteration 3347, dc_loss: 0.023173216730356216, tv_loss: 0.033847589045763016\n",
      "iteration 3348, dc_loss: 0.0231617521494627, tv_loss: 0.03385799378156662\n",
      "iteration 3349, dc_loss: 0.023162269964814186, tv_loss: 0.03385438397526741\n",
      "iteration 3350, dc_loss: 0.02316751517355442, tv_loss: 0.03385790437459946\n",
      "iteration 3351, dc_loss: 0.023158710449934006, tv_loss: 0.033867768943309784\n",
      "iteration 3352, dc_loss: 0.023161668330430984, tv_loss: 0.03385539352893829\n",
      "iteration 3353, dc_loss: 0.02317061647772789, tv_loss: 0.03383876010775566\n",
      "iteration 3354, dc_loss: 0.023153211921453476, tv_loss: 0.03386366367340088\n",
      "iteration 3355, dc_loss: 0.023154277354478836, tv_loss: 0.03386601433157921\n",
      "iteration 3356, dc_loss: 0.023158399388194084, tv_loss: 0.03385303169488907\n",
      "iteration 3357, dc_loss: 0.023146046325564384, tv_loss: 0.033858783543109894\n",
      "iteration 3358, dc_loss: 0.023150332272052765, tv_loss: 0.03385290503501892\n",
      "iteration 3359, dc_loss: 0.023157238960266113, tv_loss: 0.03385147824883461\n",
      "iteration 3360, dc_loss: 0.023145100101828575, tv_loss: 0.033859606832265854\n",
      "iteration 3361, dc_loss: 0.02314523421227932, tv_loss: 0.03385476768016815\n",
      "iteration 3362, dc_loss: 0.023148884996771812, tv_loss: 0.0338478609919548\n",
      "iteration 3363, dc_loss: 0.023138584569096565, tv_loss: 0.03385898843407631\n",
      "iteration 3364, dc_loss: 0.02314123883843422, tv_loss: 0.03386050462722778\n",
      "iteration 3365, dc_loss: 0.023142319172620773, tv_loss: 0.03385148569941521\n",
      "iteration 3366, dc_loss: 0.02314198762178421, tv_loss: 0.033846665173769\n",
      "iteration 3367, dc_loss: 0.02312895469367504, tv_loss: 0.03386002779006958\n",
      "iteration 3368, dc_loss: 0.02313765324652195, tv_loss: 0.03384692594408989\n",
      "iteration 3369, dc_loss: 0.023137807846069336, tv_loss: 0.03385457769036293\n",
      "iteration 3370, dc_loss: 0.02312709577381611, tv_loss: 0.0338565818965435\n",
      "iteration 3371, dc_loss: 0.023127131164073944, tv_loss: 0.03385705500841141\n",
      "iteration 3372, dc_loss: 0.02312801592051983, tv_loss: 0.03384821489453316\n",
      "iteration 3373, dc_loss: 0.023129506036639214, tv_loss: 0.03384830057621002\n",
      "iteration 3374, dc_loss: 0.023129427805542946, tv_loss: 0.03385113924741745\n",
      "iteration 3375, dc_loss: 0.023119129240512848, tv_loss: 0.03385553136467934\n",
      "iteration 3376, dc_loss: 0.02312154695391655, tv_loss: 0.033847738057374954\n",
      "iteration 3377, dc_loss: 0.023124374449253082, tv_loss: 0.03384503722190857\n",
      "iteration 3378, dc_loss: 0.023109883069992065, tv_loss: 0.0338592603802681\n",
      "iteration 3379, dc_loss: 0.023115236312150955, tv_loss: 0.03385547921061516\n",
      "iteration 3380, dc_loss: 0.023122070357203484, tv_loss: 0.033842191100120544\n",
      "iteration 3381, dc_loss: 0.02310819737613201, tv_loss: 0.033855777233839035\n",
      "iteration 3382, dc_loss: 0.02310974895954132, tv_loss: 0.03385075554251671\n",
      "iteration 3383, dc_loss: 0.023120110854506493, tv_loss: 0.03383924439549446\n",
      "iteration 3384, dc_loss: 0.023107051849365234, tv_loss: 0.033853624016046524\n",
      "iteration 3385, dc_loss: 0.023096635937690735, tv_loss: 0.03386535868048668\n",
      "iteration 3386, dc_loss: 0.023107612505555153, tv_loss: 0.033853914588689804\n",
      "iteration 3387, dc_loss: 0.02310905233025551, tv_loss: 0.03384140878915787\n",
      "iteration 3388, dc_loss: 0.023097781464457512, tv_loss: 0.033858075737953186\n",
      "iteration 3389, dc_loss: 0.023094935342669487, tv_loss: 0.03385226055979729\n",
      "iteration 3390, dc_loss: 0.023105204105377197, tv_loss: 0.03384517505764961\n",
      "iteration 3391, dc_loss: 0.023097675293684006, tv_loss: 0.03384508937597275\n",
      "iteration 3392, dc_loss: 0.023090418428182602, tv_loss: 0.03384804725646973\n",
      "iteration 3393, dc_loss: 0.023096848279237747, tv_loss: 0.03384483605623245\n",
      "iteration 3394, dc_loss: 0.023095883429050446, tv_loss: 0.03384588286280632\n",
      "iteration 3395, dc_loss: 0.023079518228769302, tv_loss: 0.033858705312013626\n",
      "iteration 3396, dc_loss: 0.023092415183782578, tv_loss: 0.03385293111205101\n",
      "iteration 3397, dc_loss: 0.02309316396713257, tv_loss: 0.033843182027339935\n",
      "iteration 3398, dc_loss: 0.02307789772748947, tv_loss: 0.03385617956519127\n",
      "iteration 3399, dc_loss: 0.02308250404894352, tv_loss: 0.03385047987103462\n",
      "iteration 3400, dc_loss: 0.0230938158929348, tv_loss: 0.03384103998541832\n",
      "iteration 3401, dc_loss: 0.023075252771377563, tv_loss: 0.03385774791240692\n",
      "iteration 3402, dc_loss: 0.02308025397360325, tv_loss: 0.033847376704216\n",
      "iteration 3403, dc_loss: 0.02308330498635769, tv_loss: 0.03384128957986832\n",
      "iteration 3404, dc_loss: 0.023066719993948936, tv_loss: 0.03385460004210472\n",
      "iteration 3405, dc_loss: 0.02306678332388401, tv_loss: 0.033853039145469666\n",
      "iteration 3406, dc_loss: 0.023089615628123283, tv_loss: 0.03383222967386246\n",
      "iteration 3407, dc_loss: 0.0230674110352993, tv_loss: 0.03386395797133446\n",
      "iteration 3408, dc_loss: 0.023071765899658203, tv_loss: 0.03385106101632118\n",
      "iteration 3409, dc_loss: 0.02307184599339962, tv_loss: 0.03384355083107948\n",
      "iteration 3410, dc_loss: 0.023069677874445915, tv_loss: 0.033845074474811554\n",
      "iteration 3411, dc_loss: 0.023059258237481117, tv_loss: 0.033864110708236694\n",
      "iteration 3412, dc_loss: 0.02307007648050785, tv_loss: 0.03385128080844879\n",
      "iteration 3413, dc_loss: 0.023060904815793037, tv_loss: 0.03385816887021065\n",
      "iteration 3414, dc_loss: 0.023070257157087326, tv_loss: 0.033842530101537704\n",
      "iteration 3415, dc_loss: 0.023057814687490463, tv_loss: 0.033847905695438385\n",
      "iteration 3416, dc_loss: 0.0230672936886549, tv_loss: 0.033841267228126526\n",
      "iteration 3417, dc_loss: 0.023054609075188637, tv_loss: 0.03384769707918167\n",
      "iteration 3418, dc_loss: 0.023055696859955788, tv_loss: 0.033847346901893616\n",
      "iteration 3419, dc_loss: 0.023047275841236115, tv_loss: 0.03385135903954506\n",
      "iteration 3420, dc_loss: 0.023049967363476753, tv_loss: 0.03384236618876457\n",
      "iteration 3421, dc_loss: 0.023046158254146576, tv_loss: 0.03384591266512871\n",
      "iteration 3422, dc_loss: 0.023050978779792786, tv_loss: 0.033839717507362366\n",
      "iteration 3423, dc_loss: 0.02304946631193161, tv_loss: 0.03383983299136162\n",
      "iteration 3424, dc_loss: 0.02303571254014969, tv_loss: 0.033861201256513596\n",
      "iteration 3425, dc_loss: 0.023048441857099533, tv_loss: 0.033848151564598083\n",
      "iteration 3426, dc_loss: 0.023037385195493698, tv_loss: 0.03385390713810921\n",
      "iteration 3427, dc_loss: 0.02304403856396675, tv_loss: 0.033843379467725754\n",
      "iteration 3428, dc_loss: 0.023034725338220596, tv_loss: 0.033845946192741394\n",
      "iteration 3429, dc_loss: 0.023036882281303406, tv_loss: 0.03384111449122429\n",
      "iteration 3430, dc_loss: 0.023031018674373627, tv_loss: 0.033848270773887634\n",
      "iteration 3431, dc_loss: 0.023029960691928864, tv_loss: 0.033848028630018234\n",
      "iteration 3432, dc_loss: 0.023036833852529526, tv_loss: 0.03384627401828766\n",
      "iteration 3433, dc_loss: 0.023030629381537437, tv_loss: 0.03384576365351677\n",
      "iteration 3434, dc_loss: 0.023025374859571457, tv_loss: 0.03385273367166519\n",
      "iteration 3435, dc_loss: 0.023025376722216606, tv_loss: 0.033842120319604874\n",
      "iteration 3436, dc_loss: 0.023027069866657257, tv_loss: 0.03383874148130417\n",
      "iteration 3437, dc_loss: 0.023016635328531265, tv_loss: 0.033854056149721146\n",
      "iteration 3438, dc_loss: 0.023025648668408394, tv_loss: 0.0338391549885273\n",
      "iteration 3439, dc_loss: 0.023018499836325645, tv_loss: 0.03384413570165634\n",
      "iteration 3440, dc_loss: 0.023019731044769287, tv_loss: 0.03383965045213699\n",
      "iteration 3441, dc_loss: 0.023014530539512634, tv_loss: 0.033840861171483994\n",
      "iteration 3442, dc_loss: 0.02301015518605709, tv_loss: 0.03384409099817276\n",
      "iteration 3443, dc_loss: 0.02301529422402382, tv_loss: 0.033841513097286224\n",
      "iteration 3444, dc_loss: 0.023012030869722366, tv_loss: 0.03384382650256157\n",
      "iteration 3445, dc_loss: 0.02300650253891945, tv_loss: 0.03385248780250549\n",
      "iteration 3446, dc_loss: 0.02300783433020115, tv_loss: 0.03385166823863983\n",
      "iteration 3447, dc_loss: 0.023012882098555565, tv_loss: 0.03384597599506378\n",
      "iteration 3448, dc_loss: 0.02300049178302288, tv_loss: 0.03384961932897568\n",
      "iteration 3449, dc_loss: 0.023012682795524597, tv_loss: 0.03383299335837364\n",
      "iteration 3450, dc_loss: 0.02299155294895172, tv_loss: 0.03385152295231819\n",
      "iteration 3451, dc_loss: 0.02300022915005684, tv_loss: 0.03384018689393997\n",
      "iteration 3452, dc_loss: 0.023004762828350067, tv_loss: 0.033837564289569855\n",
      "iteration 3453, dc_loss: 0.023001648485660553, tv_loss: 0.03384094312787056\n",
      "iteration 3454, dc_loss: 0.022993052378296852, tv_loss: 0.033854492008686066\n",
      "iteration 3455, dc_loss: 0.02299908548593521, tv_loss: 0.033845894038677216\n",
      "iteration 3456, dc_loss: 0.022992128506302834, tv_loss: 0.03384515270590782\n",
      "iteration 3457, dc_loss: 0.022991791367530823, tv_loss: 0.03383729234337807\n",
      "iteration 3458, dc_loss: 0.022977665066719055, tv_loss: 0.03384937718510628\n",
      "iteration 3459, dc_loss: 0.02298903837800026, tv_loss: 0.033837977796792984\n",
      "iteration 3460, dc_loss: 0.022992832586169243, tv_loss: 0.033832017332315445\n",
      "iteration 3461, dc_loss: 0.02298067696392536, tv_loss: 0.033851489424705505\n",
      "iteration 3462, dc_loss: 0.022977981716394424, tv_loss: 0.03385088965296745\n",
      "iteration 3463, dc_loss: 0.022986536845564842, tv_loss: 0.03383754566311836\n",
      "iteration 3464, dc_loss: 0.022977132350206375, tv_loss: 0.033844102174043655\n",
      "iteration 3465, dc_loss: 0.022969117388129234, tv_loss: 0.03384752944111824\n",
      "iteration 3466, dc_loss: 0.022979125380516052, tv_loss: 0.033834826201200485\n",
      "iteration 3467, dc_loss: 0.022977132350206375, tv_loss: 0.03384093567728996\n",
      "iteration 3468, dc_loss: 0.022970782592892647, tv_loss: 0.03384219482541084\n",
      "iteration 3469, dc_loss: 0.022968050092458725, tv_loss: 0.03384655714035034\n",
      "iteration 3470, dc_loss: 0.022975008934736252, tv_loss: 0.03383436053991318\n",
      "iteration 3471, dc_loss: 0.02295893430709839, tv_loss: 0.033848028630018234\n",
      "iteration 3472, dc_loss: 0.022968806326389313, tv_loss: 0.033836815506219864\n",
      "iteration 3473, dc_loss: 0.022971272468566895, tv_loss: 0.033836837857961655\n",
      "iteration 3474, dc_loss: 0.022959500551223755, tv_loss: 0.03384922072291374\n",
      "iteration 3475, dc_loss: 0.022959010675549507, tv_loss: 0.03385775536298752\n",
      "iteration 3476, dc_loss: 0.022962206974625587, tv_loss: 0.03384110704064369\n",
      "iteration 3477, dc_loss: 0.022959435358643532, tv_loss: 0.0338471457362175\n",
      "iteration 3478, dc_loss: 0.022959277033805847, tv_loss: 0.03383830562233925\n",
      "iteration 3479, dc_loss: 0.022953584790229797, tv_loss: 0.03385290876030922\n",
      "iteration 3480, dc_loss: 0.022960128262639046, tv_loss: 0.03383902832865715\n",
      "iteration 3481, dc_loss: 0.022949611768126488, tv_loss: 0.03384849801659584\n",
      "iteration 3482, dc_loss: 0.022955873981118202, tv_loss: 0.03383892402052879\n",
      "iteration 3483, dc_loss: 0.022942638024687767, tv_loss: 0.03386005014181137\n",
      "iteration 3484, dc_loss: 0.022958196699619293, tv_loss: 0.033846415579319\n",
      "iteration 3485, dc_loss: 0.022954856976866722, tv_loss: 0.03384048864245415\n",
      "iteration 3486, dc_loss: 0.022956086322665215, tv_loss: 0.033838193863630295\n",
      "iteration 3487, dc_loss: 0.02293279767036438, tv_loss: 0.03386346623301506\n",
      "iteration 3488, dc_loss: 0.022956782951951027, tv_loss: 0.03383566811680794\n",
      "iteration 3489, dc_loss: 0.022941110655665398, tv_loss: 0.03384413570165634\n",
      "iteration 3490, dc_loss: 0.022943826392292976, tv_loss: 0.03383670747280121\n",
      "iteration 3491, dc_loss: 0.02293027937412262, tv_loss: 0.033850595355033875\n",
      "iteration 3492, dc_loss: 0.02294125221669674, tv_loss: 0.03383516147732735\n",
      "iteration 3493, dc_loss: 0.02293752133846283, tv_loss: 0.033837832510471344\n",
      "iteration 3494, dc_loss: 0.022922107949852943, tv_loss: 0.03384767472743988\n",
      "iteration 3495, dc_loss: 0.022929349914193153, tv_loss: 0.033844757825136185\n",
      "iteration 3496, dc_loss: 0.022933365777134895, tv_loss: 0.03383299335837364\n",
      "iteration 3497, dc_loss: 0.02293451689183712, tv_loss: 0.03383222594857216\n",
      "iteration 3498, dc_loss: 0.02291875146329403, tv_loss: 0.03384576737880707\n",
      "iteration 3499, dc_loss: 0.022932322695851326, tv_loss: 0.03382888436317444\n",
      "iteration 3500, dc_loss: 0.02291896939277649, tv_loss: 0.033849652856588364\n",
      "iteration 3501, dc_loss: 0.022921552881598473, tv_loss: 0.03383701294660568\n",
      "iteration 3502, dc_loss: 0.02292133867740631, tv_loss: 0.033839285373687744\n",
      "iteration 3503, dc_loss: 0.02291988581418991, tv_loss: 0.033832672983407974\n",
      "iteration 3504, dc_loss: 0.022913305088877678, tv_loss: 0.03384222835302353\n",
      "iteration 3505, dc_loss: 0.022912446409463882, tv_loss: 0.033837299793958664\n",
      "iteration 3506, dc_loss: 0.022914456203579903, tv_loss: 0.03383916616439819\n",
      "iteration 3507, dc_loss: 0.022911032661795616, tv_loss: 0.033842865377664566\n",
      "iteration 3508, dc_loss: 0.02291211485862732, tv_loss: 0.03383558616042137\n",
      "iteration 3509, dc_loss: 0.022905845195055008, tv_loss: 0.033839840441942215\n",
      "iteration 3510, dc_loss: 0.022915851324796677, tv_loss: 0.033827539533376694\n",
      "iteration 3511, dc_loss: 0.02289845608174801, tv_loss: 0.033842723816633224\n",
      "iteration 3512, dc_loss: 0.0228984747081995, tv_loss: 0.033846497535705566\n",
      "iteration 3513, dc_loss: 0.02290322817862034, tv_loss: 0.03383870795369148\n",
      "iteration 3514, dc_loss: 0.022904926910996437, tv_loss: 0.03383926674723625\n",
      "iteration 3515, dc_loss: 0.02289540134370327, tv_loss: 0.03383960574865341\n",
      "iteration 3516, dc_loss: 0.02289636619389057, tv_loss: 0.03383887559175491\n",
      "iteration 3517, dc_loss: 0.022900491952896118, tv_loss: 0.03383328765630722\n",
      "iteration 3518, dc_loss: 0.02288726717233658, tv_loss: 0.03385576605796814\n",
      "iteration 3519, dc_loss: 0.022890208289027214, tv_loss: 0.03384252265095711\n",
      "iteration 3520, dc_loss: 0.022890402004122734, tv_loss: 0.03384024649858475\n",
      "iteration 3521, dc_loss: 0.022896703332662582, tv_loss: 0.03383300080895424\n",
      "iteration 3522, dc_loss: 0.02288489229977131, tv_loss: 0.033841218799352646\n",
      "iteration 3523, dc_loss: 0.022887052968144417, tv_loss: 0.03383984789252281\n",
      "iteration 3524, dc_loss: 0.022882862016558647, tv_loss: 0.033838484436273575\n",
      "iteration 3525, dc_loss: 0.02288075163960457, tv_loss: 0.03383611887693405\n",
      "iteration 3526, dc_loss: 0.022883132100105286, tv_loss: 0.033836480230093\n",
      "iteration 3527, dc_loss: 0.022884642705321312, tv_loss: 0.03382943943142891\n",
      "iteration 3528, dc_loss: 0.02287229150533676, tv_loss: 0.033844590187072754\n",
      "iteration 3529, dc_loss: 0.022880464792251587, tv_loss: 0.03383272513747215\n",
      "iteration 3530, dc_loss: 0.022874776273965836, tv_loss: 0.033838964998722076\n",
      "iteration 3531, dc_loss: 0.022876789793372154, tv_loss: 0.033831920474767685\n",
      "iteration 3532, dc_loss: 0.02286786027252674, tv_loss: 0.033841609954833984\n",
      "iteration 3533, dc_loss: 0.022881098091602325, tv_loss: 0.033825602382421494\n",
      "iteration 3534, dc_loss: 0.02286292426288128, tv_loss: 0.03384228050708771\n",
      "iteration 3535, dc_loss: 0.022880464792251587, tv_loss: 0.03382500261068344\n",
      "iteration 3536, dc_loss: 0.022870155051350594, tv_loss: 0.033837929368019104\n",
      "iteration 3537, dc_loss: 0.02287040278315544, tv_loss: 0.03383985161781311\n",
      "iteration 3538, dc_loss: 0.02285933680832386, tv_loss: 0.033851247280836105\n",
      "iteration 3539, dc_loss: 0.02287948690354824, tv_loss: 0.033823516219854355\n",
      "iteration 3540, dc_loss: 0.02285449020564556, tv_loss: 0.03384506702423096\n",
      "iteration 3541, dc_loss: 0.022866997867822647, tv_loss: 0.03383195400238037\n",
      "iteration 3542, dc_loss: 0.022855745628476143, tv_loss: 0.033838339149951935\n",
      "iteration 3543, dc_loss: 0.02286073938012123, tv_loss: 0.03383621200919151\n",
      "iteration 3544, dc_loss: 0.02284928783774376, tv_loss: 0.03383644297719002\n",
      "iteration 3545, dc_loss: 0.022855712100863457, tv_loss: 0.03383113071322441\n",
      "iteration 3546, dc_loss: 0.02284999005496502, tv_loss: 0.033831074833869934\n",
      "iteration 3547, dc_loss: 0.02284218557178974, tv_loss: 0.033838529139757156\n",
      "iteration 3548, dc_loss: 0.022854739800095558, tv_loss: 0.03382555767893791\n",
      "iteration 3549, dc_loss: 0.022843312472105026, tv_loss: 0.03384184464812279\n",
      "iteration 3550, dc_loss: 0.0228451918810606, tv_loss: 0.03383302688598633\n",
      "iteration 3551, dc_loss: 0.02284107357263565, tv_loss: 0.03383997455239296\n",
      "iteration 3552, dc_loss: 0.022847935557365417, tv_loss: 0.03382755443453789\n",
      "iteration 3553, dc_loss: 0.022830385714769363, tv_loss: 0.033837102353572845\n",
      "iteration 3554, dc_loss: 0.022844333201646805, tv_loss: 0.03382865712046623\n",
      "iteration 3555, dc_loss: 0.022835223004221916, tv_loss: 0.033829834312200546\n",
      "iteration 3556, dc_loss: 0.022838452830910683, tv_loss: 0.03382522240281105\n",
      "iteration 3557, dc_loss: 0.022829975932836533, tv_loss: 0.033836446702480316\n",
      "iteration 3558, dc_loss: 0.022830545902252197, tv_loss: 0.03382840380072594\n",
      "iteration 3559, dc_loss: 0.022829430177807808, tv_loss: 0.0338311605155468\n",
      "iteration 3560, dc_loss: 0.022826654836535454, tv_loss: 0.03383321315050125\n",
      "iteration 3561, dc_loss: 0.022832069545984268, tv_loss: 0.033823274075984955\n",
      "iteration 3562, dc_loss: 0.022817425429821014, tv_loss: 0.033835191279649734\n",
      "iteration 3563, dc_loss: 0.022825701162219048, tv_loss: 0.03383282944560051\n",
      "iteration 3564, dc_loss: 0.022823937237262726, tv_loss: 0.03383469581604004\n",
      "iteration 3565, dc_loss: 0.02282334864139557, tv_loss: 0.033832889050245285\n",
      "iteration 3566, dc_loss: 0.022816086187958717, tv_loss: 0.03383888304233551\n",
      "iteration 3567, dc_loss: 0.022825466468930244, tv_loss: 0.03382953628897667\n",
      "iteration 3568, dc_loss: 0.022806694731116295, tv_loss: 0.03384154662489891\n",
      "iteration 3569, dc_loss: 0.02281402423977852, tv_loss: 0.03382869064807892\n",
      "iteration 3570, dc_loss: 0.022815968841314316, tv_loss: 0.033832188695669174\n",
      "iteration 3571, dc_loss: 0.022808749228715897, tv_loss: 0.033833812922239304\n",
      "iteration 3572, dc_loss: 0.0228151585906744, tv_loss: 0.033825185149908066\n",
      "iteration 3573, dc_loss: 0.02281356230378151, tv_loss: 0.033824048936367035\n",
      "iteration 3574, dc_loss: 0.02279483713209629, tv_loss: 0.03383868932723999\n",
      "iteration 3575, dc_loss: 0.022816557437181473, tv_loss: 0.03382566198706627\n",
      "iteration 3576, dc_loss: 0.022800687700510025, tv_loss: 0.03385007381439209\n",
      "iteration 3577, dc_loss: 0.0227997787296772, tv_loss: 0.03384662792086601\n",
      "iteration 3578, dc_loss: 0.02280975878238678, tv_loss: 0.033826109021902084\n",
      "iteration 3579, dc_loss: 0.02279745601117611, tv_loss: 0.03383094444870949\n",
      "iteration 3580, dc_loss: 0.022791940718889236, tv_loss: 0.03384600952267647\n",
      "iteration 3581, dc_loss: 0.022809498012065887, tv_loss: 0.033830273896455765\n",
      "iteration 3582, dc_loss: 0.0227825827896595, tv_loss: 0.03385500982403755\n",
      "iteration 3583, dc_loss: 0.022804243490099907, tv_loss: 0.03382263705134392\n",
      "iteration 3584, dc_loss: 0.022794464603066444, tv_loss: 0.033832550048828125\n",
      "iteration 3585, dc_loss: 0.022799182683229446, tv_loss: 0.03382689878344536\n",
      "iteration 3586, dc_loss: 0.022794121876358986, tv_loss: 0.03383481130003929\n",
      "iteration 3587, dc_loss: 0.022801680490374565, tv_loss: 0.03382933512330055\n",
      "iteration 3588, dc_loss: 0.022777115926146507, tv_loss: 0.03385208919644356\n",
      "iteration 3589, dc_loss: 0.02279680036008358, tv_loss: 0.0338260643184185\n",
      "iteration 3590, dc_loss: 0.02278568409383297, tv_loss: 0.03382890298962593\n",
      "iteration 3591, dc_loss: 0.02278793975710869, tv_loss: 0.03382444009184837\n",
      "iteration 3592, dc_loss: 0.02277643047273159, tv_loss: 0.03384137153625488\n",
      "iteration 3593, dc_loss: 0.022790588438510895, tv_loss: 0.0338318757712841\n",
      "iteration 3594, dc_loss: 0.022764991968870163, tv_loss: 0.03385813161730766\n",
      "iteration 3595, dc_loss: 0.022780301049351692, tv_loss: 0.03382744640111923\n",
      "iteration 3596, dc_loss: 0.02277308702468872, tv_loss: 0.03382724151015282\n",
      "iteration 3597, dc_loss: 0.022767554968595505, tv_loss: 0.033836450427770615\n",
      "iteration 3598, dc_loss: 0.02277502790093422, tv_loss: 0.03383828327059746\n",
      "iteration 3599, dc_loss: 0.02276911959052086, tv_loss: 0.033844783902168274\n",
      "iteration 3600, dc_loss: 0.022772151976823807, tv_loss: 0.03382594883441925\n",
      "iteration 3601, dc_loss: 0.022764449939131737, tv_loss: 0.03383207321166992\n",
      "iteration 3602, dc_loss: 0.022756898775696754, tv_loss: 0.033840034157037735\n",
      "iteration 3603, dc_loss: 0.022759350016713142, tv_loss: 0.03383180871605873\n",
      "iteration 3604, dc_loss: 0.022765425965189934, tv_loss: 0.033820990473032\n",
      "iteration 3605, dc_loss: 0.022762613371014595, tv_loss: 0.033827897161245346\n",
      "iteration 3606, dc_loss: 0.02275584451854229, tv_loss: 0.033830415457487106\n",
      "iteration 3607, dc_loss: 0.02275945618748665, tv_loss: 0.03382089361548424\n",
      "iteration 3608, dc_loss: 0.022763581946492195, tv_loss: 0.03381626680493355\n",
      "iteration 3609, dc_loss: 0.022754359990358353, tv_loss: 0.03382702171802521\n",
      "iteration 3610, dc_loss: 0.02274978719651699, tv_loss: 0.03383331000804901\n",
      "iteration 3611, dc_loss: 0.02275351621210575, tv_loss: 0.03382309898734093\n",
      "iteration 3612, dc_loss: 0.02274949476122856, tv_loss: 0.033824026584625244\n",
      "iteration 3613, dc_loss: 0.022748637944459915, tv_loss: 0.03382157161831856\n",
      "iteration 3614, dc_loss: 0.022752750664949417, tv_loss: 0.03382343426346779\n",
      "iteration 3615, dc_loss: 0.022745780646800995, tv_loss: 0.03383195027709007\n",
      "iteration 3616, dc_loss: 0.022745812311768532, tv_loss: 0.0338268019258976\n",
      "iteration 3617, dc_loss: 0.022747155278921127, tv_loss: 0.0338154174387455\n",
      "iteration 3618, dc_loss: 0.02274470403790474, tv_loss: 0.03382163122296333\n",
      "iteration 3619, dc_loss: 0.022741053253412247, tv_loss: 0.0338200144469738\n",
      "iteration 3620, dc_loss: 0.022743232548236847, tv_loss: 0.033822156488895416\n",
      "iteration 3621, dc_loss: 0.022745436057448387, tv_loss: 0.033817511051893234\n",
      "iteration 3622, dc_loss: 0.022737594321370125, tv_loss: 0.03382615000009537\n",
      "iteration 3623, dc_loss: 0.022731706500053406, tv_loss: 0.03382737562060356\n",
      "iteration 3624, dc_loss: 0.02273801527917385, tv_loss: 0.03381779044866562\n",
      "iteration 3625, dc_loss: 0.02274210937321186, tv_loss: 0.03381110727787018\n",
      "iteration 3626, dc_loss: 0.02273157797753811, tv_loss: 0.03382343053817749\n",
      "iteration 3627, dc_loss: 0.022732025012373924, tv_loss: 0.03381766378879547\n",
      "iteration 3628, dc_loss: 0.022734051570296288, tv_loss: 0.03381851688027382\n",
      "iteration 3629, dc_loss: 0.022727062925696373, tv_loss: 0.03382497653365135\n",
      "iteration 3630, dc_loss: 0.02272893860936165, tv_loss: 0.033821020275354385\n",
      "iteration 3631, dc_loss: 0.022729625925421715, tv_loss: 0.03381860628724098\n",
      "iteration 3632, dc_loss: 0.02272677794098854, tv_loss: 0.03381801396608353\n",
      "iteration 3633, dc_loss: 0.022726640105247498, tv_loss: 0.03381824120879173\n",
      "iteration 3634, dc_loss: 0.022724727168679237, tv_loss: 0.03381775692105293\n",
      "iteration 3635, dc_loss: 0.022722937166690826, tv_loss: 0.03381513059139252\n",
      "iteration 3636, dc_loss: 0.022721799090504646, tv_loss: 0.033815644681453705\n",
      "iteration 3637, dc_loss: 0.022720497101545334, tv_loss: 0.03381836786866188\n",
      "iteration 3638, dc_loss: 0.022720852866768837, tv_loss: 0.03381665050983429\n",
      "iteration 3639, dc_loss: 0.022719699889421463, tv_loss: 0.03381464257836342\n",
      "iteration 3640, dc_loss: 0.02271515317261219, tv_loss: 0.03381798043847084\n",
      "iteration 3641, dc_loss: 0.022718023508787155, tv_loss: 0.03381918743252754\n",
      "iteration 3642, dc_loss: 0.022713232785463333, tv_loss: 0.03381728753447533\n",
      "iteration 3643, dc_loss: 0.022710762917995453, tv_loss: 0.03381684049963951\n",
      "iteration 3644, dc_loss: 0.022717339918017387, tv_loss: 0.03381314501166344\n",
      "iteration 3645, dc_loss: 0.022708628326654434, tv_loss: 0.033822741359472275\n",
      "iteration 3646, dc_loss: 0.02270694263279438, tv_loss: 0.033822204917669296\n",
      "iteration 3647, dc_loss: 0.022713808342814445, tv_loss: 0.033814385533332825\n",
      "iteration 3648, dc_loss: 0.022707421332597733, tv_loss: 0.03382449969649315\n",
      "iteration 3649, dc_loss: 0.022702261805534363, tv_loss: 0.03382192924618721\n",
      "iteration 3650, dc_loss: 0.0227088313549757, tv_loss: 0.033817484974861145\n",
      "iteration 3651, dc_loss: 0.0227042343467474, tv_loss: 0.033814702183008194\n",
      "iteration 3652, dc_loss: 0.022697191685438156, tv_loss: 0.03382369875907898\n",
      "iteration 3653, dc_loss: 0.022704899311065674, tv_loss: 0.033816900104284286\n",
      "iteration 3654, dc_loss: 0.0227048322558403, tv_loss: 0.03381802886724472\n",
      "iteration 3655, dc_loss: 0.022695347666740417, tv_loss: 0.033826954662799835\n",
      "iteration 3656, dc_loss: 0.022692600265145302, tv_loss: 0.03381940722465515\n",
      "iteration 3657, dc_loss: 0.0227033868432045, tv_loss: 0.03381098806858063\n",
      "iteration 3658, dc_loss: 0.022694315761327744, tv_loss: 0.033815667033195496\n",
      "iteration 3659, dc_loss: 0.022687768563628197, tv_loss: 0.03382589668035507\n",
      "iteration 3660, dc_loss: 0.022701609879732132, tv_loss: 0.03381635248661041\n",
      "iteration 3661, dc_loss: 0.02269335836172104, tv_loss: 0.033818695694208145\n",
      "iteration 3662, dc_loss: 0.022682663053274155, tv_loss: 0.033827364444732666\n",
      "iteration 3663, dc_loss: 0.022690581157803535, tv_loss: 0.033812765032052994\n",
      "iteration 3664, dc_loss: 0.02269606478512287, tv_loss: 0.033807553350925446\n",
      "iteration 3665, dc_loss: 0.022683661431074142, tv_loss: 0.03381917253136635\n",
      "iteration 3666, dc_loss: 0.022684330120682716, tv_loss: 0.03382163867354393\n",
      "iteration 3667, dc_loss: 0.022688699886202812, tv_loss: 0.033823411911726\n",
      "iteration 3668, dc_loss: 0.0226791650056839, tv_loss: 0.033828843384981155\n",
      "iteration 3669, dc_loss: 0.022679327055811882, tv_loss: 0.03381998464465141\n",
      "iteration 3670, dc_loss: 0.022685257717967033, tv_loss: 0.033811893314123154\n",
      "iteration 3671, dc_loss: 0.022679833695292473, tv_loss: 0.03381938859820366\n",
      "iteration 3672, dc_loss: 0.022678274661302567, tv_loss: 0.03382379189133644\n",
      "iteration 3673, dc_loss: 0.022678323090076447, tv_loss: 0.03382384032011032\n",
      "iteration 3674, dc_loss: 0.022675761952996254, tv_loss: 0.03381473198533058\n",
      "iteration 3675, dc_loss: 0.022675054147839546, tv_loss: 0.03381775692105293\n",
      "iteration 3676, dc_loss: 0.022674469277262688, tv_loss: 0.033815648406744\n",
      "iteration 3677, dc_loss: 0.02267196960747242, tv_loss: 0.033816855400800705\n",
      "iteration 3678, dc_loss: 0.022670406848192215, tv_loss: 0.03382071107625961\n",
      "iteration 3679, dc_loss: 0.022672947496175766, tv_loss: 0.033814139664173126\n",
      "iteration 3680, dc_loss: 0.022667279466986656, tv_loss: 0.03381988778710365\n",
      "iteration 3681, dc_loss: 0.02266673929989338, tv_loss: 0.03381604328751564\n",
      "iteration 3682, dc_loss: 0.022668922320008278, tv_loss: 0.03381214290857315\n",
      "iteration 3683, dc_loss: 0.022664284333586693, tv_loss: 0.03381555154919624\n",
      "iteration 3684, dc_loss: 0.022664383053779602, tv_loss: 0.033810898661613464\n",
      "iteration 3685, dc_loss: 0.022664573043584824, tv_loss: 0.03381281718611717\n",
      "iteration 3686, dc_loss: 0.022662047296762466, tv_loss: 0.03381165862083435\n",
      "iteration 3687, dc_loss: 0.022661117836833, tv_loss: 0.033813782036304474\n",
      "iteration 3688, dc_loss: 0.022660603746771812, tv_loss: 0.03381429240107536\n",
      "iteration 3689, dc_loss: 0.022657200694084167, tv_loss: 0.03381045162677765\n",
      "iteration 3690, dc_loss: 0.02265494503080845, tv_loss: 0.03381884843111038\n",
      "iteration 3691, dc_loss: 0.02266019769012928, tv_loss: 0.033810555934906006\n",
      "iteration 3692, dc_loss: 0.02265608124434948, tv_loss: 0.03381584957242012\n",
      "iteration 3693, dc_loss: 0.022649435326457024, tv_loss: 0.03382003307342529\n",
      "iteration 3694, dc_loss: 0.022653626278042793, tv_loss: 0.033812399953603745\n",
      "iteration 3695, dc_loss: 0.022653555497527122, tv_loss: 0.03381006792187691\n",
      "iteration 3696, dc_loss: 0.02264869399368763, tv_loss: 0.03381529450416565\n",
      "iteration 3697, dc_loss: 0.02265227772295475, tv_loss: 0.03380752354860306\n",
      "iteration 3698, dc_loss: 0.022646544501185417, tv_loss: 0.033821769058704376\n",
      "iteration 3699, dc_loss: 0.02264346182346344, tv_loss: 0.03382468596100807\n",
      "iteration 3700, dc_loss: 0.022645821794867516, tv_loss: 0.03381883725523949\n",
      "iteration 3701, dc_loss: 0.022649480029940605, tv_loss: 0.033805906772613525\n",
      "iteration 3702, dc_loss: 0.022638767957687378, tv_loss: 0.033817946910858154\n",
      "iteration 3703, dc_loss: 0.02264047972857952, tv_loss: 0.03381894901394844\n",
      "iteration 3704, dc_loss: 0.022645898163318634, tv_loss: 0.03382290527224541\n",
      "iteration 3705, dc_loss: 0.022635342553257942, tv_loss: 0.033825330436229706\n",
      "iteration 3706, dc_loss: 0.022637929767370224, tv_loss: 0.03381795808672905\n",
      "iteration 3707, dc_loss: 0.02263963595032692, tv_loss: 0.03381621465086937\n",
      "iteration 3708, dc_loss: 0.022633511573076248, tv_loss: 0.033827103674411774\n",
      "iteration 3709, dc_loss: 0.022635390982031822, tv_loss: 0.033818814903497696\n",
      "iteration 3710, dc_loss: 0.022638509050011635, tv_loss: 0.03380999341607094\n",
      "iteration 3711, dc_loss: 0.022629663348197937, tv_loss: 0.03382083773612976\n",
      "iteration 3712, dc_loss: 0.022623449563980103, tv_loss: 0.033830828964710236\n",
      "iteration 3713, dc_loss: 0.022632092237472534, tv_loss: 0.033816494047641754\n",
      "iteration 3714, dc_loss: 0.022638235241174698, tv_loss: 0.03380381315946579\n",
      "iteration 3715, dc_loss: 0.02262318879365921, tv_loss: 0.03382551670074463\n",
      "iteration 3716, dc_loss: 0.022620443254709244, tv_loss: 0.03382376208901405\n",
      "iteration 3717, dc_loss: 0.022629166021943092, tv_loss: 0.03381354361772537\n",
      "iteration 3718, dc_loss: 0.022624971345067024, tv_loss: 0.033810265362262726\n",
      "iteration 3719, dc_loss: 0.02262108586728573, tv_loss: 0.03381989523768425\n",
      "iteration 3720, dc_loss: 0.02262735366821289, tv_loss: 0.03381506726145744\n",
      "iteration 3721, dc_loss: 0.02262365259230137, tv_loss: 0.033816900104284286\n",
      "iteration 3722, dc_loss: 0.02261471189558506, tv_loss: 0.033817365765571594\n",
      "iteration 3723, dc_loss: 0.022615987807512283, tv_loss: 0.0338156633079052\n",
      "iteration 3724, dc_loss: 0.02261795848608017, tv_loss: 0.03381180390715599\n",
      "iteration 3725, dc_loss: 0.022615019232034683, tv_loss: 0.033822569996118546\n",
      "iteration 3726, dc_loss: 0.022617733106017113, tv_loss: 0.03381021320819855\n",
      "iteration 3727, dc_loss: 0.02261444739997387, tv_loss: 0.033811718225479126\n",
      "iteration 3728, dc_loss: 0.02260820008814335, tv_loss: 0.03381581977009773\n",
      "iteration 3729, dc_loss: 0.022613493725657463, tv_loss: 0.03381400927901268\n",
      "iteration 3730, dc_loss: 0.022615015506744385, tv_loss: 0.03381846472620964\n",
      "iteration 3731, dc_loss: 0.02260332927107811, tv_loss: 0.03382141888141632\n",
      "iteration 3732, dc_loss: 0.02260422706604004, tv_loss: 0.03381795063614845\n",
      "iteration 3733, dc_loss: 0.02261538803577423, tv_loss: 0.03380318731069565\n",
      "iteration 3734, dc_loss: 0.022604988887906075, tv_loss: 0.03381365165114403\n",
      "iteration 3735, dc_loss: 0.022598639130592346, tv_loss: 0.03382788971066475\n",
      "iteration 3736, dc_loss: 0.022603515535593033, tv_loss: 0.033814914524555206\n",
      "iteration 3737, dc_loss: 0.02261110208928585, tv_loss: 0.033805325627326965\n",
      "iteration 3738, dc_loss: 0.022598128765821457, tv_loss: 0.0338146798312664\n",
      "iteration 3739, dc_loss: 0.02259352058172226, tv_loss: 0.033820703625679016\n",
      "iteration 3740, dc_loss: 0.02260548062622547, tv_loss: 0.0338115319609642\n",
      "iteration 3741, dc_loss: 0.02259927988052368, tv_loss: 0.03381253778934479\n",
      "iteration 3742, dc_loss: 0.022589685395359993, tv_loss: 0.03381956368684769\n",
      "iteration 3743, dc_loss: 0.022595103830099106, tv_loss: 0.033811554312705994\n",
      "iteration 3744, dc_loss: 0.022597884759306908, tv_loss: 0.033813126385211945\n",
      "iteration 3745, dc_loss: 0.02258908376097679, tv_loss: 0.03382379561662674\n",
      "iteration 3746, dc_loss: 0.022588282823562622, tv_loss: 0.03381679952144623\n",
      "iteration 3747, dc_loss: 0.022596752271056175, tv_loss: 0.0338054783642292\n",
      "iteration 3748, dc_loss: 0.022590065374970436, tv_loss: 0.03380962833762169\n",
      "iteration 3749, dc_loss: 0.02258184552192688, tv_loss: 0.03381911292672157\n",
      "iteration 3750, dc_loss: 0.022592835128307343, tv_loss: 0.03381514921784401\n",
      "iteration 3751, dc_loss: 0.022589746862649918, tv_loss: 0.03381248563528061\n",
      "iteration 3752, dc_loss: 0.022578099742531776, tv_loss: 0.03381922468543053\n",
      "iteration 3753, dc_loss: 0.022581715136766434, tv_loss: 0.033811312168836594\n",
      "iteration 3754, dc_loss: 0.02259211800992489, tv_loss: 0.03380335494875908\n",
      "iteration 3755, dc_loss: 0.0225763451308012, tv_loss: 0.033822573721408844\n",
      "iteration 3756, dc_loss: 0.02257443033158779, tv_loss: 0.03382354974746704\n",
      "iteration 3757, dc_loss: 0.022590167820453644, tv_loss: 0.0337991788983345\n",
      "iteration 3758, dc_loss: 0.022577546536922455, tv_loss: 0.03380646929144859\n",
      "iteration 3759, dc_loss: 0.022563325241208076, tv_loss: 0.03382721170783043\n",
      "iteration 3760, dc_loss: 0.022582387551665306, tv_loss: 0.033805880695581436\n",
      "iteration 3761, dc_loss: 0.02257886715233326, tv_loss: 0.033804137259721756\n",
      "iteration 3762, dc_loss: 0.02256390079855919, tv_loss: 0.033817969262599945\n",
      "iteration 3763, dc_loss: 0.02257705107331276, tv_loss: 0.033803101629018784\n",
      "iteration 3764, dc_loss: 0.022575676441192627, tv_loss: 0.03380396217107773\n",
      "iteration 3765, dc_loss: 0.02256082557141781, tv_loss: 0.03381620720028877\n",
      "iteration 3766, dc_loss: 0.022569777444005013, tv_loss: 0.033806413412094116\n",
      "iteration 3767, dc_loss: 0.02257305383682251, tv_loss: 0.033804185688495636\n",
      "iteration 3768, dc_loss: 0.022560294717550278, tv_loss: 0.033812470734119415\n",
      "iteration 3769, dc_loss: 0.02256147190928459, tv_loss: 0.03381209075450897\n",
      "iteration 3770, dc_loss: 0.022568069398403168, tv_loss: 0.03380381688475609\n",
      "iteration 3771, dc_loss: 0.022562555968761444, tv_loss: 0.03380627930164337\n",
      "iteration 3772, dc_loss: 0.0225578211247921, tv_loss: 0.03380788117647171\n",
      "iteration 3773, dc_loss: 0.022562570869922638, tv_loss: 0.03380483388900757\n",
      "iteration 3774, dc_loss: 0.02255989797413349, tv_loss: 0.03381100669503212\n",
      "iteration 3775, dc_loss: 0.022556807845830917, tv_loss: 0.0338200218975544\n",
      "iteration 3776, dc_loss: 0.0225551538169384, tv_loss: 0.033819109201431274\n",
      "iteration 3777, dc_loss: 0.022554652765393257, tv_loss: 0.033810805529356\n",
      "iteration 3778, dc_loss: 0.0225527286529541, tv_loss: 0.03381258249282837\n",
      "iteration 3779, dc_loss: 0.022555939853191376, tv_loss: 0.033811330795288086\n",
      "iteration 3780, dc_loss: 0.022551631554961205, tv_loss: 0.033816516399383545\n",
      "iteration 3781, dc_loss: 0.022547826170921326, tv_loss: 0.033813897520303726\n",
      "iteration 3782, dc_loss: 0.022549770772457123, tv_loss: 0.033809904009103775\n",
      "iteration 3783, dc_loss: 0.02255292795598507, tv_loss: 0.03381579741835594\n",
      "iteration 3784, dc_loss: 0.022544926032423973, tv_loss: 0.03382405266165733\n",
      "iteration 3785, dc_loss: 0.02254049852490425, tv_loss: 0.03382060304284096\n",
      "iteration 3786, dc_loss: 0.022550897672772408, tv_loss: 0.0338054858148098\n",
      "iteration 3787, dc_loss: 0.02254647947847843, tv_loss: 0.033817898482084274\n",
      "iteration 3788, dc_loss: 0.022534441202878952, tv_loss: 0.03382527455687523\n",
      "iteration 3789, dc_loss: 0.022543935105204582, tv_loss: 0.033811066299676895\n",
      "iteration 3790, dc_loss: 0.022546498104929924, tv_loss: 0.033808302134275436\n",
      "iteration 3791, dc_loss: 0.02253422513604164, tv_loss: 0.033826809376478195\n",
      "iteration 3792, dc_loss: 0.022535227239131927, tv_loss: 0.03381655737757683\n",
      "iteration 3793, dc_loss: 0.02254313789308071, tv_loss: 0.03380376473069191\n",
      "iteration 3794, dc_loss: 0.02253478765487671, tv_loss: 0.03381676226854324\n",
      "iteration 3795, dc_loss: 0.022528521716594696, tv_loss: 0.03382513299584389\n",
      "iteration 3796, dc_loss: 0.02253558114171028, tv_loss: 0.03380642458796501\n",
      "iteration 3797, dc_loss: 0.022535618394613266, tv_loss: 0.03380800038576126\n",
      "iteration 3798, dc_loss: 0.022528618574142456, tv_loss: 0.033819954842329025\n",
      "iteration 3799, dc_loss: 0.022532330825924873, tv_loss: 0.03381168097257614\n",
      "iteration 3800, dc_loss: 0.022533686831593513, tv_loss: 0.03380553796887398\n",
      "iteration 3801, dc_loss: 0.02252250164747238, tv_loss: 0.033816300332546234\n",
      "iteration 3802, dc_loss: 0.02252274937927723, tv_loss: 0.03381458669900894\n",
      "iteration 3803, dc_loss: 0.022529983893036842, tv_loss: 0.03380874544382095\n",
      "iteration 3804, dc_loss: 0.022519832476973534, tv_loss: 0.03381194546818733\n",
      "iteration 3805, dc_loss: 0.0225212424993515, tv_loss: 0.03380604460835457\n",
      "iteration 3806, dc_loss: 0.022528842091560364, tv_loss: 0.03380103036761284\n",
      "iteration 3807, dc_loss: 0.022514577955007553, tv_loss: 0.033815063536167145\n",
      "iteration 3808, dc_loss: 0.02251717634499073, tv_loss: 0.033808063715696335\n",
      "iteration 3809, dc_loss: 0.022526433691382408, tv_loss: 0.033799346536397934\n",
      "iteration 3810, dc_loss: 0.022516200318932533, tv_loss: 0.03380943834781647\n",
      "iteration 3811, dc_loss: 0.022507932037115097, tv_loss: 0.033813897520303726\n",
      "iteration 3812, dc_loss: 0.022523296996951103, tv_loss: 0.033799875527620316\n",
      "iteration 3813, dc_loss: 0.022512521594762802, tv_loss: 0.033806074410676956\n",
      "iteration 3814, dc_loss: 0.022506656125187874, tv_loss: 0.03380823880434036\n",
      "iteration 3815, dc_loss: 0.022514842450618744, tv_loss: 0.03379964083433151\n",
      "iteration 3816, dc_loss: 0.022510666400194168, tv_loss: 0.033808641135692596\n",
      "iteration 3817, dc_loss: 0.02250627987086773, tv_loss: 0.0338120274245739\n",
      "iteration 3818, dc_loss: 0.022515758872032166, tv_loss: 0.03380115330219269\n",
      "iteration 3819, dc_loss: 0.022504957392811775, tv_loss: 0.03381101414561272\n",
      "iteration 3820, dc_loss: 0.02250167541205883, tv_loss: 0.03381020575761795\n",
      "iteration 3821, dc_loss: 0.022503219544887543, tv_loss: 0.033806268125772476\n",
      "iteration 3822, dc_loss: 0.022510742768645287, tv_loss: 0.03380046412348747\n",
      "iteration 3823, dc_loss: 0.02250310406088829, tv_loss: 0.033803850412368774\n",
      "iteration 3824, dc_loss: 0.02250591665506363, tv_loss: 0.03379952535033226\n",
      "iteration 3825, dc_loss: 0.022498875856399536, tv_loss: 0.03380674868822098\n",
      "iteration 3826, dc_loss: 0.022500723600387573, tv_loss: 0.033806294202804565\n",
      "iteration 3827, dc_loss: 0.022496752440929413, tv_loss: 0.033810507506132126\n",
      "iteration 3828, dc_loss: 0.02251219004392624, tv_loss: 0.03379129618406296\n",
      "iteration 3829, dc_loss: 0.022494062781333923, tv_loss: 0.033806901425123215\n",
      "iteration 3830, dc_loss: 0.02250050939619541, tv_loss: 0.03380141779780388\n",
      "iteration 3831, dc_loss: 0.02249605767428875, tv_loss: 0.03380062058568001\n",
      "iteration 3832, dc_loss: 0.022491756826639175, tv_loss: 0.03380265086889267\n",
      "iteration 3833, dc_loss: 0.022485872730612755, tv_loss: 0.033814702183008194\n",
      "iteration 3834, dc_loss: 0.02249240316450596, tv_loss: 0.03381113335490227\n",
      "iteration 3835, dc_loss: 0.02248469553887844, tv_loss: 0.03381424769759178\n",
      "iteration 3836, dc_loss: 0.02248542197048664, tv_loss: 0.03380456939339638\n",
      "iteration 3837, dc_loss: 0.02249649167060852, tv_loss: 0.033792756497859955\n",
      "iteration 3838, dc_loss: 0.022482069209218025, tv_loss: 0.033814653754234314\n",
      "iteration 3839, dc_loss: 0.02248556725680828, tv_loss: 0.03381552919745445\n",
      "iteration 3840, dc_loss: 0.02248086966574192, tv_loss: 0.03380928561091423\n",
      "iteration 3841, dc_loss: 0.022483618929982185, tv_loss: 0.033800896257162094\n",
      "iteration 3842, dc_loss: 0.02247779257595539, tv_loss: 0.0338112935423851\n",
      "iteration 3843, dc_loss: 0.0224797073751688, tv_loss: 0.03380981460213661\n",
      "iteration 3844, dc_loss: 0.022480856627225876, tv_loss: 0.033807139843702316\n",
      "iteration 3845, dc_loss: 0.022475294768810272, tv_loss: 0.03380296379327774\n",
      "iteration 3846, dc_loss: 0.02247672528028488, tv_loss: 0.033804282546043396\n",
      "iteration 3847, dc_loss: 0.02247307077050209, tv_loss: 0.03380397707223892\n",
      "iteration 3848, dc_loss: 0.022472873330116272, tv_loss: 0.03380507603287697\n",
      "iteration 3849, dc_loss: 0.022473536431789398, tv_loss: 0.03380560129880905\n",
      "iteration 3850, dc_loss: 0.022478237748146057, tv_loss: 0.03379853814840317\n",
      "iteration 3851, dc_loss: 0.022467242553830147, tv_loss: 0.03380805626511574\n",
      "iteration 3852, dc_loss: 0.02246478945016861, tv_loss: 0.033806707710027695\n",
      "iteration 3853, dc_loss: 0.022472485899925232, tv_loss: 0.033796604722738266\n",
      "iteration 3854, dc_loss: 0.022466661408543587, tv_loss: 0.03379925340414047\n",
      "iteration 3855, dc_loss: 0.022461924701929092, tv_loss: 0.03380374610424042\n",
      "iteration 3856, dc_loss: 0.022468537092208862, tv_loss: 0.033795032650232315\n",
      "iteration 3857, dc_loss: 0.022463100031018257, tv_loss: 0.03379938006401062\n",
      "iteration 3858, dc_loss: 0.022455625236034393, tv_loss: 0.03380672261118889\n",
      "iteration 3859, dc_loss: 0.0224680844694376, tv_loss: 0.033793866634368896\n",
      "iteration 3860, dc_loss: 0.022460278123617172, tv_loss: 0.03380795568227768\n",
      "iteration 3861, dc_loss: 0.02245345339179039, tv_loss: 0.03381715342402458\n",
      "iteration 3862, dc_loss: 0.0224592387676239, tv_loss: 0.03380689397454262\n",
      "iteration 3863, dc_loss: 0.02245897799730301, tv_loss: 0.033798132091760635\n",
      "iteration 3864, dc_loss: 0.022453943267464638, tv_loss: 0.033801209181547165\n",
      "iteration 3865, dc_loss: 0.0224542748183012, tv_loss: 0.03380054235458374\n",
      "iteration 3866, dc_loss: 0.02245020680129528, tv_loss: 0.03380220755934715\n",
      "iteration 3867, dc_loss: 0.022450370714068413, tv_loss: 0.03380092605948448\n",
      "iteration 3868, dc_loss: 0.022454552352428436, tv_loss: 0.03380102291703224\n",
      "iteration 3869, dc_loss: 0.02244546450674534, tv_loss: 0.033807530999183655\n",
      "iteration 3870, dc_loss: 0.02244982123374939, tv_loss: 0.03380560129880905\n",
      "iteration 3871, dc_loss: 0.022451050579547882, tv_loss: 0.033803027123212814\n",
      "iteration 3872, dc_loss: 0.022444678470492363, tv_loss: 0.033803462982177734\n",
      "iteration 3873, dc_loss: 0.022444942966103554, tv_loss: 0.033800482749938965\n",
      "iteration 3874, dc_loss: 0.022446805611252785, tv_loss: 0.033798087388277054\n",
      "iteration 3875, dc_loss: 0.022440144792199135, tv_loss: 0.033803701400756836\n",
      "iteration 3876, dc_loss: 0.022438645362854004, tv_loss: 0.03380195423960686\n",
      "iteration 3877, dc_loss: 0.022440230473876, tv_loss: 0.03379720449447632\n",
      "iteration 3878, dc_loss: 0.02243698760867119, tv_loss: 0.03379976376891136\n",
      "iteration 3879, dc_loss: 0.022441381588578224, tv_loss: 0.03379364311695099\n",
      "iteration 3880, dc_loss: 0.022439507767558098, tv_loss: 0.033794865012168884\n",
      "iteration 3881, dc_loss: 0.022432789206504822, tv_loss: 0.0337972454726696\n",
      "iteration 3882, dc_loss: 0.02243669703602791, tv_loss: 0.03379271551966667\n",
      "iteration 3883, dc_loss: 0.022435301914811134, tv_loss: 0.03380260616540909\n",
      "iteration 3884, dc_loss: 0.022429248318076134, tv_loss: 0.033818140625953674\n",
      "iteration 3885, dc_loss: 0.022434521466493607, tv_loss: 0.03380589932203293\n",
      "iteration 3886, dc_loss: 0.0224306583404541, tv_loss: 0.03379913419485092\n",
      "iteration 3887, dc_loss: 0.022426022216677666, tv_loss: 0.03380051627755165\n",
      "iteration 3888, dc_loss: 0.022430358454585075, tv_loss: 0.033802226185798645\n",
      "iteration 3889, dc_loss: 0.022431574761867523, tv_loss: 0.033814750611782074\n",
      "iteration 3890, dc_loss: 0.02241699956357479, tv_loss: 0.03381679952144623\n",
      "iteration 3891, dc_loss: 0.022431474179029465, tv_loss: 0.03379487991333008\n",
      "iteration 3892, dc_loss: 0.02242983505129814, tv_loss: 0.03380632400512695\n",
      "iteration 3893, dc_loss: 0.022421495988965034, tv_loss: 0.03381837159395218\n",
      "iteration 3894, dc_loss: 0.022418303415179253, tv_loss: 0.03381028026342392\n",
      "iteration 3895, dc_loss: 0.022429194301366806, tv_loss: 0.03379218652844429\n",
      "iteration 3896, dc_loss: 0.02241833135485649, tv_loss: 0.033815108239650726\n",
      "iteration 3897, dc_loss: 0.022424310445785522, tv_loss: 0.03380458429455757\n",
      "iteration 3898, dc_loss: 0.022415246814489365, tv_loss: 0.03381171077489853\n",
      "iteration 3899, dc_loss: 0.022431207820773125, tv_loss: 0.03379292041063309\n",
      "iteration 3900, dc_loss: 0.022418122738599777, tv_loss: 0.03380730375647545\n",
      "iteration 3901, dc_loss: 0.022415146231651306, tv_loss: 0.03381146490573883\n",
      "iteration 3902, dc_loss: 0.022409379482269287, tv_loss: 0.03380636125802994\n",
      "iteration 3903, dc_loss: 0.02241704612970352, tv_loss: 0.033799801021814346\n",
      "iteration 3904, dc_loss: 0.022405562922358513, tv_loss: 0.03381427749991417\n",
      "iteration 3905, dc_loss: 0.022409971803426743, tv_loss: 0.03380174934864044\n",
      "iteration 3906, dc_loss: 0.022414041683077812, tv_loss: 0.033795882016420364\n",
      "iteration 3907, dc_loss: 0.022404491901397705, tv_loss: 0.03380962461233139\n",
      "iteration 3908, dc_loss: 0.02240701951086521, tv_loss: 0.033805668354034424\n",
      "iteration 3909, dc_loss: 0.02240452915430069, tv_loss: 0.03379980847239494\n",
      "iteration 3910, dc_loss: 0.022410348057746887, tv_loss: 0.033794060349464417\n",
      "iteration 3911, dc_loss: 0.022395608946681023, tv_loss: 0.033807769417762756\n",
      "iteration 3912, dc_loss: 0.022403081879019737, tv_loss: 0.033806342631578445\n",
      "iteration 3913, dc_loss: 0.022406071424484253, tv_loss: 0.033802032470703125\n",
      "iteration 3914, dc_loss: 0.02239338681101799, tv_loss: 0.033801544457674026\n",
      "iteration 3915, dc_loss: 0.022390667349100113, tv_loss: 0.03380913287401199\n",
      "iteration 3916, dc_loss: 0.022403933107852936, tv_loss: 0.03379461169242859\n",
      "iteration 3917, dc_loss: 0.02239796333014965, tv_loss: 0.03379608690738678\n",
      "iteration 3918, dc_loss: 0.022388780489563942, tv_loss: 0.033806461840867996\n",
      "iteration 3919, dc_loss: 0.02239828184247017, tv_loss: 0.03379233926534653\n",
      "iteration 3920, dc_loss: 0.022392617538571358, tv_loss: 0.03379441052675247\n",
      "iteration 3921, dc_loss: 0.022388070821762085, tv_loss: 0.03380168229341507\n",
      "iteration 3922, dc_loss: 0.022391466423869133, tv_loss: 0.0337957926094532\n",
      "iteration 3923, dc_loss: 0.022389132529497147, tv_loss: 0.03379766270518303\n",
      "iteration 3924, dc_loss: 0.02238708920776844, tv_loss: 0.03379925712943077\n",
      "iteration 3925, dc_loss: 0.022391771897673607, tv_loss: 0.033791884779930115\n",
      "iteration 3926, dc_loss: 0.022380324080586433, tv_loss: 0.033805616199970245\n",
      "iteration 3927, dc_loss: 0.022379720583558083, tv_loss: 0.03380000591278076\n",
      "iteration 3928, dc_loss: 0.022388866171240807, tv_loss: 0.03378788009285927\n",
      "iteration 3929, dc_loss: 0.02237887866795063, tv_loss: 0.03379792720079422\n",
      "iteration 3930, dc_loss: 0.022380605340003967, tv_loss: 0.033798232674598694\n",
      "iteration 3931, dc_loss: 0.0223837960511446, tv_loss: 0.03378751501441002\n",
      "iteration 3932, dc_loss: 0.02237795479595661, tv_loss: 0.03379424661397934\n",
      "iteration 3933, dc_loss: 0.02237161435186863, tv_loss: 0.03380204364657402\n",
      "iteration 3934, dc_loss: 0.022381799295544624, tv_loss: 0.03379165381193161\n",
      "iteration 3935, dc_loss: 0.022373899817466736, tv_loss: 0.03379135578870773\n",
      "iteration 3936, dc_loss: 0.022372271865606308, tv_loss: 0.03379650413990021\n",
      "iteration 3937, dc_loss: 0.022380342707037926, tv_loss: 0.03379037231206894\n",
      "iteration 3938, dc_loss: 0.022369977086782455, tv_loss: 0.03379840403795242\n",
      "iteration 3939, dc_loss: 0.022366685792803764, tv_loss: 0.033802829682826996\n",
      "iteration 3940, dc_loss: 0.022370945662260056, tv_loss: 0.033796049654483795\n",
      "iteration 3941, dc_loss: 0.02236819826066494, tv_loss: 0.0337991937994957\n",
      "iteration 3942, dc_loss: 0.022367598488926888, tv_loss: 0.0337972417473793\n",
      "iteration 3943, dc_loss: 0.022368622943758965, tv_loss: 0.03379083052277565\n",
      "iteration 3944, dc_loss: 0.022363530471920967, tv_loss: 0.033794477581977844\n",
      "iteration 3945, dc_loss: 0.02236100658774376, tv_loss: 0.03379922732710838\n",
      "iteration 3946, dc_loss: 0.0223693884909153, tv_loss: 0.03378672897815704\n",
      "iteration 3947, dc_loss: 0.022363198921084404, tv_loss: 0.03379523754119873\n",
      "iteration 3948, dc_loss: 0.02235412783920765, tv_loss: 0.0338064469397068\n",
      "iteration 3949, dc_loss: 0.02236287109553814, tv_loss: 0.03379230573773384\n",
      "iteration 3950, dc_loss: 0.02236095443367958, tv_loss: 0.033796217292547226\n",
      "iteration 3951, dc_loss: 0.022349784150719643, tv_loss: 0.03380025923252106\n",
      "iteration 3952, dc_loss: 0.022361421957612038, tv_loss: 0.03378516435623169\n",
      "iteration 3953, dc_loss: 0.022365044802427292, tv_loss: 0.03378463536500931\n",
      "iteration 3954, dc_loss: 0.022344840690493584, tv_loss: 0.03380008786916733\n",
      "iteration 3955, dc_loss: 0.022354643791913986, tv_loss: 0.03378691524267197\n",
      "iteration 3956, dc_loss: 0.022358747199177742, tv_loss: 0.03378552943468094\n",
      "iteration 3957, dc_loss: 0.02234065718948841, tv_loss: 0.0338028147816658\n",
      "iteration 3958, dc_loss: 0.022354496642947197, tv_loss: 0.033783040940761566\n",
      "iteration 3959, dc_loss: 0.022352926433086395, tv_loss: 0.03378685563802719\n",
      "iteration 3960, dc_loss: 0.02233933098614216, tv_loss: 0.03380133584141731\n",
      "iteration 3961, dc_loss: 0.02235097996890545, tv_loss: 0.03378474712371826\n",
      "iteration 3962, dc_loss: 0.022345978766679764, tv_loss: 0.03378981351852417\n",
      "iteration 3963, dc_loss: 0.022343680262565613, tv_loss: 0.03379521518945694\n",
      "iteration 3964, dc_loss: 0.02234824188053608, tv_loss: 0.033793844282627106\n",
      "iteration 3965, dc_loss: 0.022337883710861206, tv_loss: 0.03380826488137245\n",
      "iteration 3966, dc_loss: 0.022341566160321236, tv_loss: 0.033798132091760635\n",
      "iteration 3967, dc_loss: 0.02234014868736267, tv_loss: 0.033795975148677826\n",
      "iteration 3968, dc_loss: 0.022343231365084648, tv_loss: 0.033788617700338364\n",
      "iteration 3969, dc_loss: 0.022335534915328026, tv_loss: 0.03379277512431145\n",
      "iteration 3970, dc_loss: 0.022334475070238113, tv_loss: 0.0337996631860733\n",
      "iteration 3971, dc_loss: 0.022335823625326157, tv_loss: 0.033799923956394196\n",
      "iteration 3972, dc_loss: 0.022351974621415138, tv_loss: 0.03378824517130852\n",
      "iteration 3973, dc_loss: 0.022335244342684746, tv_loss: 0.03380369767546654\n",
      "iteration 3974, dc_loss: 0.022348225116729736, tv_loss: 0.03378867730498314\n",
      "iteration 3975, dc_loss: 0.022354915738105774, tv_loss: 0.03379783779382706\n",
      "iteration 3976, dc_loss: 0.022362036630511284, tv_loss: 0.03378446400165558\n",
      "iteration 3977, dc_loss: 0.022328924387693405, tv_loss: 0.03381587564945221\n",
      "iteration 3978, dc_loss: 0.022348955273628235, tv_loss: 0.033783286809921265\n",
      "iteration 3979, dc_loss: 0.022327950224280357, tv_loss: 0.033795084804296494\n",
      "iteration 3980, dc_loss: 0.02232166938483715, tv_loss: 0.033792827278375626\n",
      "iteration 3981, dc_loss: 0.02233748883008957, tv_loss: 0.03378380089998245\n",
      "iteration 3982, dc_loss: 0.02232719026505947, tv_loss: 0.03380018472671509\n",
      "iteration 3983, dc_loss: 0.022330129519104958, tv_loss: 0.033792052417993546\n",
      "iteration 3984, dc_loss: 0.02232440747320652, tv_loss: 0.03379233554005623\n",
      "iteration 3985, dc_loss: 0.022325748577713966, tv_loss: 0.03379350155591965\n",
      "iteration 3986, dc_loss: 0.02231295220553875, tv_loss: 0.033799201250076294\n",
      "iteration 3987, dc_loss: 0.022325290367007256, tv_loss: 0.03378704562783241\n",
      "iteration 3988, dc_loss: 0.02232755348086357, tv_loss: 0.03378884866833687\n",
      "iteration 3989, dc_loss: 0.022311940789222717, tv_loss: 0.03379631042480469\n",
      "iteration 3990, dc_loss: 0.022317107766866684, tv_loss: 0.03379150852560997\n",
      "iteration 3991, dc_loss: 0.02231782302260399, tv_loss: 0.03379184007644653\n",
      "iteration 3992, dc_loss: 0.02231161668896675, tv_loss: 0.03379614278674126\n",
      "iteration 3993, dc_loss: 0.02231461927294731, tv_loss: 0.033789440989494324\n",
      "iteration 3994, dc_loss: 0.02231491170823574, tv_loss: 0.03378749266266823\n",
      "iteration 3995, dc_loss: 0.022307822480797768, tv_loss: 0.03378977254033089\n",
      "iteration 3996, dc_loss: 0.02231244370341301, tv_loss: 0.03378221392631531\n",
      "iteration 3997, dc_loss: 0.02230473980307579, tv_loss: 0.033793047070503235\n",
      "iteration 3998, dc_loss: 0.022309012711048126, tv_loss: 0.03378859534859657\n",
      "iteration 3999, dc_loss: 0.022307774052023888, tv_loss: 0.03378436341881752\n",
      "iteration 4000, dc_loss: 0.022299151867628098, tv_loss: 0.03379298746585846\n",
      "PSNR Value mt1: 35.77876077524474\n",
      "SSIM Value mt1: 0.7715722252164936\n",
      "tensor([[[6.0409e-04, 7.4484e-05]]]) tensor([[[1.2870, 1.2171]]])\n",
      "torch.Size([1, 1, 2]) torch.Size([1, 1, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| params = dict(fi: dict_keys(['net.0.linear.weight', 'net.0.linear.bias', 'net.1.linear.weight', 'net.1.linear.bias', 'net.2.linear.weight', 'net.2.linear.bias', 'net.3.linear.weight', 'net.3.linear.bias', 'net.4.linear.weight', 'net.4.linear.bias', 'net.5.weight', 'net.5.bias'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "iteration 1, dc_loss: 3.005403518676758, tv_loss: 0.0005004759877920151\n",
      "iteration 2, dc_loss: 2.8783397674560547, tv_loss: 0.0033010724000632763\n",
      "iteration 3, dc_loss: 2.8058218955993652, tv_loss: 0.005649450235068798\n",
      "iteration 4, dc_loss: 2.7543725967407227, tv_loss: 0.007261334452778101\n",
      "iteration 5, dc_loss: 2.715136766433716, tv_loss: 0.008402793668210506\n",
      "iteration 6, dc_loss: 2.6857385635375977, tv_loss: 0.009193306788802147\n",
      "iteration 7, dc_loss: 2.664520263671875, tv_loss: 0.009729486890137196\n",
      "iteration 8, dc_loss: 2.647268533706665, tv_loss: 0.010113631375133991\n",
      "iteration 9, dc_loss: 2.6308071613311768, tv_loss: 0.010357657447457314\n",
      "iteration 10, dc_loss: 2.614577054977417, tv_loss: 0.010506504215300083\n",
      "iteration 11, dc_loss: 2.5979108810424805, tv_loss: 0.010666261427104473\n",
      "iteration 12, dc_loss: 2.582097291946411, tv_loss: 0.010824855417013168\n",
      "iteration 13, dc_loss: 2.567143678665161, tv_loss: 0.010946669615805149\n",
      "iteration 14, dc_loss: 2.552508592605591, tv_loss: 0.011023906990885735\n",
      "iteration 15, dc_loss: 2.5382063388824463, tv_loss: 0.011045851744711399\n",
      "iteration 16, dc_loss: 2.524513006210327, tv_loss: 0.010988466441631317\n",
      "iteration 17, dc_loss: 2.511547088623047, tv_loss: 0.010830056853592396\n",
      "iteration 18, dc_loss: 2.4989452362060547, tv_loss: 0.010592401959002018\n",
      "iteration 19, dc_loss: 2.4861350059509277, tv_loss: 0.010356188751757145\n",
      "iteration 20, dc_loss: 2.4726688861846924, tv_loss: 0.010141187347471714\n",
      "iteration 21, dc_loss: 2.4590635299682617, tv_loss: 0.009822860360145569\n",
      "iteration 22, dc_loss: 2.4456140995025635, tv_loss: 0.009373526088893414\n",
      "iteration 23, dc_loss: 2.4321444034576416, tv_loss: 0.008904878050088882\n",
      "iteration 24, dc_loss: 2.418938398361206, tv_loss: 0.008352904580533504\n",
      "iteration 25, dc_loss: 2.4058876037597656, tv_loss: 0.007716033142060041\n",
      "iteration 26, dc_loss: 2.392751932144165, tv_loss: 0.007261931896209717\n",
      "iteration 27, dc_loss: 2.379934549331665, tv_loss: 0.007037212606519461\n",
      "iteration 28, dc_loss: 2.367453098297119, tv_loss: 0.0070050559006631374\n",
      "iteration 29, dc_loss: 2.355013132095337, tv_loss: 0.007128631696105003\n",
      "iteration 30, dc_loss: 2.342592239379883, tv_loss: 0.007321435492485762\n",
      "iteration 31, dc_loss: 2.3300955295562744, tv_loss: 0.00751302158460021\n",
      "iteration 32, dc_loss: 2.3178024291992188, tv_loss: 0.007651710417121649\n",
      "iteration 33, dc_loss: 2.305851936340332, tv_loss: 0.00773429311811924\n",
      "iteration 34, dc_loss: 2.293928623199463, tv_loss: 0.0077711972407996655\n",
      "iteration 35, dc_loss: 2.2820894718170166, tv_loss: 0.0077624088153243065\n",
      "iteration 36, dc_loss: 2.2705109119415283, tv_loss: 0.007742073852568865\n",
      "iteration 37, dc_loss: 2.259014844894409, tv_loss: 0.007750635500997305\n",
      "iteration 38, dc_loss: 2.2476377487182617, tv_loss: 0.007723807357251644\n",
      "iteration 39, dc_loss: 2.236121416091919, tv_loss: 0.00782078132033348\n",
      "iteration 40, dc_loss: 2.2245209217071533, tv_loss: 0.007823939435184002\n",
      "iteration 41, dc_loss: 2.2128746509552, tv_loss: 0.007967411540448666\n",
      "iteration 42, dc_loss: 2.2016680240631104, tv_loss: 0.00806622114032507\n",
      "iteration 43, dc_loss: 2.190800189971924, tv_loss: 0.008129526861011982\n",
      "iteration 44, dc_loss: 2.1798417568206787, tv_loss: 0.008316927589476109\n",
      "iteration 45, dc_loss: 2.1689305305480957, tv_loss: 0.008353465236723423\n",
      "iteration 46, dc_loss: 2.157749891281128, tv_loss: 0.00853655394166708\n",
      "iteration 47, dc_loss: 2.1467056274414062, tv_loss: 0.00859930831938982\n",
      "iteration 48, dc_loss: 2.135918140411377, tv_loss: 0.00870150700211525\n",
      "iteration 49, dc_loss: 2.1253373622894287, tv_loss: 0.00882942508906126\n",
      "iteration 50, dc_loss: 2.1149332523345947, tv_loss: 0.008877679705619812\n",
      "iteration 51, dc_loss: 2.104421615600586, tv_loss: 0.00905125867575407\n",
      "iteration 52, dc_loss: 2.0939207077026367, tv_loss: 0.00909622386097908\n",
      "iteration 53, dc_loss: 2.0832865238189697, tv_loss: 0.009254984557628632\n",
      "iteration 54, dc_loss: 2.072852849960327, tv_loss: 0.009327978827059269\n",
      "iteration 55, dc_loss: 2.0625970363616943, tv_loss: 0.00943055097013712\n",
      "iteration 56, dc_loss: 2.0524656772613525, tv_loss: 0.009565934538841248\n",
      "iteration 57, dc_loss: 2.042552947998047, tv_loss: 0.009625429287552834\n",
      "iteration 58, dc_loss: 2.0326385498046875, tv_loss: 0.009817948564887047\n",
      "iteration 59, dc_loss: 2.022890329360962, tv_loss: 0.00982617773115635\n",
      "iteration 60, dc_loss: 2.0129663944244385, tv_loss: 0.010039460845291615\n",
      "iteration 61, dc_loss: 2.002916097640991, tv_loss: 0.01004334632307291\n",
      "iteration 62, dc_loss: 1.9927213191986084, tv_loss: 0.010204512625932693\n",
      "iteration 63, dc_loss: 1.9828613996505737, tv_loss: 0.010287965647876263\n",
      "iteration 64, dc_loss: 1.9733631610870361, tv_loss: 0.010366247966885567\n",
      "iteration 65, dc_loss: 1.963962435722351, tv_loss: 0.010544816963374615\n",
      "iteration 66, dc_loss: 1.9546414613723755, tv_loss: 0.010569468140602112\n",
      "iteration 67, dc_loss: 1.945040225982666, tv_loss: 0.010768059641122818\n",
      "iteration 68, dc_loss: 1.9354273080825806, tv_loss: 0.010785525664687157\n",
      "iteration 69, dc_loss: 1.9258224964141846, tv_loss: 0.010919236578047276\n",
      "iteration 70, dc_loss: 1.9165265560150146, tv_loss: 0.01101002935320139\n",
      "iteration 71, dc_loss: 1.9074729681015015, tv_loss: 0.01108019519597292\n",
      "iteration 72, dc_loss: 1.8984297513961792, tv_loss: 0.011259776540100574\n",
      "iteration 73, dc_loss: 1.8894779682159424, tv_loss: 0.011291788890957832\n",
      "iteration 74, dc_loss: 1.8803437948226929, tv_loss: 0.011474397964775562\n",
      "iteration 75, dc_loss: 1.8712505102157593, tv_loss: 0.011483488604426384\n",
      "iteration 76, dc_loss: 1.8620706796646118, tv_loss: 0.011628254316747189\n",
      "iteration 77, dc_loss: 1.853108525276184, tv_loss: 0.011677813716232777\n",
      "iteration 78, dc_loss: 1.8442695140838623, tv_loss: 0.011776057071983814\n",
      "iteration 79, dc_loss: 1.8355531692504883, tv_loss: 0.011901166290044785\n",
      "iteration 80, dc_loss: 1.8270262479782104, tv_loss: 0.011984449811279774\n",
      "iteration 81, dc_loss: 1.8185871839523315, tv_loss: 0.01219018455594778\n",
      "iteration 82, dc_loss: 1.8104021549224854, tv_loss: 0.012179543264210224\n",
      "iteration 83, dc_loss: 1.8021148443222046, tv_loss: 0.01236309576779604\n",
      "iteration 84, dc_loss: 1.7935969829559326, tv_loss: 0.012319532223045826\n",
      "iteration 85, dc_loss: 1.784569501876831, tv_loss: 0.012548830360174179\n",
      "iteration 86, dc_loss: 1.7760268449783325, tv_loss: 0.012509261257946491\n",
      "iteration 87, dc_loss: 1.7678667306900024, tv_loss: 0.012583468109369278\n",
      "iteration 88, dc_loss: 1.75963294506073, tv_loss: 0.012736240401864052\n",
      "iteration 89, dc_loss: 1.7511365413665771, tv_loss: 0.012834398075938225\n",
      "iteration 90, dc_loss: 1.742692232131958, tv_loss: 0.01298576407134533\n",
      "iteration 91, dc_loss: 1.734651803970337, tv_loss: 0.012937679886817932\n",
      "iteration 92, dc_loss: 1.7263877391815186, tv_loss: 0.01300707645714283\n",
      "iteration 93, dc_loss: 1.7181439399719238, tv_loss: 0.013128429651260376\n",
      "iteration 94, dc_loss: 1.7103757858276367, tv_loss: 0.013246490620076656\n",
      "iteration 95, dc_loss: 1.7025095224380493, tv_loss: 0.013412289321422577\n",
      "iteration 96, dc_loss: 1.694434404373169, tv_loss: 0.013352216221392155\n",
      "iteration 97, dc_loss: 1.6863347291946411, tv_loss: 0.013465364463627338\n",
      "iteration 98, dc_loss: 1.6785880327224731, tv_loss: 0.013541872613132\n",
      "iteration 99, dc_loss: 1.6708968877792358, tv_loss: 0.013691895641386509\n",
      "iteration 100, dc_loss: 1.6630510091781616, tv_loss: 0.013808371499180794\n",
      "iteration 101, dc_loss: 1.6553620100021362, tv_loss: 0.013756364583969116\n",
      "iteration 102, dc_loss: 1.6475884914398193, tv_loss: 0.013928675092756748\n",
      "iteration 103, dc_loss: 1.6400529146194458, tv_loss: 0.013900280930101871\n",
      "iteration 104, dc_loss: 1.6324186325073242, tv_loss: 0.014055321924388409\n",
      "iteration 105, dc_loss: 1.625010371208191, tv_loss: 0.014080815948545933\n",
      "iteration 106, dc_loss: 1.6176027059555054, tv_loss: 0.01423618383705616\n",
      "iteration 107, dc_loss: 1.6103630065917969, tv_loss: 0.014273865148425102\n",
      "iteration 108, dc_loss: 1.6029890775680542, tv_loss: 0.014360334724187851\n",
      "iteration 109, dc_loss: 1.5954391956329346, tv_loss: 0.014419502578675747\n",
      "iteration 110, dc_loss: 1.5878472328186035, tv_loss: 0.014465061016380787\n",
      "iteration 111, dc_loss: 1.5804126262664795, tv_loss: 0.014545287005603313\n",
      "iteration 112, dc_loss: 1.5732548236846924, tv_loss: 0.014566315338015556\n",
      "iteration 113, dc_loss: 1.566192865371704, tv_loss: 0.014741674065589905\n",
      "iteration 114, dc_loss: 1.5595623254776, tv_loss: 0.014776666648685932\n",
      "iteration 115, dc_loss: 1.5531286001205444, tv_loss: 0.014948510564863682\n",
      "iteration 116, dc_loss: 1.5464223623275757, tv_loss: 0.014823324047029018\n",
      "iteration 117, dc_loss: 1.5388710498809814, tv_loss: 0.015112305991351604\n",
      "iteration 118, dc_loss: 1.5311915874481201, tv_loss: 0.014898843131959438\n",
      "iteration 119, dc_loss: 1.5241702795028687, tv_loss: 0.015012809075415134\n",
      "iteration 120, dc_loss: 1.517244577407837, tv_loss: 0.015134935267269611\n",
      "iteration 121, dc_loss: 1.5101407766342163, tv_loss: 0.015055486932396889\n",
      "iteration 122, dc_loss: 1.503316044807434, tv_loss: 0.015276466496288776\n",
      "iteration 123, dc_loss: 1.4965318441390991, tv_loss: 0.015387831255793571\n",
      "iteration 124, dc_loss: 1.4894176721572876, tv_loss: 0.015529824420809746\n",
      "iteration 125, dc_loss: 1.4827123880386353, tv_loss: 0.015440675429999828\n",
      "iteration 126, dc_loss: 1.4758938550949097, tv_loss: 0.01541148405522108\n",
      "iteration 127, dc_loss: 1.4689854383468628, tv_loss: 0.015570922754704952\n",
      "iteration 128, dc_loss: 1.462563395500183, tv_loss: 0.015731163322925568\n",
      "iteration 129, dc_loss: 1.455909013748169, tv_loss: 0.01576746068894863\n",
      "iteration 130, dc_loss: 1.449059009552002, tv_loss: 0.01587323285639286\n",
      "iteration 131, dc_loss: 1.4426453113555908, tv_loss: 0.015769928693771362\n",
      "iteration 132, dc_loss: 1.4359660148620605, tv_loss: 0.01595558412373066\n",
      "iteration 133, dc_loss: 1.429456353187561, tv_loss: 0.015959596261382103\n",
      "iteration 134, dc_loss: 1.4230118989944458, tv_loss: 0.016082387417554855\n",
      "iteration 135, dc_loss: 1.4165593385696411, tv_loss: 0.01611614041030407\n",
      "iteration 136, dc_loss: 1.4101848602294922, tv_loss: 0.016160540282726288\n",
      "iteration 137, dc_loss: 1.4037748575210571, tv_loss: 0.016232717782258987\n",
      "iteration 138, dc_loss: 1.3973842859268188, tv_loss: 0.01626322790980339\n",
      "iteration 139, dc_loss: 1.3910738229751587, tv_loss: 0.016363101080060005\n",
      "iteration 140, dc_loss: 1.3848809003829956, tv_loss: 0.016405019909143448\n",
      "iteration 141, dc_loss: 1.3785409927368164, tv_loss: 0.016453489661216736\n",
      "iteration 142, dc_loss: 1.372255802154541, tv_loss: 0.016525965183973312\n",
      "iteration 143, dc_loss: 1.3661624193191528, tv_loss: 0.016532287001609802\n",
      "iteration 144, dc_loss: 1.3599177598953247, tv_loss: 0.016642291098833084\n",
      "iteration 145, dc_loss: 1.3538119792938232, tv_loss: 0.016652898862957954\n",
      "iteration 146, dc_loss: 1.3478095531463623, tv_loss: 0.016754774376749992\n",
      "iteration 147, dc_loss: 1.3418272733688354, tv_loss: 0.016808301210403442\n",
      "iteration 148, dc_loss: 1.3357828855514526, tv_loss: 0.01689063012599945\n",
      "iteration 149, dc_loss: 1.329879641532898, tv_loss: 0.016859091818332672\n",
      "iteration 150, dc_loss: 1.3237829208374023, tv_loss: 0.017038004472851753\n",
      "iteration 151, dc_loss: 1.3179805278778076, tv_loss: 0.016911102458834648\n",
      "iteration 152, dc_loss: 1.3119581937789917, tv_loss: 0.01715726964175701\n",
      "iteration 153, dc_loss: 1.3064186573028564, tv_loss: 0.016964141279459\n",
      "iteration 154, dc_loss: 1.3004711866378784, tv_loss: 0.01731112413108349\n",
      "iteration 155, dc_loss: 1.2951616048812866, tv_loss: 0.016978729516267776\n",
      "iteration 156, dc_loss: 1.2891371250152588, tv_loss: 0.017455441877245903\n",
      "iteration 157, dc_loss: 1.2833364009857178, tv_loss: 0.017126014456152916\n",
      "iteration 158, dc_loss: 1.2771469354629517, tv_loss: 0.017361750826239586\n",
      "iteration 159, dc_loss: 1.2713886499404907, tv_loss: 0.017550088465213776\n",
      "iteration 160, dc_loss: 1.2662516832351685, tv_loss: 0.017397036775946617\n",
      "iteration 161, dc_loss: 1.2607218027114868, tv_loss: 0.01775270327925682\n",
      "iteration 162, dc_loss: 1.2554887533187866, tv_loss: 0.017436912283301353\n",
      "iteration 163, dc_loss: 1.2492820024490356, tv_loss: 0.01778622902929783\n",
      "iteration 164, dc_loss: 1.2434130907058716, tv_loss: 0.017624584957957268\n",
      "iteration 165, dc_loss: 1.2378236055374146, tv_loss: 0.017637699842453003\n",
      "iteration 166, dc_loss: 1.2323238849639893, tv_loss: 0.01785130426287651\n",
      "iteration 167, dc_loss: 1.227060079574585, tv_loss: 0.017646992579102516\n",
      "iteration 168, dc_loss: 1.221268653869629, tv_loss: 0.017935248091816902\n",
      "iteration 169, dc_loss: 1.2159792184829712, tv_loss: 0.01790730282664299\n",
      "iteration 170, dc_loss: 1.2107163667678833, tv_loss: 0.01797000877559185\n",
      "iteration 171, dc_loss: 1.2052546739578247, tv_loss: 0.018132109194993973\n",
      "iteration 172, dc_loss: 1.1999653577804565, tv_loss: 0.017973758280277252\n",
      "iteration 173, dc_loss: 1.1943162679672241, tv_loss: 0.018243536353111267\n",
      "iteration 174, dc_loss: 1.1892261505126953, tv_loss: 0.018074266612529755\n",
      "iteration 175, dc_loss: 1.1838465929031372, tv_loss: 0.018226319923996925\n",
      "iteration 176, dc_loss: 1.1785087585449219, tv_loss: 0.01825176551938057\n",
      "iteration 177, dc_loss: 1.1732226610183716, tv_loss: 0.018262552097439766\n",
      "iteration 178, dc_loss: 1.167986512184143, tv_loss: 0.01837797649204731\n",
      "iteration 179, dc_loss: 1.1628960371017456, tv_loss: 0.018409013748168945\n",
      "iteration 180, dc_loss: 1.1577332019805908, tv_loss: 0.01847030408680439\n",
      "iteration 181, dc_loss: 1.152590274810791, tv_loss: 0.018503345549106598\n",
      "iteration 182, dc_loss: 1.1474727392196655, tv_loss: 0.01852433942258358\n",
      "iteration 183, dc_loss: 1.1423008441925049, tv_loss: 0.01860795170068741\n",
      "iteration 184, dc_loss: 1.1372288465499878, tv_loss: 0.018648294731974602\n",
      "iteration 185, dc_loss: 1.1322089433670044, tv_loss: 0.01872371882200241\n",
      "iteration 186, dc_loss: 1.1272966861724854, tv_loss: 0.018708860501646996\n",
      "iteration 187, dc_loss: 1.1222162246704102, tv_loss: 0.018854817375540733\n",
      "iteration 188, dc_loss: 1.117443323135376, tv_loss: 0.018710685893893242\n",
      "iteration 189, dc_loss: 1.1123573780059814, tv_loss: 0.019039830192923546\n",
      "iteration 190, dc_loss: 1.1079797744750977, tv_loss: 0.018711093813180923\n",
      "iteration 191, dc_loss: 1.1030186414718628, tv_loss: 0.019250789657235146\n",
      "iteration 192, dc_loss: 1.0986254215240479, tv_loss: 0.01878228411078453\n",
      "iteration 193, dc_loss: 1.0934041738510132, tv_loss: 0.01925741136074066\n",
      "iteration 194, dc_loss: 1.0885562896728516, tv_loss: 0.01897464133799076\n",
      "iteration 195, dc_loss: 1.0834580659866333, tv_loss: 0.0191066712141037\n",
      "iteration 196, dc_loss: 1.0785419940948486, tv_loss: 0.019225727766752243\n",
      "iteration 197, dc_loss: 1.0740888118743896, tv_loss: 0.01907375082373619\n",
      "iteration 198, dc_loss: 1.0691462755203247, tv_loss: 0.019517652690410614\n",
      "iteration 199, dc_loss: 1.0648282766342163, tv_loss: 0.019139889627695084\n",
      "iteration 200, dc_loss: 1.0599626302719116, tv_loss: 0.019476469606161118\n",
      "iteration 201, dc_loss: 1.055393099784851, tv_loss: 0.019382232800126076\n",
      "iteration 202, dc_loss: 1.0505398511886597, tv_loss: 0.019470136612653732\n",
      "iteration 203, dc_loss: 1.045594334602356, tv_loss: 0.019566036760807037\n",
      "iteration 204, dc_loss: 1.0411442518234253, tv_loss: 0.01942955143749714\n",
      "iteration 205, dc_loss: 1.036547303199768, tv_loss: 0.01970558986067772\n",
      "iteration 206, dc_loss: 1.0321552753448486, tv_loss: 0.01950974389910698\n",
      "iteration 207, dc_loss: 1.0273120403289795, tv_loss: 0.019765079021453857\n",
      "iteration 208, dc_loss: 1.022825002670288, tv_loss: 0.01966291479766369\n",
      "iteration 209, dc_loss: 1.0183329582214355, tv_loss: 0.019778450950980186\n",
      "iteration 210, dc_loss: 1.013831377029419, tv_loss: 0.019837720319628716\n",
      "iteration 211, dc_loss: 1.009272813796997, tv_loss: 0.019802339375019073\n",
      "iteration 212, dc_loss: 1.0046441555023193, tv_loss: 0.019963892176747322\n",
      "iteration 213, dc_loss: 1.0004932880401611, tv_loss: 0.019811030477285385\n",
      "iteration 214, dc_loss: 0.996013343334198, tv_loss: 0.020048798993229866\n",
      "iteration 215, dc_loss: 0.9918909668922424, tv_loss: 0.01984354294836521\n",
      "iteration 216, dc_loss: 0.9872995615005493, tv_loss: 0.020206734538078308\n",
      "iteration 217, dc_loss: 0.9832471013069153, tv_loss: 0.019966475665569305\n",
      "iteration 218, dc_loss: 0.9786314964294434, tv_loss: 0.020286057144403458\n",
      "iteration 219, dc_loss: 0.974484920501709, tv_loss: 0.01999567449092865\n",
      "iteration 220, dc_loss: 0.9698579907417297, tv_loss: 0.02034885063767433\n",
      "iteration 221, dc_loss: 0.9659214615821838, tv_loss: 0.020075524225831032\n",
      "iteration 222, dc_loss: 0.9613738059997559, tv_loss: 0.020396249368786812\n",
      "iteration 223, dc_loss: 0.9572553634643555, tv_loss: 0.020209142938256264\n",
      "iteration 224, dc_loss: 0.95277339220047, tv_loss: 0.02047055959701538\n",
      "iteration 225, dc_loss: 0.9486726522445679, tv_loss: 0.02041606791317463\n",
      "iteration 226, dc_loss: 0.9446002244949341, tv_loss: 0.020418815314769745\n",
      "iteration 227, dc_loss: 0.9403740763664246, tv_loss: 0.020636681467294693\n",
      "iteration 228, dc_loss: 0.9367607235908508, tv_loss: 0.020349081605672836\n",
      "iteration 229, dc_loss: 0.9328663945198059, tv_loss: 0.02093450538814068\n",
      "iteration 230, dc_loss: 0.9299612045288086, tv_loss: 0.02022765390574932\n",
      "iteration 231, dc_loss: 0.9261857867240906, tv_loss: 0.021135609596967697\n",
      "iteration 232, dc_loss: 0.9214450716972351, tv_loss: 0.020411070436239243\n",
      "iteration 233, dc_loss: 0.9166246056556702, tv_loss: 0.020567724481225014\n",
      "iteration 234, dc_loss: 0.9125335216522217, tv_loss: 0.02105671353638172\n",
      "iteration 235, dc_loss: 0.9092344045639038, tv_loss: 0.020454544574022293\n",
      "iteration 236, dc_loss: 0.9046246409416199, tv_loss: 0.020970234647393227\n",
      "iteration 237, dc_loss: 0.900380551815033, tv_loss: 0.020834889262914658\n",
      "iteration 238, dc_loss: 0.8966975212097168, tv_loss: 0.020746255293488503\n",
      "iteration 239, dc_loss: 0.8925750851631165, tv_loss: 0.02118174359202385\n",
      "iteration 240, dc_loss: 0.8888165950775146, tv_loss: 0.02083803154528141\n",
      "iteration 241, dc_loss: 0.8846184015274048, tv_loss: 0.0210141409188509\n",
      "iteration 242, dc_loss: 0.880340039730072, tv_loss: 0.021135238930583\n",
      "iteration 243, dc_loss: 0.8767353892326355, tv_loss: 0.020929379388689995\n",
      "iteration 244, dc_loss: 0.8726881742477417, tv_loss: 0.021193360909819603\n",
      "iteration 245, dc_loss: 0.8687121868133545, tv_loss: 0.021113885566592216\n",
      "iteration 246, dc_loss: 0.8648070693016052, tv_loss: 0.02112760953605175\n",
      "iteration 247, dc_loss: 0.8610270023345947, tv_loss: 0.02131566032767296\n",
      "iteration 248, dc_loss: 0.8573339581489563, tv_loss: 0.02122391015291214\n",
      "iteration 249, dc_loss: 0.8532745838165283, tv_loss: 0.0213841013610363\n",
      "iteration 250, dc_loss: 0.8495832085609436, tv_loss: 0.021262764930725098\n",
      "iteration 251, dc_loss: 0.8457815051078796, tv_loss: 0.021322285756468773\n",
      "iteration 252, dc_loss: 0.8418087363243103, tv_loss: 0.021510642021894455\n",
      "iteration 253, dc_loss: 0.8384112119674683, tv_loss: 0.021293478086590767\n",
      "iteration 254, dc_loss: 0.8344690799713135, tv_loss: 0.021546723321080208\n",
      "iteration 255, dc_loss: 0.8306807279586792, tv_loss: 0.02153681218624115\n",
      "iteration 256, dc_loss: 0.8273089528083801, tv_loss: 0.02146558277308941\n",
      "iteration 257, dc_loss: 0.8234493732452393, tv_loss: 0.02171640843153\n",
      "iteration 258, dc_loss: 0.8198685646057129, tv_loss: 0.021538009867072105\n",
      "iteration 259, dc_loss: 0.8162729144096375, tv_loss: 0.021671170368790627\n",
      "iteration 260, dc_loss: 0.8124876618385315, tv_loss: 0.021695997565984726\n",
      "iteration 261, dc_loss: 0.8089806437492371, tv_loss: 0.02166789211332798\n",
      "iteration 262, dc_loss: 0.8052861094474792, tv_loss: 0.021781381219625473\n",
      "iteration 263, dc_loss: 0.8018301129341125, tv_loss: 0.021681345999240875\n",
      "iteration 264, dc_loss: 0.7984043955802917, tv_loss: 0.02192900888621807\n",
      "iteration 265, dc_loss: 0.795011579990387, tv_loss: 0.02180948667228222\n",
      "iteration 266, dc_loss: 0.7916921973228455, tv_loss: 0.02196485921740532\n",
      "iteration 267, dc_loss: 0.7885310649871826, tv_loss: 0.021747509017586708\n",
      "iteration 268, dc_loss: 0.7852965593338013, tv_loss: 0.02204534411430359\n",
      "iteration 269, dc_loss: 0.782201886177063, tv_loss: 0.021759742870926857\n",
      "iteration 270, dc_loss: 0.7782312631607056, tv_loss: 0.022049522027373314\n",
      "iteration 271, dc_loss: 0.7741706371307373, tv_loss: 0.022229250520467758\n",
      "iteration 272, dc_loss: 0.7703126668930054, tv_loss: 0.02203483134508133\n",
      "iteration 273, dc_loss: 0.766907811164856, tv_loss: 0.022058259695768356\n",
      "iteration 274, dc_loss: 0.7637953162193298, tv_loss: 0.02209249697625637\n",
      "iteration 275, dc_loss: 0.7604510188102722, tv_loss: 0.022200696170330048\n",
      "iteration 276, dc_loss: 0.7568337321281433, tv_loss: 0.02219722606241703\n",
      "iteration 277, dc_loss: 0.7530860304832458, tv_loss: 0.022300366312265396\n",
      "iteration 278, dc_loss: 0.7499779462814331, tv_loss: 0.022241337224841118\n",
      "iteration 279, dc_loss: 0.7468104362487793, tv_loss: 0.02239340916275978\n",
      "iteration 280, dc_loss: 0.7436285614967346, tv_loss: 0.022421635687351227\n",
      "iteration 281, dc_loss: 0.7402727007865906, tv_loss: 0.022303881123661995\n",
      "iteration 282, dc_loss: 0.7366854548454285, tv_loss: 0.02264772541821003\n",
      "iteration 283, dc_loss: 0.7340108752250671, tv_loss: 0.022217649966478348\n",
      "iteration 284, dc_loss: 0.7304501533508301, tv_loss: 0.022821558639407158\n",
      "iteration 285, dc_loss: 0.7273252606391907, tv_loss: 0.022308122366666794\n",
      "iteration 286, dc_loss: 0.7234156727790833, tv_loss: 0.022626884281635284\n",
      "iteration 287, dc_loss: 0.7200124859809875, tv_loss: 0.022639427334070206\n",
      "iteration 288, dc_loss: 0.717269778251648, tv_loss: 0.022460486739873886\n",
      "iteration 289, dc_loss: 0.7137654423713684, tv_loss: 0.02277315780520439\n",
      "iteration 290, dc_loss: 0.7106868028640747, tv_loss: 0.022529657930135727\n",
      "iteration 291, dc_loss: 0.7071821093559265, tv_loss: 0.02278107777237892\n",
      "iteration 292, dc_loss: 0.7039965391159058, tv_loss: 0.022799644619226456\n",
      "iteration 293, dc_loss: 0.7011311650276184, tv_loss: 0.022651707753539085\n",
      "iteration 294, dc_loss: 0.6976535320281982, tv_loss: 0.022867100313305855\n",
      "iteration 295, dc_loss: 0.6944730281829834, tv_loss: 0.02278945781290531\n",
      "iteration 296, dc_loss: 0.6911917924880981, tv_loss: 0.02284979447722435\n",
      "iteration 297, dc_loss: 0.6881709694862366, tv_loss: 0.02289765514433384\n",
      "iteration 298, dc_loss: 0.6852702498435974, tv_loss: 0.02297462709248066\n",
      "iteration 299, dc_loss: 0.6826455593109131, tv_loss: 0.022832218557596207\n",
      "iteration 300, dc_loss: 0.6803318858146667, tv_loss: 0.023265346884727478\n",
      "iteration 301, dc_loss: 0.6776609420776367, tv_loss: 0.02266945131123066\n",
      "iteration 302, dc_loss: 0.674042284488678, tv_loss: 0.023355398327112198\n",
      "iteration 303, dc_loss: 0.6700510382652283, tv_loss: 0.022838685661554337\n",
      "iteration 304, dc_loss: 0.6671869158744812, tv_loss: 0.023069478571414948\n",
      "iteration 305, dc_loss: 0.6641255021095276, tv_loss: 0.023292237892746925\n",
      "iteration 306, dc_loss: 0.6616090536117554, tv_loss: 0.02281114086508751\n",
      "iteration 307, dc_loss: 0.6575706601142883, tv_loss: 0.023304156959056854\n",
      "iteration 308, dc_loss: 0.6544464230537415, tv_loss: 0.023115286603569984\n",
      "iteration 309, dc_loss: 0.6516391634941101, tv_loss: 0.023130930960178375\n",
      "iteration 310, dc_loss: 0.6487203240394592, tv_loss: 0.023507675155997276\n",
      "iteration 311, dc_loss: 0.6459830403327942, tv_loss: 0.023112649098038673\n",
      "iteration 312, dc_loss: 0.642372190952301, tv_loss: 0.023349814116954803\n",
      "iteration 313, dc_loss: 0.6393250823020935, tv_loss: 0.023419149219989777\n",
      "iteration 314, dc_loss: 0.6367491483688354, tv_loss: 0.023242143914103508\n",
      "iteration 315, dc_loss: 0.6335317492485046, tv_loss: 0.023529548197984695\n",
      "iteration 316, dc_loss: 0.6308183670043945, tv_loss: 0.023358158767223358\n",
      "iteration 317, dc_loss: 0.6277329325675964, tv_loss: 0.023436710238456726\n",
      "iteration 318, dc_loss: 0.6248636841773987, tv_loss: 0.02367229573428631\n",
      "iteration 319, dc_loss: 0.6223931312561035, tv_loss: 0.023392561823129654\n",
      "iteration 320, dc_loss: 0.6191378831863403, tv_loss: 0.0237012580037117\n",
      "iteration 321, dc_loss: 0.6163160800933838, tv_loss: 0.023591574281454086\n",
      "iteration 322, dc_loss: 0.6134241819381714, tv_loss: 0.023580016568303108\n",
      "iteration 323, dc_loss: 0.6105259656906128, tv_loss: 0.02374640479683876\n",
      "iteration 324, dc_loss: 0.6079265475273132, tv_loss: 0.02359158731997013\n",
      "iteration 325, dc_loss: 0.604989230632782, tv_loss: 0.02381335198879242\n",
      "iteration 326, dc_loss: 0.6023343205451965, tv_loss: 0.023737406358122826\n",
      "iteration 327, dc_loss: 0.5994742512702942, tv_loss: 0.023798972368240356\n",
      "iteration 328, dc_loss: 0.59670090675354, tv_loss: 0.023863179609179497\n",
      "iteration 329, dc_loss: 0.5940899848937988, tv_loss: 0.023814747110009193\n",
      "iteration 330, dc_loss: 0.5913213491439819, tv_loss: 0.02398848906159401\n",
      "iteration 331, dc_loss: 0.5888280868530273, tv_loss: 0.023849770426750183\n",
      "iteration 332, dc_loss: 0.5860340595245361, tv_loss: 0.02404179610311985\n",
      "iteration 333, dc_loss: 0.5835445523262024, tv_loss: 0.02389807254076004\n",
      "iteration 334, dc_loss: 0.5807607173919678, tv_loss: 0.024135148152709007\n",
      "iteration 335, dc_loss: 0.5783073306083679, tv_loss: 0.023953258991241455\n",
      "iteration 336, dc_loss: 0.5755352973937988, tv_loss: 0.024167250841856003\n",
      "iteration 337, dc_loss: 0.5730807185173035, tv_loss: 0.02405879832804203\n",
      "iteration 338, dc_loss: 0.5704137682914734, tv_loss: 0.024190548807382584\n",
      "iteration 339, dc_loss: 0.5678612589836121, tv_loss: 0.0241532064974308\n",
      "iteration 340, dc_loss: 0.5651803612709045, tv_loss: 0.02422359026968479\n",
      "iteration 341, dc_loss: 0.5625304579734802, tv_loss: 0.02422931231558323\n",
      "iteration 342, dc_loss: 0.5599719285964966, tv_loss: 0.024260401725769043\n",
      "iteration 343, dc_loss: 0.5574299693107605, tv_loss: 0.024326348677277565\n",
      "iteration 344, dc_loss: 0.5549740195274353, tv_loss: 0.024337420240044594\n",
      "iteration 345, dc_loss: 0.5525100827217102, tv_loss: 0.024335656315088272\n",
      "iteration 346, dc_loss: 0.5499183535575867, tv_loss: 0.024482423439621925\n",
      "iteration 347, dc_loss: 0.5476990342140198, tv_loss: 0.02428765967488289\n",
      "iteration 348, dc_loss: 0.5451545119285583, tv_loss: 0.024690520018339157\n",
      "iteration 349, dc_loss: 0.543549120426178, tv_loss: 0.024171216413378716\n",
      "iteration 350, dc_loss: 0.5413485169410706, tv_loss: 0.024914033710956573\n",
      "iteration 351, dc_loss: 0.5394159555435181, tv_loss: 0.024125024676322937\n",
      "iteration 352, dc_loss: 0.5365748405456543, tv_loss: 0.02476181462407112\n",
      "iteration 353, dc_loss: 0.5338945388793945, tv_loss: 0.024477267637848854\n",
      "iteration 354, dc_loss: 0.5317359566688538, tv_loss: 0.024665959179401398\n",
      "iteration 355, dc_loss: 0.5288676619529724, tv_loss: 0.02487531304359436\n",
      "iteration 356, dc_loss: 0.5271655917167664, tv_loss: 0.024349957704544067\n",
      "iteration 357, dc_loss: 0.5244577527046204, tv_loss: 0.024869130924344063\n",
      "iteration 358, dc_loss: 0.5224023461341858, tv_loss: 0.024670826271176338\n",
      "iteration 359, dc_loss: 0.519784688949585, tv_loss: 0.024769920855760574\n",
      "iteration 360, dc_loss: 0.5177140235900879, tv_loss: 0.02493409253656864\n",
      "iteration 361, dc_loss: 0.5148301124572754, tv_loss: 0.02469697967171669\n",
      "iteration 362, dc_loss: 0.5126986503601074, tv_loss: 0.02477852813899517\n",
      "iteration 363, dc_loss: 0.510002076625824, tv_loss: 0.024893125519156456\n",
      "iteration 364, dc_loss: 0.507951557636261, tv_loss: 0.024724485352635384\n",
      "iteration 365, dc_loss: 0.5053267478942871, tv_loss: 0.025200840085744858\n",
      "iteration 366, dc_loss: 0.5029956102371216, tv_loss: 0.02491598017513752\n",
      "iteration 367, dc_loss: 0.5009722709655762, tv_loss: 0.02482862025499344\n",
      "iteration 368, dc_loss: 0.49837344884872437, tv_loss: 0.025067027658224106\n",
      "iteration 369, dc_loss: 0.4962204694747925, tv_loss: 0.02489040605723858\n",
      "iteration 370, dc_loss: 0.4939211905002594, tv_loss: 0.02511381171643734\n",
      "iteration 371, dc_loss: 0.49159955978393555, tv_loss: 0.025305695831775665\n",
      "iteration 372, dc_loss: 0.48956775665283203, tv_loss: 0.024994347244501114\n",
      "iteration 373, dc_loss: 0.48713505268096924, tv_loss: 0.025187185034155846\n",
      "iteration 374, dc_loss: 0.4850517809391022, tv_loss: 0.02539362758398056\n",
      "iteration 375, dc_loss: 0.4827711582183838, tv_loss: 0.02520626224577427\n",
      "iteration 376, dc_loss: 0.48065873980522156, tv_loss: 0.025352872908115387\n",
      "iteration 377, dc_loss: 0.4786146879196167, tv_loss: 0.025355946272611618\n",
      "iteration 378, dc_loss: 0.4762111008167267, tv_loss: 0.02535405568778515\n",
      "iteration 379, dc_loss: 0.47421756386756897, tv_loss: 0.025407548993825912\n",
      "iteration 380, dc_loss: 0.47199442982673645, tv_loss: 0.025430817157030106\n",
      "iteration 381, dc_loss: 0.46988657116889954, tv_loss: 0.025336388498544693\n",
      "iteration 382, dc_loss: 0.4678472876548767, tv_loss: 0.02538742497563362\n",
      "iteration 383, dc_loss: 0.4657238721847534, tv_loss: 0.025497032329440117\n",
      "iteration 384, dc_loss: 0.4635680317878723, tv_loss: 0.02548026479780674\n",
      "iteration 385, dc_loss: 0.4615377187728882, tv_loss: 0.025392435491085052\n",
      "iteration 386, dc_loss: 0.45939838886260986, tv_loss: 0.025628473609685898\n",
      "iteration 387, dc_loss: 0.4574820399284363, tv_loss: 0.025553368031978607\n",
      "iteration 388, dc_loss: 0.45532315969467163, tv_loss: 0.025637025013566017\n",
      "iteration 389, dc_loss: 0.45359548926353455, tv_loss: 0.025393741205334663\n",
      "iteration 390, dc_loss: 0.45150187611579895, tv_loss: 0.02572924830019474\n",
      "iteration 391, dc_loss: 0.4501720666885376, tv_loss: 0.025348583236336708\n",
      "iteration 392, dc_loss: 0.4484599232673645, tv_loss: 0.02595681883394718\n",
      "iteration 393, dc_loss: 0.44686785340309143, tv_loss: 0.025479601696133614\n",
      "iteration 394, dc_loss: 0.444295734167099, tv_loss: 0.025960804894566536\n",
      "iteration 395, dc_loss: 0.44193413853645325, tv_loss: 0.025599125772714615\n",
      "iteration 396, dc_loss: 0.43955230712890625, tv_loss: 0.02569705992937088\n",
      "iteration 397, dc_loss: 0.43755772709846497, tv_loss: 0.02585620805621147\n",
      "iteration 398, dc_loss: 0.4360470473766327, tv_loss: 0.025759398937225342\n",
      "iteration 399, dc_loss: 0.4339854419231415, tv_loss: 0.026078056544065475\n",
      "iteration 400, dc_loss: 0.4322294294834137, tv_loss: 0.025609353557229042\n",
      "iteration 401, dc_loss: 0.42973411083221436, tv_loss: 0.025962449610233307\n",
      "iteration 402, dc_loss: 0.42808762192726135, tv_loss: 0.025821944698691368\n",
      "iteration 403, dc_loss: 0.42685121297836304, tv_loss: 0.025711778551340103\n",
      "iteration 404, dc_loss: 0.4250861704349518, tv_loss: 0.026023756712675095\n",
      "iteration 405, dc_loss: 0.42349863052368164, tv_loss: 0.025938358157873154\n",
      "iteration 406, dc_loss: 0.4222545027732849, tv_loss: 0.025883717462420464\n",
      "iteration 407, dc_loss: 0.4205358028411865, tv_loss: 0.026089150458574295\n",
      "iteration 408, dc_loss: 0.41899433732032776, tv_loss: 0.025936974212527275\n",
      "iteration 409, dc_loss: 0.4177263677120209, tv_loss: 0.025867335498332977\n",
      "iteration 410, dc_loss: 0.41603124141693115, tv_loss: 0.0261674951761961\n",
      "iteration 411, dc_loss: 0.4146146774291992, tv_loss: 0.02605346590280533\n",
      "iteration 412, dc_loss: 0.41322171688079834, tv_loss: 0.026023996993899345\n",
      "iteration 413, dc_loss: 0.4116002917289734, tv_loss: 0.02620258927345276\n",
      "iteration 414, dc_loss: 0.41023755073547363, tv_loss: 0.026007073000073433\n",
      "iteration 415, dc_loss: 0.40877896547317505, tv_loss: 0.026066644117236137\n",
      "iteration 416, dc_loss: 0.40720894932746887, tv_loss: 0.0263753030449152\n",
      "iteration 417, dc_loss: 0.40593817830085754, tv_loss: 0.026066500693559647\n",
      "iteration 418, dc_loss: 0.4043922424316406, tv_loss: 0.026114534586668015\n",
      "iteration 419, dc_loss: 0.4028787314891815, tv_loss: 0.02624782733619213\n",
      "iteration 420, dc_loss: 0.40162205696105957, tv_loss: 0.026104869320988655\n",
      "iteration 421, dc_loss: 0.4001004695892334, tv_loss: 0.026280514895915985\n",
      "iteration 422, dc_loss: 0.39863526821136475, tv_loss: 0.026417572051286697\n",
      "iteration 423, dc_loss: 0.39733970165252686, tv_loss: 0.026192205026745796\n",
      "iteration 424, dc_loss: 0.3958355784416199, tv_loss: 0.026287443935871124\n",
      "iteration 425, dc_loss: 0.3944656252861023, tv_loss: 0.02629271149635315\n",
      "iteration 426, dc_loss: 0.3931054472923279, tv_loss: 0.02643416076898575\n",
      "iteration 427, dc_loss: 0.3916378617286682, tv_loss: 0.02639980986714363\n",
      "iteration 428, dc_loss: 0.3903321623802185, tv_loss: 0.026300035417079926\n",
      "iteration 429, dc_loss: 0.38892608880996704, tv_loss: 0.026394430547952652\n",
      "iteration 430, dc_loss: 0.3875178396701813, tv_loss: 0.026595599949359894\n",
      "iteration 431, dc_loss: 0.3862190544605255, tv_loss: 0.026372704654932022\n",
      "iteration 432, dc_loss: 0.3848249018192291, tv_loss: 0.02651360258460045\n",
      "iteration 433, dc_loss: 0.38348618149757385, tv_loss: 0.02666981890797615\n",
      "iteration 434, dc_loss: 0.3821098208427429, tv_loss: 0.026477541774511337\n",
      "iteration 435, dc_loss: 0.3807893693447113, tv_loss: 0.026683460921049118\n",
      "iteration 436, dc_loss: 0.3794597387313843, tv_loss: 0.026536790654063225\n",
      "iteration 437, dc_loss: 0.3780786991119385, tv_loss: 0.026563383638858795\n",
      "iteration 438, dc_loss: 0.37683114409446716, tv_loss: 0.02674851194024086\n",
      "iteration 439, dc_loss: 0.37545621395111084, tv_loss: 0.026556164026260376\n",
      "iteration 440, dc_loss: 0.3741413950920105, tv_loss: 0.026817748323082924\n",
      "iteration 441, dc_loss: 0.3728390634059906, tv_loss: 0.026664480566978455\n",
      "iteration 442, dc_loss: 0.3715129494667053, tv_loss: 0.026615740731358528\n",
      "iteration 443, dc_loss: 0.37023478746414185, tv_loss: 0.02667233720421791\n",
      "iteration 444, dc_loss: 0.3689296543598175, tv_loss: 0.026777736842632294\n",
      "iteration 445, dc_loss: 0.36760666966438293, tv_loss: 0.026699015870690346\n",
      "iteration 446, dc_loss: 0.36631742119789124, tv_loss: 0.026691440492868423\n",
      "iteration 447, dc_loss: 0.36506569385528564, tv_loss: 0.026833008974790573\n",
      "iteration 448, dc_loss: 0.36375322937965393, tv_loss: 0.026813935488462448\n",
      "iteration 449, dc_loss: 0.3624701201915741, tv_loss: 0.026754774153232574\n",
      "iteration 450, dc_loss: 0.3612436354160309, tv_loss: 0.026793383061885834\n",
      "iteration 451, dc_loss: 0.3599274754524231, tv_loss: 0.02693070098757744\n",
      "iteration 452, dc_loss: 0.3586784303188324, tv_loss: 0.026847077533602715\n",
      "iteration 453, dc_loss: 0.3574293851852417, tv_loss: 0.02682088129222393\n",
      "iteration 454, dc_loss: 0.356170654296875, tv_loss: 0.026979604735970497\n",
      "iteration 455, dc_loss: 0.3549632430076599, tv_loss: 0.02688329853117466\n",
      "iteration 456, dc_loss: 0.3537061810493469, tv_loss: 0.026858584955334663\n",
      "iteration 457, dc_loss: 0.3524627387523651, tv_loss: 0.026912467554211617\n",
      "iteration 458, dc_loss: 0.3512111008167267, tv_loss: 0.02705296128988266\n",
      "iteration 459, dc_loss: 0.35001853108406067, tv_loss: 0.026953069493174553\n",
      "iteration 460, dc_loss: 0.3487361967563629, tv_loss: 0.026989469304680824\n",
      "iteration 461, dc_loss: 0.3476111590862274, tv_loss: 0.026994556188583374\n",
      "iteration 462, dc_loss: 0.34632986783981323, tv_loss: 0.027138378471136093\n",
      "iteration 463, dc_loss: 0.34514063596725464, tv_loss: 0.02702534943819046\n",
      "iteration 464, dc_loss: 0.3439435064792633, tv_loss: 0.02704976685345173\n",
      "iteration 465, dc_loss: 0.3427513539791107, tv_loss: 0.027179254218935966\n",
      "iteration 466, dc_loss: 0.34150880575180054, tv_loss: 0.027119802311062813\n",
      "iteration 467, dc_loss: 0.34038665890693665, tv_loss: 0.027048377320170403\n",
      "iteration 468, dc_loss: 0.33913934230804443, tv_loss: 0.027244411408901215\n",
      "iteration 469, dc_loss: 0.3380158543586731, tv_loss: 0.027127735316753387\n",
      "iteration 470, dc_loss: 0.3367888629436493, tv_loss: 0.027140280231833458\n",
      "iteration 471, dc_loss: 0.33566269278526306, tv_loss: 0.027107587084174156\n",
      "iteration 472, dc_loss: 0.33445656299591064, tv_loss: 0.027187442407011986\n",
      "iteration 473, dc_loss: 0.3334084749221802, tv_loss: 0.027188360691070557\n",
      "iteration 474, dc_loss: 0.33219972252845764, tv_loss: 0.02738157846033573\n",
      "iteration 475, dc_loss: 0.3313612937927246, tv_loss: 0.02709774486720562\n",
      "iteration 476, dc_loss: 0.3301549553871155, tv_loss: 0.027418099343776703\n",
      "iteration 477, dc_loss: 0.3298102021217346, tv_loss: 0.02707601897418499\n",
      "iteration 478, dc_loss: 0.328960657119751, tv_loss: 0.02772459201514721\n",
      "iteration 479, dc_loss: 0.32818925380706787, tv_loss: 0.02703140117228031\n",
      "iteration 480, dc_loss: 0.3264233469963074, tv_loss: 0.027541957795619965\n",
      "iteration 481, dc_loss: 0.32458487153053284, tv_loss: 0.027432510629296303\n",
      "iteration 482, dc_loss: 0.323382705450058, tv_loss: 0.027209240943193436\n",
      "iteration 483, dc_loss: 0.32251641154289246, tv_loss: 0.02765880525112152\n",
      "iteration 484, dc_loss: 0.3218713104724884, tv_loss: 0.027317818254232407\n",
      "iteration 485, dc_loss: 0.31991708278656006, tv_loss: 0.02760079875588417\n",
      "iteration 486, dc_loss: 0.3187344968318939, tv_loss: 0.027579640969634056\n",
      "iteration 487, dc_loss: 0.318043053150177, tv_loss: 0.02726837620139122\n",
      "iteration 488, dc_loss: 0.3167625069618225, tv_loss: 0.027719974517822266\n",
      "iteration 489, dc_loss: 0.31589362025260925, tv_loss: 0.0273683350533247\n",
      "iteration 490, dc_loss: 0.31427761912345886, tv_loss: 0.027550779283046722\n",
      "iteration 491, dc_loss: 0.31314393877983093, tv_loss: 0.027604110538959503\n",
      "iteration 492, dc_loss: 0.3125874400138855, tv_loss: 0.02730668894946575\n",
      "iteration 493, dc_loss: 0.31111401319503784, tv_loss: 0.027799196541309357\n",
      "iteration 494, dc_loss: 0.31003832817077637, tv_loss: 0.02749870903789997\n",
      "iteration 495, dc_loss: 0.30897536873817444, tv_loss: 0.027454590424895287\n",
      "iteration 496, dc_loss: 0.30785486102104187, tv_loss: 0.027690712362527847\n",
      "iteration 497, dc_loss: 0.30707794427871704, tv_loss: 0.027369989082217216\n",
      "iteration 498, dc_loss: 0.3056236803531647, tv_loss: 0.02765565738081932\n",
      "iteration 499, dc_loss: 0.3045814037322998, tv_loss: 0.02767406404018402\n",
      "iteration 500, dc_loss: 0.3037645220756531, tv_loss: 0.027601905167102814\n",
      "iteration 501, dc_loss: 0.3024924099445343, tv_loss: 0.02781040221452713\n",
      "iteration 502, dc_loss: 0.30163657665252686, tv_loss: 0.027541520074009895\n",
      "iteration 503, dc_loss: 0.300455242395401, tv_loss: 0.027634914964437485\n",
      "iteration 504, dc_loss: 0.29932910203933716, tv_loss: 0.027738837525248528\n",
      "iteration 505, dc_loss: 0.2985309660434723, tv_loss: 0.027666185051202774\n",
      "iteration 506, dc_loss: 0.2973223626613617, tv_loss: 0.02795039676129818\n",
      "iteration 507, dc_loss: 0.29637661576271057, tv_loss: 0.027677392587065697\n",
      "iteration 508, dc_loss: 0.29530709981918335, tv_loss: 0.02775443345308304\n",
      "iteration 509, dc_loss: 0.2942720949649811, tv_loss: 0.028041528537869453\n",
      "iteration 510, dc_loss: 0.29335111379623413, tv_loss: 0.027728796005249023\n",
      "iteration 511, dc_loss: 0.2922866642475128, tv_loss: 0.028071898967027664\n",
      "iteration 512, dc_loss: 0.2913508713245392, tv_loss: 0.027752693742513657\n",
      "iteration 513, dc_loss: 0.2902000844478607, tv_loss: 0.027871448546648026\n",
      "iteration 514, dc_loss: 0.2892472445964813, tv_loss: 0.0280101727694273\n",
      "iteration 515, dc_loss: 0.28833916783332825, tv_loss: 0.02784297615289688\n",
      "iteration 516, dc_loss: 0.2871938645839691, tv_loss: 0.027978932484984398\n",
      "iteration 517, dc_loss: 0.2864517867565155, tv_loss: 0.027762066572904587\n",
      "iteration 518, dc_loss: 0.2852795422077179, tv_loss: 0.028031421825289726\n",
      "iteration 519, dc_loss: 0.2843787372112274, tv_loss: 0.02802763134241104\n",
      "iteration 520, dc_loss: 0.2834078371524811, tv_loss: 0.027926437556743622\n",
      "iteration 521, dc_loss: 0.28237125277519226, tv_loss: 0.02801978774368763\n",
      "iteration 522, dc_loss: 0.28156760334968567, tv_loss: 0.02806585282087326\n",
      "iteration 523, dc_loss: 0.2805180847644806, tv_loss: 0.02805512212216854\n",
      "iteration 524, dc_loss: 0.2795754373073578, tv_loss: 0.02800724282860756\n",
      "iteration 525, dc_loss: 0.2786467671394348, tv_loss: 0.028052112087607384\n",
      "iteration 526, dc_loss: 0.27768653631210327, tv_loss: 0.028172750025987625\n",
      "iteration 527, dc_loss: 0.2768368124961853, tv_loss: 0.028051065281033516\n",
      "iteration 528, dc_loss: 0.27588769793510437, tv_loss: 0.02813107892870903\n",
      "iteration 529, dc_loss: 0.2751747667789459, tv_loss: 0.02804940566420555\n",
      "iteration 530, dc_loss: 0.27433204650878906, tv_loss: 0.02831677533686161\n",
      "iteration 531, dc_loss: 0.27364063262939453, tv_loss: 0.028064122423529625\n",
      "iteration 532, dc_loss: 0.2726829946041107, tv_loss: 0.028246823698282242\n",
      "iteration 533, dc_loss: 0.27175238728523254, tv_loss: 0.028243569657206535\n",
      "iteration 534, dc_loss: 0.27065640687942505, tv_loss: 0.028191139921545982\n",
      "iteration 535, dc_loss: 0.269368439912796, tv_loss: 0.028278473764657974\n",
      "iteration 536, dc_loss: 0.2685371935367584, tv_loss: 0.02813875675201416\n",
      "iteration 537, dc_loss: 0.26745977997779846, tv_loss: 0.028392555192112923\n",
      "iteration 538, dc_loss: 0.26685193181037903, tv_loss: 0.028134401887655258\n",
      "iteration 539, dc_loss: 0.265790194272995, tv_loss: 0.02833896316587925\n",
      "iteration 540, dc_loss: 0.26498499512672424, tv_loss: 0.028168903663754463\n",
      "iteration 541, dc_loss: 0.2639335095882416, tv_loss: 0.02830631472170353\n",
      "iteration 542, dc_loss: 0.2629075348377228, tv_loss: 0.028408106416463852\n",
      "iteration 543, dc_loss: 0.26217103004455566, tv_loss: 0.02819182351231575\n",
      "iteration 544, dc_loss: 0.26109468936920166, tv_loss: 0.028419354930520058\n",
      "iteration 545, dc_loss: 0.26055389642715454, tv_loss: 0.02818816527724266\n",
      "iteration 546, dc_loss: 0.25946640968322754, tv_loss: 0.02859051339328289\n",
      "iteration 547, dc_loss: 0.25863558053970337, tv_loss: 0.028312556445598602\n",
      "iteration 548, dc_loss: 0.25755393505096436, tv_loss: 0.028394317254424095\n",
      "iteration 549, dc_loss: 0.256665974855423, tv_loss: 0.028563451021909714\n",
      "iteration 550, dc_loss: 0.255980908870697, tv_loss: 0.02832420915365219\n",
      "iteration 551, dc_loss: 0.2549467086791992, tv_loss: 0.028569037094712257\n",
      "iteration 552, dc_loss: 0.254426509141922, tv_loss: 0.028317946940660477\n",
      "iteration 553, dc_loss: 0.253266841173172, tv_loss: 0.0286758653819561\n",
      "iteration 554, dc_loss: 0.2525809109210968, tv_loss: 0.02839481085538864\n",
      "iteration 555, dc_loss: 0.25153928995132446, tv_loss: 0.02851025015115738\n",
      "iteration 556, dc_loss: 0.25073546171188354, tv_loss: 0.028478870168328285\n",
      "iteration 557, dc_loss: 0.2498699277639389, tv_loss: 0.02862050011754036\n",
      "iteration 558, dc_loss: 0.24896985292434692, tv_loss: 0.02859647013247013\n",
      "iteration 559, dc_loss: 0.24821333587169647, tv_loss: 0.02854226343333721\n",
      "iteration 560, dc_loss: 0.24733436107635498, tv_loss: 0.02873840555548668\n",
      "iteration 561, dc_loss: 0.2466878443956375, tv_loss: 0.028481682762503624\n",
      "iteration 562, dc_loss: 0.24563264846801758, tv_loss: 0.02872697450220585\n",
      "iteration 563, dc_loss: 0.24511189758777618, tv_loss: 0.028535209596157074\n",
      "iteration 564, dc_loss: 0.24404381215572357, tv_loss: 0.028756840154528618\n",
      "iteration 565, dc_loss: 0.24349108338356018, tv_loss: 0.028494469821453094\n",
      "iteration 566, dc_loss: 0.24251551926136017, tv_loss: 0.028806516900658607\n",
      "iteration 567, dc_loss: 0.24199044704437256, tv_loss: 0.028579246252775192\n",
      "iteration 568, dc_loss: 0.2409999817609787, tv_loss: 0.028845714405179024\n",
      "iteration 569, dc_loss: 0.24035313725471497, tv_loss: 0.028528008610010147\n",
      "iteration 570, dc_loss: 0.23930490016937256, tv_loss: 0.028805335983633995\n",
      "iteration 571, dc_loss: 0.23857061564922333, tv_loss: 0.028769314289093018\n",
      "iteration 572, dc_loss: 0.23777638375759125, tv_loss: 0.02873106673359871\n",
      "iteration 573, dc_loss: 0.2369116246700287, tv_loss: 0.028811048716306686\n",
      "iteration 574, dc_loss: 0.23640459775924683, tv_loss: 0.028677450492978096\n",
      "iteration 575, dc_loss: 0.23545178771018982, tv_loss: 0.028955016285181046\n",
      "iteration 576, dc_loss: 0.234844371676445, tv_loss: 0.028656961396336555\n",
      "iteration 577, dc_loss: 0.2336849570274353, tv_loss: 0.028911011293530464\n",
      "iteration 578, dc_loss: 0.23303820192813873, tv_loss: 0.028751330450177193\n",
      "iteration 579, dc_loss: 0.23209773004055023, tv_loss: 0.028970517218112946\n",
      "iteration 580, dc_loss: 0.23148280382156372, tv_loss: 0.028805451467633247\n",
      "iteration 581, dc_loss: 0.23068206012248993, tv_loss: 0.028861667960882187\n",
      "iteration 582, dc_loss: 0.22999168932437897, tv_loss: 0.028902851045131683\n",
      "iteration 583, dc_loss: 0.22918111085891724, tv_loss: 0.02895674668252468\n",
      "iteration 584, dc_loss: 0.2283477485179901, tv_loss: 0.02895110845565796\n",
      "iteration 585, dc_loss: 0.22757165133953094, tv_loss: 0.028911355882883072\n",
      "iteration 586, dc_loss: 0.22680692374706268, tv_loss: 0.028923703357577324\n",
      "iteration 587, dc_loss: 0.22597308456897736, tv_loss: 0.02903478778898716\n",
      "iteration 588, dc_loss: 0.2253645658493042, tv_loss: 0.028881333768367767\n",
      "iteration 589, dc_loss: 0.2244812250137329, tv_loss: 0.029068704694509506\n",
      "iteration 590, dc_loss: 0.22403140366077423, tv_loss: 0.02886488102376461\n",
      "iteration 591, dc_loss: 0.22311870753765106, tv_loss: 0.029176142066717148\n",
      "iteration 592, dc_loss: 0.222617506980896, tv_loss: 0.028905192390084267\n",
      "iteration 593, dc_loss: 0.22180026769638062, tv_loss: 0.02913028560578823\n",
      "iteration 594, dc_loss: 0.22147144377231598, tv_loss: 0.02896123193204403\n",
      "iteration 595, dc_loss: 0.22098106145858765, tv_loss: 0.029219303280115128\n",
      "iteration 596, dc_loss: 0.22082076966762543, tv_loss: 0.028894083574414253\n",
      "iteration 597, dc_loss: 0.22019505500793457, tv_loss: 0.029356781393289566\n",
      "iteration 598, dc_loss: 0.21941344439983368, tv_loss: 0.028869152069091797\n",
      "iteration 599, dc_loss: 0.21760372817516327, tv_loss: 0.029277732595801353\n",
      "iteration 600, dc_loss: 0.21682694554328918, tv_loss: 0.02893228456377983\n",
      "iteration 601, dc_loss: 0.2162448614835739, tv_loss: 0.029212910681962967\n",
      "iteration 602, dc_loss: 0.21597230434417725, tv_loss: 0.029155846685171127\n",
      "iteration 603, dc_loss: 0.2149246633052826, tv_loss: 0.02921123243868351\n",
      "iteration 604, dc_loss: 0.2141738086938858, tv_loss: 0.029095662757754326\n",
      "iteration 605, dc_loss: 0.21332359313964844, tv_loss: 0.029252154752612114\n",
      "iteration 606, dc_loss: 0.2126792073249817, tv_loss: 0.02916235662996769\n",
      "iteration 607, dc_loss: 0.21182484924793243, tv_loss: 0.029229197651147842\n",
      "iteration 608, dc_loss: 0.2112085521221161, tv_loss: 0.029168156906962395\n",
      "iteration 609, dc_loss: 0.21059353649616241, tv_loss: 0.02925996668636799\n",
      "iteration 610, dc_loss: 0.20974282920360565, tv_loss: 0.02934895269572735\n",
      "iteration 611, dc_loss: 0.20918476581573486, tv_loss: 0.02916083298623562\n",
      "iteration 612, dc_loss: 0.20828163623809814, tv_loss: 0.029354214668273926\n",
      "iteration 613, dc_loss: 0.20764845609664917, tv_loss: 0.029287496581673622\n",
      "iteration 614, dc_loss: 0.20700682699680328, tv_loss: 0.029291681945323944\n",
      "iteration 615, dc_loss: 0.2063361257314682, tv_loss: 0.029250042513012886\n",
      "iteration 616, dc_loss: 0.20558857917785645, tv_loss: 0.029339101165533066\n",
      "iteration 617, dc_loss: 0.20489896833896637, tv_loss: 0.029425110667943954\n",
      "iteration 618, dc_loss: 0.20429736375808716, tv_loss: 0.029310766607522964\n",
      "iteration 619, dc_loss: 0.2035352885723114, tv_loss: 0.0294494666159153\n",
      "iteration 620, dc_loss: 0.20299692451953888, tv_loss: 0.029328932985663414\n",
      "iteration 621, dc_loss: 0.2021895796060562, tv_loss: 0.029475396499037743\n",
      "iteration 622, dc_loss: 0.20170807838439941, tv_loss: 0.02927817590534687\n",
      "iteration 623, dc_loss: 0.20090395212173462, tv_loss: 0.029456499963998795\n",
      "iteration 624, dc_loss: 0.200342059135437, tv_loss: 0.029402054846286774\n",
      "iteration 625, dc_loss: 0.19959436357021332, tv_loss: 0.029482262209057808\n",
      "iteration 626, dc_loss: 0.199031263589859, tv_loss: 0.029349735006690025\n",
      "iteration 627, dc_loss: 0.19828324019908905, tv_loss: 0.029471447691321373\n",
      "iteration 628, dc_loss: 0.1977395862340927, tv_loss: 0.02946738712489605\n",
      "iteration 629, dc_loss: 0.19703251123428345, tv_loss: 0.029524335637688637\n",
      "iteration 630, dc_loss: 0.19642548263072968, tv_loss: 0.02946154586970806\n",
      "iteration 631, dc_loss: 0.19576594233512878, tv_loss: 0.02948491834104061\n",
      "iteration 632, dc_loss: 0.19514039158821106, tv_loss: 0.029589034616947174\n",
      "iteration 633, dc_loss: 0.19454409182071686, tv_loss: 0.029545310884714127\n",
      "iteration 634, dc_loss: 0.19393783807754517, tv_loss: 0.029542332515120506\n",
      "iteration 635, dc_loss: 0.19333134591579437, tv_loss: 0.029578689485788345\n",
      "iteration 636, dc_loss: 0.19263780117034912, tv_loss: 0.02961781993508339\n",
      "iteration 637, dc_loss: 0.19218119978904724, tv_loss: 0.029489245265722275\n",
      "iteration 638, dc_loss: 0.19147567451000214, tv_loss: 0.029671551659703255\n",
      "iteration 639, dc_loss: 0.1910824477672577, tv_loss: 0.029453184455633163\n",
      "iteration 640, dc_loss: 0.1903482973575592, tv_loss: 0.029705945402383804\n",
      "iteration 641, dc_loss: 0.19016017019748688, tv_loss: 0.02940746210515499\n",
      "iteration 642, dc_loss: 0.18934689462184906, tv_loss: 0.029859434813261032\n",
      "iteration 643, dc_loss: 0.18944503366947174, tv_loss: 0.029268043115735054\n",
      "iteration 644, dc_loss: 0.1884884387254715, tv_loss: 0.030116312205791473\n",
      "iteration 645, dc_loss: 0.1887444704771042, tv_loss: 0.02920057252049446\n",
      "iteration 646, dc_loss: 0.18736770749092102, tv_loss: 0.030114982277154922\n",
      "iteration 647, dc_loss: 0.18708360195159912, tv_loss: 0.029310323297977448\n",
      "iteration 648, dc_loss: 0.18573477864265442, tv_loss: 0.02974809892475605\n",
      "iteration 649, dc_loss: 0.18514855206012726, tv_loss: 0.029805021360516548\n",
      "iteration 650, dc_loss: 0.18502096831798553, tv_loss: 0.02946368418633938\n",
      "iteration 651, dc_loss: 0.1840413361787796, tv_loss: 0.029988033697009087\n",
      "iteration 652, dc_loss: 0.18369060754776, tv_loss: 0.029498282819986343\n",
      "iteration 653, dc_loss: 0.1826842576265335, tv_loss: 0.029701627790927887\n",
      "iteration 654, dc_loss: 0.1820807307958603, tv_loss: 0.029878396540880203\n",
      "iteration 655, dc_loss: 0.18197157979011536, tv_loss: 0.0296369306743145\n",
      "iteration 656, dc_loss: 0.18098527193069458, tv_loss: 0.02993888594210148\n",
      "iteration 657, dc_loss: 0.1805242896080017, tv_loss: 0.029676778241991997\n",
      "iteration 658, dc_loss: 0.17988577485084534, tv_loss: 0.029883990064263344\n",
      "iteration 659, dc_loss: 0.17911668121814728, tv_loss: 0.02992069348692894\n",
      "iteration 660, dc_loss: 0.17886731028556824, tv_loss: 0.029782332479953766\n",
      "iteration 661, dc_loss: 0.17815081775188446, tv_loss: 0.030058743432164192\n",
      "iteration 662, dc_loss: 0.17764827609062195, tv_loss: 0.029780149459838867\n",
      "iteration 663, dc_loss: 0.17704029381275177, tv_loss: 0.029938187450170517\n",
      "iteration 664, dc_loss: 0.17636431753635406, tv_loss: 0.029962068423628807\n",
      "iteration 665, dc_loss: 0.17600026726722717, tv_loss: 0.029763473197817802\n",
      "iteration 666, dc_loss: 0.17528662085533142, tv_loss: 0.030023492872714996\n",
      "iteration 667, dc_loss: 0.17479434609413147, tv_loss: 0.029890136793255806\n",
      "iteration 668, dc_loss: 0.174289733171463, tv_loss: 0.029826974496245384\n",
      "iteration 669, dc_loss: 0.17366662621498108, tv_loss: 0.029956459999084473\n",
      "iteration 670, dc_loss: 0.17322605848312378, tv_loss: 0.029894430190324783\n",
      "iteration 671, dc_loss: 0.17261487245559692, tv_loss: 0.029902292415499687\n",
      "iteration 672, dc_loss: 0.17203283309936523, tv_loss: 0.029934823513031006\n",
      "iteration 673, dc_loss: 0.17158746719360352, tv_loss: 0.02982505038380623\n",
      "iteration 674, dc_loss: 0.17090865969657898, tv_loss: 0.02998983860015869\n",
      "iteration 675, dc_loss: 0.17051799595355988, tv_loss: 0.029893487691879272\n",
      "iteration 676, dc_loss: 0.16992774605751038, tv_loss: 0.030023040249943733\n",
      "iteration 677, dc_loss: 0.1693844348192215, tv_loss: 0.030059101060032845\n",
      "iteration 678, dc_loss: 0.16898512840270996, tv_loss: 0.02989093028008938\n",
      "iteration 679, dc_loss: 0.16829034686088562, tv_loss: 0.030073381960392\n",
      "iteration 680, dc_loss: 0.16797827184200287, tv_loss: 0.02991560474038124\n",
      "iteration 681, dc_loss: 0.16728274524211884, tv_loss: 0.030222931876778603\n",
      "iteration 682, dc_loss: 0.1669624298810959, tv_loss: 0.029965460300445557\n",
      "iteration 683, dc_loss: 0.1662880927324295, tv_loss: 0.030066099017858505\n",
      "iteration 684, dc_loss: 0.1659182608127594, tv_loss: 0.02994660474359989\n",
      "iteration 685, dc_loss: 0.16537365317344666, tv_loss: 0.03017968125641346\n",
      "iteration 686, dc_loss: 0.16500328481197357, tv_loss: 0.030126383528113365\n",
      "iteration 687, dc_loss: 0.16460366547107697, tv_loss: 0.03011331520974636\n",
      "iteration 688, dc_loss: 0.1642305999994278, tv_loss: 0.030017053708434105\n",
      "iteration 689, dc_loss: 0.16374816000461578, tv_loss: 0.030229343101382256\n",
      "iteration 690, dc_loss: 0.16317813098430634, tv_loss: 0.030087560415267944\n",
      "iteration 691, dc_loss: 0.1624031811952591, tv_loss: 0.030217094346880913\n",
      "iteration 692, dc_loss: 0.1621156632900238, tv_loss: 0.02996710315346718\n",
      "iteration 693, dc_loss: 0.16143502295017242, tv_loss: 0.03027527593076229\n",
      "iteration 694, dc_loss: 0.16120217740535736, tv_loss: 0.03009396232664585\n",
      "iteration 695, dc_loss: 0.16039304435253143, tv_loss: 0.03026387095451355\n",
      "iteration 696, dc_loss: 0.159899041056633, tv_loss: 0.030113020911812782\n",
      "iteration 697, dc_loss: 0.15939579904079437, tv_loss: 0.030189884826540947\n",
      "iteration 698, dc_loss: 0.15892843902111053, tv_loss: 0.030375709757208824\n",
      "iteration 699, dc_loss: 0.1586005538702011, tv_loss: 0.03013623133301735\n",
      "iteration 700, dc_loss: 0.15796640515327454, tv_loss: 0.030360758304595947\n",
      "iteration 701, dc_loss: 0.15764856338500977, tv_loss: 0.03023418039083481\n",
      "iteration 702, dc_loss: 0.15690293908119202, tv_loss: 0.03035554103553295\n",
      "iteration 703, dc_loss: 0.15667106211185455, tv_loss: 0.030196061357855797\n",
      "iteration 704, dc_loss: 0.15602977573871613, tv_loss: 0.030419358983635902\n",
      "iteration 705, dc_loss: 0.15561968088150024, tv_loss: 0.030257463455200195\n",
      "iteration 706, dc_loss: 0.15523067116737366, tv_loss: 0.03023555874824524\n",
      "iteration 707, dc_loss: 0.15462429821491241, tv_loss: 0.03047950193285942\n",
      "iteration 708, dc_loss: 0.15425744652748108, tv_loss: 0.030240103602409363\n",
      "iteration 709, dc_loss: 0.15369798243045807, tv_loss: 0.030485982075333595\n",
      "iteration 710, dc_loss: 0.1533547192811966, tv_loss: 0.030287038534879684\n",
      "iteration 711, dc_loss: 0.15279141068458557, tv_loss: 0.030394908040761948\n",
      "iteration 712, dc_loss: 0.15247155725955963, tv_loss: 0.030417274683713913\n",
      "iteration 713, dc_loss: 0.15194670855998993, tv_loss: 0.030353054404258728\n",
      "iteration 714, dc_loss: 0.1515311896800995, tv_loss: 0.03038572520017624\n",
      "iteration 715, dc_loss: 0.15111427009105682, tv_loss: 0.03053201362490654\n",
      "iteration 716, dc_loss: 0.15076200664043427, tv_loss: 0.030321475118398666\n",
      "iteration 717, dc_loss: 0.15026718378067017, tv_loss: 0.030698103830218315\n",
      "iteration 718, dc_loss: 0.15021559596061707, tv_loss: 0.030266789719462395\n",
      "iteration 719, dc_loss: 0.14977163076400757, tv_loss: 0.030639780685305595\n",
      "iteration 720, dc_loss: 0.14993658661842346, tv_loss: 0.0302583035081625\n",
      "iteration 721, dc_loss: 0.1492721289396286, tv_loss: 0.0307143684476614\n",
      "iteration 722, dc_loss: 0.14925752580165863, tv_loss: 0.030276481062173843\n",
      "iteration 723, dc_loss: 0.14791052043437958, tv_loss: 0.030719855800271034\n",
      "iteration 724, dc_loss: 0.14743752777576447, tv_loss: 0.0304111335426569\n",
      "iteration 725, dc_loss: 0.1467069387435913, tv_loss: 0.030596677213907242\n",
      "iteration 726, dc_loss: 0.1465415060520172, tv_loss: 0.030538372695446014\n",
      "iteration 727, dc_loss: 0.1464911848306656, tv_loss: 0.030491283163428307\n",
      "iteration 728, dc_loss: 0.14562322199344635, tv_loss: 0.030719537287950516\n",
      "iteration 729, dc_loss: 0.14535927772521973, tv_loss: 0.030485616996884346\n",
      "iteration 730, dc_loss: 0.1445397287607193, tv_loss: 0.0305978674441576\n",
      "iteration 731, dc_loss: 0.14429375529289246, tv_loss: 0.03076651319861412\n",
      "iteration 732, dc_loss: 0.14412802457809448, tv_loss: 0.030473828315734863\n",
      "iteration 733, dc_loss: 0.14347323775291443, tv_loss: 0.03076835162937641\n",
      "iteration 734, dc_loss: 0.14311537146568298, tv_loss: 0.030490655452013016\n",
      "iteration 735, dc_loss: 0.14244115352630615, tv_loss: 0.030703868716955185\n",
      "iteration 736, dc_loss: 0.14227961003780365, tv_loss: 0.0306283850222826\n",
      "iteration 737, dc_loss: 0.14193780720233917, tv_loss: 0.030490262433886528\n",
      "iteration 738, dc_loss: 0.1413765400648117, tv_loss: 0.03081652894616127\n",
      "iteration 739, dc_loss: 0.1409563571214676, tv_loss: 0.03052699565887451\n",
      "iteration 740, dc_loss: 0.1404661238193512, tv_loss: 0.03068714588880539\n",
      "iteration 741, dc_loss: 0.14014844596385956, tv_loss: 0.03067493438720703\n",
      "iteration 742, dc_loss: 0.1397525668144226, tv_loss: 0.030616646632552147\n",
      "iteration 743, dc_loss: 0.13936814665794373, tv_loss: 0.030811108648777008\n",
      "iteration 744, dc_loss: 0.1388714760541916, tv_loss: 0.030648747459053993\n",
      "iteration 745, dc_loss: 0.13844238221645355, tv_loss: 0.03085187077522278\n",
      "iteration 746, dc_loss: 0.13803306221961975, tv_loss: 0.030738193541765213\n",
      "iteration 747, dc_loss: 0.13780763745307922, tv_loss: 0.030792493373155594\n",
      "iteration 748, dc_loss: 0.1372927725315094, tv_loss: 0.030770815908908844\n",
      "iteration 749, dc_loss: 0.13686184585094452, tv_loss: 0.030903689563274384\n",
      "iteration 750, dc_loss: 0.13650593161582947, tv_loss: 0.03070818819105625\n",
      "iteration 751, dc_loss: 0.13611078262329102, tv_loss: 0.03095012530684471\n",
      "iteration 752, dc_loss: 0.135801762342453, tv_loss: 0.030702246353030205\n",
      "iteration 753, dc_loss: 0.13533727824687958, tv_loss: 0.03106866218149662\n",
      "iteration 754, dc_loss: 0.13499657809734344, tv_loss: 0.03085377626121044\n",
      "iteration 755, dc_loss: 0.13464277982711792, tv_loss: 0.03096102364361286\n",
      "iteration 756, dc_loss: 0.13427530229091644, tv_loss: 0.03079255297780037\n",
      "iteration 757, dc_loss: 0.13385871052742004, tv_loss: 0.031006140634417534\n",
      "iteration 758, dc_loss: 0.13346189260482788, tv_loss: 0.030869213864207268\n",
      "iteration 759, dc_loss: 0.13312183320522308, tv_loss: 0.03099079057574272\n",
      "iteration 760, dc_loss: 0.13276395201683044, tv_loss: 0.03084753267467022\n",
      "iteration 761, dc_loss: 0.13235028088092804, tv_loss: 0.031028369441628456\n",
      "iteration 762, dc_loss: 0.13199307024478912, tv_loss: 0.03092866763472557\n",
      "iteration 763, dc_loss: 0.13160304725170135, tv_loss: 0.030971771106123924\n",
      "iteration 764, dc_loss: 0.13125497102737427, tv_loss: 0.03091001883149147\n",
      "iteration 765, dc_loss: 0.13084788620471954, tv_loss: 0.03099624067544937\n",
      "iteration 766, dc_loss: 0.1304989904165268, tv_loss: 0.030937964096665382\n",
      "iteration 767, dc_loss: 0.1301707774400711, tv_loss: 0.03097262606024742\n",
      "iteration 768, dc_loss: 0.12977007031440735, tv_loss: 0.030958188697695732\n",
      "iteration 769, dc_loss: 0.12939788401126862, tv_loss: 0.031017934903502464\n",
      "iteration 770, dc_loss: 0.12903566658496857, tv_loss: 0.030967721715569496\n",
      "iteration 771, dc_loss: 0.12872087955474854, tv_loss: 0.030979255214333534\n",
      "iteration 772, dc_loss: 0.12833885848522186, tv_loss: 0.030947860330343246\n",
      "iteration 773, dc_loss: 0.12796464562416077, tv_loss: 0.031053194776177406\n",
      "iteration 774, dc_loss: 0.1276031881570816, tv_loss: 0.031009111553430557\n",
      "iteration 775, dc_loss: 0.127360999584198, tv_loss: 0.030986277386546135\n",
      "iteration 776, dc_loss: 0.12692661583423615, tv_loss: 0.031034233048558235\n",
      "iteration 777, dc_loss: 0.12671609222888947, tv_loss: 0.03099703975021839\n",
      "iteration 778, dc_loss: 0.12625767290592194, tv_loss: 0.03112693689763546\n",
      "iteration 779, dc_loss: 0.12635545432567596, tv_loss: 0.030872520059347153\n",
      "iteration 780, dc_loss: 0.1258329153060913, tv_loss: 0.031242219731211662\n",
      "iteration 781, dc_loss: 0.12619845569133759, tv_loss: 0.030814053490757942\n",
      "iteration 782, dc_loss: 0.12561576068401337, tv_loss: 0.03142653778195381\n",
      "iteration 783, dc_loss: 0.12608258426189423, tv_loss: 0.03065718151628971\n",
      "iteration 784, dc_loss: 0.12514041364192963, tv_loss: 0.031380023807287216\n",
      "iteration 785, dc_loss: 0.12507180869579315, tv_loss: 0.03089350089430809\n",
      "iteration 786, dc_loss: 0.12416009604930878, tv_loss: 0.031100573018193245\n",
      "iteration 787, dc_loss: 0.12364021688699722, tv_loss: 0.031239114701747894\n",
      "iteration 788, dc_loss: 0.12359602749347687, tv_loss: 0.03080449253320694\n",
      "iteration 789, dc_loss: 0.12283197045326233, tv_loss: 0.03140755370259285\n",
      "iteration 790, dc_loss: 0.12271876633167267, tv_loss: 0.030934104695916176\n",
      "iteration 791, dc_loss: 0.12227071821689606, tv_loss: 0.03110009990632534\n",
      "iteration 792, dc_loss: 0.12176765501499176, tv_loss: 0.0311775803565979\n",
      "iteration 793, dc_loss: 0.12166760116815567, tv_loss: 0.031015345826745033\n",
      "iteration 794, dc_loss: 0.12096960097551346, tv_loss: 0.031227372586727142\n",
      "iteration 795, dc_loss: 0.12098319083452225, tv_loss: 0.031068997457623482\n",
      "iteration 796, dc_loss: 0.12051279842853546, tv_loss: 0.031014468520879745\n",
      "iteration 797, dc_loss: 0.11996254324913025, tv_loss: 0.031346388161182404\n",
      "iteration 798, dc_loss: 0.11982062458992004, tv_loss: 0.031004412099719048\n",
      "iteration 799, dc_loss: 0.11947428435087204, tv_loss: 0.03125326707959175\n",
      "iteration 800, dc_loss: 0.1190914586186409, tv_loss: 0.0311525147408247\n",
      "iteration 801, dc_loss: 0.11884425580501556, tv_loss: 0.03113033063709736\n",
      "iteration 802, dc_loss: 0.11832574009895325, tv_loss: 0.031243031844496727\n",
      "iteration 803, dc_loss: 0.11819799244403839, tv_loss: 0.031170114874839783\n",
      "iteration 804, dc_loss: 0.11799293756484985, tv_loss: 0.031062938272953033\n",
      "iteration 805, dc_loss: 0.11765975505113602, tv_loss: 0.03113488107919693\n",
      "iteration 806, dc_loss: 0.11735933274030685, tv_loss: 0.031172899529337883\n",
      "iteration 807, dc_loss: 0.11719710379838943, tv_loss: 0.031067144125699997\n",
      "iteration 808, dc_loss: 0.11691462248563766, tv_loss: 0.03111843951046467\n",
      "iteration 809, dc_loss: 0.1165747195482254, tv_loss: 0.031191984191536903\n",
      "iteration 810, dc_loss: 0.11642372608184814, tv_loss: 0.031089169904589653\n",
      "iteration 811, dc_loss: 0.11613739281892776, tv_loss: 0.031167881563305855\n",
      "iteration 812, dc_loss: 0.11585645377635956, tv_loss: 0.031132416799664497\n",
      "iteration 813, dc_loss: 0.11567205935716629, tv_loss: 0.03117360547184944\n",
      "iteration 814, dc_loss: 0.11535516381263733, tv_loss: 0.031152402982115746\n",
      "iteration 815, dc_loss: 0.11512826383113861, tv_loss: 0.03123452514410019\n",
      "iteration 816, dc_loss: 0.1149386391043663, tv_loss: 0.031094398349523544\n",
      "iteration 817, dc_loss: 0.11466053128242493, tv_loss: 0.03123543970286846\n",
      "iteration 818, dc_loss: 0.11437136679887772, tv_loss: 0.03118733875453472\n",
      "iteration 819, dc_loss: 0.11421389877796173, tv_loss: 0.031230393797159195\n",
      "iteration 820, dc_loss: 0.11393469572067261, tv_loss: 0.031162723898887634\n",
      "iteration 821, dc_loss: 0.11369023472070694, tv_loss: 0.031291838735342026\n",
      "iteration 822, dc_loss: 0.11344179511070251, tv_loss: 0.031213944777846336\n",
      "iteration 823, dc_loss: 0.11323682963848114, tv_loss: 0.031287018209695816\n",
      "iteration 824, dc_loss: 0.11298556625843048, tv_loss: 0.031249646097421646\n",
      "iteration 825, dc_loss: 0.11276097595691681, tv_loss: 0.03127817437052727\n",
      "iteration 826, dc_loss: 0.11248931288719177, tv_loss: 0.031293559819459915\n",
      "iteration 827, dc_loss: 0.11228817701339722, tv_loss: 0.03125917166471481\n",
      "iteration 828, dc_loss: 0.11205879598855972, tv_loss: 0.031230716034770012\n",
      "iteration 829, dc_loss: 0.11181814968585968, tv_loss: 0.03128731995820999\n",
      "iteration 830, dc_loss: 0.11156082153320312, tv_loss: 0.03125671297311783\n",
      "iteration 831, dc_loss: 0.11138948798179626, tv_loss: 0.0312945693731308\n",
      "iteration 832, dc_loss: 0.11112090200185776, tv_loss: 0.031260691583156586\n",
      "iteration 833, dc_loss: 0.11089004576206207, tv_loss: 0.03136136010289192\n",
      "iteration 834, dc_loss: 0.11065997928380966, tv_loss: 0.03130905330181122\n",
      "iteration 835, dc_loss: 0.11045646667480469, tv_loss: 0.031332433223724365\n",
      "iteration 836, dc_loss: 0.11020764708518982, tv_loss: 0.03132664039731026\n",
      "iteration 837, dc_loss: 0.10999643802642822, tv_loss: 0.03132207691669464\n",
      "iteration 838, dc_loss: 0.10974881052970886, tv_loss: 0.031321246176958084\n",
      "iteration 839, dc_loss: 0.10955983400344849, tv_loss: 0.03132738173007965\n",
      "iteration 840, dc_loss: 0.10932696610689163, tv_loss: 0.0312887467443943\n",
      "iteration 841, dc_loss: 0.10908296704292297, tv_loss: 0.03139247000217438\n",
      "iteration 842, dc_loss: 0.1088457927107811, tv_loss: 0.031347811222076416\n",
      "iteration 843, dc_loss: 0.10867714136838913, tv_loss: 0.03137033432722092\n",
      "iteration 844, dc_loss: 0.10843220353126526, tv_loss: 0.03134904429316521\n",
      "iteration 845, dc_loss: 0.10820829123258591, tv_loss: 0.031394556164741516\n",
      "iteration 846, dc_loss: 0.10796966403722763, tv_loss: 0.031381867825984955\n",
      "iteration 847, dc_loss: 0.1077902764081955, tv_loss: 0.03137342631816864\n",
      "iteration 848, dc_loss: 0.1075696274638176, tv_loss: 0.03133561834692955\n",
      "iteration 849, dc_loss: 0.10733771324157715, tv_loss: 0.031412649899721146\n",
      "iteration 850, dc_loss: 0.10709597915410995, tv_loss: 0.03138520568609238\n",
      "iteration 851, dc_loss: 0.10693374276161194, tv_loss: 0.03140920028090477\n",
      "iteration 852, dc_loss: 0.10669654607772827, tv_loss: 0.03137660026550293\n",
      "iteration 853, dc_loss: 0.10647276788949966, tv_loss: 0.031449224799871445\n",
      "iteration 854, dc_loss: 0.1062430590391159, tv_loss: 0.03142380714416504\n",
      "iteration 855, dc_loss: 0.10606739670038223, tv_loss: 0.03142661228775978\n",
      "iteration 856, dc_loss: 0.10585141181945801, tv_loss: 0.03139762580394745\n",
      "iteration 857, dc_loss: 0.1056274026632309, tv_loss: 0.03144431486725807\n",
      "iteration 858, dc_loss: 0.10539215058088303, tv_loss: 0.03143101558089256\n",
      "iteration 859, dc_loss: 0.10522935539484024, tv_loss: 0.031439170241355896\n",
      "iteration 860, dc_loss: 0.10500285774469376, tv_loss: 0.03140487149357796\n",
      "iteration 861, dc_loss: 0.10478182882070541, tv_loss: 0.03148917481303215\n",
      "iteration 862, dc_loss: 0.10455800592899323, tv_loss: 0.03145578131079674\n",
      "iteration 863, dc_loss: 0.10438574850559235, tv_loss: 0.031477201730012894\n",
      "iteration 864, dc_loss: 0.1041751280426979, tv_loss: 0.03144362196326256\n",
      "iteration 865, dc_loss: 0.10395834594964981, tv_loss: 0.03148467838764191\n",
      "iteration 866, dc_loss: 0.10372805595397949, tv_loss: 0.03147711604833603\n",
      "iteration 867, dc_loss: 0.10356004536151886, tv_loss: 0.03148406744003296\n",
      "iteration 868, dc_loss: 0.10335142910480499, tv_loss: 0.031449027359485626\n",
      "iteration 869, dc_loss: 0.10313493758440018, tv_loss: 0.031513385474681854\n",
      "iteration 870, dc_loss: 0.1029156818985939, tv_loss: 0.031482916325330734\n",
      "iteration 871, dc_loss: 0.10274845361709595, tv_loss: 0.031515125185251236\n",
      "iteration 872, dc_loss: 0.10254178196191788, tv_loss: 0.031472254544496536\n",
      "iteration 873, dc_loss: 0.10232701152563095, tv_loss: 0.03153754025697708\n",
      "iteration 874, dc_loss: 0.10210337489843369, tv_loss: 0.03152434155344963\n",
      "iteration 875, dc_loss: 0.10193624347448349, tv_loss: 0.03152967989444733\n",
      "iteration 876, dc_loss: 0.1017392948269844, tv_loss: 0.031497035175561905\n",
      "iteration 877, dc_loss: 0.10152920335531235, tv_loss: 0.03154122456908226\n",
      "iteration 878, dc_loss: 0.10130970180034637, tv_loss: 0.03152627870440483\n",
      "iteration 879, dc_loss: 0.10114540159702301, tv_loss: 0.0315464623272419\n",
      "iteration 880, dc_loss: 0.10095297545194626, tv_loss: 0.03149395436048508\n",
      "iteration 881, dc_loss: 0.10073689371347427, tv_loss: 0.03157613426446915\n",
      "iteration 882, dc_loss: 0.10051962733268738, tv_loss: 0.03155091032385826\n",
      "iteration 883, dc_loss: 0.10035741329193115, tv_loss: 0.03157855197787285\n",
      "iteration 884, dc_loss: 0.10016787797212601, tv_loss: 0.03153492510318756\n",
      "iteration 885, dc_loss: 0.09995809942483902, tv_loss: 0.03158826753497124\n",
      "iteration 886, dc_loss: 0.09973981231451035, tv_loss: 0.031580183655023575\n",
      "iteration 887, dc_loss: 0.09958553314208984, tv_loss: 0.03158285468816757\n",
      "iteration 888, dc_loss: 0.09940238296985626, tv_loss: 0.031536590307950974\n",
      "iteration 889, dc_loss: 0.09918555617332458, tv_loss: 0.0316070131957531\n",
      "iteration 890, dc_loss: 0.09897257387638092, tv_loss: 0.031589411199092865\n",
      "iteration 891, dc_loss: 0.09881408512592316, tv_loss: 0.0316118448972702\n",
      "iteration 892, dc_loss: 0.09863532334566116, tv_loss: 0.03156079724431038\n",
      "iteration 893, dc_loss: 0.0984235405921936, tv_loss: 0.031630489975214005\n",
      "iteration 894, dc_loss: 0.0982123389840126, tv_loss: 0.03161453455686569\n",
      "iteration 895, dc_loss: 0.09806666523218155, tv_loss: 0.03162204846739769\n",
      "iteration 896, dc_loss: 0.09788680076599121, tv_loss: 0.03157413750886917\n",
      "iteration 897, dc_loss: 0.09766700863838196, tv_loss: 0.031655073165893555\n",
      "iteration 898, dc_loss: 0.09746216982603073, tv_loss: 0.03163667023181915\n",
      "iteration 899, dc_loss: 0.09731535613536835, tv_loss: 0.031641993671655655\n",
      "iteration 900, dc_loss: 0.09714222699403763, tv_loss: 0.031594760715961456\n",
      "iteration 901, dc_loss: 0.09692437201738358, tv_loss: 0.03167020156979561\n",
      "iteration 902, dc_loss: 0.09672027826309204, tv_loss: 0.031653258949518204\n",
      "iteration 903, dc_loss: 0.09658537060022354, tv_loss: 0.031652841717004776\n",
      "iteration 904, dc_loss: 0.09640520066022873, tv_loss: 0.03160921111702919\n",
      "iteration 905, dc_loss: 0.0961889773607254, tv_loss: 0.03169351816177368\n",
      "iteration 906, dc_loss: 0.09599019587039948, tv_loss: 0.03167227283120155\n",
      "iteration 907, dc_loss: 0.09585423767566681, tv_loss: 0.031674984842538834\n",
      "iteration 908, dc_loss: 0.09567716717720032, tv_loss: 0.03163542598485947\n",
      "iteration 909, dc_loss: 0.0954633355140686, tv_loss: 0.031710460782051086\n",
      "iteration 910, dc_loss: 0.09526733309030533, tv_loss: 0.03168823942542076\n",
      "iteration 911, dc_loss: 0.09514326602220535, tv_loss: 0.03168575093150139\n",
      "iteration 912, dc_loss: 0.09495726227760315, tv_loss: 0.03165121749043465\n",
      "iteration 913, dc_loss: 0.09474906325340271, tv_loss: 0.03173219785094261\n",
      "iteration 914, dc_loss: 0.09455399960279465, tv_loss: 0.03171096742153168\n",
      "iteration 915, dc_loss: 0.09443926066160202, tv_loss: 0.03170013427734375\n",
      "iteration 916, dc_loss: 0.0942486971616745, tv_loss: 0.03168153762817383\n",
      "iteration 917, dc_loss: 0.09406525641679764, tv_loss: 0.031738102436065674\n",
      "iteration 918, dc_loss: 0.09386401623487473, tv_loss: 0.03173689916729927\n",
      "iteration 919, dc_loss: 0.09380537271499634, tv_loss: 0.031699057668447495\n",
      "iteration 920, dc_loss: 0.09362445771694183, tv_loss: 0.03171965852379799\n",
      "iteration 921, dc_loss: 0.09355935454368591, tv_loss: 0.031732212752103806\n",
      "iteration 922, dc_loss: 0.09343724697828293, tv_loss: 0.03180518373847008\n",
      "iteration 923, dc_loss: 0.09356001019477844, tv_loss: 0.03167196363210678\n",
      "iteration 924, dc_loss: 0.09352043271064758, tv_loss: 0.031798746436834335\n",
      "iteration 925, dc_loss: 0.09334450215101242, tv_loss: 0.0317060761153698\n",
      "iteration 926, dc_loss: 0.09287601709365845, tv_loss: 0.031834788620471954\n",
      "iteration 927, dc_loss: 0.09263370931148529, tv_loss: 0.03167487680912018\n",
      "iteration 928, dc_loss: 0.09220552444458008, tv_loss: 0.0317520834505558\n",
      "iteration 929, dc_loss: 0.09208032488822937, tv_loss: 0.031781625002622604\n",
      "iteration 930, dc_loss: 0.09197250753641129, tv_loss: 0.03175162896513939\n",
      "iteration 931, dc_loss: 0.09191890805959702, tv_loss: 0.0317830853164196\n",
      "iteration 932, dc_loss: 0.09171346575021744, tv_loss: 0.031690336763858795\n",
      "iteration 933, dc_loss: 0.09132913500070572, tv_loss: 0.0318925678730011\n",
      "iteration 934, dc_loss: 0.09123887121677399, tv_loss: 0.03171461448073387\n",
      "iteration 935, dc_loss: 0.09105544537305832, tv_loss: 0.03182145580649376\n",
      "iteration 936, dc_loss: 0.09097141772508621, tv_loss: 0.03173397481441498\n",
      "iteration 937, dc_loss: 0.09077751636505127, tv_loss: 0.0317985862493515\n",
      "iteration 938, dc_loss: 0.09048164635896683, tv_loss: 0.03184808790683746\n",
      "iteration 939, dc_loss: 0.09049433469772339, tv_loss: 0.0316925048828125\n",
      "iteration 940, dc_loss: 0.09013336896896362, tv_loss: 0.03185165300965309\n",
      "iteration 941, dc_loss: 0.09006890654563904, tv_loss: 0.031783849000930786\n",
      "iteration 942, dc_loss: 0.08989110589027405, tv_loss: 0.03178144991397858\n",
      "iteration 943, dc_loss: 0.08967217803001404, tv_loss: 0.03184591606259346\n",
      "iteration 944, dc_loss: 0.08961715549230576, tv_loss: 0.031728412955999374\n",
      "iteration 945, dc_loss: 0.08930262923240662, tv_loss: 0.03187454119324684\n",
      "iteration 946, dc_loss: 0.0892619639635086, tv_loss: 0.03173951432108879\n",
      "iteration 947, dc_loss: 0.08904480189085007, tv_loss: 0.03182227537035942\n",
      "iteration 948, dc_loss: 0.0888623520731926, tv_loss: 0.03181533142924309\n",
      "iteration 949, dc_loss: 0.08878667652606964, tv_loss: 0.0317772701382637\n",
      "iteration 950, dc_loss: 0.08849772810935974, tv_loss: 0.03186894208192825\n",
      "iteration 951, dc_loss: 0.08847701549530029, tv_loss: 0.0317750982940197\n",
      "iteration 952, dc_loss: 0.08820436894893646, tv_loss: 0.03185151144862175\n",
      "iteration 953, dc_loss: 0.08812005817890167, tv_loss: 0.0318126417696476\n",
      "iteration 954, dc_loss: 0.08789949119091034, tv_loss: 0.03183303028345108\n",
      "iteration 955, dc_loss: 0.08773225545883179, tv_loss: 0.03188793733716011\n",
      "iteration 956, dc_loss: 0.087651826441288, tv_loss: 0.03178022429347038\n",
      "iteration 957, dc_loss: 0.08741702139377594, tv_loss: 0.031912028789520264\n",
      "iteration 958, dc_loss: 0.08732685446739197, tv_loss: 0.031823307275772095\n",
      "iteration 959, dc_loss: 0.08710739761590958, tv_loss: 0.03189048543572426\n",
      "iteration 960, dc_loss: 0.08701139688491821, tv_loss: 0.03181377798318863\n",
      "iteration 961, dc_loss: 0.08680756390094757, tv_loss: 0.03187503293156624\n",
      "iteration 962, dc_loss: 0.08665051311254501, tv_loss: 0.03186246007680893\n",
      "iteration 963, dc_loss: 0.08652600646018982, tv_loss: 0.031854093074798584\n",
      "iteration 964, dc_loss: 0.08634047955274582, tv_loss: 0.031880054622888565\n",
      "iteration 965, dc_loss: 0.08625772595405579, tv_loss: 0.03182922676205635\n",
      "iteration 966, dc_loss: 0.0860000029206276, tv_loss: 0.03191645070910454\n",
      "iteration 967, dc_loss: 0.08596145361661911, tv_loss: 0.03183075040578842\n",
      "iteration 968, dc_loss: 0.08572525531053543, tv_loss: 0.031892385333776474\n",
      "iteration 969, dc_loss: 0.08563315123319626, tv_loss: 0.03186650201678276\n",
      "iteration 970, dc_loss: 0.08543252944946289, tv_loss: 0.03189259395003319\n",
      "iteration 971, dc_loss: 0.08530903607606888, tv_loss: 0.03189747408032417\n",
      "iteration 972, dc_loss: 0.08517344295978546, tv_loss: 0.031871408224105835\n",
      "iteration 973, dc_loss: 0.08502005785703659, tv_loss: 0.03191015124320984\n",
      "iteration 974, dc_loss: 0.08486874401569366, tv_loss: 0.03190207853913307\n",
      "iteration 975, dc_loss: 0.08475594967603683, tv_loss: 0.03190644085407257\n",
      "iteration 976, dc_loss: 0.08460181951522827, tv_loss: 0.031923793256282806\n",
      "iteration 977, dc_loss: 0.08451589196920395, tv_loss: 0.031896915286779404\n",
      "iteration 978, dc_loss: 0.08434173464775085, tv_loss: 0.03196832165122032\n",
      "iteration 979, dc_loss: 0.08434334397315979, tv_loss: 0.03186006844043732\n",
      "iteration 980, dc_loss: 0.0841563493013382, tv_loss: 0.03199552372097969\n",
      "iteration 981, dc_loss: 0.08421333134174347, tv_loss: 0.03184693679213524\n",
      "iteration 982, dc_loss: 0.0839814618229866, tv_loss: 0.03205413371324539\n",
      "iteration 983, dc_loss: 0.0840359628200531, tv_loss: 0.03182607144117355\n",
      "iteration 984, dc_loss: 0.08370018005371094, tv_loss: 0.032054342329502106\n",
      "iteration 985, dc_loss: 0.08367277681827545, tv_loss: 0.03181346505880356\n",
      "iteration 986, dc_loss: 0.08318586647510529, tv_loss: 0.032079312950372696\n",
      "iteration 987, dc_loss: 0.08315593749284744, tv_loss: 0.03183512017130852\n",
      "iteration 988, dc_loss: 0.08278349041938782, tv_loss: 0.03202323243021965\n",
      "iteration 989, dc_loss: 0.08280012011528015, tv_loss: 0.031880684196949005\n",
      "iteration 990, dc_loss: 0.08261507749557495, tv_loss: 0.03200109302997589\n",
      "iteration 991, dc_loss: 0.08252473175525665, tv_loss: 0.03198898583650589\n",
      "iteration 992, dc_loss: 0.0824311152100563, tv_loss: 0.031917754560709\n",
      "iteration 993, dc_loss: 0.08217500895261765, tv_loss: 0.03199918568134308\n",
      "iteration 994, dc_loss: 0.08207245916128159, tv_loss: 0.03191014751791954\n",
      "iteration 995, dc_loss: 0.08180537074804306, tv_loss: 0.032002151012420654\n",
      "iteration 996, dc_loss: 0.08172279596328735, tv_loss: 0.031938545405864716\n",
      "iteration 997, dc_loss: 0.08158068358898163, tv_loss: 0.03196955472230911\n",
      "iteration 998, dc_loss: 0.08145540207624435, tv_loss: 0.03200686722993851\n",
      "iteration 999, dc_loss: 0.08141263574361801, tv_loss: 0.03193458169698715\n",
      "iteration 1000, dc_loss: 0.08114104717969894, tv_loss: 0.03207052871584892\n",
      "iteration 1001, dc_loss: 0.0811375230550766, tv_loss: 0.03191125765442848\n",
      "iteration 1002, dc_loss: 0.08081454038619995, tv_loss: 0.0320819690823555\n",
      "iteration 1003, dc_loss: 0.08085460960865021, tv_loss: 0.031915318220853806\n",
      "iteration 1004, dc_loss: 0.08057533949613571, tv_loss: 0.03210573270916939\n",
      "iteration 1005, dc_loss: 0.08064545691013336, tv_loss: 0.03194178640842438\n",
      "iteration 1006, dc_loss: 0.08038081973791122, tv_loss: 0.0320974662899971\n",
      "iteration 1007, dc_loss: 0.08041590452194214, tv_loss: 0.03194174915552139\n",
      "iteration 1008, dc_loss: 0.08010359853506088, tv_loss: 0.032102711498737335\n",
      "iteration 1009, dc_loss: 0.08008035272359848, tv_loss: 0.03197052702307701\n",
      "iteration 1010, dc_loss: 0.07977594435214996, tv_loss: 0.03209373727440834\n",
      "iteration 1011, dc_loss: 0.07971702516078949, tv_loss: 0.031981345266103745\n",
      "iteration 1012, dc_loss: 0.0794903114438057, tv_loss: 0.03206424415111542\n",
      "iteration 1013, dc_loss: 0.07940483093261719, tv_loss: 0.032020214945077896\n",
      "iteration 1014, dc_loss: 0.07925700396299362, tv_loss: 0.032045893371105194\n",
      "iteration 1015, dc_loss: 0.07912179082632065, tv_loss: 0.03205681964755058\n",
      "iteration 1016, dc_loss: 0.07900803536176682, tv_loss: 0.032034147530794144\n",
      "iteration 1017, dc_loss: 0.07885473966598511, tv_loss: 0.03204948455095291\n",
      "iteration 1018, dc_loss: 0.07871853560209274, tv_loss: 0.03206738084554672\n",
      "iteration 1019, dc_loss: 0.07863454520702362, tv_loss: 0.03205280378460884\n",
      "iteration 1020, dc_loss: 0.07842868566513062, tv_loss: 0.03211378678679466\n",
      "iteration 1021, dc_loss: 0.07842282950878143, tv_loss: 0.03199389949440956\n",
      "iteration 1022, dc_loss: 0.07816773653030396, tv_loss: 0.03215654194355011\n",
      "iteration 1023, dc_loss: 0.07823295146226883, tv_loss: 0.031977422535419464\n",
      "iteration 1024, dc_loss: 0.07793274521827698, tv_loss: 0.032184626907110214\n",
      "iteration 1025, dc_loss: 0.07808925956487656, tv_loss: 0.03195040300488472\n",
      "iteration 1026, dc_loss: 0.07771961390972137, tv_loss: 0.03228265047073364\n",
      "iteration 1027, dc_loss: 0.07799538969993591, tv_loss: 0.03191274777054787\n",
      "iteration 1028, dc_loss: 0.0775635614991188, tv_loss: 0.03229784592986107\n",
      "iteration 1029, dc_loss: 0.07786872237920761, tv_loss: 0.03190034627914429\n",
      "iteration 1030, dc_loss: 0.07738546282052994, tv_loss: 0.032325293868780136\n",
      "iteration 1031, dc_loss: 0.07756483554840088, tv_loss: 0.03193984180688858\n",
      "iteration 1032, dc_loss: 0.07709681242704391, tv_loss: 0.03223744034767151\n",
      "iteration 1033, dc_loss: 0.07706711441278458, tv_loss: 0.03203928470611572\n",
      "iteration 1034, dc_loss: 0.07675832509994507, tv_loss: 0.0321490541100502\n",
      "iteration 1035, dc_loss: 0.0766119435429573, tv_loss: 0.03213054686784744\n",
      "iteration 1036, dc_loss: 0.07657229900360107, tv_loss: 0.032065100967884064\n",
      "iteration 1037, dc_loss: 0.07629433274269104, tv_loss: 0.03224801644682884\n",
      "iteration 1038, dc_loss: 0.07642216980457306, tv_loss: 0.03199966251850128\n",
      "iteration 1039, dc_loss: 0.07602372765541077, tv_loss: 0.032274242490530014\n",
      "iteration 1040, dc_loss: 0.07614468038082123, tv_loss: 0.032009635120630264\n",
      "iteration 1041, dc_loss: 0.0757722333073616, tv_loss: 0.032240986824035645\n",
      "iteration 1042, dc_loss: 0.0758020430803299, tv_loss: 0.032077573239803314\n",
      "iteration 1043, dc_loss: 0.07559297233819962, tv_loss: 0.03216787427663803\n",
      "iteration 1044, dc_loss: 0.07545918971300125, tv_loss: 0.03218218311667442\n",
      "iteration 1045, dc_loss: 0.07541381567716599, tv_loss: 0.03210671991109848\n",
      "iteration 1046, dc_loss: 0.07518262416124344, tv_loss: 0.03222348168492317\n",
      "iteration 1047, dc_loss: 0.07520197331905365, tv_loss: 0.03208493813872337\n",
      "iteration 1048, dc_loss: 0.07495633512735367, tv_loss: 0.03222404047846794\n",
      "iteration 1049, dc_loss: 0.07493843883275986, tv_loss: 0.03214098885655403\n",
      "iteration 1050, dc_loss: 0.07479534298181534, tv_loss: 0.0321890190243721\n",
      "iteration 1051, dc_loss: 0.07467875629663467, tv_loss: 0.032203637063503265\n",
      "iteration 1052, dc_loss: 0.07465820014476776, tv_loss: 0.03213458135724068\n",
      "iteration 1053, dc_loss: 0.0744486004114151, tv_loss: 0.032253626734018326\n",
      "iteration 1054, dc_loss: 0.07449474930763245, tv_loss: 0.032118480652570724\n",
      "iteration 1055, dc_loss: 0.07420028001070023, tv_loss: 0.03228382021188736\n",
      "iteration 1056, dc_loss: 0.07425088435411453, tv_loss: 0.03211930766701698\n",
      "iteration 1057, dc_loss: 0.07395300269126892, tv_loss: 0.03228835389018059\n",
      "iteration 1058, dc_loss: 0.07394995540380478, tv_loss: 0.03214496001601219\n",
      "iteration 1059, dc_loss: 0.07367891818284988, tv_loss: 0.03226719796657562\n",
      "iteration 1060, dc_loss: 0.07362526655197144, tv_loss: 0.032175738364458084\n",
      "iteration 1061, dc_loss: 0.0734289363026619, tv_loss: 0.03224051743745804\n",
      "iteration 1062, dc_loss: 0.07333515584468842, tv_loss: 0.0322086401283741\n",
      "iteration 1063, dc_loss: 0.07319948822259903, tv_loss: 0.03223245218396187\n",
      "iteration 1064, dc_loss: 0.07310973107814789, tv_loss: 0.03221095725893974\n",
      "iteration 1065, dc_loss: 0.07299164682626724, tv_loss: 0.03222553804516792\n",
      "iteration 1066, dc_loss: 0.07289683818817139, tv_loss: 0.03222076594829559\n",
      "iteration 1067, dc_loss: 0.0727669969201088, tv_loss: 0.032254401594400406\n",
      "iteration 1068, dc_loss: 0.07270723581314087, tv_loss: 0.03222907707095146\n",
      "iteration 1069, dc_loss: 0.07253298908472061, tv_loss: 0.0322854220867157\n",
      "iteration 1070, dc_loss: 0.0725099965929985, tv_loss: 0.0321950800716877\n",
      "iteration 1071, dc_loss: 0.07230179756879807, tv_loss: 0.032318536192178726\n",
      "iteration 1072, dc_loss: 0.072306327521801, tv_loss: 0.032202742993831635\n",
      "iteration 1073, dc_loss: 0.07207315415143967, tv_loss: 0.03232816234230995\n",
      "iteration 1074, dc_loss: 0.07210593670606613, tv_loss: 0.03219452500343323\n",
      "iteration 1075, dc_loss: 0.07187161594629288, tv_loss: 0.032370515167713165\n",
      "iteration 1076, dc_loss: 0.0719732716679573, tv_loss: 0.03218507394194603\n",
      "iteration 1077, dc_loss: 0.07169168442487717, tv_loss: 0.03240063413977623\n",
      "iteration 1078, dc_loss: 0.07191477715969086, tv_loss: 0.03211808204650879\n",
      "iteration 1079, dc_loss: 0.07154545933008194, tv_loss: 0.03248772770166397\n",
      "iteration 1080, dc_loss: 0.07182388752698898, tv_loss: 0.03208335489034653\n",
      "iteration 1081, dc_loss: 0.0713370218873024, tv_loss: 0.03249787539243698\n",
      "iteration 1082, dc_loss: 0.07156091183423996, tv_loss: 0.03210742771625519\n",
      "iteration 1083, dc_loss: 0.07099466770887375, tv_loss: 0.03249317407608032\n",
      "iteration 1084, dc_loss: 0.07109960168600082, tv_loss: 0.03215990588068962\n",
      "iteration 1085, dc_loss: 0.0706990659236908, tv_loss: 0.03240501880645752\n",
      "iteration 1086, dc_loss: 0.07070225477218628, tv_loss: 0.03226882591843605\n",
      "iteration 1087, dc_loss: 0.07059811800718307, tv_loss: 0.03228330612182617\n",
      "iteration 1088, dc_loss: 0.07044976949691772, tv_loss: 0.03238602727651596\n",
      "iteration 1089, dc_loss: 0.07056211680173874, tv_loss: 0.032208070158958435\n",
      "iteration 1090, dc_loss: 0.07026098668575287, tv_loss: 0.0324489027261734\n",
      "iteration 1091, dc_loss: 0.07038507610559464, tv_loss: 0.03219909220933914\n",
      "iteration 1092, dc_loss: 0.07003241032361984, tv_loss: 0.03244325891137123\n",
      "iteration 1093, dc_loss: 0.07004456222057343, tv_loss: 0.032264597713947296\n",
      "iteration 1094, dc_loss: 0.06984014809131622, tv_loss: 0.03235020861029625\n",
      "iteration 1095, dc_loss: 0.06970976293087006, tv_loss: 0.03237604722380638\n",
      "iteration 1096, dc_loss: 0.06973940134048462, tv_loss: 0.03226200118660927\n",
      "iteration 1097, dc_loss: 0.06949145346879959, tv_loss: 0.03245778754353523\n",
      "iteration 1098, dc_loss: 0.0696585550904274, tv_loss: 0.03222205117344856\n",
      "iteration 1099, dc_loss: 0.0693155899643898, tv_loss: 0.03249940276145935\n",
      "iteration 1100, dc_loss: 0.06948935985565186, tv_loss: 0.03221016749739647\n",
      "iteration 1101, dc_loss: 0.06909869611263275, tv_loss: 0.03247839957475662\n",
      "iteration 1102, dc_loss: 0.06914716213941574, tv_loss: 0.03226487338542938\n",
      "iteration 1103, dc_loss: 0.06886400282382965, tv_loss: 0.0324116013944149\n",
      "iteration 1104, dc_loss: 0.0688040554523468, tv_loss: 0.03237224370241165\n",
      "iteration 1105, dc_loss: 0.06871452927589417, tv_loss: 0.03234380856156349\n",
      "iteration 1106, dc_loss: 0.06857267022132874, tv_loss: 0.03240933641791344\n",
      "iteration 1107, dc_loss: 0.06859543919563293, tv_loss: 0.03231048211455345\n",
      "iteration 1108, dc_loss: 0.06834327429533005, tv_loss: 0.032462622970342636\n",
      "iteration 1109, dc_loss: 0.06842344254255295, tv_loss: 0.032275572419166565\n",
      "iteration 1110, dc_loss: 0.06811562180519104, tv_loss: 0.03248954564332962\n",
      "iteration 1111, dc_loss: 0.06818916648626328, tv_loss: 0.032296936959028244\n",
      "iteration 1112, dc_loss: 0.06791026890277863, tv_loss: 0.032464444637298584\n",
      "iteration 1113, dc_loss: 0.06794998794794083, tv_loss: 0.032316904515028\n",
      "iteration 1114, dc_loss: 0.06772158294916153, tv_loss: 0.03245840221643448\n",
      "iteration 1115, dc_loss: 0.06772235035896301, tv_loss: 0.032352011650800705\n",
      "iteration 1116, dc_loss: 0.06754343211650848, tv_loss: 0.03244253247976303\n",
      "iteration 1117, dc_loss: 0.06748167425394058, tv_loss: 0.0324244350194931\n",
      "iteration 1118, dc_loss: 0.06735824793577194, tv_loss: 0.03243327513337135\n",
      "iteration 1119, dc_loss: 0.06726477295160294, tv_loss: 0.03241746500134468\n",
      "iteration 1120, dc_loss: 0.0671815425157547, tv_loss: 0.032402265816926956\n",
      "iteration 1121, dc_loss: 0.06704114377498627, tv_loss: 0.03244036063551903\n",
      "iteration 1122, dc_loss: 0.06702332943677902, tv_loss: 0.03241071477532387\n",
      "iteration 1123, dc_loss: 0.06684960424900055, tv_loss: 0.032485008239746094\n",
      "iteration 1124, dc_loss: 0.06687015295028687, tv_loss: 0.03238215297460556\n",
      "iteration 1125, dc_loss: 0.06667251884937286, tv_loss: 0.03251619637012482\n",
      "iteration 1126, dc_loss: 0.06678512692451477, tv_loss: 0.03234652429819107\n",
      "iteration 1127, dc_loss: 0.06654955446720123, tv_loss: 0.03252813592553139\n",
      "iteration 1128, dc_loss: 0.06668238341808319, tv_loss: 0.03237069770693779\n",
      "iteration 1129, dc_loss: 0.06647153943777084, tv_loss: 0.032535914331674576\n",
      "iteration 1130, dc_loss: 0.06660464406013489, tv_loss: 0.03237161785364151\n",
      "iteration 1131, dc_loss: 0.06636969745159149, tv_loss: 0.032544516026973724\n",
      "iteration 1132, dc_loss: 0.06640863418579102, tv_loss: 0.03243746981024742\n",
      "iteration 1133, dc_loss: 0.06618691235780716, tv_loss: 0.032481156289577484\n",
      "iteration 1134, dc_loss: 0.06603903323411942, tv_loss: 0.03248323127627373\n",
      "iteration 1135, dc_loss: 0.06597522646188736, tv_loss: 0.032399117946624756\n",
      "iteration 1136, dc_loss: 0.06574919819831848, tv_loss: 0.03259632736444473\n",
      "iteration 1137, dc_loss: 0.06602521240711212, tv_loss: 0.032295674085617065\n",
      "iteration 1138, dc_loss: 0.06562274694442749, tv_loss: 0.032744377851486206\n",
      "iteration 1139, dc_loss: 0.06605012714862823, tv_loss: 0.03219424560666084\n",
      "iteration 1140, dc_loss: 0.06537309288978577, tv_loss: 0.032760489732027054\n",
      "iteration 1141, dc_loss: 0.06556978821754456, tv_loss: 0.03227255865931511\n",
      "iteration 1142, dc_loss: 0.06504727154970169, tv_loss: 0.0326014906167984\n",
      "iteration 1143, dc_loss: 0.06501510739326477, tv_loss: 0.03248412907123566\n",
      "iteration 1144, dc_loss: 0.06505852937698364, tv_loss: 0.03240413963794708\n",
      "iteration 1145, dc_loss: 0.06480027735233307, tv_loss: 0.03265288844704628\n",
      "iteration 1146, dc_loss: 0.06502719968557358, tv_loss: 0.032338857650756836\n",
      "iteration 1147, dc_loss: 0.0646006315946579, tv_loss: 0.03264391049742699\n",
      "iteration 1148, dc_loss: 0.06467816233634949, tv_loss: 0.03240944445133209\n",
      "iteration 1149, dc_loss: 0.0644327700138092, tv_loss: 0.03253662586212158\n",
      "iteration 1150, dc_loss: 0.06435441970825195, tv_loss: 0.03256174176931381\n",
      "iteration 1151, dc_loss: 0.06443973630666733, tv_loss: 0.0324152335524559\n",
      "iteration 1152, dc_loss: 0.06416812539100647, tv_loss: 0.032613568007946014\n",
      "iteration 1153, dc_loss: 0.06421024352312088, tv_loss: 0.03245958313345909\n",
      "iteration 1154, dc_loss: 0.06399643421173096, tv_loss: 0.032549988478422165\n",
      "iteration 1155, dc_loss: 0.06391023099422455, tv_loss: 0.03251922130584717\n",
      "iteration 1156, dc_loss: 0.06385598331689835, tv_loss: 0.032488759607076645\n",
      "iteration 1157, dc_loss: 0.06370167434215546, tv_loss: 0.032577890902757645\n",
      "iteration 1158, dc_loss: 0.06372314691543579, tv_loss: 0.03249742463231087\n",
      "iteration 1159, dc_loss: 0.06354338675737381, tv_loss: 0.03260594606399536\n",
      "iteration 1160, dc_loss: 0.06349468976259232, tv_loss: 0.03255156800150871\n",
      "iteration 1161, dc_loss: 0.06341033428907394, tv_loss: 0.032532233744859695\n",
      "iteration 1162, dc_loss: 0.06328727304935455, tv_loss: 0.03258451074361801\n",
      "iteration 1163, dc_loss: 0.06323549151420593, tv_loss: 0.03254188597202301\n",
      "iteration 1164, dc_loss: 0.06309561431407928, tv_loss: 0.03260510787367821\n",
      "iteration 1165, dc_loss: 0.06306174397468567, tv_loss: 0.03254377469420433\n",
      "iteration 1166, dc_loss: 0.06295714527368546, tv_loss: 0.03257601708173752\n",
      "iteration 1167, dc_loss: 0.06289637088775635, tv_loss: 0.0325629860162735\n",
      "iteration 1168, dc_loss: 0.06277967244386673, tv_loss: 0.032589033246040344\n",
      "iteration 1169, dc_loss: 0.06272923946380615, tv_loss: 0.032549548894166946\n",
      "iteration 1170, dc_loss: 0.0625782236456871, tv_loss: 0.03263537958264351\n",
      "iteration 1171, dc_loss: 0.06261702626943588, tv_loss: 0.032510556280612946\n",
      "iteration 1172, dc_loss: 0.062408749014139175, tv_loss: 0.03265516832470894\n",
      "iteration 1173, dc_loss: 0.06252281367778778, tv_loss: 0.0324968658387661\n",
      "iteration 1174, dc_loss: 0.062324345111846924, tv_loss: 0.03269192576408386\n",
      "iteration 1175, dc_loss: 0.06251616030931473, tv_loss: 0.03249990940093994\n",
      "iteration 1176, dc_loss: 0.06238611787557602, tv_loss: 0.03271899372339249\n",
      "iteration 1177, dc_loss: 0.06267186999320984, tv_loss: 0.032473862171173096\n",
      "iteration 1178, dc_loss: 0.06261269748210907, tv_loss: 0.032788604497909546\n",
      "iteration 1179, dc_loss: 0.06286568939685822, tv_loss: 0.0324389673769474\n",
      "iteration 1180, dc_loss: 0.062413375824689865, tv_loss: 0.03282535448670387\n",
      "iteration 1181, dc_loss: 0.06238526105880737, tv_loss: 0.032415036112070084\n",
      "iteration 1182, dc_loss: 0.06168144568800926, tv_loss: 0.032781071960926056\n",
      "iteration 1183, dc_loss: 0.061822559684515, tv_loss: 0.032490722835063934\n",
      "iteration 1184, dc_loss: 0.06169680878520012, tv_loss: 0.03270135074853897\n",
      "iteration 1185, dc_loss: 0.061896372586488724, tv_loss: 0.032586537301540375\n",
      "iteration 1186, dc_loss: 0.06165573373436928, tv_loss: 0.032638803124427795\n",
      "iteration 1187, dc_loss: 0.06136666238307953, tv_loss: 0.03263643756508827\n",
      "iteration 1188, dc_loss: 0.06115177646279335, tv_loss: 0.03261604532599449\n",
      "iteration 1189, dc_loss: 0.06117360666394234, tv_loss: 0.03258461877703667\n",
      "iteration 1190, dc_loss: 0.06114695221185684, tv_loss: 0.032670311629772186\n",
      "iteration 1191, dc_loss: 0.06110252067446709, tv_loss: 0.032619789242744446\n",
      "iteration 1192, dc_loss: 0.06093095242977142, tv_loss: 0.03262679651379585\n",
      "iteration 1193, dc_loss: 0.0607195645570755, tv_loss: 0.03269267454743385\n",
      "iteration 1194, dc_loss: 0.060823697596788406, tv_loss: 0.03255446255207062\n",
      "iteration 1195, dc_loss: 0.060620564967393875, tv_loss: 0.03273237869143486\n",
      "iteration 1196, dc_loss: 0.060717690736055374, tv_loss: 0.03254758566617966\n",
      "iteration 1197, dc_loss: 0.0604107640683651, tv_loss: 0.03274330124258995\n",
      "iteration 1198, dc_loss: 0.06043720245361328, tv_loss: 0.032590676099061966\n",
      "iteration 1199, dc_loss: 0.06026013568043709, tv_loss: 0.03268759325146675\n",
      "iteration 1200, dc_loss: 0.06027178093791008, tv_loss: 0.03262509033083916\n",
      "iteration 1201, dc_loss: 0.06015359237790108, tv_loss: 0.032688651233911514\n",
      "iteration 1202, dc_loss: 0.06005197390913963, tv_loss: 0.032651130110025406\n",
      "iteration 1203, dc_loss: 0.060007061809301376, tv_loss: 0.032644808292388916\n",
      "iteration 1204, dc_loss: 0.05994664877653122, tv_loss: 0.03268691152334213\n",
      "iteration 1205, dc_loss: 0.05986740067601204, tv_loss: 0.03265167027711868\n",
      "iteration 1206, dc_loss: 0.05980310216546059, tv_loss: 0.03266911953687668\n",
      "iteration 1207, dc_loss: 0.059744853526353836, tv_loss: 0.032691169530153275\n",
      "iteration 1208, dc_loss: 0.05969352275133133, tv_loss: 0.0326409712433815\n",
      "iteration 1209, dc_loss: 0.05962157994508743, tv_loss: 0.032662373036146164\n",
      "iteration 1210, dc_loss: 0.05955051630735397, tv_loss: 0.03269163891673088\n",
      "iteration 1211, dc_loss: 0.05949930474162102, tv_loss: 0.032652564346790314\n",
      "iteration 1212, dc_loss: 0.05942603573203087, tv_loss: 0.03267333656549454\n",
      "iteration 1213, dc_loss: 0.05937480553984642, tv_loss: 0.03267889469861984\n",
      "iteration 1214, dc_loss: 0.05931318551301956, tv_loss: 0.03266692906618118\n",
      "iteration 1215, dc_loss: 0.059233199805021286, tv_loss: 0.032685667276382446\n",
      "iteration 1216, dc_loss: 0.059198249131441116, tv_loss: 0.03266538679599762\n",
      "iteration 1217, dc_loss: 0.05911879613995552, tv_loss: 0.03268815949559212\n",
      "iteration 1218, dc_loss: 0.05904895067214966, tv_loss: 0.032692499458789825\n",
      "iteration 1219, dc_loss: 0.05901710316538811, tv_loss: 0.03266815468668938\n",
      "iteration 1220, dc_loss: 0.05893591418862343, tv_loss: 0.03269055113196373\n",
      "iteration 1221, dc_loss: 0.058869704604148865, tv_loss: 0.03271143510937691\n",
      "iteration 1222, dc_loss: 0.05883047357201576, tv_loss: 0.032701194286346436\n",
      "iteration 1223, dc_loss: 0.05875786393880844, tv_loss: 0.03270215168595314\n",
      "iteration 1224, dc_loss: 0.058695029467344284, tv_loss: 0.03269915655255318\n",
      "iteration 1225, dc_loss: 0.05864380672574043, tv_loss: 0.03269219398498535\n",
      "iteration 1226, dc_loss: 0.05856944993138313, tv_loss: 0.03270535543560982\n",
      "iteration 1227, dc_loss: 0.058529775589704514, tv_loss: 0.032703302800655365\n",
      "iteration 1228, dc_loss: 0.058464765548706055, tv_loss: 0.032726678997278214\n",
      "iteration 1229, dc_loss: 0.05838463827967644, tv_loss: 0.032728761434555054\n",
      "iteration 1230, dc_loss: 0.05835612490773201, tv_loss: 0.03269222751259804\n",
      "iteration 1231, dc_loss: 0.058280088007450104, tv_loss: 0.032716210931539536\n",
      "iteration 1232, dc_loss: 0.05822752043604851, tv_loss: 0.032714154571294785\n",
      "iteration 1233, dc_loss: 0.05817379802465439, tv_loss: 0.03272281959652901\n",
      "iteration 1234, dc_loss: 0.058090243488550186, tv_loss: 0.032754085958004\n",
      "iteration 1235, dc_loss: 0.05806352570652962, tv_loss: 0.03271125629544258\n",
      "iteration 1236, dc_loss: 0.057996828109025955, tv_loss: 0.032712746411561966\n",
      "iteration 1237, dc_loss: 0.05792280286550522, tv_loss: 0.03273364156484604\n",
      "iteration 1238, dc_loss: 0.0578918419778347, tv_loss: 0.032708898186683655\n",
      "iteration 1239, dc_loss: 0.057810552418231964, tv_loss: 0.03273361921310425\n",
      "iteration 1240, dc_loss: 0.05776108056306839, tv_loss: 0.0327252633869648\n",
      "iteration 1241, dc_loss: 0.05770950764417648, tv_loss: 0.03274466097354889\n",
      "iteration 1242, dc_loss: 0.05763126537203789, tv_loss: 0.032752007246017456\n",
      "iteration 1243, dc_loss: 0.057600971311330795, tv_loss: 0.0327284038066864\n",
      "iteration 1244, dc_loss: 0.057546455413103104, tv_loss: 0.032723959535360336\n",
      "iteration 1245, dc_loss: 0.05745813250541687, tv_loss: 0.03275619447231293\n",
      "iteration 1246, dc_loss: 0.057426586747169495, tv_loss: 0.032741401344537735\n",
      "iteration 1247, dc_loss: 0.057360030710697174, tv_loss: 0.032753828912973404\n",
      "iteration 1248, dc_loss: 0.05729975923895836, tv_loss: 0.03275582566857338\n",
      "iteration 1249, dc_loss: 0.057264164090156555, tv_loss: 0.032731544226408005\n",
      "iteration 1250, dc_loss: 0.057184696197509766, tv_loss: 0.0327538326382637\n",
      "iteration 1251, dc_loss: 0.057125892490148544, tv_loss: 0.03276051953434944\n",
      "iteration 1252, dc_loss: 0.057087380439043045, tv_loss: 0.03276032954454422\n",
      "iteration 1253, dc_loss: 0.057018619030714035, tv_loss: 0.032769251614809036\n",
      "iteration 1254, dc_loss: 0.05696985125541687, tv_loss: 0.03276694566011429\n",
      "iteration 1255, dc_loss: 0.05691487714648247, tv_loss: 0.0327579490840435\n",
      "iteration 1256, dc_loss: 0.056862231343984604, tv_loss: 0.0327548123896122\n",
      "iteration 1257, dc_loss: 0.056792039424180984, tv_loss: 0.0327763631939888\n",
      "iteration 1258, dc_loss: 0.056744761765003204, tv_loss: 0.03277641907334328\n",
      "iteration 1259, dc_loss: 0.05669570341706276, tv_loss: 0.032765503972768784\n",
      "iteration 1260, dc_loss: 0.056629423052072525, tv_loss: 0.032766323536634445\n",
      "iteration 1261, dc_loss: 0.05659271031618118, tv_loss: 0.03275756165385246\n",
      "iteration 1262, dc_loss: 0.056510671973228455, tv_loss: 0.032786186784505844\n",
      "iteration 1263, dc_loss: 0.05646970495581627, tv_loss: 0.03277480602264404\n",
      "iteration 1264, dc_loss: 0.05642621964216232, tv_loss: 0.032760441303253174\n",
      "iteration 1265, dc_loss: 0.05634529888629913, tv_loss: 0.032797448337078094\n",
      "iteration 1266, dc_loss: 0.05632079765200615, tv_loss: 0.03277656063437462\n",
      "iteration 1267, dc_loss: 0.05624153092503548, tv_loss: 0.03279914706945419\n",
      "iteration 1268, dc_loss: 0.056194983422756195, tv_loss: 0.03279193863272667\n",
      "iteration 1269, dc_loss: 0.056138522922992706, tv_loss: 0.032785866409540176\n",
      "iteration 1270, dc_loss: 0.056085363030433655, tv_loss: 0.0327872559428215\n",
      "iteration 1271, dc_loss: 0.056038159877061844, tv_loss: 0.03277625888586044\n",
      "iteration 1272, dc_loss: 0.055977724492549896, tv_loss: 0.03278496116399765\n",
      "iteration 1273, dc_loss: 0.05593548342585564, tv_loss: 0.032777126878499985\n",
      "iteration 1274, dc_loss: 0.05586232990026474, tv_loss: 0.03279932588338852\n",
      "iteration 1275, dc_loss: 0.055811118334531784, tv_loss: 0.032794371247291565\n",
      "iteration 1276, dc_loss: 0.055770233273506165, tv_loss: 0.03278912976384163\n",
      "iteration 1277, dc_loss: 0.05570852383971214, tv_loss: 0.032808803021907806\n",
      "iteration 1278, dc_loss: 0.055660832673311234, tv_loss: 0.0328066311776638\n",
      "iteration 1279, dc_loss: 0.055599432438611984, tv_loss: 0.03281345218420029\n",
      "iteration 1280, dc_loss: 0.05556318908929825, tv_loss: 0.03279741108417511\n",
      "iteration 1281, dc_loss: 0.05548406392335892, tv_loss: 0.03282308578491211\n",
      "iteration 1282, dc_loss: 0.055442534387111664, tv_loss: 0.032805588096380234\n",
      "iteration 1283, dc_loss: 0.055390432476997375, tv_loss: 0.032802119851112366\n",
      "iteration 1284, dc_loss: 0.05533576011657715, tv_loss: 0.03281015530228615\n",
      "iteration 1285, dc_loss: 0.05529240146279335, tv_loss: 0.0328122153878212\n",
      "iteration 1286, dc_loss: 0.05523194000124931, tv_loss: 0.03281854838132858\n",
      "iteration 1287, dc_loss: 0.05517369881272316, tv_loss: 0.03283229470252991\n",
      "iteration 1288, dc_loss: 0.05514033883810043, tv_loss: 0.032837286591529846\n",
      "iteration 1289, dc_loss: 0.05506633594632149, tv_loss: 0.03284822031855583\n",
      "iteration 1290, dc_loss: 0.055028922855854034, tv_loss: 0.03283071145415306\n",
      "iteration 1291, dc_loss: 0.054964225739240646, tv_loss: 0.032841719686985016\n",
      "iteration 1292, dc_loss: 0.054931119084358215, tv_loss: 0.032843928784132004\n",
      "iteration 1293, dc_loss: 0.054861340671777725, tv_loss: 0.03285321965813637\n",
      "iteration 1294, dc_loss: 0.054825086146593094, tv_loss: 0.03282633051276207\n",
      "iteration 1295, dc_loss: 0.054764438420534134, tv_loss: 0.03284492716193199\n",
      "iteration 1296, dc_loss: 0.05470649525523186, tv_loss: 0.032858606427907944\n",
      "iteration 1297, dc_loss: 0.054662223905324936, tv_loss: 0.03286339342594147\n",
      "iteration 1298, dc_loss: 0.054619111120700836, tv_loss: 0.03283087536692619\n",
      "iteration 1299, dc_loss: 0.05456402897834778, tv_loss: 0.03283355012536049\n",
      "iteration 1300, dc_loss: 0.054495953023433685, tv_loss: 0.03286579251289368\n",
      "iteration 1301, dc_loss: 0.05446482077240944, tv_loss: 0.03285166248679161\n",
      "iteration 1302, dc_loss: 0.05439490079879761, tv_loss: 0.03287310153245926\n",
      "iteration 1303, dc_loss: 0.05436144024133682, tv_loss: 0.03286133334040642\n",
      "iteration 1304, dc_loss: 0.05430765822529793, tv_loss: 0.032843589782714844\n",
      "iteration 1305, dc_loss: 0.05426105484366417, tv_loss: 0.03285063058137894\n",
      "iteration 1306, dc_loss: 0.05420424044132233, tv_loss: 0.03288324922323227\n",
      "iteration 1307, dc_loss: 0.05413774773478508, tv_loss: 0.03288158401846886\n",
      "iteration 1308, dc_loss: 0.05410805344581604, tv_loss: 0.03286369517445564\n",
      "iteration 1309, dc_loss: 0.05405830591917038, tv_loss: 0.03287041187286377\n",
      "iteration 1310, dc_loss: 0.054003212600946426, tv_loss: 0.03287482261657715\n",
      "iteration 1311, dc_loss: 0.05394725501537323, tv_loss: 0.03289042040705681\n",
      "iteration 1312, dc_loss: 0.053911589086055756, tv_loss: 0.032867204397916794\n",
      "iteration 1313, dc_loss: 0.053833238780498505, tv_loss: 0.0329120010137558\n",
      "iteration 1314, dc_loss: 0.053832072764635086, tv_loss: 0.0328545980155468\n",
      "iteration 1315, dc_loss: 0.0537344329059124, tv_loss: 0.032902251929044724\n",
      "iteration 1316, dc_loss: 0.05372607335448265, tv_loss: 0.0328662246465683\n",
      "iteration 1317, dc_loss: 0.053640495985746384, tv_loss: 0.032912492752075195\n",
      "iteration 1318, dc_loss: 0.05362492799758911, tv_loss: 0.032879505306482315\n",
      "iteration 1319, dc_loss: 0.05353249981999397, tv_loss: 0.03290447220206261\n",
      "iteration 1320, dc_loss: 0.053544338792562485, tv_loss: 0.03285958990454674\n",
      "iteration 1321, dc_loss: 0.05343272164463997, tv_loss: 0.03293723613023758\n",
      "iteration 1322, dc_loss: 0.05347288027405739, tv_loss: 0.032850466668605804\n",
      "iteration 1323, dc_loss: 0.053323887288570404, tv_loss: 0.03295733034610748\n",
      "iteration 1324, dc_loss: 0.05342256277799606, tv_loss: 0.032806020230054855\n",
      "iteration 1325, dc_loss: 0.05323753133416176, tv_loss: 0.032972969114780426\n",
      "iteration 1326, dc_loss: 0.05337497591972351, tv_loss: 0.03281901404261589\n",
      "iteration 1327, dc_loss: 0.053161852061748505, tv_loss: 0.03301730006933212\n",
      "iteration 1328, dc_loss: 0.05336059257388115, tv_loss: 0.03277001902461052\n",
      "iteration 1329, dc_loss: 0.053089726716279984, tv_loss: 0.033038750290870667\n",
      "iteration 1330, dc_loss: 0.05331072583794594, tv_loss: 0.0327720008790493\n",
      "iteration 1331, dc_loss: 0.05295370891690254, tv_loss: 0.03306245431303978\n",
      "iteration 1332, dc_loss: 0.053119707852602005, tv_loss: 0.03280026838183403\n",
      "iteration 1333, dc_loss: 0.05284745246171951, tv_loss: 0.032980069518089294\n",
      "iteration 1334, dc_loss: 0.0528830923140049, tv_loss: 0.032882533967494965\n",
      "iteration 1335, dc_loss: 0.05277992784976959, tv_loss: 0.03292839601635933\n",
      "iteration 1336, dc_loss: 0.052709754556417465, tv_loss: 0.03294915705919266\n",
      "iteration 1337, dc_loss: 0.05275119096040726, tv_loss: 0.03286098688840866\n",
      "iteration 1338, dc_loss: 0.05261165276169777, tv_loss: 0.03299301490187645\n",
      "iteration 1339, dc_loss: 0.052707474678754807, tv_loss: 0.03285134583711624\n",
      "iteration 1340, dc_loss: 0.05252929776906967, tv_loss: 0.03298819810152054\n",
      "iteration 1341, dc_loss: 0.05259135738015175, tv_loss: 0.03288853168487549\n",
      "iteration 1342, dc_loss: 0.05240897461771965, tv_loss: 0.03300294280052185\n",
      "iteration 1343, dc_loss: 0.05248786136507988, tv_loss: 0.032866816967725754\n",
      "iteration 1344, dc_loss: 0.052328143268823624, tv_loss: 0.03297007083892822\n",
      "iteration 1345, dc_loss: 0.05233302712440491, tv_loss: 0.03290558606386185\n",
      "iteration 1346, dc_loss: 0.052268192172050476, tv_loss: 0.032949481159448624\n",
      "iteration 1347, dc_loss: 0.052184849977493286, tv_loss: 0.03297032415866852\n",
      "iteration 1348, dc_loss: 0.05221206322312355, tv_loss: 0.03289920091629028\n",
      "iteration 1349, dc_loss: 0.05210372060537338, tv_loss: 0.03297363594174385\n",
      "iteration 1350, dc_loss: 0.052121348679065704, tv_loss: 0.03290993347764015\n",
      "iteration 1351, dc_loss: 0.0520118772983551, tv_loss: 0.0329715721309185\n",
      "iteration 1352, dc_loss: 0.052042704075574875, tv_loss: 0.03289858624339104\n",
      "iteration 1353, dc_loss: 0.051902107894420624, tv_loss: 0.03298630565404892\n",
      "iteration 1354, dc_loss: 0.051945824176073074, tv_loss: 0.032899875193834305\n",
      "iteration 1355, dc_loss: 0.051814425736665726, tv_loss: 0.0329873226583004\n",
      "iteration 1356, dc_loss: 0.0518600232899189, tv_loss: 0.032911162823438644\n",
      "iteration 1357, dc_loss: 0.05172562599182129, tv_loss: 0.03298885002732277\n",
      "iteration 1358, dc_loss: 0.051785100251436234, tv_loss: 0.03289800137281418\n",
      "iteration 1359, dc_loss: 0.05165243148803711, tv_loss: 0.032992053776979446\n",
      "iteration 1360, dc_loss: 0.05168839916586876, tv_loss: 0.032915256917476654\n",
      "iteration 1361, dc_loss: 0.05154569819569588, tv_loss: 0.033014606684446335\n",
      "iteration 1362, dc_loss: 0.05162068083882332, tv_loss: 0.03290848061442375\n",
      "iteration 1363, dc_loss: 0.05146721377968788, tv_loss: 0.03300878778100014\n",
      "iteration 1364, dc_loss: 0.05154182389378548, tv_loss: 0.032889124006032944\n",
      "iteration 1365, dc_loss: 0.05136098340153694, tv_loss: 0.03302731364965439\n",
      "iteration 1366, dc_loss: 0.05147101730108261, tv_loss: 0.032875895500183105\n",
      "iteration 1367, dc_loss: 0.05126428231596947, tv_loss: 0.033044010400772095\n",
      "iteration 1368, dc_loss: 0.05141100287437439, tv_loss: 0.03286658227443695\n",
      "iteration 1369, dc_loss: 0.05118735134601593, tv_loss: 0.03307684510946274\n",
      "iteration 1370, dc_loss: 0.05135833099484444, tv_loss: 0.03287769481539726\n",
      "iteration 1371, dc_loss: 0.05110583081841469, tv_loss: 0.03309333324432373\n",
      "iteration 1372, dc_loss: 0.05131125450134277, tv_loss: 0.0328400693833828\n",
      "iteration 1373, dc_loss: 0.05102745443582535, tv_loss: 0.033092912286520004\n",
      "iteration 1374, dc_loss: 0.051194120198488235, tv_loss: 0.032864443957805634\n",
      "iteration 1375, dc_loss: 0.05091991275548935, tv_loss: 0.033091749995946884\n",
      "iteration 1376, dc_loss: 0.05105186253786087, tv_loss: 0.03288527950644493\n",
      "iteration 1377, dc_loss: 0.05083712562918663, tv_loss: 0.03304765000939369\n",
      "iteration 1378, dc_loss: 0.05088333413004875, tv_loss: 0.032927919179201126\n",
      "iteration 1379, dc_loss: 0.05075184628367424, tv_loss: 0.03300761431455612\n",
      "iteration 1380, dc_loss: 0.050738364458084106, tv_loss: 0.03297050669789314\n",
      "iteration 1381, dc_loss: 0.050703294575214386, tv_loss: 0.03297022357583046\n",
      "iteration 1382, dc_loss: 0.05062362551689148, tv_loss: 0.033009663224220276\n",
      "iteration 1383, dc_loss: 0.0506720170378685, tv_loss: 0.03292335569858551\n",
      "iteration 1384, dc_loss: 0.05051495134830475, tv_loss: 0.03304458037018776\n",
      "iteration 1385, dc_loss: 0.05062609165906906, tv_loss: 0.03290478512644768\n",
      "iteration 1386, dc_loss: 0.050434328615665436, tv_loss: 0.03307991102337837\n",
      "iteration 1387, dc_loss: 0.05058402940630913, tv_loss: 0.03290117532014847\n",
      "iteration 1388, dc_loss: 0.050346408039331436, tv_loss: 0.03310519456863403\n",
      "iteration 1389, dc_loss: 0.05047791078686714, tv_loss: 0.03291280195116997\n",
      "iteration 1390, dc_loss: 0.050265274941921234, tv_loss: 0.03308524936437607\n",
      "iteration 1391, dc_loss: 0.05036172270774841, tv_loss: 0.03292300924658775\n",
      "iteration 1392, dc_loss: 0.05018049478530884, tv_loss: 0.03306210786104202\n",
      "iteration 1393, dc_loss: 0.05024897679686546, tv_loss: 0.032943833619356155\n",
      "iteration 1394, dc_loss: 0.05009884759783745, tv_loss: 0.03305038809776306\n",
      "iteration 1395, dc_loss: 0.05010700970888138, tv_loss: 0.03299752250313759\n",
      "iteration 1396, dc_loss: 0.050016533583402634, tv_loss: 0.03302851691842079\n",
      "iteration 1397, dc_loss: 0.050009533762931824, tv_loss: 0.03299945592880249\n",
      "iteration 1398, dc_loss: 0.049958955496549606, tv_loss: 0.03300165385007858\n",
      "iteration 1399, dc_loss: 0.04989152029156685, tv_loss: 0.03302322328090668\n",
      "iteration 1400, dc_loss: 0.04988543316721916, tv_loss: 0.032995570451021194\n",
      "iteration 1401, dc_loss: 0.04979246109724045, tv_loss: 0.03305453062057495\n",
      "iteration 1402, dc_loss: 0.049827881157398224, tv_loss: 0.032966919243335724\n",
      "iteration 1403, dc_loss: 0.04970742389559746, tv_loss: 0.03306099772453308\n",
      "iteration 1404, dc_loss: 0.049769189208745956, tv_loss: 0.03297439590096474\n",
      "iteration 1405, dc_loss: 0.04963776841759682, tv_loss: 0.033082786947488785\n",
      "iteration 1406, dc_loss: 0.049766696989536285, tv_loss: 0.032931167632341385\n",
      "iteration 1407, dc_loss: 0.049568064510822296, tv_loss: 0.03314170241355896\n",
      "iteration 1408, dc_loss: 0.049802251160144806, tv_loss: 0.03289330378174782\n",
      "iteration 1409, dc_loss: 0.04953509569168091, tv_loss: 0.03316613659262657\n",
      "iteration 1410, dc_loss: 0.049815334379673004, tv_loss: 0.0328645333647728\n",
      "iteration 1411, dc_loss: 0.04947622865438461, tv_loss: 0.03320407494902611\n",
      "iteration 1412, dc_loss: 0.049711257219314575, tv_loss: 0.03286853805184364\n",
      "iteration 1413, dc_loss: 0.04933569207787514, tv_loss: 0.03316565603017807\n",
      "iteration 1414, dc_loss: 0.04945898801088333, tv_loss: 0.03293055295944214\n",
      "iteration 1415, dc_loss: 0.04921623691916466, tv_loss: 0.0331004336476326\n",
      "iteration 1416, dc_loss: 0.04925481975078583, tv_loss: 0.0330209955573082\n",
      "iteration 1417, dc_loss: 0.04921691492199898, tv_loss: 0.03302321955561638\n",
      "iteration 1418, dc_loss: 0.049152132123708725, tv_loss: 0.033066630363464355\n",
      "iteration 1419, dc_loss: 0.0491984486579895, tv_loss: 0.03299146145582199\n",
      "iteration 1420, dc_loss: 0.049042824655771255, tv_loss: 0.03309369832277298\n",
      "iteration 1421, dc_loss: 0.04908445104956627, tv_loss: 0.03298332914710045\n",
      "iteration 1422, dc_loss: 0.04895133152604103, tv_loss: 0.03306742012500763\n",
      "iteration 1423, dc_loss: 0.048945602029561996, tv_loss: 0.03302321955561638\n",
      "iteration 1424, dc_loss: 0.04888005182147026, tv_loss: 0.03304510936141014\n",
      "iteration 1425, dc_loss: 0.048849884420633316, tv_loss: 0.033047113567590714\n",
      "iteration 1426, dc_loss: 0.04882849007844925, tv_loss: 0.033028244972229004\n",
      "iteration 1427, dc_loss: 0.04876365512609482, tv_loss: 0.03304983675479889\n",
      "iteration 1428, dc_loss: 0.04875665903091431, tv_loss: 0.03301757201552391\n",
      "iteration 1429, dc_loss: 0.04869239404797554, tv_loss: 0.03305152431130409\n",
      "iteration 1430, dc_loss: 0.0486336424946785, tv_loss: 0.033080872148275375\n",
      "iteration 1431, dc_loss: 0.04862280562520027, tv_loss: 0.03303162381052971\n",
      "iteration 1432, dc_loss: 0.04852414131164551, tv_loss: 0.0330970473587513\n",
      "iteration 1433, dc_loss: 0.04856939613819122, tv_loss: 0.033033423125743866\n",
      "iteration 1434, dc_loss: 0.048431187868118286, tv_loss: 0.03311535716056824\n",
      "iteration 1435, dc_loss: 0.048480741679668427, tv_loss: 0.03303315490484238\n",
      "iteration 1436, dc_loss: 0.048366524279117584, tv_loss: 0.03311586380004883\n",
      "iteration 1437, dc_loss: 0.04842058941721916, tv_loss: 0.033033426851034164\n",
      "iteration 1438, dc_loss: 0.04829433932900429, tv_loss: 0.033135708421468735\n",
      "iteration 1439, dc_loss: 0.048354532569646835, tv_loss: 0.033030543476343155\n",
      "iteration 1440, dc_loss: 0.04823211953043938, tv_loss: 0.03311944380402565\n",
      "iteration 1441, dc_loss: 0.048332035541534424, tv_loss: 0.033002205193042755\n",
      "iteration 1442, dc_loss: 0.04815695807337761, tv_loss: 0.03318240866065025\n",
      "iteration 1443, dc_loss: 0.04840141534805298, tv_loss: 0.032963238656520844\n",
      "iteration 1444, dc_loss: 0.04817550629377365, tv_loss: 0.033247921615839005\n",
      "iteration 1445, dc_loss: 0.04851716011762619, tv_loss: 0.03289635106921196\n",
      "iteration 1446, dc_loss: 0.048179466277360916, tv_loss: 0.03328538313508034\n",
      "iteration 1447, dc_loss: 0.048501912504434586, tv_loss: 0.0328693650662899\n",
      "iteration 1448, dc_loss: 0.048057205975055695, tv_loss: 0.033297859132289886\n",
      "iteration 1449, dc_loss: 0.04826304689049721, tv_loss: 0.03291897103190422\n",
      "iteration 1450, dc_loss: 0.04784642159938812, tv_loss: 0.03319036588072777\n",
      "iteration 1451, dc_loss: 0.0479089729487896, tv_loss: 0.033026546239852905\n",
      "iteration 1452, dc_loss: 0.04781365767121315, tv_loss: 0.03307940065860748\n",
      "iteration 1453, dc_loss: 0.04776684567332268, tv_loss: 0.0331304557621479\n",
      "iteration 1454, dc_loss: 0.047919582575559616, tv_loss: 0.033001258969306946\n",
      "iteration 1455, dc_loss: 0.047663502395153046, tv_loss: 0.033239733427762985\n",
      "iteration 1456, dc_loss: 0.047867968678474426, tv_loss: 0.0329740084707737\n",
      "iteration 1457, dc_loss: 0.04756031185388565, tv_loss: 0.03319487348198891\n",
      "iteration 1458, dc_loss: 0.04763330519199371, tv_loss: 0.03304173797369003\n",
      "iteration 1459, dc_loss: 0.047541387379169464, tv_loss: 0.03310408070683479\n",
      "iteration 1460, dc_loss: 0.04748492315411568, tv_loss: 0.033126555383205414\n",
      "iteration 1461, dc_loss: 0.04756690934300423, tv_loss: 0.03302062302827835\n",
      "iteration 1462, dc_loss: 0.04735880345106125, tv_loss: 0.033208370208740234\n",
      "iteration 1463, dc_loss: 0.047500140964984894, tv_loss: 0.0330214723944664\n",
      "iteration 1464, dc_loss: 0.04729895293712616, tv_loss: 0.03317719325423241\n",
      "iteration 1465, dc_loss: 0.04736404865980148, tv_loss: 0.03305364400148392\n",
      "iteration 1466, dc_loss: 0.04725935310125351, tv_loss: 0.03310968726873398\n",
      "iteration 1467, dc_loss: 0.04721875116229057, tv_loss: 0.03311515599489212\n",
      "iteration 1468, dc_loss: 0.04720371216535568, tv_loss: 0.033092811703681946\n",
      "iteration 1469, dc_loss: 0.04711199179291725, tv_loss: 0.03316134214401245\n",
      "iteration 1470, dc_loss: 0.0471595861017704, tv_loss: 0.03308436647057533\n",
      "iteration 1471, dc_loss: 0.04705698788166046, tv_loss: 0.0331403948366642\n",
      "iteration 1472, dc_loss: 0.04707951098680496, tv_loss: 0.03306790813803673\n",
      "iteration 1473, dc_loss: 0.046959295868873596, tv_loss: 0.033148858696222305\n",
      "iteration 1474, dc_loss: 0.046967409551143646, tv_loss: 0.03309299051761627\n",
      "iteration 1475, dc_loss: 0.04688997194170952, tv_loss: 0.03313916549086571\n",
      "iteration 1476, dc_loss: 0.04690041020512581, tv_loss: 0.03310355916619301\n",
      "iteration 1477, dc_loss: 0.04683057591319084, tv_loss: 0.03315522521734238\n",
      "iteration 1478, dc_loss: 0.04681069031357765, tv_loss: 0.03313891217112541\n",
      "iteration 1479, dc_loss: 0.04677877947688103, tv_loss: 0.03311625495553017\n",
      "iteration 1480, dc_loss: 0.0467124842107296, tv_loss: 0.033145636320114136\n",
      "iteration 1481, dc_loss: 0.04669513925909996, tv_loss: 0.033124301582574844\n",
      "iteration 1482, dc_loss: 0.04664260894060135, tv_loss: 0.033135611563920975\n",
      "iteration 1483, dc_loss: 0.046631935983896255, tv_loss: 0.03310703858733177\n",
      "iteration 1484, dc_loss: 0.04654959961771965, tv_loss: 0.03315142169594765\n",
      "iteration 1485, dc_loss: 0.04654917120933533, tv_loss: 0.03312460705637932\n",
      "iteration 1486, dc_loss: 0.04649904742836952, tv_loss: 0.03313332423567772\n",
      "iteration 1487, dc_loss: 0.046484071761369705, tv_loss: 0.03311854973435402\n",
      "iteration 1488, dc_loss: 0.046426922082901, tv_loss: 0.03314793109893799\n",
      "iteration 1489, dc_loss: 0.04641680046916008, tv_loss: 0.033129867166280746\n",
      "iteration 1490, dc_loss: 0.04634442552924156, tv_loss: 0.03316657617688179\n",
      "iteration 1491, dc_loss: 0.046376269310712814, tv_loss: 0.033108003437519073\n",
      "iteration 1492, dc_loss: 0.046277932822704315, tv_loss: 0.03317525237798691\n",
      "iteration 1493, dc_loss: 0.04635099694132805, tv_loss: 0.03308588266372681\n",
      "iteration 1494, dc_loss: 0.04620932415127754, tv_loss: 0.033220238983631134\n",
      "iteration 1495, dc_loss: 0.04637099802494049, tv_loss: 0.03304622694849968\n",
      "iteration 1496, dc_loss: 0.04613751545548439, tv_loss: 0.033281441777944565\n",
      "iteration 1497, dc_loss: 0.04638713598251343, tv_loss: 0.03301779180765152\n",
      "iteration 1498, dc_loss: 0.04610050469636917, tv_loss: 0.033319100737571716\n",
      "iteration 1499, dc_loss: 0.04645455256104469, tv_loss: 0.03295523673295975\n",
      "iteration 1500, dc_loss: 0.04608849808573723, tv_loss: 0.03338237106800079\n",
      "iteration 1501, dc_loss: 0.04649181291460991, tv_loss: 0.032923176884651184\n",
      "iteration 1502, dc_loss: 0.04605556279420853, tv_loss: 0.03336786851286888\n",
      "iteration 1503, dc_loss: 0.046315748244524, tv_loss: 0.032966092228889465\n",
      "iteration 1504, dc_loss: 0.045898716896772385, tv_loss: 0.03327670320868492\n",
      "iteration 1505, dc_loss: 0.04593998193740845, tv_loss: 0.033110104501247406\n",
      "iteration 1506, dc_loss: 0.04585929214954376, tv_loss: 0.033133652061223984\n",
      "iteration 1507, dc_loss: 0.04576000198721886, tv_loss: 0.033233970403671265\n",
      "iteration 1508, dc_loss: 0.04595576599240303, tv_loss: 0.03304855152964592\n",
      "iteration 1509, dc_loss: 0.04572305083274841, tv_loss: 0.033300235867500305\n",
      "iteration 1510, dc_loss: 0.04592645913362503, tv_loss: 0.033019550144672394\n",
      "iteration 1511, dc_loss: 0.04560241475701332, tv_loss: 0.03327729180455208\n",
      "iteration 1512, dc_loss: 0.04567219689488411, tv_loss: 0.03310332074761391\n",
      "iteration 1513, dc_loss: 0.045554205775260925, tv_loss: 0.03316628560423851\n",
      "iteration 1514, dc_loss: 0.04549676179885864, tv_loss: 0.033214908093214035\n",
      "iteration 1515, dc_loss: 0.045626670122146606, tv_loss: 0.033072322607040405\n",
      "iteration 1516, dc_loss: 0.04541642218828201, tv_loss: 0.03328236564993858\n",
      "iteration 1517, dc_loss: 0.045583710074424744, tv_loss: 0.03306933119893074\n",
      "iteration 1518, dc_loss: 0.04535871744155884, tv_loss: 0.03324786201119423\n",
      "iteration 1519, dc_loss: 0.04541221633553505, tv_loss: 0.03313503786921501\n",
      "iteration 1520, dc_loss: 0.04529690742492676, tv_loss: 0.03319074958562851\n",
      "iteration 1521, dc_loss: 0.04527739807963371, tv_loss: 0.03318445011973381\n",
      "iteration 1522, dc_loss: 0.04529181122779846, tv_loss: 0.03313995152711868\n",
      "iteration 1523, dc_loss: 0.04518456384539604, tv_loss: 0.03322727978229523\n",
      "iteration 1524, dc_loss: 0.04526733607053757, tv_loss: 0.033113762736320496\n",
      "iteration 1525, dc_loss: 0.045123253017663956, tv_loss: 0.03323425352573395\n",
      "iteration 1526, dc_loss: 0.045169759541749954, tv_loss: 0.0331433042883873\n",
      "iteration 1527, dc_loss: 0.04506244137883186, tv_loss: 0.033209044486284256\n",
      "iteration 1528, dc_loss: 0.0450538694858551, tv_loss: 0.033183347433805466\n",
      "iteration 1529, dc_loss: 0.04502391815185547, tv_loss: 0.03316831216216087\n",
      "iteration 1530, dc_loss: 0.04496745392680168, tv_loss: 0.033199019730091095\n",
      "iteration 1531, dc_loss: 0.044967763125896454, tv_loss: 0.033164747059345245\n",
      "iteration 1532, dc_loss: 0.04490135982632637, tv_loss: 0.033208806067705154\n",
      "iteration 1533, dc_loss: 0.04493609070777893, tv_loss: 0.033148813992738724\n",
      "iteration 1534, dc_loss: 0.04485045373439789, tv_loss: 0.03322172909975052\n",
      "iteration 1535, dc_loss: 0.0449201762676239, tv_loss: 0.03315296396613121\n",
      "iteration 1536, dc_loss: 0.0448615625500679, tv_loss: 0.03322425112128258\n",
      "iteration 1537, dc_loss: 0.044916197657585144, tv_loss: 0.03316102176904678\n",
      "iteration 1538, dc_loss: 0.04480234161019325, tv_loss: 0.033226873725652695\n",
      "iteration 1539, dc_loss: 0.04479339346289635, tv_loss: 0.03316088020801544\n",
      "iteration 1540, dc_loss: 0.04473253712058067, tv_loss: 0.03318672627210617\n",
      "iteration 1541, dc_loss: 0.0446803942322731, tv_loss: 0.03322916850447655\n",
      "iteration 1542, dc_loss: 0.04472867771983147, tv_loss: 0.033202312886714935\n",
      "iteration 1543, dc_loss: 0.04456165060400963, tv_loss: 0.03328782692551613\n",
      "iteration 1544, dc_loss: 0.044661957770586014, tv_loss: 0.03311682119965553\n",
      "iteration 1545, dc_loss: 0.04447430372238159, tv_loss: 0.033290795981884\n",
      "iteration 1546, dc_loss: 0.04462825879454613, tv_loss: 0.03317764401435852\n",
      "iteration 1547, dc_loss: 0.044532593339681625, tv_loss: 0.033275485038757324\n",
      "iteration 1548, dc_loss: 0.04456157609820366, tv_loss: 0.03316812962293625\n",
      "iteration 1549, dc_loss: 0.044410910457372665, tv_loss: 0.03326670452952385\n",
      "iteration 1550, dc_loss: 0.044563621282577515, tv_loss: 0.03314550220966339\n",
      "iteration 1551, dc_loss: 0.04434719681739807, tv_loss: 0.03334710747003555\n",
      "iteration 1552, dc_loss: 0.04452628642320633, tv_loss: 0.0330723412334919\n",
      "iteration 1553, dc_loss: 0.04424440115690231, tv_loss: 0.03333887830376625\n",
      "iteration 1554, dc_loss: 0.04438071325421333, tv_loss: 0.0331558957695961\n",
      "iteration 1555, dc_loss: 0.044259000569581985, tv_loss: 0.03324935957789421\n",
      "iteration 1556, dc_loss: 0.04420929029583931, tv_loss: 0.03323429822921753\n",
      "iteration 1557, dc_loss: 0.04419402405619621, tv_loss: 0.03319530561566353\n",
      "iteration 1558, dc_loss: 0.0441463328897953, tv_loss: 0.03325071930885315\n",
      "iteration 1559, dc_loss: 0.044132255017757416, tv_loss: 0.03321712091565132\n",
      "iteration 1560, dc_loss: 0.044067345559597015, tv_loss: 0.033229485154151917\n",
      "iteration 1561, dc_loss: 0.04406512528657913, tv_loss: 0.03319605812430382\n",
      "iteration 1562, dc_loss: 0.0439378097653389, tv_loss: 0.03330022096633911\n",
      "iteration 1563, dc_loss: 0.044071998447179794, tv_loss: 0.033158447593450546\n",
      "iteration 1564, dc_loss: 0.043910253793001175, tv_loss: 0.03327008709311485\n",
      "iteration 1565, dc_loss: 0.04397926852107048, tv_loss: 0.03316175937652588\n",
      "iteration 1566, dc_loss: 0.04382500424981117, tv_loss: 0.033306654542684555\n",
      "iteration 1567, dc_loss: 0.04392710700631142, tv_loss: 0.033161357045173645\n",
      "iteration 1568, dc_loss: 0.043758559972047806, tv_loss: 0.03328879922628403\n",
      "iteration 1569, dc_loss: 0.04380819946527481, tv_loss: 0.033206842839717865\n",
      "iteration 1570, dc_loss: 0.043691329658031464, tv_loss: 0.033271778374910355\n",
      "iteration 1571, dc_loss: 0.043713320046663284, tv_loss: 0.033211153000593185\n",
      "iteration 1572, dc_loss: 0.04365380108356476, tv_loss: 0.03322568163275719\n",
      "iteration 1573, dc_loss: 0.04361637681722641, tv_loss: 0.03324493020772934\n",
      "iteration 1574, dc_loss: 0.04360467195510864, tv_loss: 0.03322586789727211\n",
      "iteration 1575, dc_loss: 0.04354798421263695, tv_loss: 0.033254869282245636\n",
      "iteration 1576, dc_loss: 0.043547503650188446, tv_loss: 0.033203113824129105\n",
      "iteration 1577, dc_loss: 0.043467793613672256, tv_loss: 0.033245280385017395\n",
      "iteration 1578, dc_loss: 0.04349783435463905, tv_loss: 0.03321220725774765\n",
      "iteration 1579, dc_loss: 0.04340628907084465, tv_loss: 0.03326854109764099\n",
      "iteration 1580, dc_loss: 0.04341856762766838, tv_loss: 0.033206187188625336\n",
      "iteration 1581, dc_loss: 0.043317437171936035, tv_loss: 0.033292870968580246\n",
      "iteration 1582, dc_loss: 0.043385155498981476, tv_loss: 0.03320447355508804\n",
      "iteration 1583, dc_loss: 0.0432593896985054, tv_loss: 0.03329843282699585\n",
      "iteration 1584, dc_loss: 0.043339088559150696, tv_loss: 0.03319764882326126\n",
      "iteration 1585, dc_loss: 0.043207183480262756, tv_loss: 0.033287499099969864\n",
      "iteration 1586, dc_loss: 0.043297868221998215, tv_loss: 0.033184826374053955\n",
      "iteration 1587, dc_loss: 0.04313036799430847, tv_loss: 0.03336359187960625\n",
      "iteration 1588, dc_loss: 0.04330079257488251, tv_loss: 0.03315342217683792\n",
      "iteration 1589, dc_loss: 0.04308361932635307, tv_loss: 0.0333721786737442\n",
      "iteration 1590, dc_loss: 0.043309275060892105, tv_loss: 0.033141616731882095\n",
      "iteration 1591, dc_loss: 0.043071482330560684, tv_loss: 0.03339708223938942\n",
      "iteration 1592, dc_loss: 0.043335895985364914, tv_loss: 0.03312919661402702\n",
      "iteration 1593, dc_loss: 0.04302675649523735, tv_loss: 0.033459439873695374\n",
      "iteration 1594, dc_loss: 0.0433502122759819, tv_loss: 0.03306637331843376\n",
      "iteration 1595, dc_loss: 0.04294608160853386, tv_loss: 0.03343343734741211\n",
      "iteration 1596, dc_loss: 0.04317319393157959, tv_loss: 0.033109020441770554\n",
      "iteration 1597, dc_loss: 0.04282980039715767, tv_loss: 0.033376943320035934\n",
      "iteration 1598, dc_loss: 0.042900972068309784, tv_loss: 0.0332157239317894\n",
      "iteration 1599, dc_loss: 0.04281468316912651, tv_loss: 0.03326043114066124\n",
      "iteration 1600, dc_loss: 0.042732395231723785, tv_loss: 0.033328913152217865\n",
      "iteration 1601, dc_loss: 0.04288751259446144, tv_loss: 0.033166322857141495\n",
      "iteration 1602, dc_loss: 0.04267454892396927, tv_loss: 0.03332098200917244\n",
      "iteration 1603, dc_loss: 0.042675577104091644, tv_loss: 0.03326738625764847\n",
      "iteration 1604, dc_loss: 0.04277372732758522, tv_loss: 0.033184703439474106\n",
      "iteration 1605, dc_loss: 0.04261491075158119, tv_loss: 0.03331591561436653\n",
      "iteration 1606, dc_loss: 0.04262440279126167, tv_loss: 0.033266156911849976\n",
      "iteration 1607, dc_loss: 0.04266820847988129, tv_loss: 0.03321525454521179\n",
      "iteration 1608, dc_loss: 0.04253688454627991, tv_loss: 0.033319927752017975\n",
      "iteration 1609, dc_loss: 0.042570095509290695, tv_loss: 0.033239517360925674\n",
      "iteration 1610, dc_loss: 0.04257320985198021, tv_loss: 0.03322241082787514\n",
      "iteration 1611, dc_loss: 0.04246283695101738, tv_loss: 0.03331777825951576\n",
      "iteration 1612, dc_loss: 0.04250221326947212, tv_loss: 0.033231835812330246\n",
      "iteration 1613, dc_loss: 0.04248928651213646, tv_loss: 0.03322065621614456\n",
      "iteration 1614, dc_loss: 0.04239761829376221, tv_loss: 0.03329930081963539\n",
      "iteration 1615, dc_loss: 0.042426951229572296, tv_loss: 0.03323420137166977\n",
      "iteration 1616, dc_loss: 0.04240080341696739, tv_loss: 0.033248644322156906\n",
      "iteration 1617, dc_loss: 0.042338140308856964, tv_loss: 0.03331002593040466\n",
      "iteration 1618, dc_loss: 0.04236859455704689, tv_loss: 0.03324954956769943\n",
      "iteration 1619, dc_loss: 0.042307958006858826, tv_loss: 0.03326459228992462\n",
      "iteration 1620, dc_loss: 0.0422666035592556, tv_loss: 0.03328494355082512\n",
      "iteration 1621, dc_loss: 0.04229806736111641, tv_loss: 0.03322797268629074\n",
      "iteration 1622, dc_loss: 0.04223429039120674, tv_loss: 0.03326764330267906\n",
      "iteration 1623, dc_loss: 0.04220985993742943, tv_loss: 0.03328050300478935\n",
      "iteration 1624, dc_loss: 0.042215995490550995, tv_loss: 0.03326112776994705\n",
      "iteration 1625, dc_loss: 0.042160212993621826, tv_loss: 0.033290307968854904\n",
      "iteration 1626, dc_loss: 0.04214964434504509, tv_loss: 0.03327289596199989\n",
      "iteration 1627, dc_loss: 0.04213767871260643, tv_loss: 0.03326092287898064\n",
      "iteration 1628, dc_loss: 0.04208408296108246, tv_loss: 0.03329014405608177\n",
      "iteration 1629, dc_loss: 0.04209063947200775, tv_loss: 0.03326703608036041\n",
      "iteration 1630, dc_loss: 0.0420660637319088, tv_loss: 0.03326814994215965\n",
      "iteration 1631, dc_loss: 0.04202267527580261, tv_loss: 0.033291276544332504\n",
      "iteration 1632, dc_loss: 0.04201515018939972, tv_loss: 0.0332779623568058\n",
      "iteration 1633, dc_loss: 0.041986025869846344, tv_loss: 0.03327517956495285\n",
      "iteration 1634, dc_loss: 0.041962698101997375, tv_loss: 0.03327434882521629\n",
      "iteration 1635, dc_loss: 0.04194728657603264, tv_loss: 0.033273618668317795\n",
      "iteration 1636, dc_loss: 0.041914746165275574, tv_loss: 0.033280231058597565\n",
      "iteration 1637, dc_loss: 0.041897837072610855, tv_loss: 0.033269379287958145\n",
      "iteration 1638, dc_loss: 0.04187353327870369, tv_loss: 0.033270739018917084\n",
      "iteration 1639, dc_loss: 0.04184645414352417, tv_loss: 0.033285144716501236\n",
      "iteration 1640, dc_loss: 0.04183436185121536, tv_loss: 0.03326587751507759\n",
      "iteration 1641, dc_loss: 0.041805945336818695, tv_loss: 0.033273693174123764\n",
      "iteration 1642, dc_loss: 0.04178272932767868, tv_loss: 0.033274419605731964\n",
      "iteration 1643, dc_loss: 0.04176971688866615, tv_loss: 0.03326890617609024\n",
      "iteration 1644, dc_loss: 0.04172113165259361, tv_loss: 0.03330446779727936\n",
      "iteration 1645, dc_loss: 0.04171319305896759, tv_loss: 0.03332610800862312\n",
      "iteration 1646, dc_loss: 0.041704125702381134, tv_loss: 0.03330226242542267\n",
      "iteration 1647, dc_loss: 0.04166697338223457, tv_loss: 0.03328360989689827\n",
      "iteration 1648, dc_loss: 0.0416516549885273, tv_loss: 0.033306363970041275\n",
      "iteration 1649, dc_loss: 0.04163126274943352, tv_loss: 0.03331628069281578\n",
      "iteration 1650, dc_loss: 0.041600532829761505, tv_loss: 0.033302903175354004\n",
      "iteration 1651, dc_loss: 0.04158221557736397, tv_loss: 0.03328564763069153\n",
      "iteration 1652, dc_loss: 0.041557811200618744, tv_loss: 0.03329747915267944\n",
      "iteration 1653, dc_loss: 0.04153706133365631, tv_loss: 0.03331030532717705\n",
      "iteration 1654, dc_loss: 0.04151371866464615, tv_loss: 0.03331425040960312\n",
      "iteration 1655, dc_loss: 0.04149094596505165, tv_loss: 0.03330133855342865\n",
      "iteration 1656, dc_loss: 0.041474342346191406, tv_loss: 0.03328549861907959\n",
      "iteration 1657, dc_loss: 0.0414491668343544, tv_loss: 0.03329499810934067\n",
      "iteration 1658, dc_loss: 0.04142599180340767, tv_loss: 0.03331165388226509\n",
      "iteration 1659, dc_loss: 0.04140530154109001, tv_loss: 0.03331628441810608\n",
      "iteration 1660, dc_loss: 0.04138270020484924, tv_loss: 0.03329966589808464\n",
      "iteration 1661, dc_loss: 0.041361089795827866, tv_loss: 0.03329348564147949\n",
      "iteration 1662, dc_loss: 0.04133491590619087, tv_loss: 0.03330034017562866\n",
      "iteration 1663, dc_loss: 0.04131712019443512, tv_loss: 0.0332946740090847\n",
      "iteration 1664, dc_loss: 0.041295550763607025, tv_loss: 0.03330012783408165\n",
      "iteration 1665, dc_loss: 0.04126908630132675, tv_loss: 0.03331061452627182\n",
      "iteration 1666, dc_loss: 0.041250184178352356, tv_loss: 0.03330469876527786\n",
      "iteration 1667, dc_loss: 0.041235920041799545, tv_loss: 0.03329246863722801\n",
      "iteration 1668, dc_loss: 0.04120588302612305, tv_loss: 0.033307626843452454\n",
      "iteration 1669, dc_loss: 0.041188471019268036, tv_loss: 0.033298004418611526\n",
      "iteration 1670, dc_loss: 0.0411592535674572, tv_loss: 0.03330080583691597\n",
      "iteration 1671, dc_loss: 0.04114213213324547, tv_loss: 0.03329508751630783\n",
      "iteration 1672, dc_loss: 0.04112211614847183, tv_loss: 0.03329605609178543\n",
      "iteration 1673, dc_loss: 0.04109609127044678, tv_loss: 0.03329671546816826\n",
      "iteration 1674, dc_loss: 0.041077326983213425, tv_loss: 0.03330029547214508\n",
      "iteration 1675, dc_loss: 0.04106113314628601, tv_loss: 0.03330518305301666\n",
      "iteration 1676, dc_loss: 0.04103042930364609, tv_loss: 0.033350713551044464\n",
      "iteration 1677, dc_loss: 0.04101888835430145, tv_loss: 0.033330995589494705\n",
      "iteration 1678, dc_loss: 0.04098882898688316, tv_loss: 0.03330915793776512\n",
      "iteration 1679, dc_loss: 0.04096450284123421, tv_loss: 0.03333364054560661\n",
      "iteration 1680, dc_loss: 0.04095061495900154, tv_loss: 0.03335953131318092\n",
      "iteration 1681, dc_loss: 0.040922991931438446, tv_loss: 0.033342525362968445\n",
      "iteration 1682, dc_loss: 0.04091301187872887, tv_loss: 0.03332259878516197\n",
      "iteration 1683, dc_loss: 0.040893714874982834, tv_loss: 0.033361852169036865\n",
      "iteration 1684, dc_loss: 0.040851809084415436, tv_loss: 0.03334902599453926\n",
      "iteration 1685, dc_loss: 0.04085060581564903, tv_loss: 0.03333507105708122\n",
      "iteration 1686, dc_loss: 0.04083679988980293, tv_loss: 0.03334443271160126\n",
      "iteration 1687, dc_loss: 0.04079042002558708, tv_loss: 0.0333498977124691\n",
      "iteration 1688, dc_loss: 0.04078632593154907, tv_loss: 0.03333185240626335\n",
      "iteration 1689, dc_loss: 0.04078572988510132, tv_loss: 0.033335864543914795\n",
      "iteration 1690, dc_loss: 0.040731772780418396, tv_loss: 0.03336895629763603\n",
      "iteration 1691, dc_loss: 0.040725227445364, tv_loss: 0.03333767503499985\n",
      "iteration 1692, dc_loss: 0.04073113203048706, tv_loss: 0.03332488611340523\n",
      "iteration 1693, dc_loss: 0.04068267345428467, tv_loss: 0.033348117023706436\n",
      "iteration 1694, dc_loss: 0.040662504732608795, tv_loss: 0.033334117382764816\n",
      "iteration 1695, dc_loss: 0.040637608617544174, tv_loss: 0.033333003520965576\n",
      "iteration 1696, dc_loss: 0.04060981422662735, tv_loss: 0.03334713727235794\n",
      "iteration 1697, dc_loss: 0.040599558502435684, tv_loss: 0.03333447501063347\n",
      "iteration 1698, dc_loss: 0.04056726396083832, tv_loss: 0.03333212435245514\n",
      "iteration 1699, dc_loss: 0.040537212044000626, tv_loss: 0.033337898552417755\n",
      "iteration 1700, dc_loss: 0.040519118309020996, tv_loss: 0.033344391733407974\n",
      "iteration 1701, dc_loss: 0.040517594665288925, tv_loss: 0.033314384520053864\n",
      "iteration 1702, dc_loss: 0.04049033299088478, tv_loss: 0.03332038223743439\n",
      "iteration 1703, dc_loss: 0.040456950664520264, tv_loss: 0.0333375446498394\n",
      "iteration 1704, dc_loss: 0.040449127554893494, tv_loss: 0.03332480043172836\n",
      "iteration 1705, dc_loss: 0.040425971150398254, tv_loss: 0.03332367166876793\n",
      "iteration 1706, dc_loss: 0.04038985073566437, tv_loss: 0.033338986337184906\n",
      "iteration 1707, dc_loss: 0.04038085788488388, tv_loss: 0.03333912417292595\n",
      "iteration 1708, dc_loss: 0.04037294536828995, tv_loss: 0.03333038091659546\n",
      "iteration 1709, dc_loss: 0.04034982621669769, tv_loss: 0.03334445133805275\n",
      "iteration 1710, dc_loss: 0.04032423719763756, tv_loss: 0.03335275873541832\n",
      "iteration 1711, dc_loss: 0.040318239480257034, tv_loss: 0.03334957733750343\n",
      "iteration 1712, dc_loss: 0.04030834138393402, tv_loss: 0.03333383798599243\n",
      "iteration 1713, dc_loss: 0.04027866944670677, tv_loss: 0.0333402119576931\n",
      "iteration 1714, dc_loss: 0.040252797305583954, tv_loss: 0.033333562314510345\n",
      "iteration 1715, dc_loss: 0.04022742062807083, tv_loss: 0.03333393856883049\n",
      "iteration 1716, dc_loss: 0.0401962473988533, tv_loss: 0.03334281966090202\n",
      "iteration 1717, dc_loss: 0.040184758603572845, tv_loss: 0.03333958983421326\n",
      "iteration 1718, dc_loss: 0.04015615954995155, tv_loss: 0.03336217626929283\n",
      "iteration 1719, dc_loss: 0.04013088718056679, tv_loss: 0.033364035189151764\n",
      "iteration 1720, dc_loss: 0.04011773318052292, tv_loss: 0.03334120288491249\n",
      "iteration 1721, dc_loss: 0.040095265954732895, tv_loss: 0.033331580460071564\n",
      "iteration 1722, dc_loss: 0.04006299749016762, tv_loss: 0.03335673734545708\n",
      "iteration 1723, dc_loss: 0.04005458205938339, tv_loss: 0.033365145325660706\n",
      "iteration 1724, dc_loss: 0.04004170000553131, tv_loss: 0.03336295112967491\n",
      "iteration 1725, dc_loss: 0.040004223585128784, tv_loss: 0.03337115794420242\n",
      "iteration 1726, dc_loss: 0.04000415280461311, tv_loss: 0.033337511122226715\n",
      "iteration 1727, dc_loss: 0.03999623283743858, tv_loss: 0.033331479877233505\n",
      "iteration 1728, dc_loss: 0.039963528513908386, tv_loss: 0.03335871919989586\n",
      "iteration 1729, dc_loss: 0.03994292765855789, tv_loss: 0.03337507322430611\n",
      "iteration 1730, dc_loss: 0.03992123529314995, tv_loss: 0.03337451070547104\n",
      "iteration 1731, dc_loss: 0.03989654779434204, tv_loss: 0.03336047753691673\n",
      "iteration 1732, dc_loss: 0.03988143056631088, tv_loss: 0.03335147723555565\n",
      "iteration 1733, dc_loss: 0.03986576199531555, tv_loss: 0.033348891884088516\n",
      "iteration 1734, dc_loss: 0.03983338177204132, tv_loss: 0.03336695209145546\n",
      "iteration 1735, dc_loss: 0.03981298208236694, tv_loss: 0.03336730971932411\n",
      "iteration 1736, dc_loss: 0.03979756310582161, tv_loss: 0.03335229307413101\n",
      "iteration 1737, dc_loss: 0.039764124900102615, tv_loss: 0.03335721045732498\n",
      "iteration 1738, dc_loss: 0.039760734885931015, tv_loss: 0.03334031626582146\n",
      "iteration 1739, dc_loss: 0.039714265614748, tv_loss: 0.03336285054683685\n",
      "iteration 1740, dc_loss: 0.03971545025706291, tv_loss: 0.033344537019729614\n",
      "iteration 1741, dc_loss: 0.03968760743737221, tv_loss: 0.03334575518965721\n",
      "iteration 1742, dc_loss: 0.039663445204496384, tv_loss: 0.03335297480225563\n",
      "iteration 1743, dc_loss: 0.039654336869716644, tv_loss: 0.03335420414805412\n",
      "iteration 1744, dc_loss: 0.039620544761419296, tv_loss: 0.0333823636174202\n",
      "iteration 1745, dc_loss: 0.03961917757987976, tv_loss: 0.03337014093995094\n",
      "iteration 1746, dc_loss: 0.039590831845998764, tv_loss: 0.033366382122039795\n",
      "iteration 1747, dc_loss: 0.039561253041028976, tv_loss: 0.03336778283119202\n",
      "iteration 1748, dc_loss: 0.039538297802209854, tv_loss: 0.033374059945344925\n",
      "iteration 1749, dc_loss: 0.0395459346473217, tv_loss: 0.033344168215990067\n",
      "iteration 1750, dc_loss: 0.03950921818614006, tv_loss: 0.033365584909915924\n",
      "iteration 1751, dc_loss: 0.0394911915063858, tv_loss: 0.03336429223418236\n",
      "iteration 1752, dc_loss: 0.039475709199905396, tv_loss: 0.0333745963871479\n",
      "iteration 1753, dc_loss: 0.03946072980761528, tv_loss: 0.03339068964123726\n",
      "iteration 1754, dc_loss: 0.039464958012104034, tv_loss: 0.03336590528488159\n",
      "iteration 1755, dc_loss: 0.03946401923894882, tv_loss: 0.03338285908102989\n",
      "iteration 1756, dc_loss: 0.03944304212927818, tv_loss: 0.033372655510902405\n",
      "iteration 1757, dc_loss: 0.03943319991230965, tv_loss: 0.03336752951145172\n",
      "iteration 1758, dc_loss: 0.039378341287374496, tv_loss: 0.03338037431240082\n",
      "iteration 1759, dc_loss: 0.03934741020202637, tv_loss: 0.03338086977601051\n",
      "iteration 1760, dc_loss: 0.03930830955505371, tv_loss: 0.03340176120400429\n",
      "iteration 1761, dc_loss: 0.03936275467276573, tv_loss: 0.03333535045385361\n",
      "iteration 1762, dc_loss: 0.039305590093135834, tv_loss: 0.03339720517396927\n",
      "iteration 1763, dc_loss: 0.03932339325547218, tv_loss: 0.03335157409310341\n",
      "iteration 1764, dc_loss: 0.0392456091940403, tv_loss: 0.03339199721813202\n",
      "iteration 1765, dc_loss: 0.03922820836305618, tv_loss: 0.033389151096343994\n",
      "iteration 1766, dc_loss: 0.03924272581934929, tv_loss: 0.033345047384500504\n",
      "iteration 1767, dc_loss: 0.039180684834718704, tv_loss: 0.03341813385486603\n",
      "iteration 1768, dc_loss: 0.039231669157743454, tv_loss: 0.0333501361310482\n",
      "iteration 1769, dc_loss: 0.03915569558739662, tv_loss: 0.033406589180231094\n",
      "iteration 1770, dc_loss: 0.03917737677693367, tv_loss: 0.033349987119436264\n",
      "iteration 1771, dc_loss: 0.03909023106098175, tv_loss: 0.033397890627384186\n",
      "iteration 1772, dc_loss: 0.039079632610082626, tv_loss: 0.033383484929800034\n",
      "iteration 1773, dc_loss: 0.03909742087125778, tv_loss: 0.03336292505264282\n",
      "iteration 1774, dc_loss: 0.03904646262526512, tv_loss: 0.033394575119018555\n",
      "iteration 1775, dc_loss: 0.039070818573236465, tv_loss: 0.033352576196193695\n",
      "iteration 1776, dc_loss: 0.039004914462566376, tv_loss: 0.03340292349457741\n",
      "iteration 1777, dc_loss: 0.03902086243033409, tv_loss: 0.03338829427957535\n",
      "iteration 1778, dc_loss: 0.03896923363208771, tv_loss: 0.033426303416490555\n",
      "iteration 1779, dc_loss: 0.03896051272749901, tv_loss: 0.03337893635034561\n",
      "iteration 1780, dc_loss: 0.038934580981731415, tv_loss: 0.03338826447725296\n",
      "iteration 1781, dc_loss: 0.03891440108418465, tv_loss: 0.033399369567632675\n",
      "iteration 1782, dc_loss: 0.038903072476387024, tv_loss: 0.03340240567922592\n",
      "iteration 1783, dc_loss: 0.038885585963726044, tv_loss: 0.0333956703543663\n",
      "iteration 1784, dc_loss: 0.03887514770030975, tv_loss: 0.03337504714727402\n",
      "iteration 1785, dc_loss: 0.03884095326066017, tv_loss: 0.03339719772338867\n",
      "iteration 1786, dc_loss: 0.03883599489927292, tv_loss: 0.03337690234184265\n",
      "iteration 1787, dc_loss: 0.038802068680524826, tv_loss: 0.033393651247024536\n",
      "iteration 1788, dc_loss: 0.03878113627433777, tv_loss: 0.03340284526348114\n",
      "iteration 1789, dc_loss: 0.03879319131374359, tv_loss: 0.03336595371365547\n",
      "iteration 1790, dc_loss: 0.03872223198413849, tv_loss: 0.03341686725616455\n",
      "iteration 1791, dc_loss: 0.038758985698223114, tv_loss: 0.033377762883901596\n",
      "iteration 1792, dc_loss: 0.03870387747883797, tv_loss: 0.033404167741537094\n",
      "iteration 1793, dc_loss: 0.038744255900382996, tv_loss: 0.03335414454340935\n",
      "iteration 1794, dc_loss: 0.03864428028464317, tv_loss: 0.03344319760799408\n",
      "iteration 1795, dc_loss: 0.038744762539863586, tv_loss: 0.033326517790555954\n",
      "iteration 1796, dc_loss: 0.03861941769719124, tv_loss: 0.03345690667629242\n",
      "iteration 1797, dc_loss: 0.03873342275619507, tv_loss: 0.0333331823348999\n",
      "iteration 1798, dc_loss: 0.038614578545093536, tv_loss: 0.03345925360918045\n",
      "iteration 1799, dc_loss: 0.03873376548290253, tv_loss: 0.03334314376115799\n",
      "iteration 1800, dc_loss: 0.03855428844690323, tv_loss: 0.033501625061035156\n",
      "iteration 1801, dc_loss: 0.038654740899801254, tv_loss: 0.033344343304634094\n",
      "iteration 1802, dc_loss: 0.03850480541586876, tv_loss: 0.03344901651144028\n",
      "iteration 1803, dc_loss: 0.0385490320622921, tv_loss: 0.03336870297789574\n",
      "iteration 1804, dc_loss: 0.038474828004837036, tv_loss: 0.03343130648136139\n",
      "iteration 1805, dc_loss: 0.03847062587738037, tv_loss: 0.03339892625808716\n",
      "iteration 1806, dc_loss: 0.038447946310043335, tv_loss: 0.033411964774131775\n",
      "iteration 1807, dc_loss: 0.03843565285205841, tv_loss: 0.03340497985482216\n",
      "iteration 1808, dc_loss: 0.03844425454735756, tv_loss: 0.03339983522891998\n",
      "iteration 1809, dc_loss: 0.03839351609349251, tv_loss: 0.03346136584877968\n",
      "iteration 1810, dc_loss: 0.038430411368608475, tv_loss: 0.03338688239455223\n",
      "iteration 1811, dc_loss: 0.038347773253917694, tv_loss: 0.0334414504468441\n",
      "iteration 1812, dc_loss: 0.03838082775473595, tv_loss: 0.03340751305222511\n",
      "iteration 1813, dc_loss: 0.038315724581480026, tv_loss: 0.03345310315489769\n",
      "iteration 1814, dc_loss: 0.03834206983447075, tv_loss: 0.03339659422636032\n",
      "iteration 1815, dc_loss: 0.03826019540429115, tv_loss: 0.03345159813761711\n",
      "iteration 1816, dc_loss: 0.0382719412446022, tv_loss: 0.03344615176320076\n",
      "iteration 1817, dc_loss: 0.03826090320944786, tv_loss: 0.033416565507650375\n",
      "iteration 1818, dc_loss: 0.03821796551346779, tv_loss: 0.03344513475894928\n",
      "iteration 1819, dc_loss: 0.0382252037525177, tv_loss: 0.03345496952533722\n",
      "iteration 1820, dc_loss: 0.03819219022989273, tv_loss: 0.03342602774500847\n",
      "iteration 1821, dc_loss: 0.03817256540060043, tv_loss: 0.033447716385126114\n",
      "iteration 1822, dc_loss: 0.03813029080629349, tv_loss: 0.03347904980182648\n",
      "iteration 1823, dc_loss: 0.038188111037015915, tv_loss: 0.033385150134563446\n",
      "iteration 1824, dc_loss: 0.03809487819671631, tv_loss: 0.03347153961658478\n",
      "iteration 1825, dc_loss: 0.03812740370631218, tv_loss: 0.03342166170477867\n",
      "iteration 1826, dc_loss: 0.03808412700891495, tv_loss: 0.03344672545790672\n",
      "iteration 1827, dc_loss: 0.03812265396118164, tv_loss: 0.033395715057849884\n",
      "iteration 1828, dc_loss: 0.03803695738315582, tv_loss: 0.03348428010940552\n",
      "iteration 1829, dc_loss: 0.038131289184093475, tv_loss: 0.033391259610652924\n",
      "iteration 1830, dc_loss: 0.03801882639527321, tv_loss: 0.033479250967502594\n",
      "iteration 1831, dc_loss: 0.03808685392141342, tv_loss: 0.033380039036273956\n",
      "iteration 1832, dc_loss: 0.03797008469700813, tv_loss: 0.03349221497774124\n",
      "iteration 1833, dc_loss: 0.03802488371729851, tv_loss: 0.03339152783155441\n",
      "iteration 1834, dc_loss: 0.037928659468889236, tv_loss: 0.03346579894423485\n",
      "iteration 1835, dc_loss: 0.0379817821085453, tv_loss: 0.033404238522052765\n",
      "iteration 1836, dc_loss: 0.03787629306316376, tv_loss: 0.03348397836089134\n",
      "iteration 1837, dc_loss: 0.037954289466142654, tv_loss: 0.03340084105730057\n",
      "iteration 1838, dc_loss: 0.0378948412835598, tv_loss: 0.03344554081559181\n",
      "iteration 1839, dc_loss: 0.03792406991124153, tv_loss: 0.033404309302568436\n",
      "iteration 1840, dc_loss: 0.037850189954042435, tv_loss: 0.03348315507173538\n",
      "iteration 1841, dc_loss: 0.037899550050497055, tv_loss: 0.033387716859579086\n",
      "iteration 1842, dc_loss: 0.03778940811753273, tv_loss: 0.0334772951900959\n",
      "iteration 1843, dc_loss: 0.03785316273570061, tv_loss: 0.03340804576873779\n",
      "iteration 1844, dc_loss: 0.03774845227599144, tv_loss: 0.033475473523139954\n",
      "iteration 1845, dc_loss: 0.03780355677008629, tv_loss: 0.03340376913547516\n",
      "iteration 1846, dc_loss: 0.037710100412368774, tv_loss: 0.033474184572696686\n",
      "iteration 1847, dc_loss: 0.037759844213724136, tv_loss: 0.03341035917401314\n",
      "iteration 1848, dc_loss: 0.03768492117524147, tv_loss: 0.03346775844693184\n",
      "iteration 1849, dc_loss: 0.03772193565964699, tv_loss: 0.03339463099837303\n",
      "iteration 1850, dc_loss: 0.037632476538419724, tv_loss: 0.033473823219537735\n",
      "iteration 1851, dc_loss: 0.03768020123243332, tv_loss: 0.033411309123039246\n",
      "iteration 1852, dc_loss: 0.0376160629093647, tv_loss: 0.03345893695950508\n",
      "iteration 1853, dc_loss: 0.037644267082214355, tv_loss: 0.0334073044359684\n",
      "iteration 1854, dc_loss: 0.037575628608465195, tv_loss: 0.03347153216600418\n",
      "iteration 1855, dc_loss: 0.03762960806488991, tv_loss: 0.033391356468200684\n",
      "iteration 1856, dc_loss: 0.03756806626915932, tv_loss: 0.03345399722456932\n",
      "iteration 1857, dc_loss: 0.03758932277560234, tv_loss: 0.033423446118831635\n",
      "iteration 1858, dc_loss: 0.037529364228248596, tv_loss: 0.03348419442772865\n",
      "iteration 1859, dc_loss: 0.03757128864526749, tv_loss: 0.03341899812221527\n",
      "iteration 1860, dc_loss: 0.03749594837427139, tv_loss: 0.03346720710396767\n",
      "iteration 1861, dc_loss: 0.03754623606801033, tv_loss: 0.03339860215783119\n",
      "iteration 1862, dc_loss: 0.03743177652359009, tv_loss: 0.03350578621029854\n",
      "iteration 1863, dc_loss: 0.0375126376748085, tv_loss: 0.03340717777609825\n",
      "iteration 1864, dc_loss: 0.03739561140537262, tv_loss: 0.03349902480840683\n",
      "iteration 1865, dc_loss: 0.03748588636517525, tv_loss: 0.0333826020359993\n",
      "iteration 1866, dc_loss: 0.03735538572072983, tv_loss: 0.03351388871669769\n",
      "iteration 1867, dc_loss: 0.0374910905957222, tv_loss: 0.033368952572345734\n",
      "iteration 1868, dc_loss: 0.037338901311159134, tv_loss: 0.03352149948477745\n",
      "iteration 1869, dc_loss: 0.037490375339984894, tv_loss: 0.033363040536642075\n",
      "iteration 1870, dc_loss: 0.03733311593532562, tv_loss: 0.033543217927217484\n",
      "iteration 1871, dc_loss: 0.0374998040497303, tv_loss: 0.03335348889231682\n",
      "iteration 1872, dc_loss: 0.037297118455171585, tv_loss: 0.03355421498417854\n",
      "iteration 1873, dc_loss: 0.03745628148317337, tv_loss: 0.033344071358442307\n",
      "iteration 1874, dc_loss: 0.03723990172147751, tv_loss: 0.033517006784677505\n",
      "iteration 1875, dc_loss: 0.03731848672032356, tv_loss: 0.03338657692074776\n",
      "iteration 1876, dc_loss: 0.03719756007194519, tv_loss: 0.0334717258810997\n",
      "iteration 1877, dc_loss: 0.037203919142484665, tv_loss: 0.03343375772237778\n",
      "iteration 1878, dc_loss: 0.03718789666891098, tv_loss: 0.033427465707063675\n",
      "iteration 1879, dc_loss: 0.037137556821107864, tv_loss: 0.03347253426909447\n",
      "iteration 1880, dc_loss: 0.037202950567007065, tv_loss: 0.033414360135793686\n",
      "iteration 1881, dc_loss: 0.03711598366498947, tv_loss: 0.033509302884340286\n",
      "iteration 1882, dc_loss: 0.03718894347548485, tv_loss: 0.03341908007860184\n",
      "iteration 1883, dc_loss: 0.03706802800297737, tv_loss: 0.03350861370563507\n",
      "iteration 1884, dc_loss: 0.03714991360902786, tv_loss: 0.0333981029689312\n",
      "iteration 1885, dc_loss: 0.037038207054138184, tv_loss: 0.03350019454956055\n",
      "iteration 1886, dc_loss: 0.03708723187446594, tv_loss: 0.03342783451080322\n",
      "iteration 1887, dc_loss: 0.03702494874596596, tv_loss: 0.033474430441856384\n",
      "iteration 1888, dc_loss: 0.037018414586782455, tv_loss: 0.03346780687570572\n",
      "iteration 1889, dc_loss: 0.03702213242650032, tv_loss: 0.03344457969069481\n",
      "iteration 1890, dc_loss: 0.03697705268859863, tv_loss: 0.03348725661635399\n",
      "iteration 1891, dc_loss: 0.03703621029853821, tv_loss: 0.033430181443691254\n",
      "iteration 1892, dc_loss: 0.03695531561970711, tv_loss: 0.03350795432925224\n",
      "iteration 1893, dc_loss: 0.03703581541776657, tv_loss: 0.033398617058992386\n",
      "iteration 1894, dc_loss: 0.03687199577689171, tv_loss: 0.033539898693561554\n",
      "iteration 1895, dc_loss: 0.03699794411659241, tv_loss: 0.03338845819234848\n",
      "iteration 1896, dc_loss: 0.036869391798973083, tv_loss: 0.03351539373397827\n",
      "iteration 1897, dc_loss: 0.03696458414196968, tv_loss: 0.03341960534453392\n",
      "iteration 1898, dc_loss: 0.03685633838176727, tv_loss: 0.03351694345474243\n",
      "iteration 1899, dc_loss: 0.03694215416908264, tv_loss: 0.03342575579881668\n",
      "iteration 1900, dc_loss: 0.03682824596762657, tv_loss: 0.03350689262151718\n",
      "iteration 1901, dc_loss: 0.03690434992313385, tv_loss: 0.03341226652264595\n",
      "iteration 1902, dc_loss: 0.03678601607680321, tv_loss: 0.033513981848955154\n",
      "iteration 1903, dc_loss: 0.03683709353208542, tv_loss: 0.033443890511989594\n",
      "iteration 1904, dc_loss: 0.036742594093084335, tv_loss: 0.03352329134941101\n",
      "iteration 1905, dc_loss: 0.03678470104932785, tv_loss: 0.03343707695603371\n",
      "iteration 1906, dc_loss: 0.03669676557183266, tv_loss: 0.033489521592855453\n",
      "iteration 1907, dc_loss: 0.03672821819782257, tv_loss: 0.033442240208387375\n",
      "iteration 1908, dc_loss: 0.03666852414608002, tv_loss: 0.03348499536514282\n",
      "iteration 1909, dc_loss: 0.03666811063885689, tv_loss: 0.033481307327747345\n",
      "iteration 1910, dc_loss: 0.03667629882693291, tv_loss: 0.03347938880324364\n",
      "iteration 1911, dc_loss: 0.036650266498327255, tv_loss: 0.03348805010318756\n",
      "iteration 1912, dc_loss: 0.03665202856063843, tv_loss: 0.03347857668995857\n",
      "iteration 1913, dc_loss: 0.03663752228021622, tv_loss: 0.03346254676580429\n",
      "iteration 1914, dc_loss: 0.036621447652578354, tv_loss: 0.033468812704086304\n",
      "iteration 1915, dc_loss: 0.036597952246665955, tv_loss: 0.033466316759586334\n",
      "iteration 1916, dc_loss: 0.03657171502709389, tv_loss: 0.03346581012010574\n",
      "iteration 1917, dc_loss: 0.036539461463689804, tv_loss: 0.03347478061914444\n",
      "iteration 1918, dc_loss: 0.036549944430589676, tv_loss: 0.03344366326928139\n",
      "iteration 1919, dc_loss: 0.03648447245359421, tv_loss: 0.03349654749035835\n",
      "iteration 1920, dc_loss: 0.03650354593992233, tv_loss: 0.033486828207969666\n",
      "iteration 1921, dc_loss: 0.03645613417029381, tv_loss: 0.03351552411913872\n",
      "iteration 1922, dc_loss: 0.03648124262690544, tv_loss: 0.03346467390656471\n",
      "iteration 1923, dc_loss: 0.03643961250782013, tv_loss: 0.03348662704229355\n",
      "iteration 1924, dc_loss: 0.036460455507040024, tv_loss: 0.03345396742224693\n",
      "iteration 1925, dc_loss: 0.03639443591237068, tv_loss: 0.03351238742470741\n",
      "iteration 1926, dc_loss: 0.036429356783628464, tv_loss: 0.033462949097156525\n",
      "iteration 1927, dc_loss: 0.03637082502245903, tv_loss: 0.033500466495752335\n",
      "iteration 1928, dc_loss: 0.03641215339303017, tv_loss: 0.03344361111521721\n",
      "iteration 1929, dc_loss: 0.03632853180170059, tv_loss: 0.03351140767335892\n",
      "iteration 1930, dc_loss: 0.03638162463903427, tv_loss: 0.03344130143523216\n",
      "iteration 1931, dc_loss: 0.03628868609666824, tv_loss: 0.03351474553346634\n",
      "iteration 1932, dc_loss: 0.03636625409126282, tv_loss: 0.033427901566028595\n",
      "iteration 1933, dc_loss: 0.03625830262899399, tv_loss: 0.033528950065374374\n",
      "iteration 1934, dc_loss: 0.03636869788169861, tv_loss: 0.03342186659574509\n",
      "iteration 1935, dc_loss: 0.03624008223414421, tv_loss: 0.03358004242181778\n",
      "iteration 1936, dc_loss: 0.036407146602869034, tv_loss: 0.0334172323346138\n",
      "iteration 1937, dc_loss: 0.03624245896935463, tv_loss: 0.03359886631369591\n",
      "iteration 1938, dc_loss: 0.03649546951055527, tv_loss: 0.033353351056575775\n",
      "iteration 1939, dc_loss: 0.03626098856329918, tv_loss: 0.033644989132881165\n",
      "iteration 1940, dc_loss: 0.036596570163965225, tv_loss: 0.03333193063735962\n",
      "iteration 1941, dc_loss: 0.03627876564860344, tv_loss: 0.03367355838418007\n",
      "iteration 1942, dc_loss: 0.03654003515839577, tv_loss: 0.03333768621087074\n",
      "iteration 1943, dc_loss: 0.03619784116744995, tv_loss: 0.03359696641564369\n",
      "iteration 1944, dc_loss: 0.03626788407564163, tv_loss: 0.0334208682179451\n",
      "iteration 1945, dc_loss: 0.03609734773635864, tv_loss: 0.03351932018995285\n",
      "iteration 1946, dc_loss: 0.036082297563552856, tv_loss: 0.03350507840514183\n",
      "iteration 1947, dc_loss: 0.036160342395305634, tv_loss: 0.03342769294977188\n",
      "iteration 1948, dc_loss: 0.03604225069284439, tv_loss: 0.033580947667360306\n",
      "iteration 1949, dc_loss: 0.036241594702005386, tv_loss: 0.033384352922439575\n",
      "iteration 1950, dc_loss: 0.03600647673010826, tv_loss: 0.03361103683710098\n",
      "iteration 1951, dc_loss: 0.036130160093307495, tv_loss: 0.03341999277472496\n",
      "iteration 1952, dc_loss: 0.03596845269203186, tv_loss: 0.0335240513086319\n",
      "iteration 1953, dc_loss: 0.035981517285108566, tv_loss: 0.033489007502794266\n",
      "iteration 1954, dc_loss: 0.03599349036812782, tv_loss: 0.03345958888530731\n",
      "iteration 1955, dc_loss: 0.03590799868106842, tv_loss: 0.033549774438142776\n",
      "iteration 1956, dc_loss: 0.03605277091264725, tv_loss: 0.033417366445064545\n",
      "iteration 1957, dc_loss: 0.03588838875293732, tv_loss: 0.033566106110811234\n",
      "iteration 1958, dc_loss: 0.0359976701438427, tv_loss: 0.033424172550439835\n",
      "iteration 1959, dc_loss: 0.035856928676366806, tv_loss: 0.03356393054127693\n",
      "iteration 1960, dc_loss: 0.03588526323437691, tv_loss: 0.033508818596601486\n",
      "iteration 1961, dc_loss: 0.03586626425385475, tv_loss: 0.03349961340427399\n",
      "iteration 1962, dc_loss: 0.035815414041280746, tv_loss: 0.03353200480341911\n",
      "iteration 1963, dc_loss: 0.03588493913412094, tv_loss: 0.03348774090409279\n",
      "iteration 1964, dc_loss: 0.0358172208070755, tv_loss: 0.0335485115647316\n",
      "iteration 1965, dc_loss: 0.03586936369538307, tv_loss: 0.033483777195215225\n",
      "iteration 1966, dc_loss: 0.03581603243947029, tv_loss: 0.03353525698184967\n",
      "iteration 1967, dc_loss: 0.03582793101668358, tv_loss: 0.03351372107863426\n",
      "iteration 1968, dc_loss: 0.0357612781226635, tv_loss: 0.03352314233779907\n",
      "iteration 1969, dc_loss: 0.03575507923960686, tv_loss: 0.03349674493074417\n",
      "iteration 1970, dc_loss: 0.0357549712061882, tv_loss: 0.03348785266280174\n",
      "iteration 1971, dc_loss: 0.03570222854614258, tv_loss: 0.03355349972844124\n",
      "iteration 1972, dc_loss: 0.03575562685728073, tv_loss: 0.03348582610487938\n",
      "iteration 1973, dc_loss: 0.0356629304587841, tv_loss: 0.03354644402861595\n",
      "iteration 1974, dc_loss: 0.03570200502872467, tv_loss: 0.03347349539399147\n",
      "iteration 1975, dc_loss: 0.03565070405602455, tv_loss: 0.033500947058200836\n",
      "iteration 1976, dc_loss: 0.03562730923295021, tv_loss: 0.03350622206926346\n",
      "iteration 1977, dc_loss: 0.03563254699110985, tv_loss: 0.033479243516922\n",
      "iteration 1978, dc_loss: 0.03557881340384483, tv_loss: 0.0335325226187706\n",
      "iteration 1979, dc_loss: 0.03564052656292915, tv_loss: 0.03346535563468933\n",
      "iteration 1980, dc_loss: 0.035530395805835724, tv_loss: 0.03357355296611786\n",
      "iteration 1981, dc_loss: 0.03561728447675705, tv_loss: 0.03348112851381302\n",
      "iteration 1982, dc_loss: 0.03549826145172119, tv_loss: 0.03356276825070381\n",
      "iteration 1983, dc_loss: 0.03555871918797493, tv_loss: 0.033472690731287\n",
      "iteration 1984, dc_loss: 0.035496123135089874, tv_loss: 0.03353315591812134\n",
      "iteration 1985, dc_loss: 0.035500310361385345, tv_loss: 0.033507321029901505\n",
      "iteration 1986, dc_loss: 0.03547609597444534, tv_loss: 0.03352870047092438\n",
      "iteration 1987, dc_loss: 0.03546253591775894, tv_loss: 0.033525872975587845\n",
      "iteration 1988, dc_loss: 0.03545201197266579, tv_loss: 0.033503785729408264\n",
      "iteration 1989, dc_loss: 0.03544863313436508, tv_loss: 0.03350098803639412\n",
      "iteration 1990, dc_loss: 0.03541835397481918, tv_loss: 0.033517878502607346\n",
      "iteration 1991, dc_loss: 0.035388898104429245, tv_loss: 0.033545371145009995\n",
      "iteration 1992, dc_loss: 0.035400763154029846, tv_loss: 0.03352021053433418\n",
      "iteration 1993, dc_loss: 0.03538788855075836, tv_loss: 0.033518217504024506\n",
      "iteration 1994, dc_loss: 0.035394471138715744, tv_loss: 0.03351782634854317\n",
      "iteration 1995, dc_loss: 0.03539654612541199, tv_loss: 0.033519547432661057\n",
      "iteration 1996, dc_loss: 0.03545406460762024, tv_loss: 0.03350990265607834\n",
      "iteration 1997, dc_loss: 0.035406701266765594, tv_loss: 0.033548370003700256\n",
      "iteration 1998, dc_loss: 0.035405900329351425, tv_loss: 0.0335204154253006\n",
      "iteration 1999, dc_loss: 0.03534436970949173, tv_loss: 0.03354540839791298\n",
      "iteration 2000, dc_loss: 0.03537807986140251, tv_loss: 0.03350260108709335\n",
      "iteration 2001, dc_loss: 0.035331644117832184, tv_loss: 0.03353621065616608\n",
      "iteration 2002, dc_loss: 0.03530164435505867, tv_loss: 0.033507611602544785\n",
      "iteration 2003, dc_loss: 0.035247188061475754, tv_loss: 0.033500805497169495\n",
      "iteration 2004, dc_loss: 0.035268384963274, tv_loss: 0.03352503851056099\n",
      "iteration 2005, dc_loss: 0.03527506813406944, tv_loss: 0.03349357470870018\n",
      "iteration 2006, dc_loss: 0.03520110622048378, tv_loss: 0.03351808339357376\n",
      "iteration 2007, dc_loss: 0.035210225731134415, tv_loss: 0.033513475209474564\n",
      "iteration 2008, dc_loss: 0.03524310886859894, tv_loss: 0.033498384058475494\n",
      "iteration 2009, dc_loss: 0.03517228737473488, tv_loss: 0.033535219728946686\n",
      "iteration 2010, dc_loss: 0.0351596400141716, tv_loss: 0.0335240438580513\n",
      "iteration 2011, dc_loss: 0.035184334963560104, tv_loss: 0.03350590169429779\n",
      "iteration 2012, dc_loss: 0.03514659032225609, tv_loss: 0.03351471200585365\n",
      "iteration 2013, dc_loss: 0.03512910380959511, tv_loss: 0.03350260108709335\n",
      "iteration 2014, dc_loss: 0.03513350710272789, tv_loss: 0.03350236266851425\n",
      "iteration 2015, dc_loss: 0.03511165827512741, tv_loss: 0.03351633623242378\n",
      "iteration 2016, dc_loss: 0.03509720787405968, tv_loss: 0.033497750759124756\n",
      "iteration 2017, dc_loss: 0.03508346155285835, tv_loss: 0.03350505232810974\n",
      "iteration 2018, dc_loss: 0.03507177159190178, tv_loss: 0.033514101058244705\n",
      "iteration 2019, dc_loss: 0.0350663885474205, tv_loss: 0.03349391743540764\n",
      "iteration 2020, dc_loss: 0.03503488004207611, tv_loss: 0.03352447599172592\n",
      "iteration 2021, dc_loss: 0.035032324492931366, tv_loss: 0.03352905809879303\n",
      "iteration 2022, dc_loss: 0.03504001349210739, tv_loss: 0.03351595252752304\n",
      "iteration 2023, dc_loss: 0.034994155168533325, tv_loss: 0.033536557108163834\n",
      "iteration 2024, dc_loss: 0.03499512001872063, tv_loss: 0.0335221029818058\n",
      "iteration 2025, dc_loss: 0.034999508410692215, tv_loss: 0.03350929543375969\n",
      "iteration 2026, dc_loss: 0.03496107459068298, tv_loss: 0.03354176506400108\n",
      "iteration 2027, dc_loss: 0.03496135026216507, tv_loss: 0.0335344634950161\n",
      "iteration 2028, dc_loss: 0.034953679889440536, tv_loss: 0.03352176025509834\n",
      "iteration 2029, dc_loss: 0.03492678329348564, tv_loss: 0.033528149127960205\n",
      "iteration 2030, dc_loss: 0.03493684530258179, tv_loss: 0.0335257314145565\n",
      "iteration 2031, dc_loss: 0.03491239994764328, tv_loss: 0.03353026881814003\n",
      "iteration 2032, dc_loss: 0.034890953451395035, tv_loss: 0.03353555500507355\n",
      "iteration 2033, dc_loss: 0.03490590676665306, tv_loss: 0.03351199999451637\n",
      "iteration 2034, dc_loss: 0.034873709082603455, tv_loss: 0.03353460878133774\n",
      "iteration 2035, dc_loss: 0.03485807031393051, tv_loss: 0.0335451103746891\n",
      "iteration 2036, dc_loss: 0.03486870601773262, tv_loss: 0.033511508256196976\n",
      "iteration 2037, dc_loss: 0.03483712300658226, tv_loss: 0.033529527485370636\n",
      "iteration 2038, dc_loss: 0.03483104333281517, tv_loss: 0.03352571651339531\n",
      "iteration 2039, dc_loss: 0.03484116494655609, tv_loss: 0.03351633995771408\n",
      "iteration 2040, dc_loss: 0.034802794456481934, tv_loss: 0.03353486582636833\n",
      "iteration 2041, dc_loss: 0.03478977829217911, tv_loss: 0.033537574112415314\n",
      "iteration 2042, dc_loss: 0.034800976514816284, tv_loss: 0.03351578116416931\n",
      "iteration 2043, dc_loss: 0.0347716324031353, tv_loss: 0.033522360026836395\n",
      "iteration 2044, dc_loss: 0.034758757799863815, tv_loss: 0.03353195637464523\n",
      "iteration 2045, dc_loss: 0.03476160019636154, tv_loss: 0.03352797403931618\n",
      "iteration 2046, dc_loss: 0.03473908454179764, tv_loss: 0.0335325188934803\n",
      "iteration 2047, dc_loss: 0.03473304212093353, tv_loss: 0.033534105867147446\n",
      "iteration 2048, dc_loss: 0.034726642072200775, tv_loss: 0.033526502549648285\n",
      "iteration 2049, dc_loss: 0.034707508981227875, tv_loss: 0.033528883010149\n",
      "iteration 2050, dc_loss: 0.0346948467195034, tv_loss: 0.03353481739759445\n",
      "iteration 2051, dc_loss: 0.03469424322247505, tv_loss: 0.033518049865961075\n",
      "iteration 2052, dc_loss: 0.03467605635523796, tv_loss: 0.03353327512741089\n",
      "iteration 2053, dc_loss: 0.034663282334804535, tv_loss: 0.03352903202176094\n",
      "iteration 2054, dc_loss: 0.03465493395924568, tv_loss: 0.03352009877562523\n",
      "iteration 2055, dc_loss: 0.034637752920389175, tv_loss: 0.033532947301864624\n",
      "iteration 2056, dc_loss: 0.03463631123304367, tv_loss: 0.033530618995428085\n",
      "iteration 2057, dc_loss: 0.03462383523583412, tv_loss: 0.03353032097220421\n",
      "iteration 2058, dc_loss: 0.034605689346790314, tv_loss: 0.03355161100625992\n",
      "iteration 2059, dc_loss: 0.034599266946315765, tv_loss: 0.03353884443640709\n",
      "iteration 2060, dc_loss: 0.03458382561802864, tv_loss: 0.03354014828801155\n",
      "iteration 2061, dc_loss: 0.03458639606833458, tv_loss: 0.03353118523955345\n",
      "iteration 2062, dc_loss: 0.03457207605242729, tv_loss: 0.03353683277964592\n",
      "iteration 2063, dc_loss: 0.034538742154836655, tv_loss: 0.03356721252202988\n",
      "iteration 2064, dc_loss: 0.03454569727182388, tv_loss: 0.033530257642269135\n",
      "iteration 2065, dc_loss: 0.034547049552202225, tv_loss: 0.03352762758731842\n",
      "iteration 2066, dc_loss: 0.0345156267285347, tv_loss: 0.033547159284353256\n",
      "iteration 2067, dc_loss: 0.03450712934136391, tv_loss: 0.03354058414697647\n",
      "iteration 2068, dc_loss: 0.03450813144445419, tv_loss: 0.03353714942932129\n",
      "iteration 2069, dc_loss: 0.03448823094367981, tv_loss: 0.03353602811694145\n",
      "iteration 2070, dc_loss: 0.03447231277823448, tv_loss: 0.03355388343334198\n",
      "iteration 2071, dc_loss: 0.03446822240948677, tv_loss: 0.03354082256555557\n",
      "iteration 2072, dc_loss: 0.03446562960743904, tv_loss: 0.033534497022628784\n",
      "iteration 2073, dc_loss: 0.0344441719353199, tv_loss: 0.033545754849910736\n",
      "iteration 2074, dc_loss: 0.03443374112248421, tv_loss: 0.03353840857744217\n",
      "iteration 2075, dc_loss: 0.034427471458911896, tv_loss: 0.03353758156299591\n",
      "iteration 2076, dc_loss: 0.03440706059336662, tv_loss: 0.033553674817085266\n",
      "iteration 2077, dc_loss: 0.03440796956419945, tv_loss: 0.03352699056267738\n",
      "iteration 2078, dc_loss: 0.03440310060977936, tv_loss: 0.033544786274433136\n",
      "iteration 2079, dc_loss: 0.034371115267276764, tv_loss: 0.0335438996553421\n",
      "iteration 2080, dc_loss: 0.0343620628118515, tv_loss: 0.03355052322149277\n",
      "iteration 2081, dc_loss: 0.03436866030097008, tv_loss: 0.033539723604917526\n",
      "iteration 2082, dc_loss: 0.03435244783759117, tv_loss: 0.03353412076830864\n",
      "iteration 2083, dc_loss: 0.03433333709836006, tv_loss: 0.033542852848768234\n",
      "iteration 2084, dc_loss: 0.0343344546854496, tv_loss: 0.0335410051047802\n",
      "iteration 2085, dc_loss: 0.034320589154958725, tv_loss: 0.03353622928261757\n",
      "iteration 2086, dc_loss: 0.034302953630685806, tv_loss: 0.03354042395949364\n",
      "iteration 2087, dc_loss: 0.03429051861166954, tv_loss: 0.033548951148986816\n",
      "iteration 2088, dc_loss: 0.03429242968559265, tv_loss: 0.03355364128947258\n",
      "iteration 2089, dc_loss: 0.034275706857442856, tv_loss: 0.0335640124976635\n",
      "iteration 2090, dc_loss: 0.0342547670006752, tv_loss: 0.033563729375600815\n",
      "iteration 2091, dc_loss: 0.034256964921951294, tv_loss: 0.033537548035383224\n",
      "iteration 2092, dc_loss: 0.03424989432096481, tv_loss: 0.033540621399879456\n",
      "iteration 2093, dc_loss: 0.0342276468873024, tv_loss: 0.033556509763002396\n",
      "iteration 2094, dc_loss: 0.03422129899263382, tv_loss: 0.03355754166841507\n",
      "iteration 2095, dc_loss: 0.03421136736869812, tv_loss: 0.03355736657977104\n",
      "iteration 2096, dc_loss: 0.03419990837574005, tv_loss: 0.033555030822753906\n",
      "iteration 2097, dc_loss: 0.03418978676199913, tv_loss: 0.033557992428541183\n",
      "iteration 2098, dc_loss: 0.03417888283729553, tv_loss: 0.033550966531038284\n",
      "iteration 2099, dc_loss: 0.03415970504283905, tv_loss: 0.03355434536933899\n",
      "iteration 2100, dc_loss: 0.03416189178824425, tv_loss: 0.03353981301188469\n",
      "iteration 2101, dc_loss: 0.03415486589074135, tv_loss: 0.033544693142175674\n",
      "iteration 2102, dc_loss: 0.03413193300366402, tv_loss: 0.0335596464574337\n",
      "iteration 2103, dc_loss: 0.03412378579378128, tv_loss: 0.03355422243475914\n",
      "iteration 2104, dc_loss: 0.03412346541881561, tv_loss: 0.033552445471286774\n",
      "iteration 2105, dc_loss: 0.03410303592681885, tv_loss: 0.03355543687939644\n",
      "iteration 2106, dc_loss: 0.03408681973814964, tv_loss: 0.03356291353702545\n",
      "iteration 2107, dc_loss: 0.0340871699154377, tv_loss: 0.03355120122432709\n",
      "iteration 2108, dc_loss: 0.03408481925725937, tv_loss: 0.03354494273662567\n",
      "iteration 2109, dc_loss: 0.03405969217419624, tv_loss: 0.03355824202299118\n",
      "iteration 2110, dc_loss: 0.03405812755227089, tv_loss: 0.03355056047439575\n",
      "iteration 2111, dc_loss: 0.03405129536986351, tv_loss: 0.033555638045072556\n",
      "iteration 2112, dc_loss: 0.03405659273266792, tv_loss: 0.03354659304022789\n",
      "iteration 2113, dc_loss: 0.034046806395053864, tv_loss: 0.033556219190359116\n",
      "iteration 2114, dc_loss: 0.03405432403087616, tv_loss: 0.03354645520448685\n",
      "iteration 2115, dc_loss: 0.03402657061815262, tv_loss: 0.033579375594854355\n",
      "iteration 2116, dc_loss: 0.034018903970718384, tv_loss: 0.033562205731868744\n",
      "iteration 2117, dc_loss: 0.033982615917921066, tv_loss: 0.03356659784913063\n",
      "iteration 2118, dc_loss: 0.03396549075841904, tv_loss: 0.03356277942657471\n",
      "iteration 2119, dc_loss: 0.03396105021238327, tv_loss: 0.03356276825070381\n",
      "iteration 2120, dc_loss: 0.0339573472738266, tv_loss: 0.03355644643306732\n",
      "iteration 2121, dc_loss: 0.03396528214216232, tv_loss: 0.03354279324412346\n",
      "iteration 2122, dc_loss: 0.03393257036805153, tv_loss: 0.03356499597430229\n",
      "iteration 2123, dc_loss: 0.03392712026834488, tv_loss: 0.033553607761859894\n",
      "iteration 2124, dc_loss: 0.03390716016292572, tv_loss: 0.03355853632092476\n",
      "iteration 2125, dc_loss: 0.03389868512749672, tv_loss: 0.03356098383665085\n",
      "iteration 2126, dc_loss: 0.03389863297343254, tv_loss: 0.033560480922460556\n",
      "iteration 2127, dc_loss: 0.033875416964292526, tv_loss: 0.03358057513833046\n",
      "iteration 2128, dc_loss: 0.03388042002916336, tv_loss: 0.033567238599061966\n",
      "iteration 2129, dc_loss: 0.03386025130748749, tv_loss: 0.03355984017252922\n",
      "iteration 2130, dc_loss: 0.03384541720151901, tv_loss: 0.033567529171705246\n",
      "iteration 2131, dc_loss: 0.033839814364910126, tv_loss: 0.03356410190463066\n",
      "iteration 2132, dc_loss: 0.03381853178143501, tv_loss: 0.03358515352010727\n",
      "iteration 2133, dc_loss: 0.033834509551525116, tv_loss: 0.03355187922716141\n",
      "iteration 2134, dc_loss: 0.033798426389694214, tv_loss: 0.03357701376080513\n",
      "iteration 2135, dc_loss: 0.03379740193486214, tv_loss: 0.03355956822633743\n",
      "iteration 2136, dc_loss: 0.03379261493682861, tv_loss: 0.033555515110492706\n",
      "iteration 2137, dc_loss: 0.03376273065805435, tv_loss: 0.03357403352856636\n",
      "iteration 2138, dc_loss: 0.03377269580960274, tv_loss: 0.03355471417307854\n",
      "iteration 2139, dc_loss: 0.033750563859939575, tv_loss: 0.03357885032892227\n",
      "iteration 2140, dc_loss: 0.03375006467103958, tv_loss: 0.03357793390750885\n",
      "iteration 2141, dc_loss: 0.03373630344867706, tv_loss: 0.03357535973191261\n",
      "iteration 2142, dc_loss: 0.033718667924404144, tv_loss: 0.03357545658946037\n",
      "iteration 2143, dc_loss: 0.03372073546051979, tv_loss: 0.0335564948618412\n",
      "iteration 2144, dc_loss: 0.03369057551026344, tv_loss: 0.03358219563961029\n",
      "iteration 2145, dc_loss: 0.0336960144340992, tv_loss: 0.03357553854584694\n",
      "iteration 2146, dc_loss: 0.033686328679323196, tv_loss: 0.03358341008424759\n",
      "iteration 2147, dc_loss: 0.03367604687809944, tv_loss: 0.03357529267668724\n",
      "iteration 2148, dc_loss: 0.03366434574127197, tv_loss: 0.03356826677918434\n",
      "iteration 2149, dc_loss: 0.03364570066332817, tv_loss: 0.033571481704711914\n",
      "iteration 2150, dc_loss: 0.033641040325164795, tv_loss: 0.033579155802726746\n",
      "iteration 2151, dc_loss: 0.03363616019487381, tv_loss: 0.03358384966850281\n",
      "iteration 2152, dc_loss: 0.03362235054373741, tv_loss: 0.033579859882593155\n",
      "iteration 2153, dc_loss: 0.033603254705667496, tv_loss: 0.03358578681945801\n",
      "iteration 2154, dc_loss: 0.03360990434885025, tv_loss: 0.03356621041893959\n",
      "iteration 2155, dc_loss: 0.03359517082571983, tv_loss: 0.033579498529434204\n",
      "iteration 2156, dc_loss: 0.03358202800154686, tv_loss: 0.03358665853738785\n",
      "iteration 2157, dc_loss: 0.033560965210199356, tv_loss: 0.033592455089092255\n",
      "iteration 2158, dc_loss: 0.03356257081031799, tv_loss: 0.0335766077041626\n",
      "iteration 2159, dc_loss: 0.03355132043361664, tv_loss: 0.033571843057870865\n",
      "iteration 2160, dc_loss: 0.03353854641318321, tv_loss: 0.03358364850282669\n",
      "iteration 2161, dc_loss: 0.03354327753186226, tv_loss: 0.033574361354112625\n",
      "iteration 2162, dc_loss: 0.03351522237062454, tv_loss: 0.03359160199761391\n",
      "iteration 2163, dc_loss: 0.033503804355859756, tv_loss: 0.03358730301260948\n",
      "iteration 2164, dc_loss: 0.033505070954561234, tv_loss: 0.03356948122382164\n",
      "iteration 2165, dc_loss: 0.033497247844934464, tv_loss: 0.03357095643877983\n",
      "iteration 2166, dc_loss: 0.033477067947387695, tv_loss: 0.03358425199985504\n",
      "iteration 2167, dc_loss: 0.03346928581595421, tv_loss: 0.033589210361242294\n",
      "iteration 2168, dc_loss: 0.03346056863665581, tv_loss: 0.033584415912628174\n",
      "iteration 2169, dc_loss: 0.03345201537013054, tv_loss: 0.033579833805561066\n",
      "iteration 2170, dc_loss: 0.03343654423952103, tv_loss: 0.03358133137226105\n",
      "iteration 2171, dc_loss: 0.033437881618738174, tv_loss: 0.03357086703181267\n",
      "iteration 2172, dc_loss: 0.033418621867895126, tv_loss: 0.03358175978064537\n",
      "iteration 2173, dc_loss: 0.03341491147875786, tv_loss: 0.03359093889594078\n",
      "iteration 2174, dc_loss: 0.033404022455215454, tv_loss: 0.03357762470841408\n",
      "iteration 2175, dc_loss: 0.03339601680636406, tv_loss: 0.033576637506484985\n",
      "iteration 2176, dc_loss: 0.03339844197034836, tv_loss: 0.03358478099107742\n",
      "iteration 2177, dc_loss: 0.03341229259967804, tv_loss: 0.03357511758804321\n",
      "iteration 2178, dc_loss: 0.03341322019696236, tv_loss: 0.0336022712290287\n",
      "iteration 2179, dc_loss: 0.03341871500015259, tv_loss: 0.03357848897576332\n",
      "iteration 2180, dc_loss: 0.03339811787009239, tv_loss: 0.033586595207452774\n",
      "iteration 2181, dc_loss: 0.03336873650550842, tv_loss: 0.03357044234871864\n",
      "iteration 2182, dc_loss: 0.03333885595202446, tv_loss: 0.03356846794486046\n",
      "iteration 2183, dc_loss: 0.03331761807203293, tv_loss: 0.03358973562717438\n",
      "iteration 2184, dc_loss: 0.03335009887814522, tv_loss: 0.033560533076524734\n",
      "iteration 2185, dc_loss: 0.03330199047923088, tv_loss: 0.03360914811491966\n",
      "iteration 2186, dc_loss: 0.03331568092107773, tv_loss: 0.03355860337615013\n",
      "iteration 2187, dc_loss: 0.03328036144375801, tv_loss: 0.033573612570762634\n",
      "iteration 2188, dc_loss: 0.03327209874987602, tv_loss: 0.03358495980501175\n",
      "iteration 2189, dc_loss: 0.03331579267978668, tv_loss: 0.0335395485162735\n",
      "iteration 2190, dc_loss: 0.03323669359087944, tv_loss: 0.033612292259931564\n",
      "iteration 2191, dc_loss: 0.03327043727040291, tv_loss: 0.03356391191482544\n",
      "iteration 2192, dc_loss: 0.033215027302503586, tv_loss: 0.03361821547150612\n",
      "iteration 2193, dc_loss: 0.0332305021584034, tv_loss: 0.033607419580221176\n",
      "iteration 2194, dc_loss: 0.03323987126350403, tv_loss: 0.033576615154743195\n",
      "iteration 2195, dc_loss: 0.033184509724378586, tv_loss: 0.0336025170981884\n",
      "iteration 2196, dc_loss: 0.03321978822350502, tv_loss: 0.033559203147888184\n",
      "iteration 2197, dc_loss: 0.03315960243344307, tv_loss: 0.03364855796098709\n",
      "iteration 2198, dc_loss: 0.03319116681814194, tv_loss: 0.03359806537628174\n",
      "iteration 2199, dc_loss: 0.03316733241081238, tv_loss: 0.033587388694286346\n",
      "iteration 2200, dc_loss: 0.03313374146819115, tv_loss: 0.033616166561841965\n",
      "iteration 2201, dc_loss: 0.03317174315452576, tv_loss: 0.03358667716383934\n",
      "iteration 2202, dc_loss: 0.03312453255057335, tv_loss: 0.03361840173602104\n",
      "iteration 2203, dc_loss: 0.03312738612294197, tv_loss: 0.03359454497694969\n",
      "iteration 2204, dc_loss: 0.03311190754175186, tv_loss: 0.03360726684331894\n",
      "iteration 2205, dc_loss: 0.03310180827975273, tv_loss: 0.033620014786720276\n",
      "iteration 2206, dc_loss: 0.033097051084041595, tv_loss: 0.033602047711610794\n",
      "iteration 2207, dc_loss: 0.033091556280851364, tv_loss: 0.03359047323465347\n",
      "iteration 2208, dc_loss: 0.03307942673563957, tv_loss: 0.033606406301259995\n",
      "iteration 2209, dc_loss: 0.033053141087293625, tv_loss: 0.03361310809850693\n",
      "iteration 2210, dc_loss: 0.03305681049823761, tv_loss: 0.03359486535191536\n",
      "iteration 2211, dc_loss: 0.03305517137050629, tv_loss: 0.0335899256169796\n",
      "iteration 2212, dc_loss: 0.03302933648228645, tv_loss: 0.03359859809279442\n",
      "iteration 2213, dc_loss: 0.03302505612373352, tv_loss: 0.03359229117631912\n",
      "iteration 2214, dc_loss: 0.03301785886287689, tv_loss: 0.033593956381082535\n",
      "iteration 2215, dc_loss: 0.03300352022051811, tv_loss: 0.033597011119127274\n",
      "iteration 2216, dc_loss: 0.032994549721479416, tv_loss: 0.03359271213412285\n",
      "iteration 2217, dc_loss: 0.032989513128995895, tv_loss: 0.03359271213412285\n",
      "iteration 2218, dc_loss: 0.03297998383641243, tv_loss: 0.03359810262918472\n",
      "iteration 2219, dc_loss: 0.0329660139977932, tv_loss: 0.033600639551877975\n",
      "iteration 2220, dc_loss: 0.032974451780319214, tv_loss: 0.033591609448194504\n",
      "iteration 2221, dc_loss: 0.032934293150901794, tv_loss: 0.03362919017672539\n",
      "iteration 2222, dc_loss: 0.032939665019512177, tv_loss: 0.0335988812148571\n",
      "iteration 2223, dc_loss: 0.032932817935943604, tv_loss: 0.03360506519675255\n",
      "iteration 2224, dc_loss: 0.032921113073825836, tv_loss: 0.03360842168331146\n",
      "iteration 2225, dc_loss: 0.032914429903030396, tv_loss: 0.03360190615057945\n",
      "iteration 2226, dc_loss: 0.03291056677699089, tv_loss: 0.03359944000840187\n",
      "iteration 2227, dc_loss: 0.03289332613348961, tv_loss: 0.03361052647233009\n",
      "iteration 2228, dc_loss: 0.0329054519534111, tv_loss: 0.03358620032668114\n",
      "iteration 2229, dc_loss: 0.03287183493375778, tv_loss: 0.033611588180065155\n",
      "iteration 2230, dc_loss: 0.032889075577259064, tv_loss: 0.033589743077754974\n",
      "iteration 2231, dc_loss: 0.03285400941967964, tv_loss: 0.03361193835735321\n",
      "iteration 2232, dc_loss: 0.03287596255540848, tv_loss: 0.03357802703976631\n",
      "iteration 2233, dc_loss: 0.03283052146434784, tv_loss: 0.03360980004072189\n",
      "iteration 2234, dc_loss: 0.032853029668331146, tv_loss: 0.03358108550310135\n",
      "iteration 2235, dc_loss: 0.0328044593334198, tv_loss: 0.03362198546528816\n",
      "iteration 2236, dc_loss: 0.032806042581796646, tv_loss: 0.03361018747091293\n",
      "iteration 2237, dc_loss: 0.03279656171798706, tv_loss: 0.03359968215227127\n",
      "iteration 2238, dc_loss: 0.0327882245182991, tv_loss: 0.033604465425014496\n",
      "iteration 2239, dc_loss: 0.03278798609972, tv_loss: 0.03358810022473335\n",
      "iteration 2240, dc_loss: 0.03275705501437187, tv_loss: 0.033616531640291214\n",
      "iteration 2241, dc_loss: 0.03278053551912308, tv_loss: 0.03358962759375572\n",
      "iteration 2242, dc_loss: 0.032749611884355545, tv_loss: 0.033617306500673294\n",
      "iteration 2243, dc_loss: 0.03275790810585022, tv_loss: 0.033603496849536896\n",
      "iteration 2244, dc_loss: 0.0327230803668499, tv_loss: 0.03361861780285835\n",
      "iteration 2245, dc_loss: 0.03274225816130638, tv_loss: 0.033590372651815414\n",
      "iteration 2246, dc_loss: 0.03271256759762764, tv_loss: 0.03360725939273834\n",
      "iteration 2247, dc_loss: 0.03271685168147087, tv_loss: 0.03359386697411537\n",
      "iteration 2248, dc_loss: 0.03269905596971512, tv_loss: 0.03359945863485336\n",
      "iteration 2249, dc_loss: 0.0326797254383564, tv_loss: 0.03361545130610466\n",
      "iteration 2250, dc_loss: 0.03268597275018692, tv_loss: 0.03359800577163696\n",
      "iteration 2251, dc_loss: 0.03267775475978851, tv_loss: 0.03360489010810852\n",
      "iteration 2252, dc_loss: 0.03266213834285736, tv_loss: 0.0336204394698143\n",
      "iteration 2253, dc_loss: 0.032644208520650864, tv_loss: 0.03362192586064339\n",
      "iteration 2254, dc_loss: 0.03264673426747322, tv_loss: 0.03360015153884888\n",
      "iteration 2255, dc_loss: 0.032628633081912994, tv_loss: 0.033610906451940536\n",
      "iteration 2256, dc_loss: 0.03262585774064064, tv_loss: 0.03360193595290184\n",
      "iteration 2257, dc_loss: 0.032615795731544495, tv_loss: 0.0335964635014534\n",
      "iteration 2258, dc_loss: 0.03259824216365814, tv_loss: 0.03360684961080551\n",
      "iteration 2259, dc_loss: 0.032587986439466476, tv_loss: 0.033608727157115936\n",
      "iteration 2260, dc_loss: 0.03259831294417381, tv_loss: 0.03359201177954674\n",
      "iteration 2261, dc_loss: 0.03258547931909561, tv_loss: 0.03360060229897499\n",
      "iteration 2262, dc_loss: 0.032560378313064575, tv_loss: 0.03362075611948967\n",
      "iteration 2263, dc_loss: 0.032567474991083145, tv_loss: 0.03361450135707855\n",
      "iteration 2264, dc_loss: 0.032550837844610214, tv_loss: 0.033632270991802216\n",
      "iteration 2265, dc_loss: 0.03256592899560928, tv_loss: 0.033608682453632355\n",
      "iteration 2266, dc_loss: 0.03253091126680374, tv_loss: 0.03362853080034256\n",
      "iteration 2267, dc_loss: 0.03255605697631836, tv_loss: 0.033591657876968384\n",
      "iteration 2268, dc_loss: 0.03250804543495178, tv_loss: 0.03363413363695145\n",
      "iteration 2269, dc_loss: 0.03255346789956093, tv_loss: 0.03358915448188782\n",
      "iteration 2270, dc_loss: 0.03251048922538757, tv_loss: 0.033624544739723206\n",
      "iteration 2271, dc_loss: 0.03254058212041855, tv_loss: 0.03358062729239464\n",
      "iteration 2272, dc_loss: 0.03246976435184479, tv_loss: 0.03364007547497749\n",
      "iteration 2273, dc_loss: 0.032520733773708344, tv_loss: 0.03357864171266556\n",
      "iteration 2274, dc_loss: 0.032444801181554794, tv_loss: 0.0336405448615551\n",
      "iteration 2275, dc_loss: 0.03245854005217552, tv_loss: 0.033616531640291214\n",
      "iteration 2276, dc_loss: 0.03244200721383095, tv_loss: 0.033622946590185165\n",
      "iteration 2277, dc_loss: 0.03242884948849678, tv_loss: 0.03362547233700752\n",
      "iteration 2278, dc_loss: 0.03242836147546768, tv_loss: 0.03360714763402939\n",
      "iteration 2279, dc_loss: 0.032409556210041046, tv_loss: 0.03361902013421059\n",
      "iteration 2280, dc_loss: 0.032416265457868576, tv_loss: 0.0336136519908905\n",
      "iteration 2281, dc_loss: 0.03238082677125931, tv_loss: 0.033636048436164856\n",
      "iteration 2282, dc_loss: 0.03242125362157822, tv_loss: 0.03358633816242218\n",
      "iteration 2283, dc_loss: 0.03237596154212952, tv_loss: 0.03363504260778427\n",
      "iteration 2284, dc_loss: 0.032396458089351654, tv_loss: 0.03360229358077049\n",
      "iteration 2285, dc_loss: 0.032357946038246155, tv_loss: 0.033631566911935806\n",
      "iteration 2286, dc_loss: 0.03237701579928398, tv_loss: 0.03362555056810379\n",
      "iteration 2287, dc_loss: 0.03234269097447395, tv_loss: 0.033637259155511856\n",
      "iteration 2288, dc_loss: 0.03234870731830597, tv_loss: 0.033617328852415085\n",
      "iteration 2289, dc_loss: 0.032314643263816833, tv_loss: 0.033636465668678284\n",
      "iteration 2290, dc_loss: 0.03232676908373833, tv_loss: 0.033611055463552475\n",
      "iteration 2291, dc_loss: 0.03229004517197609, tv_loss: 0.03365879878401756\n",
      "iteration 2292, dc_loss: 0.03231377899646759, tv_loss: 0.03360074758529663\n",
      "iteration 2293, dc_loss: 0.03228578343987465, tv_loss: 0.03362857550382614\n",
      "iteration 2294, dc_loss: 0.03227424621582031, tv_loss: 0.03362780436873436\n",
      "iteration 2295, dc_loss: 0.032276201993227005, tv_loss: 0.0336172841489315\n",
      "iteration 2296, dc_loss: 0.032267771661281586, tv_loss: 0.03364265710115433\n",
      "iteration 2297, dc_loss: 0.03226333111524582, tv_loss: 0.03363264352083206\n",
      "iteration 2298, dc_loss: 0.032247282564640045, tv_loss: 0.033638231456279755\n",
      "iteration 2299, dc_loss: 0.03225390240550041, tv_loss: 0.0336175300180912\n",
      "iteration 2300, dc_loss: 0.032231178134679794, tv_loss: 0.03364600986242294\n",
      "iteration 2301, dc_loss: 0.032242301851511, tv_loss: 0.033629417419433594\n",
      "iteration 2302, dc_loss: 0.03221547231078148, tv_loss: 0.0336410216987133\n",
      "iteration 2303, dc_loss: 0.0322304293513298, tv_loss: 0.03360506519675255\n",
      "iteration 2304, dc_loss: 0.03219430893659592, tv_loss: 0.03364355117082596\n",
      "iteration 2305, dc_loss: 0.03221110627055168, tv_loss: 0.033630095422267914\n",
      "iteration 2306, dc_loss: 0.03217323124408722, tv_loss: 0.03363986685872078\n",
      "iteration 2307, dc_loss: 0.032193414866924286, tv_loss: 0.03361992910504341\n",
      "iteration 2308, dc_loss: 0.032147567719221115, tv_loss: 0.03363918513059616\n",
      "iteration 2309, dc_loss: 0.032170794904232025, tv_loss: 0.03361142426729202\n",
      "iteration 2310, dc_loss: 0.032125283032655716, tv_loss: 0.033653080463409424\n",
      "iteration 2311, dc_loss: 0.03214560076594353, tv_loss: 0.03362143784761429\n",
      "iteration 2312, dc_loss: 0.03210814669728279, tv_loss: 0.033643171191215515\n",
      "iteration 2313, dc_loss: 0.032162006944417953, tv_loss: 0.03358625993132591\n",
      "iteration 2314, dc_loss: 0.03207544609904289, tv_loss: 0.03366773948073387\n",
      "iteration 2315, dc_loss: 0.03214067965745926, tv_loss: 0.03359097242355347\n",
      "iteration 2316, dc_loss: 0.03206786885857582, tv_loss: 0.033659521490335464\n",
      "iteration 2317, dc_loss: 0.032129235565662384, tv_loss: 0.033595919609069824\n",
      "iteration 2318, dc_loss: 0.032050617039203644, tv_loss: 0.033659107983112335\n",
      "iteration 2319, dc_loss: 0.03211737051606178, tv_loss: 0.03357969596982002\n",
      "iteration 2320, dc_loss: 0.03202277421951294, tv_loss: 0.03368398919701576\n",
      "iteration 2321, dc_loss: 0.03211180865764618, tv_loss: 0.03357503190636635\n",
      "iteration 2322, dc_loss: 0.03202008828520775, tv_loss: 0.033656857907772064\n",
      "iteration 2323, dc_loss: 0.03209070488810539, tv_loss: 0.03359641879796982\n",
      "iteration 2324, dc_loss: 0.03199877589941025, tv_loss: 0.033679623156785965\n",
      "iteration 2325, dc_loss: 0.03208887204527855, tv_loss: 0.033583734184503555\n",
      "iteration 2326, dc_loss: 0.032003775238990784, tv_loss: 0.03366877883672714\n",
      "iteration 2327, dc_loss: 0.032073572278022766, tv_loss: 0.033590905368328094\n",
      "iteration 2328, dc_loss: 0.03196687623858452, tv_loss: 0.03367909789085388\n",
      "iteration 2329, dc_loss: 0.03204483166337013, tv_loss: 0.03358052298426628\n",
      "iteration 2330, dc_loss: 0.031951695680618286, tv_loss: 0.0336662158370018\n",
      "iteration 2331, dc_loss: 0.03198941424489021, tv_loss: 0.03360329940915108\n",
      "iteration 2332, dc_loss: 0.03195922449231148, tv_loss: 0.0336296521127224\n",
      "iteration 2333, dc_loss: 0.0319487564265728, tv_loss: 0.0336264967918396\n",
      "iteration 2334, dc_loss: 0.03193216770887375, tv_loss: 0.0336417555809021\n",
      "iteration 2335, dc_loss: 0.031920842826366425, tv_loss: 0.03364817053079605\n",
      "iteration 2336, dc_loss: 0.0319414921104908, tv_loss: 0.033616695553064346\n",
      "iteration 2337, dc_loss: 0.03188706934452057, tv_loss: 0.03365961089730263\n",
      "iteration 2338, dc_loss: 0.03195236995816231, tv_loss: 0.03358989953994751\n",
      "iteration 2339, dc_loss: 0.0318615697324276, tv_loss: 0.033668942749500275\n",
      "iteration 2340, dc_loss: 0.03192247822880745, tv_loss: 0.03360246494412422\n",
      "iteration 2341, dc_loss: 0.031863462179899216, tv_loss: 0.03366445004940033\n",
      "iteration 2342, dc_loss: 0.031903788447380066, tv_loss: 0.03360649198293686\n",
      "iteration 2343, dc_loss: 0.03185298666357994, tv_loss: 0.033657483756542206\n",
      "iteration 2344, dc_loss: 0.031879179179668427, tv_loss: 0.033610936254262924\n",
      "iteration 2345, dc_loss: 0.031841933727264404, tv_loss: 0.03364330530166626\n",
      "iteration 2346, dc_loss: 0.03185296803712845, tv_loss: 0.03363204374909401\n",
      "iteration 2347, dc_loss: 0.03186286985874176, tv_loss: 0.03362133353948593\n",
      "iteration 2348, dc_loss: 0.031843602657318115, tv_loss: 0.03364047408103943\n",
      "iteration 2349, dc_loss: 0.03186790272593498, tv_loss: 0.03361302614212036\n",
      "iteration 2350, dc_loss: 0.031806495040655136, tv_loss: 0.03366929292678833\n",
      "iteration 2351, dc_loss: 0.03184705600142479, tv_loss: 0.033614713698625565\n",
      "iteration 2352, dc_loss: 0.03177529573440552, tv_loss: 0.03366341441869736\n",
      "iteration 2353, dc_loss: 0.03180016577243805, tv_loss: 0.033622026443481445\n",
      "iteration 2354, dc_loss: 0.03175445646047592, tv_loss: 0.03366472199559212\n",
      "iteration 2355, dc_loss: 0.03179553896188736, tv_loss: 0.03362388163805008\n",
      "iteration 2356, dc_loss: 0.03175393491983414, tv_loss: 0.03367629274725914\n",
      "iteration 2357, dc_loss: 0.03175997734069824, tv_loss: 0.0336463525891304\n",
      "iteration 2358, dc_loss: 0.03173797205090523, tv_loss: 0.03365439176559448\n",
      "iteration 2359, dc_loss: 0.031741246581077576, tv_loss: 0.03362978622317314\n",
      "iteration 2360, dc_loss: 0.0317092165350914, tv_loss: 0.033673908561468124\n",
      "iteration 2361, dc_loss: 0.031741950660943985, tv_loss: 0.033616047352552414\n",
      "iteration 2362, dc_loss: 0.031694479286670685, tv_loss: 0.033664677292108536\n",
      "iteration 2363, dc_loss: 0.03171543404459953, tv_loss: 0.03363407775759697\n",
      "iteration 2364, dc_loss: 0.031695976853370667, tv_loss: 0.03364512696862221\n",
      "iteration 2365, dc_loss: 0.03167838975787163, tv_loss: 0.03365137428045273\n",
      "iteration 2366, dc_loss: 0.03166722506284714, tv_loss: 0.03364179655909538\n",
      "iteration 2367, dc_loss: 0.03166496753692627, tv_loss: 0.033637307584285736\n",
      "iteration 2368, dc_loss: 0.031661372631788254, tv_loss: 0.033630385994911194\n",
      "iteration 2369, dc_loss: 0.03162750229239464, tv_loss: 0.03367799520492554\n",
      "iteration 2370, dc_loss: 0.03165167570114136, tv_loss: 0.033643148839473724\n",
      "iteration 2371, dc_loss: 0.031625524163246155, tv_loss: 0.033655691891908646\n",
      "iteration 2372, dc_loss: 0.03162546083331108, tv_loss: 0.033633794635534286\n",
      "iteration 2373, dc_loss: 0.03161538764834404, tv_loss: 0.03364211320877075\n",
      "iteration 2374, dc_loss: 0.03160487115383148, tv_loss: 0.03365527465939522\n",
      "iteration 2375, dc_loss: 0.031593143939971924, tv_loss: 0.033665064722299576\n",
      "iteration 2376, dc_loss: 0.031585413962602615, tv_loss: 0.03365979716181755\n",
      "iteration 2377, dc_loss: 0.03159929811954498, tv_loss: 0.03363608196377754\n",
      "iteration 2378, dc_loss: 0.03156346455216408, tv_loss: 0.03366188704967499\n",
      "iteration 2379, dc_loss: 0.031611159443855286, tv_loss: 0.03361302986741066\n",
      "iteration 2380, dc_loss: 0.03155652433633804, tv_loss: 0.03367379680275917\n",
      "iteration 2381, dc_loss: 0.031591158360242844, tv_loss: 0.0336446538567543\n",
      "iteration 2382, dc_loss: 0.03154492378234863, tv_loss: 0.03368207812309265\n",
      "iteration 2383, dc_loss: 0.03159136325120926, tv_loss: 0.03361938148736954\n",
      "iteration 2384, dc_loss: 0.03153230994939804, tv_loss: 0.03367765620350838\n",
      "iteration 2385, dc_loss: 0.03160223737359047, tv_loss: 0.03360063582658768\n",
      "iteration 2386, dc_loss: 0.031509947031736374, tv_loss: 0.03368673101067543\n",
      "iteration 2387, dc_loss: 0.03157772123813629, tv_loss: 0.03360607475042343\n",
      "iteration 2388, dc_loss: 0.03149988502264023, tv_loss: 0.033681225031614304\n",
      "iteration 2389, dc_loss: 0.03156198933720589, tv_loss: 0.03361921384930611\n",
      "iteration 2390, dc_loss: 0.03148189187049866, tv_loss: 0.03367983177304268\n",
      "iteration 2391, dc_loss: 0.03153728321194649, tv_loss: 0.0336155891418457\n",
      "iteration 2392, dc_loss: 0.03145508095622063, tv_loss: 0.03369300439953804\n",
      "iteration 2393, dc_loss: 0.03149523586034775, tv_loss: 0.033635638654232025\n",
      "iteration 2394, dc_loss: 0.03144972026348114, tv_loss: 0.03365340083837509\n",
      "iteration 2395, dc_loss: 0.03144600987434387, tv_loss: 0.033633675426244736\n",
      "iteration 2396, dc_loss: 0.03141489997506142, tv_loss: 0.03365635871887207\n",
      "iteration 2397, dc_loss: 0.03142490237951279, tv_loss: 0.03364856541156769\n",
      "iteration 2398, dc_loss: 0.03140107914805412, tv_loss: 0.033679671585559845\n",
      "iteration 2399, dc_loss: 0.031421959400177, tv_loss: 0.03364520147442818\n",
      "iteration 2400, dc_loss: 0.03139558434486389, tv_loss: 0.03366076946258545\n",
      "iteration 2401, dc_loss: 0.03140143305063248, tv_loss: 0.03364129737019539\n",
      "iteration 2402, dc_loss: 0.03137941658496857, tv_loss: 0.03363918513059616\n",
      "iteration 2403, dc_loss: 0.03136327490210533, tv_loss: 0.03363820165395737\n",
      "iteration 2404, dc_loss: 0.03137068450450897, tv_loss: 0.033636730164289474\n",
      "iteration 2405, dc_loss: 0.031361840665340424, tv_loss: 0.03364323079586029\n",
      "iteration 2406, dc_loss: 0.0313577726483345, tv_loss: 0.03362278267741203\n",
      "iteration 2407, dc_loss: 0.031347814947366714, tv_loss: 0.033631641417741776\n",
      "iteration 2408, dc_loss: 0.03134049102663994, tv_loss: 0.03364991769194603\n",
      "iteration 2409, dc_loss: 0.03133263811469078, tv_loss: 0.03364487364888191\n",
      "iteration 2410, dc_loss: 0.03132346272468567, tv_loss: 0.03364875167608261\n",
      "iteration 2411, dc_loss: 0.031318627297878265, tv_loss: 0.03364498168230057\n",
      "iteration 2412, dc_loss: 0.0313139483332634, tv_loss: 0.033635854721069336\n",
      "iteration 2413, dc_loss: 0.031301479786634445, tv_loss: 0.033637918531894684\n",
      "iteration 2414, dc_loss: 0.03130277618765831, tv_loss: 0.03363822028040886\n",
      "iteration 2415, dc_loss: 0.031291455030441284, tv_loss: 0.03363579139113426\n",
      "iteration 2416, dc_loss: 0.031274545937776566, tv_loss: 0.03365796059370041\n",
      "iteration 2417, dc_loss: 0.031287360936403275, tv_loss: 0.033630069345235825\n",
      "iteration 2418, dc_loss: 0.0312650240957737, tv_loss: 0.03364337608218193\n",
      "iteration 2419, dc_loss: 0.0312623456120491, tv_loss: 0.03363938629627228\n",
      "iteration 2420, dc_loss: 0.03127535805106163, tv_loss: 0.03361984342336655\n",
      "iteration 2421, dc_loss: 0.031237948685884476, tv_loss: 0.03364706411957741\n",
      "iteration 2422, dc_loss: 0.031238792464137077, tv_loss: 0.03364270552992821\n",
      "iteration 2423, dc_loss: 0.03125254064798355, tv_loss: 0.033620815724134445\n",
      "iteration 2424, dc_loss: 0.031219633296132088, tv_loss: 0.033646173775196075\n",
      "iteration 2425, dc_loss: 0.031223107129335403, tv_loss: 0.03363201022148132\n",
      "iteration 2426, dc_loss: 0.031229212880134583, tv_loss: 0.03362759202718735\n",
      "iteration 2427, dc_loss: 0.03120383992791176, tv_loss: 0.0336453802883625\n",
      "iteration 2428, dc_loss: 0.031201468780636787, tv_loss: 0.033643342554569244\n",
      "iteration 2429, dc_loss: 0.031202424317598343, tv_loss: 0.03363903611898422\n",
      "iteration 2430, dc_loss: 0.031184641644358635, tv_loss: 0.03367500379681587\n",
      "iteration 2431, dc_loss: 0.031197499483823776, tv_loss: 0.03364623710513115\n",
      "iteration 2432, dc_loss: 0.031171385198831558, tv_loss: 0.03365251049399376\n",
      "iteration 2433, dc_loss: 0.03116578422486782, tv_loss: 0.03365793824195862\n",
      "iteration 2434, dc_loss: 0.031177980825304985, tv_loss: 0.03364114090800285\n",
      "iteration 2435, dc_loss: 0.031152186915278435, tv_loss: 0.03366483747959137\n",
      "iteration 2436, dc_loss: 0.031154628843069077, tv_loss: 0.033642083406448364\n",
      "iteration 2437, dc_loss: 0.031153462827205658, tv_loss: 0.03363368287682533\n",
      "iteration 2438, dc_loss: 0.031131068244576454, tv_loss: 0.03366357088088989\n",
      "iteration 2439, dc_loss: 0.03113441728055477, tv_loss: 0.03366423398256302\n",
      "iteration 2440, dc_loss: 0.031127968803048134, tv_loss: 0.033659934997558594\n",
      "iteration 2441, dc_loss: 0.031123273074626923, tv_loss: 0.033643387258052826\n",
      "iteration 2442, dc_loss: 0.031123710796236992, tv_loss: 0.0336439348757267\n",
      "iteration 2443, dc_loss: 0.031099652871489525, tv_loss: 0.03368011489510536\n",
      "iteration 2444, dc_loss: 0.03109835647046566, tv_loss: 0.03366105630993843\n",
      "iteration 2445, dc_loss: 0.031099790707230568, tv_loss: 0.03364446386694908\n",
      "iteration 2446, dc_loss: 0.031090514734387398, tv_loss: 0.03365238755941391\n",
      "iteration 2447, dc_loss: 0.03108943998813629, tv_loss: 0.0336562842130661\n",
      "iteration 2448, dc_loss: 0.031071066856384277, tv_loss: 0.0336669459939003\n",
      "iteration 2449, dc_loss: 0.03107503615319729, tv_loss: 0.03364642709493637\n",
      "iteration 2450, dc_loss: 0.03107267990708351, tv_loss: 0.03364105895161629\n",
      "iteration 2451, dc_loss: 0.031047163531184196, tv_loss: 0.03366208076477051\n",
      "iteration 2452, dc_loss: 0.03105560690164566, tv_loss: 0.03365032747387886\n",
      "iteration 2453, dc_loss: 0.031056389212608337, tv_loss: 0.033641476184129715\n",
      "iteration 2454, dc_loss: 0.03103615529835224, tv_loss: 0.03365429490804672\n",
      "iteration 2455, dc_loss: 0.03102904185652733, tv_loss: 0.033655304461717606\n",
      "iteration 2456, dc_loss: 0.03102959506213665, tv_loss: 0.03364184498786926\n",
      "iteration 2457, dc_loss: 0.031022094190120697, tv_loss: 0.033645402640104294\n",
      "iteration 2458, dc_loss: 0.03102121129631996, tv_loss: 0.03364238515496254\n",
      "iteration 2459, dc_loss: 0.031008968129754066, tv_loss: 0.033645570278167725\n",
      "iteration 2460, dc_loss: 0.031000206246972084, tv_loss: 0.03364857658743858\n",
      "iteration 2461, dc_loss: 0.03099851682782173, tv_loss: 0.03364516794681549\n",
      "iteration 2462, dc_loss: 0.03099074587225914, tv_loss: 0.03365165367722511\n",
      "iteration 2463, dc_loss: 0.03098888322710991, tv_loss: 0.03365403413772583\n",
      "iteration 2464, dc_loss: 0.030976207926869392, tv_loss: 0.03366998955607414\n",
      "iteration 2465, dc_loss: 0.030967624858021736, tv_loss: 0.03365694358944893\n",
      "iteration 2466, dc_loss: 0.030971214175224304, tv_loss: 0.03364736586809158\n",
      "iteration 2467, dc_loss: 0.03096078708767891, tv_loss: 0.03365583345293999\n",
      "iteration 2468, dc_loss: 0.03095191717147827, tv_loss: 0.033664364367723465\n",
      "iteration 2469, dc_loss: 0.030944451689720154, tv_loss: 0.03366721794009209\n",
      "iteration 2470, dc_loss: 0.03094346635043621, tv_loss: 0.033652886748313904\n",
      "iteration 2471, dc_loss: 0.030940748751163483, tv_loss: 0.03364584222435951\n",
      "iteration 2472, dc_loss: 0.03092745877802372, tv_loss: 0.03366478532552719\n",
      "iteration 2473, dc_loss: 0.030923279002308846, tv_loss: 0.03365825489163399\n",
      "iteration 2474, dc_loss: 0.030915517359972, tv_loss: 0.03366504982113838\n",
      "iteration 2475, dc_loss: 0.030909474939107895, tv_loss: 0.03365512564778328\n",
      "iteration 2476, dc_loss: 0.030911864712834358, tv_loss: 0.03364749625325203\n",
      "iteration 2477, dc_loss: 0.030901208519935608, tv_loss: 0.03365393728017807\n",
      "iteration 2478, dc_loss: 0.030887024477124214, tv_loss: 0.03366750478744507\n",
      "iteration 2479, dc_loss: 0.03089100494980812, tv_loss: 0.033656876534223557\n",
      "iteration 2480, dc_loss: 0.030880451202392578, tv_loss: 0.03365784510970116\n",
      "iteration 2481, dc_loss: 0.030870309099555016, tv_loss: 0.033660903573036194\n",
      "iteration 2482, dc_loss: 0.030869608744978905, tv_loss: 0.03364685922861099\n",
      "iteration 2483, dc_loss: 0.030865564942359924, tv_loss: 0.03365181013941765\n",
      "iteration 2484, dc_loss: 0.03085930272936821, tv_loss: 0.03365141898393631\n",
      "iteration 2485, dc_loss: 0.030845096334815025, tv_loss: 0.03365934640169144\n",
      "iteration 2486, dc_loss: 0.030841395258903503, tv_loss: 0.03366827219724655\n",
      "iteration 2487, dc_loss: 0.03084542416036129, tv_loss: 0.033659324049949646\n",
      "iteration 2488, dc_loss: 0.030832218006253242, tv_loss: 0.03365607559680939\n",
      "iteration 2489, dc_loss: 0.03082343004643917, tv_loss: 0.03366046026349068\n",
      "iteration 2490, dc_loss: 0.030820537358522415, tv_loss: 0.033658068627119064\n",
      "iteration 2491, dc_loss: 0.030809439718723297, tv_loss: 0.03366439417004585\n",
      "iteration 2492, dc_loss: 0.030809441581368446, tv_loss: 0.03366638720035553\n",
      "iteration 2493, dc_loss: 0.03080405294895172, tv_loss: 0.03365938737988472\n",
      "iteration 2494, dc_loss: 0.030795127153396606, tv_loss: 0.033658064901828766\n",
      "iteration 2495, dc_loss: 0.030786847695708275, tv_loss: 0.03365449607372284\n",
      "iteration 2496, dc_loss: 0.03078988939523697, tv_loss: 0.033651091158390045\n",
      "iteration 2497, dc_loss: 0.030779151245951653, tv_loss: 0.03365980461239815\n",
      "iteration 2498, dc_loss: 0.030762959271669388, tv_loss: 0.033677637577056885\n",
      "iteration 2499, dc_loss: 0.030764905735850334, tv_loss: 0.033661194145679474\n",
      "iteration 2500, dc_loss: 0.03076765313744545, tv_loss: 0.03364839404821396\n",
      "iteration 2501, dc_loss: 0.030751317739486694, tv_loss: 0.03365640342235565\n",
      "iteration 2502, dc_loss: 0.030745718628168106, tv_loss: 0.03365889564156532\n",
      "iteration 2503, dc_loss: 0.030742978677153587, tv_loss: 0.03366757556796074\n",
      "iteration 2504, dc_loss: 0.030736299231648445, tv_loss: 0.03366672247648239\n",
      "iteration 2505, dc_loss: 0.030730828642845154, tv_loss: 0.03366265073418617\n",
      "iteration 2506, dc_loss: 0.030721573159098625, tv_loss: 0.033655691891908646\n",
      "iteration 2507, dc_loss: 0.030713513493537903, tv_loss: 0.03366389125585556\n",
      "iteration 2508, dc_loss: 0.03070956841111183, tv_loss: 0.03366495296359062\n",
      "iteration 2509, dc_loss: 0.03070826455950737, tv_loss: 0.03365999087691307\n",
      "iteration 2510, dc_loss: 0.030705804005265236, tv_loss: 0.033660851418972015\n",
      "iteration 2511, dc_loss: 0.03069239854812622, tv_loss: 0.03366384282708168\n",
      "iteration 2512, dc_loss: 0.030684344470500946, tv_loss: 0.0336628258228302\n",
      "iteration 2513, dc_loss: 0.030679846182465553, tv_loss: 0.0336594395339489\n",
      "iteration 2514, dc_loss: 0.03067571111023426, tv_loss: 0.03366117179393768\n",
      "iteration 2515, dc_loss: 0.03067166917026043, tv_loss: 0.03366508334875107\n",
      "iteration 2516, dc_loss: 0.030666310340166092, tv_loss: 0.03366518393158913\n",
      "iteration 2517, dc_loss: 0.030655547976493835, tv_loss: 0.033669255673885345\n",
      "iteration 2518, dc_loss: 0.03065124899148941, tv_loss: 0.033664949238300323\n",
      "iteration 2519, dc_loss: 0.030648253858089447, tv_loss: 0.033657558262348175\n",
      "iteration 2520, dc_loss: 0.03063676692545414, tv_loss: 0.03365928307175636\n",
      "iteration 2521, dc_loss: 0.030633077025413513, tv_loss: 0.03366183117032051\n",
      "iteration 2522, dc_loss: 0.03063509613275528, tv_loss: 0.03365501016378403\n",
      "iteration 2523, dc_loss: 0.030619226396083832, tv_loss: 0.03366503119468689\n",
      "iteration 2524, dc_loss: 0.03061615489423275, tv_loss: 0.033670078963041306\n",
      "iteration 2525, dc_loss: 0.03061424195766449, tv_loss: 0.03366386517882347\n",
      "iteration 2526, dc_loss: 0.030596408993005753, tv_loss: 0.03367893770337105\n",
      "iteration 2527, dc_loss: 0.030599210411310196, tv_loss: 0.033661749213933945\n",
      "iteration 2528, dc_loss: 0.030595935881137848, tv_loss: 0.03365764394402504\n",
      "iteration 2529, dc_loss: 0.030582115054130554, tv_loss: 0.03366854041814804\n",
      "iteration 2530, dc_loss: 0.030584292486310005, tv_loss: 0.03366924822330475\n",
      "iteration 2531, dc_loss: 0.030579401180148125, tv_loss: 0.033667076379060745\n",
      "iteration 2532, dc_loss: 0.03056570515036583, tv_loss: 0.03367128223180771\n",
      "iteration 2533, dc_loss: 0.03055734932422638, tv_loss: 0.033674243837594986\n",
      "iteration 2534, dc_loss: 0.030556755140423775, tv_loss: 0.033658936619758606\n",
      "iteration 2535, dc_loss: 0.03055734746158123, tv_loss: 0.03366130217909813\n",
      "iteration 2536, dc_loss: 0.030545461922883987, tv_loss: 0.0336742140352726\n",
      "iteration 2537, dc_loss: 0.03054247796535492, tv_loss: 0.033669136464595795\n",
      "iteration 2538, dc_loss: 0.030529430136084557, tv_loss: 0.03367067128419876\n",
      "iteration 2539, dc_loss: 0.0305319856852293, tv_loss: 0.03365890309214592\n",
      "iteration 2540, dc_loss: 0.030521301552653313, tv_loss: 0.033663567155599594\n",
      "iteration 2541, dc_loss: 0.03051167167723179, tv_loss: 0.033669017255306244\n",
      "iteration 2542, dc_loss: 0.03051338903605938, tv_loss: 0.03366042673587799\n",
      "iteration 2543, dc_loss: 0.03050825372338295, tv_loss: 0.03365768492221832\n",
      "iteration 2544, dc_loss: 0.03049778938293457, tv_loss: 0.033663615584373474\n",
      "iteration 2545, dc_loss: 0.030492158606648445, tv_loss: 0.033663153648376465\n",
      "iteration 2546, dc_loss: 0.03049251064658165, tv_loss: 0.03366732597351074\n",
      "iteration 2547, dc_loss: 0.03048732690513134, tv_loss: 0.03368527442216873\n",
      "iteration 2548, dc_loss: 0.030477160587906837, tv_loss: 0.03368953615427017\n",
      "iteration 2549, dc_loss: 0.030477356165647507, tv_loss: 0.03367018699645996\n",
      "iteration 2550, dc_loss: 0.030468668788671494, tv_loss: 0.033670518547296524\n",
      "iteration 2551, dc_loss: 0.030475519597530365, tv_loss: 0.03366504982113838\n",
      "iteration 2552, dc_loss: 0.030456526204943657, tv_loss: 0.03367607295513153\n",
      "iteration 2553, dc_loss: 0.03045583888888359, tv_loss: 0.03366571292281151\n",
      "iteration 2554, dc_loss: 0.030451608821749687, tv_loss: 0.033664070069789886\n",
      "iteration 2555, dc_loss: 0.03043181821703911, tv_loss: 0.033671602606773376\n",
      "iteration 2556, dc_loss: 0.03043483942747116, tv_loss: 0.03366154059767723\n",
      "iteration 2557, dc_loss: 0.030421782284975052, tv_loss: 0.033667437732219696\n",
      "iteration 2558, dc_loss: 0.03042963147163391, tv_loss: 0.03366298973560333\n",
      "iteration 2559, dc_loss: 0.030411962419748306, tv_loss: 0.03367949277162552\n",
      "iteration 2560, dc_loss: 0.030423250049352646, tv_loss: 0.03367360308766365\n",
      "iteration 2561, dc_loss: 0.030406739562749863, tv_loss: 0.03368388116359711\n",
      "iteration 2562, dc_loss: 0.030407706275582314, tv_loss: 0.033680565655231476\n",
      "iteration 2563, dc_loss: 0.030390389263629913, tv_loss: 0.03368096426129341\n",
      "iteration 2564, dc_loss: 0.030401883646845818, tv_loss: 0.033664289861917496\n",
      "iteration 2565, dc_loss: 0.030381683260202408, tv_loss: 0.03367310017347336\n",
      "iteration 2566, dc_loss: 0.03038128651678562, tv_loss: 0.0336664542555809\n",
      "iteration 2567, dc_loss: 0.030365929007530212, tv_loss: 0.033685293048620224\n",
      "iteration 2568, dc_loss: 0.030372725799679756, tv_loss: 0.03367089852690697\n",
      "iteration 2569, dc_loss: 0.030357351526618004, tv_loss: 0.03368306905031204\n",
      "iteration 2570, dc_loss: 0.030352197587490082, tv_loss: 0.033676162362098694\n",
      "iteration 2571, dc_loss: 0.03035740554332733, tv_loss: 0.03366747498512268\n",
      "iteration 2572, dc_loss: 0.030341466888785362, tv_loss: 0.033678267151117325\n",
      "iteration 2573, dc_loss: 0.03033914975821972, tv_loss: 0.03368533030152321\n",
      "iteration 2574, dc_loss: 0.0303238146007061, tv_loss: 0.033678196370601654\n",
      "iteration 2575, dc_loss: 0.030323417857289314, tv_loss: 0.03367462381720543\n",
      "iteration 2576, dc_loss: 0.03031899593770504, tv_loss: 0.033671487122774124\n",
      "iteration 2577, dc_loss: 0.030299708247184753, tv_loss: 0.033687129616737366\n",
      "iteration 2578, dc_loss: 0.03030554950237274, tv_loss: 0.0336773656308651\n",
      "iteration 2579, dc_loss: 0.030309230089187622, tv_loss: 0.033678509294986725\n",
      "iteration 2580, dc_loss: 0.030296023935079575, tv_loss: 0.033675432205200195\n",
      "iteration 2581, dc_loss: 0.030280841514468193, tv_loss: 0.033682458102703094\n",
      "iteration 2582, dc_loss: 0.030292119830846786, tv_loss: 0.03367237374186516\n",
      "iteration 2583, dc_loss: 0.030283140018582344, tv_loss: 0.03367147594690323\n",
      "iteration 2584, dc_loss: 0.030264969915151596, tv_loss: 0.033681612461805344\n",
      "iteration 2585, dc_loss: 0.030264459550380707, tv_loss: 0.033678505569696426\n",
      "iteration 2586, dc_loss: 0.030254006385803223, tv_loss: 0.033686112612485886\n",
      "iteration 2587, dc_loss: 0.03026113659143448, tv_loss: 0.0336592011153698\n",
      "iteration 2588, dc_loss: 0.030247177928686142, tv_loss: 0.03367757797241211\n",
      "iteration 2589, dc_loss: 0.03024265728890896, tv_loss: 0.033682286739349365\n",
      "iteration 2590, dc_loss: 0.030238257721066475, tv_loss: 0.03367256000638008\n",
      "iteration 2591, dc_loss: 0.03023422695696354, tv_loss: 0.03367258235812187\n",
      "iteration 2592, dc_loss: 0.030220383778214455, tv_loss: 0.03368688002228737\n",
      "iteration 2593, dc_loss: 0.0302252359688282, tv_loss: 0.03367407247424126\n",
      "iteration 2594, dc_loss: 0.030216161161661148, tv_loss: 0.03367689624428749\n",
      "iteration 2595, dc_loss: 0.030217036604881287, tv_loss: 0.033683959394693375\n",
      "iteration 2596, dc_loss: 0.030201029032468796, tv_loss: 0.03368352726101875\n",
      "iteration 2597, dc_loss: 0.030202379450201988, tv_loss: 0.03367183730006218\n",
      "iteration 2598, dc_loss: 0.030179128050804138, tv_loss: 0.03369715437293053\n",
      "iteration 2599, dc_loss: 0.030198557302355766, tv_loss: 0.033665791153907776\n",
      "iteration 2600, dc_loss: 0.030186466872692108, tv_loss: 0.033671699464321136\n",
      "iteration 2601, dc_loss: 0.030178451910614967, tv_loss: 0.033676426857709885\n",
      "iteration 2602, dc_loss: 0.030170070007443428, tv_loss: 0.03369338810443878\n",
      "iteration 2603, dc_loss: 0.030186358839273453, tv_loss: 0.03367328643798828\n",
      "iteration 2604, dc_loss: 0.030174944549798965, tv_loss: 0.033683862537145615\n",
      "iteration 2605, dc_loss: 0.030167853459715843, tv_loss: 0.03369126841425896\n",
      "iteration 2606, dc_loss: 0.03016408160328865, tv_loss: 0.03368832916021347\n",
      "iteration 2607, dc_loss: 0.030161114409565926, tv_loss: 0.033677004277706146\n",
      "iteration 2608, dc_loss: 0.03014497272670269, tv_loss: 0.03368544206023216\n",
      "iteration 2609, dc_loss: 0.03014272451400757, tv_loss: 0.03366870433092117\n",
      "iteration 2610, dc_loss: 0.030129477381706238, tv_loss: 0.033680256456136703\n",
      "iteration 2611, dc_loss: 0.0301267821341753, tv_loss: 0.033676519989967346\n",
      "iteration 2612, dc_loss: 0.030115846544504166, tv_loss: 0.03368091210722923\n",
      "iteration 2613, dc_loss: 0.03012058138847351, tv_loss: 0.03367013856768608\n",
      "iteration 2614, dc_loss: 0.03010002337396145, tv_loss: 0.03369356319308281\n",
      "iteration 2615, dc_loss: 0.03009425662457943, tv_loss: 0.03370201587677002\n",
      "iteration 2616, dc_loss: 0.030100157484412193, tv_loss: 0.03367681801319122\n",
      "iteration 2617, dc_loss: 0.0300875436514616, tv_loss: 0.03368017077445984\n",
      "iteration 2618, dc_loss: 0.030072808265686035, tv_loss: 0.033686600625514984\n",
      "iteration 2619, dc_loss: 0.030080066993832588, tv_loss: 0.03367852047085762\n",
      "iteration 2620, dc_loss: 0.03007587045431137, tv_loss: 0.03367675840854645\n",
      "iteration 2621, dc_loss: 0.030070390552282333, tv_loss: 0.033684853464365005\n",
      "iteration 2622, dc_loss: 0.030065959319472313, tv_loss: 0.03369308263063431\n",
      "iteration 2623, dc_loss: 0.030054863542318344, tv_loss: 0.03370184078812599\n",
      "iteration 2624, dc_loss: 0.03005620837211609, tv_loss: 0.03368189558386803\n",
      "iteration 2625, dc_loss: 0.030044913291931152, tv_loss: 0.0336838960647583\n",
      "iteration 2626, dc_loss: 0.03004271723330021, tv_loss: 0.03367689624428749\n",
      "iteration 2627, dc_loss: 0.030016951262950897, tv_loss: 0.033705566078424454\n",
      "iteration 2628, dc_loss: 0.030037304386496544, tv_loss: 0.03369327634572983\n",
      "iteration 2629, dc_loss: 0.030018702149391174, tv_loss: 0.03370679169893265\n",
      "iteration 2630, dc_loss: 0.03001714125275612, tv_loss: 0.033686358481645584\n",
      "iteration 2631, dc_loss: 0.03001323528587818, tv_loss: 0.03368252143263817\n",
      "iteration 2632, dc_loss: 0.029997535049915314, tv_loss: 0.033695872873067856\n",
      "iteration 2633, dc_loss: 0.030010811984539032, tv_loss: 0.033685289323329926\n",
      "iteration 2634, dc_loss: 0.02999548427760601, tv_loss: 0.033692240715026855\n",
      "iteration 2635, dc_loss: 0.029989713802933693, tv_loss: 0.03368525207042694\n",
      "iteration 2636, dc_loss: 0.029986079782247543, tv_loss: 0.03368743509054184\n",
      "iteration 2637, dc_loss: 0.029989033937454224, tv_loss: 0.03367464616894722\n",
      "iteration 2638, dc_loss: 0.029966915026307106, tv_loss: 0.033688221126794815\n",
      "iteration 2639, dc_loss: 0.02996995858848095, tv_loss: 0.03369073569774628\n",
      "iteration 2640, dc_loss: 0.029975291341543198, tv_loss: 0.03369235619902611\n",
      "iteration 2641, dc_loss: 0.029961807653307915, tv_loss: 0.033704329282045364\n",
      "iteration 2642, dc_loss: 0.029958385974168777, tv_loss: 0.03369172662496567\n",
      "iteration 2643, dc_loss: 0.029959069564938545, tv_loss: 0.03368549793958664\n",
      "iteration 2644, dc_loss: 0.029950154945254326, tv_loss: 0.033688005059957504\n",
      "iteration 2645, dc_loss: 0.0299522764980793, tv_loss: 0.03369433060288429\n",
      "iteration 2646, dc_loss: 0.029947396367788315, tv_loss: 0.03369177505373955\n",
      "iteration 2647, dc_loss: 0.029940541833639145, tv_loss: 0.033696625381708145\n",
      "iteration 2648, dc_loss: 0.029934324324131012, tv_loss: 0.033694956451654434\n",
      "iteration 2649, dc_loss: 0.029927855357527733, tv_loss: 0.03368588164448738\n",
      "iteration 2650, dc_loss: 0.029912041500210762, tv_loss: 0.03369319811463356\n",
      "iteration 2651, dc_loss: 0.029911817982792854, tv_loss: 0.033682435750961304\n",
      "iteration 2652, dc_loss: 0.029894113540649414, tv_loss: 0.03370041027665138\n",
      "iteration 2653, dc_loss: 0.029903311282396317, tv_loss: 0.033682409673929214\n",
      "iteration 2654, dc_loss: 0.02988233044743538, tv_loss: 0.0337081179022789\n",
      "iteration 2655, dc_loss: 0.029888635501265526, tv_loss: 0.033687982708215714\n",
      "iteration 2656, dc_loss: 0.029867151752114296, tv_loss: 0.03369816765189171\n",
      "iteration 2657, dc_loss: 0.02987627685070038, tv_loss: 0.03367741033434868\n",
      "iteration 2658, dc_loss: 0.029853779822587967, tv_loss: 0.03369900956749916\n",
      "iteration 2659, dc_loss: 0.029866261407732964, tv_loss: 0.03367859870195389\n",
      "iteration 2660, dc_loss: 0.029837632551789284, tv_loss: 0.03370064124464989\n",
      "iteration 2661, dc_loss: 0.029859459027647972, tv_loss: 0.033677004277706146\n",
      "iteration 2662, dc_loss: 0.02984405867755413, tv_loss: 0.033689480274915695\n",
      "iteration 2663, dc_loss: 0.029840592294931412, tv_loss: 0.03368563577532768\n",
      "iteration 2664, dc_loss: 0.029825592413544655, tv_loss: 0.03370574116706848\n",
      "iteration 2665, dc_loss: 0.029830293729901314, tv_loss: 0.03369482606649399\n",
      "iteration 2666, dc_loss: 0.0298150647431612, tv_loss: 0.033707309514284134\n",
      "iteration 2667, dc_loss: 0.029810236766934395, tv_loss: 0.03369545936584473\n",
      "iteration 2668, dc_loss: 0.029812995344400406, tv_loss: 0.03369204327464104\n",
      "iteration 2669, dc_loss: 0.029799314215779305, tv_loss: 0.033692993223667145\n",
      "iteration 2670, dc_loss: 0.02979937382042408, tv_loss: 0.033691808581352234\n",
      "iteration 2671, dc_loss: 0.02979418635368347, tv_loss: 0.03369800001382828\n",
      "iteration 2672, dc_loss: 0.029809406027197838, tv_loss: 0.0336780920624733\n",
      "iteration 2673, dc_loss: 0.029776066541671753, tv_loss: 0.03371117636561394\n",
      "iteration 2674, dc_loss: 0.029795611277222633, tv_loss: 0.03369119390845299\n",
      "iteration 2675, dc_loss: 0.029795151203870773, tv_loss: 0.033687710762023926\n",
      "iteration 2676, dc_loss: 0.029801934957504272, tv_loss: 0.03368806838989258\n",
      "iteration 2677, dc_loss: 0.029774880036711693, tv_loss: 0.033709220588207245\n",
      "iteration 2678, dc_loss: 0.029819507151842117, tv_loss: 0.033668916672468185\n",
      "iteration 2679, dc_loss: 0.029772598296403885, tv_loss: 0.03370807319879532\n",
      "iteration 2680, dc_loss: 0.029790978878736496, tv_loss: 0.03369331359863281\n",
      "iteration 2681, dc_loss: 0.02975906990468502, tv_loss: 0.03371448442339897\n",
      "iteration 2682, dc_loss: 0.029774712398648262, tv_loss: 0.033678989857435226\n",
      "iteration 2683, dc_loss: 0.029723847284913063, tv_loss: 0.03370705246925354\n",
      "iteration 2684, dc_loss: 0.02974451705813408, tv_loss: 0.03367656096816063\n",
      "iteration 2685, dc_loss: 0.029711425304412842, tv_loss: 0.03369752690196037\n",
      "iteration 2686, dc_loss: 0.029722535982728004, tv_loss: 0.03368307650089264\n",
      "iteration 2687, dc_loss: 0.02969834767282009, tv_loss: 0.0337083525955677\n",
      "iteration 2688, dc_loss: 0.029711393639445305, tv_loss: 0.03370271995663643\n",
      "iteration 2689, dc_loss: 0.02971334382891655, tv_loss: 0.033708468079566956\n",
      "iteration 2690, dc_loss: 0.02970403991639614, tv_loss: 0.03370717540383339\n",
      "iteration 2691, dc_loss: 0.029695279896259308, tv_loss: 0.03369860723614693\n",
      "iteration 2692, dc_loss: 0.02969292551279068, tv_loss: 0.033694423735141754\n",
      "iteration 2693, dc_loss: 0.029687007889151573, tv_loss: 0.03369938209652901\n",
      "iteration 2694, dc_loss: 0.029680004343390465, tv_loss: 0.0337040051817894\n",
      "iteration 2695, dc_loss: 0.02967524714767933, tv_loss: 0.03370730206370354\n",
      "iteration 2696, dc_loss: 0.029677724465727806, tv_loss: 0.03369391709566116\n",
      "iteration 2697, dc_loss: 0.029658956453204155, tv_loss: 0.03370613232254982\n",
      "iteration 2698, dc_loss: 0.029662881046533585, tv_loss: 0.033689819276332855\n",
      "iteration 2699, dc_loss: 0.029651805758476257, tv_loss: 0.03369372710585594\n",
      "iteration 2700, dc_loss: 0.02964864857494831, tv_loss: 0.033681392669677734\n",
      "iteration 2701, dc_loss: 0.029627209529280663, tv_loss: 0.033699650317430496\n",
      "iteration 2702, dc_loss: 0.029632462188601494, tv_loss: 0.03369330242276192\n",
      "iteration 2703, dc_loss: 0.029625343158841133, tv_loss: 0.033699825406074524\n",
      "iteration 2704, dc_loss: 0.029614929109811783, tv_loss: 0.033715326339006424\n",
      "iteration 2705, dc_loss: 0.029613381251692772, tv_loss: 0.03370992839336395\n",
      "iteration 2706, dc_loss: 0.029617473483085632, tv_loss: 0.033689334988594055\n",
      "iteration 2707, dc_loss: 0.029597816988825798, tv_loss: 0.03370559215545654\n",
      "iteration 2708, dc_loss: 0.029601505026221275, tv_loss: 0.03370494395494461\n",
      "iteration 2709, dc_loss: 0.029596742242574692, tv_loss: 0.033697180449962616\n",
      "iteration 2710, dc_loss: 0.029589684680104256, tv_loss: 0.033700719475746155\n",
      "iteration 2711, dc_loss: 0.02956177107989788, tv_loss: 0.03372780233621597\n",
      "iteration 2712, dc_loss: 0.029594359919428825, tv_loss: 0.03367917612195015\n",
      "iteration 2713, dc_loss: 0.029568104073405266, tv_loss: 0.033705975860357285\n",
      "iteration 2714, dc_loss: 0.029585162177681923, tv_loss: 0.03369010612368584\n",
      "iteration 2715, dc_loss: 0.029547741636633873, tv_loss: 0.03372634947299957\n",
      "iteration 2716, dc_loss: 0.02960045635700226, tv_loss: 0.03366737812757492\n",
      "iteration 2717, dc_loss: 0.02953179180622101, tv_loss: 0.03373463451862335\n",
      "iteration 2718, dc_loss: 0.029589135199785233, tv_loss: 0.03367173299193382\n",
      "iteration 2719, dc_loss: 0.029543699696660042, tv_loss: 0.03372783586382866\n",
      "iteration 2720, dc_loss: 0.029591523110866547, tv_loss: 0.0336800180375576\n",
      "iteration 2721, dc_loss: 0.029535723850131035, tv_loss: 0.03372864052653313\n",
      "iteration 2722, dc_loss: 0.029608603566884995, tv_loss: 0.03365955874323845\n",
      "iteration 2723, dc_loss: 0.029518427327275276, tv_loss: 0.033742647618055344\n",
      "iteration 2724, dc_loss: 0.029571261256933212, tv_loss: 0.0336780920624733\n",
      "iteration 2725, dc_loss: 0.029513543471693993, tv_loss: 0.03371358662843704\n",
      "iteration 2726, dc_loss: 0.029534809291362762, tv_loss: 0.03368847072124481\n",
      "iteration 2727, dc_loss: 0.029500817880034447, tv_loss: 0.03371540084481239\n",
      "iteration 2728, dc_loss: 0.029506536200642586, tv_loss: 0.033695656806230545\n",
      "iteration 2729, dc_loss: 0.029493344947695732, tv_loss: 0.0337018258869648\n",
      "iteration 2730, dc_loss: 0.029495183378458023, tv_loss: 0.03369981050491333\n",
      "iteration 2731, dc_loss: 0.029499230906367302, tv_loss: 0.033699698746204376\n",
      "iteration 2732, dc_loss: 0.02949037216603756, tv_loss: 0.03370228037238121\n",
      "iteration 2733, dc_loss: 0.029498929157853127, tv_loss: 0.033695973455905914\n",
      "iteration 2734, dc_loss: 0.029490552842617035, tv_loss: 0.03370973840355873\n",
      "iteration 2735, dc_loss: 0.02949804998934269, tv_loss: 0.03371018171310425\n",
      "iteration 2736, dc_loss: 0.029493892565369606, tv_loss: 0.03369525074958801\n",
      "iteration 2737, dc_loss: 0.029454927891492844, tv_loss: 0.03373531997203827\n",
      "iteration 2738, dc_loss: 0.02947673760354519, tv_loss: 0.03369079902768135\n",
      "iteration 2739, dc_loss: 0.02943393960595131, tv_loss: 0.03371351584792137\n",
      "iteration 2740, dc_loss: 0.02946072816848755, tv_loss: 0.03368464484810829\n",
      "iteration 2741, dc_loss: 0.029432719573378563, tv_loss: 0.03372451290488243\n",
      "iteration 2742, dc_loss: 0.029459280893206596, tv_loss: 0.03370261192321777\n",
      "iteration 2743, dc_loss: 0.029427241533994675, tv_loss: 0.03374366834759712\n",
      "iteration 2744, dc_loss: 0.029470134526491165, tv_loss: 0.033672988414764404\n",
      "iteration 2745, dc_loss: 0.02941027469933033, tv_loss: 0.03371518477797508\n",
      "iteration 2746, dc_loss: 0.02942560985684395, tv_loss: 0.03370097279548645\n",
      "iteration 2747, dc_loss: 0.029398087412118912, tv_loss: 0.03373244032263756\n",
      "iteration 2748, dc_loss: 0.029436280950903893, tv_loss: 0.03368474543094635\n",
      "iteration 2749, dc_loss: 0.029374118894338608, tv_loss: 0.03374074026942253\n",
      "iteration 2750, dc_loss: 0.029408540576696396, tv_loss: 0.033688005059957504\n",
      "iteration 2751, dc_loss: 0.029380660504102707, tv_loss: 0.03370565548539162\n",
      "iteration 2752, dc_loss: 0.02937786653637886, tv_loss: 0.03369763121008873\n",
      "iteration 2753, dc_loss: 0.02936774119734764, tv_loss: 0.03371017426252365\n",
      "iteration 2754, dc_loss: 0.02937246859073639, tv_loss: 0.03370712324976921\n",
      "iteration 2755, dc_loss: 0.029362326487898827, tv_loss: 0.033705782145261765\n",
      "iteration 2756, dc_loss: 0.02935202419757843, tv_loss: 0.03371269255876541\n",
      "iteration 2757, dc_loss: 0.029365357011556625, tv_loss: 0.03368758037686348\n",
      "iteration 2758, dc_loss: 0.029329411685466766, tv_loss: 0.03371584787964821\n",
      "iteration 2759, dc_loss: 0.029346533119678497, tv_loss: 0.03369801118969917\n",
      "iteration 2760, dc_loss: 0.029330920428037643, tv_loss: 0.03370710834860802\n",
      "iteration 2761, dc_loss: 0.029351871460676193, tv_loss: 0.033676620572805405\n",
      "iteration 2762, dc_loss: 0.029299845919013023, tv_loss: 0.03373095393180847\n",
      "iteration 2763, dc_loss: 0.029352158308029175, tv_loss: 0.03366996347904205\n",
      "iteration 2764, dc_loss: 0.02929098904132843, tv_loss: 0.033729057759046555\n",
      "iteration 2765, dc_loss: 0.029349589720368385, tv_loss: 0.03368457406759262\n",
      "iteration 2766, dc_loss: 0.029277879744768143, tv_loss: 0.03376784920692444\n",
      "iteration 2767, dc_loss: 0.0293685682117939, tv_loss: 0.03367030248045921\n",
      "iteration 2768, dc_loss: 0.029266418889164925, tv_loss: 0.033755626529455185\n",
      "iteration 2769, dc_loss: 0.02935011312365532, tv_loss: 0.033675163984298706\n",
      "iteration 2770, dc_loss: 0.029273290187120438, tv_loss: 0.03375820815563202\n",
      "iteration 2771, dc_loss: 0.029341183602809906, tv_loss: 0.03368103504180908\n",
      "iteration 2772, dc_loss: 0.029249217361211777, tv_loss: 0.03375530242919922\n",
      "iteration 2773, dc_loss: 0.029319753870368004, tv_loss: 0.03367244079709053\n",
      "iteration 2774, dc_loss: 0.02925781160593033, tv_loss: 0.03373430296778679\n",
      "iteration 2775, dc_loss: 0.029279248788952827, tv_loss: 0.03371182084083557\n",
      "iteration 2776, dc_loss: 0.029270602390170097, tv_loss: 0.03372468054294586\n",
      "iteration 2777, dc_loss: 0.029255926609039307, tv_loss: 0.03372885659337044\n",
      "iteration 2778, dc_loss: 0.029259929433465004, tv_loss: 0.03370879217982292\n",
      "iteration 2779, dc_loss: 0.029242994263768196, tv_loss: 0.03372230380773544\n",
      "iteration 2780, dc_loss: 0.029288126155734062, tv_loss: 0.03367882966995239\n",
      "iteration 2781, dc_loss: 0.02922901138663292, tv_loss: 0.03375732898712158\n",
      "iteration 2782, dc_loss: 0.029294947162270546, tv_loss: 0.03368647024035454\n",
      "iteration 2783, dc_loss: 0.02924880012869835, tv_loss: 0.033742573112249374\n",
      "iteration 2784, dc_loss: 0.029295993968844414, tv_loss: 0.033675871789455414\n",
      "iteration 2785, dc_loss: 0.02920065075159073, tv_loss: 0.033763282001018524\n",
      "iteration 2786, dc_loss: 0.0292750783264637, tv_loss: 0.03367280960083008\n",
      "iteration 2787, dc_loss: 0.029213927686214447, tv_loss: 0.033731576055288315\n",
      "iteration 2788, dc_loss: 0.02923392318189144, tv_loss: 0.033702295273542404\n",
      "iteration 2789, dc_loss: 0.029195396229624748, tv_loss: 0.033731963485479355\n",
      "iteration 2790, dc_loss: 0.02921268530189991, tv_loss: 0.03370528295636177\n",
      "iteration 2791, dc_loss: 0.02917838655412197, tv_loss: 0.0337209478020668\n",
      "iteration 2792, dc_loss: 0.029162663966417313, tv_loss: 0.03372688591480255\n",
      "iteration 2793, dc_loss: 0.02919659949839115, tv_loss: 0.03369556739926338\n",
      "iteration 2794, dc_loss: 0.029166867956519127, tv_loss: 0.03372307866811752\n",
      "iteration 2795, dc_loss: 0.029190178960561752, tv_loss: 0.033702727407217026\n",
      "iteration 2796, dc_loss: 0.029151013121008873, tv_loss: 0.03373349830508232\n",
      "iteration 2797, dc_loss: 0.029187213629484177, tv_loss: 0.0336935818195343\n",
      "iteration 2798, dc_loss: 0.029134519398212433, tv_loss: 0.033744316548109055\n",
      "iteration 2799, dc_loss: 0.029169753193855286, tv_loss: 0.033704470843076706\n",
      "iteration 2800, dc_loss: 0.02914131060242653, tv_loss: 0.03372626751661301\n",
      "iteration 2801, dc_loss: 0.029158079996705055, tv_loss: 0.03369644656777382\n",
      "iteration 2802, dc_loss: 0.029115213081240654, tv_loss: 0.033718857914209366\n",
      "iteration 2803, dc_loss: 0.02911454439163208, tv_loss: 0.033713940531015396\n",
      "iteration 2804, dc_loss: 0.029135189950466156, tv_loss: 0.03369645029306412\n",
      "iteration 2805, dc_loss: 0.029101114720106125, tv_loss: 0.033723317086696625\n",
      "iteration 2806, dc_loss: 0.02910797670483589, tv_loss: 0.0337042436003685\n",
      "iteration 2807, dc_loss: 0.029122712090611458, tv_loss: 0.03368840739130974\n",
      "iteration 2808, dc_loss: 0.02909066528081894, tv_loss: 0.033717501908540726\n",
      "iteration 2809, dc_loss: 0.02909756824374199, tv_loss: 0.03370116278529167\n",
      "iteration 2810, dc_loss: 0.029101891443133354, tv_loss: 0.033693406730890274\n",
      "iteration 2811, dc_loss: 0.029082244262099266, tv_loss: 0.0337153896689415\n",
      "iteration 2812, dc_loss: 0.02908608503639698, tv_loss: 0.033701393753290176\n",
      "iteration 2813, dc_loss: 0.02908208966255188, tv_loss: 0.03370300680398941\n",
      "iteration 2814, dc_loss: 0.029069431126117706, tv_loss: 0.03371371701359749\n",
      "iteration 2815, dc_loss: 0.029075201600790024, tv_loss: 0.033697500824928284\n",
      "iteration 2816, dc_loss: 0.029071830213069916, tv_loss: 0.033697571605443954\n",
      "iteration 2817, dc_loss: 0.02905866876244545, tv_loss: 0.03370276466012001\n",
      "iteration 2818, dc_loss: 0.029067199677228928, tv_loss: 0.03368944674730301\n",
      "iteration 2819, dc_loss: 0.029051274061203003, tv_loss: 0.03370336815714836\n",
      "iteration 2820, dc_loss: 0.029048362746834755, tv_loss: 0.03370791673660278\n",
      "iteration 2821, dc_loss: 0.02905779704451561, tv_loss: 0.03371044993400574\n",
      "iteration 2822, dc_loss: 0.029040662571787834, tv_loss: 0.0337211973965168\n",
      "iteration 2823, dc_loss: 0.029035069048404694, tv_loss: 0.03371391445398331\n",
      "iteration 2824, dc_loss: 0.029041720554232597, tv_loss: 0.033697567880153656\n",
      "iteration 2825, dc_loss: 0.029029522091150284, tv_loss: 0.03370671346783638\n",
      "iteration 2826, dc_loss: 0.029028447344899178, tv_loss: 0.03370172530412674\n",
      "iteration 2827, dc_loss: 0.029021086171269417, tv_loss: 0.03370409458875656\n",
      "iteration 2828, dc_loss: 0.029018867760896683, tv_loss: 0.03370525315403938\n",
      "iteration 2829, dc_loss: 0.029028108343482018, tv_loss: 0.03369816020131111\n",
      "iteration 2830, dc_loss: 0.02900836057960987, tv_loss: 0.03372013941407204\n",
      "iteration 2831, dc_loss: 0.02899869531393051, tv_loss: 0.03372234106063843\n",
      "iteration 2832, dc_loss: 0.029011821374297142, tv_loss: 0.03370177745819092\n",
      "iteration 2833, dc_loss: 0.029003236442804337, tv_loss: 0.03369958698749542\n",
      "iteration 2834, dc_loss: 0.028990136459469795, tv_loss: 0.033714521676301956\n",
      "iteration 2835, dc_loss: 0.02899434231221676, tv_loss: 0.03370661288499832\n",
      "iteration 2836, dc_loss: 0.028994448482990265, tv_loss: 0.03370239585638046\n",
      "iteration 2837, dc_loss: 0.02898293361067772, tv_loss: 0.03371395170688629\n",
      "iteration 2838, dc_loss: 0.02897786535322666, tv_loss: 0.033711593598127365\n",
      "iteration 2839, dc_loss: 0.028974713757634163, tv_loss: 0.03371121361851692\n",
      "iteration 2840, dc_loss: 0.028984995558857918, tv_loss: 0.03369283303618431\n",
      "iteration 2841, dc_loss: 0.02896951697766781, tv_loss: 0.03370685875415802\n",
      "iteration 2842, dc_loss: 0.028955182060599327, tv_loss: 0.03371691703796387\n",
      "iteration 2843, dc_loss: 0.028963904827833176, tv_loss: 0.0337008573114872\n",
      "iteration 2844, dc_loss: 0.028964098542928696, tv_loss: 0.03370511531829834\n",
      "iteration 2845, dc_loss: 0.02895115315914154, tv_loss: 0.03370997682213783\n",
      "iteration 2846, dc_loss: 0.028946734964847565, tv_loss: 0.033711399883031845\n",
      "iteration 2847, dc_loss: 0.028949476778507233, tv_loss: 0.03370586410164833\n",
      "iteration 2848, dc_loss: 0.02895362116396427, tv_loss: 0.03370242565870285\n",
      "iteration 2849, dc_loss: 0.028934475034475327, tv_loss: 0.033709824085235596\n",
      "iteration 2850, dc_loss: 0.028925130143761635, tv_loss: 0.03371971473097801\n",
      "iteration 2851, dc_loss: 0.02893761359155178, tv_loss: 0.03369686380028725\n",
      "iteration 2852, dc_loss: 0.028931481763720512, tv_loss: 0.03370777145028114\n",
      "iteration 2853, dc_loss: 0.028916077688336372, tv_loss: 0.033724136650562286\n",
      "iteration 2854, dc_loss: 0.028920229524374008, tv_loss: 0.033711161464452744\n",
      "iteration 2855, dc_loss: 0.028925156220793724, tv_loss: 0.03370293229818344\n",
      "iteration 2856, dc_loss: 0.028911786153912544, tv_loss: 0.03371389955282211\n",
      "iteration 2857, dc_loss: 0.028899960219860077, tv_loss: 0.033716559410095215\n",
      "iteration 2858, dc_loss: 0.028908317908644676, tv_loss: 0.03370840847492218\n",
      "iteration 2859, dc_loss: 0.028908951207995415, tv_loss: 0.0337052159011364\n",
      "iteration 2860, dc_loss: 0.028895532712340355, tv_loss: 0.03370767831802368\n",
      "iteration 2861, dc_loss: 0.028890086337924004, tv_loss: 0.033716779202222824\n",
      "iteration 2862, dc_loss: 0.028891028836369514, tv_loss: 0.03370616212487221\n",
      "iteration 2863, dc_loss: 0.028886405751109123, tv_loss: 0.033711545169353485\n",
      "iteration 2864, dc_loss: 0.02887558378279209, tv_loss: 0.03370894864201546\n",
      "iteration 2865, dc_loss: 0.02887885831296444, tv_loss: 0.03371229022741318\n",
      "iteration 2866, dc_loss: 0.028882918879389763, tv_loss: 0.03370806202292442\n",
      "iteration 2867, dc_loss: 0.028870638459920883, tv_loss: 0.03371522203087807\n",
      "iteration 2868, dc_loss: 0.028861993923783302, tv_loss: 0.03372277319431305\n",
      "iteration 2869, dc_loss: 0.02886364422738552, tv_loss: 0.033710915595293045\n",
      "iteration 2870, dc_loss: 0.028865445405244827, tv_loss: 0.033704038709402084\n",
      "iteration 2871, dc_loss: 0.028850432485342026, tv_loss: 0.03372437506914139\n",
      "iteration 2872, dc_loss: 0.02884676121175289, tv_loss: 0.03371795639395714\n",
      "iteration 2873, dc_loss: 0.028852984309196472, tv_loss: 0.033708736300468445\n",
      "iteration 2874, dc_loss: 0.028846854344010353, tv_loss: 0.033714208751916885\n",
      "iteration 2875, dc_loss: 0.028840811923146248, tv_loss: 0.03371398150920868\n",
      "iteration 2876, dc_loss: 0.028830282390117645, tv_loss: 0.03372303768992424\n",
      "iteration 2877, dc_loss: 0.02882845513522625, tv_loss: 0.03372449055314064\n",
      "iteration 2878, dc_loss: 0.028838716447353363, tv_loss: 0.03370710834860802\n",
      "iteration 2879, dc_loss: 0.02882957085967064, tv_loss: 0.03370676562190056\n",
      "iteration 2880, dc_loss: 0.02881082147359848, tv_loss: 0.033729493618011475\n",
      "iteration 2881, dc_loss: 0.028819454833865166, tv_loss: 0.03371913731098175\n",
      "iteration 2882, dc_loss: 0.028817858546972275, tv_loss: 0.03370515629649162\n",
      "iteration 2883, dc_loss: 0.028804754838347435, tv_loss: 0.03372887894511223\n",
      "iteration 2884, dc_loss: 0.028803661465644836, tv_loss: 0.03371928259730339\n",
      "iteration 2885, dc_loss: 0.028808755800127983, tv_loss: 0.03370467200875282\n",
      "iteration 2886, dc_loss: 0.028800392523407936, tv_loss: 0.0337178073823452\n",
      "iteration 2887, dc_loss: 0.02879733219742775, tv_loss: 0.03371576964855194\n",
      "iteration 2888, dc_loss: 0.028788261115550995, tv_loss: 0.03370916098356247\n",
      "iteration 2889, dc_loss: 0.02878700941801071, tv_loss: 0.03370916470885277\n",
      "iteration 2890, dc_loss: 0.0287895780056715, tv_loss: 0.03371606767177582\n",
      "iteration 2891, dc_loss: 0.028775012120604515, tv_loss: 0.033714067190885544\n",
      "iteration 2892, dc_loss: 0.028773268684744835, tv_loss: 0.03371884673833847\n",
      "iteration 2893, dc_loss: 0.02877732366323471, tv_loss: 0.03371960297226906\n",
      "iteration 2894, dc_loss: 0.028766116127371788, tv_loss: 0.03371209651231766\n",
      "iteration 2895, dc_loss: 0.028758246451616287, tv_loss: 0.0337224118411541\n",
      "iteration 2896, dc_loss: 0.02876829169690609, tv_loss: 0.03371544927358627\n",
      "iteration 2897, dc_loss: 0.028761642053723335, tv_loss: 0.03370678424835205\n",
      "iteration 2898, dc_loss: 0.028750527650117874, tv_loss: 0.03372194245457649\n",
      "iteration 2899, dc_loss: 0.028749171644449234, tv_loss: 0.033720310777425766\n",
      "iteration 2900, dc_loss: 0.02874867245554924, tv_loss: 0.033708009868860245\n",
      "iteration 2901, dc_loss: 0.028739066794514656, tv_loss: 0.03372037783265114\n",
      "iteration 2902, dc_loss: 0.028739312663674355, tv_loss: 0.033719588071107864\n",
      "iteration 2903, dc_loss: 0.028734128922224045, tv_loss: 0.033712275326251984\n",
      "iteration 2904, dc_loss: 0.02873389795422554, tv_loss: 0.03370951488614082\n",
      "iteration 2905, dc_loss: 0.028728777542710304, tv_loss: 0.03371430188417435\n",
      "iteration 2906, dc_loss: 0.028722897171974182, tv_loss: 0.03371323645114899\n",
      "iteration 2907, dc_loss: 0.028719140216708183, tv_loss: 0.03371366485953331\n",
      "iteration 2908, dc_loss: 0.02871984802186489, tv_loss: 0.03372185304760933\n",
      "iteration 2909, dc_loss: 0.028708510100841522, tv_loss: 0.03372777998447418\n",
      "iteration 2910, dc_loss: 0.028704598546028137, tv_loss: 0.03373049572110176\n",
      "iteration 2911, dc_loss: 0.0287153460085392, tv_loss: 0.03370806202292442\n",
      "iteration 2912, dc_loss: 0.02870027720928192, tv_loss: 0.033712130039930344\n",
      "iteration 2913, dc_loss: 0.02868703380227089, tv_loss: 0.03373407945036888\n",
      "iteration 2914, dc_loss: 0.028701262548565865, tv_loss: 0.03372041508555412\n",
      "iteration 2915, dc_loss: 0.02869771420955658, tv_loss: 0.03372558578848839\n",
      "iteration 2916, dc_loss: 0.0286802276968956, tv_loss: 0.03373376652598381\n",
      "iteration 2917, dc_loss: 0.028681546449661255, tv_loss: 0.03371875733137131\n",
      "iteration 2918, dc_loss: 0.028688140213489532, tv_loss: 0.03371328487992287\n",
      "iteration 2919, dc_loss: 0.02866494283080101, tv_loss: 0.03373437747359276\n",
      "iteration 2920, dc_loss: 0.02866755612194538, tv_loss: 0.033721115440130234\n",
      "iteration 2921, dc_loss: 0.028684474527835846, tv_loss: 0.03370630368590355\n",
      "iteration 2922, dc_loss: 0.02866394631564617, tv_loss: 0.03371783718466759\n",
      "iteration 2923, dc_loss: 0.028652844950556755, tv_loss: 0.03372545167803764\n",
      "iteration 2924, dc_loss: 0.02865871600806713, tv_loss: 0.033720169216394424\n",
      "iteration 2925, dc_loss: 0.02865220047533512, tv_loss: 0.033720407634973526\n",
      "iteration 2926, dc_loss: 0.028652893379330635, tv_loss: 0.033719003200531006\n",
      "iteration 2927, dc_loss: 0.028646590188145638, tv_loss: 0.03372000530362129\n",
      "iteration 2928, dc_loss: 0.0286429263651371, tv_loss: 0.03372159227728844\n",
      "iteration 2929, dc_loss: 0.028636686503887177, tv_loss: 0.03372224420309067\n",
      "iteration 2930, dc_loss: 0.02863340452313423, tv_loss: 0.033716507256031036\n",
      "iteration 2931, dc_loss: 0.02863313816487789, tv_loss: 0.03372327610850334\n",
      "iteration 2932, dc_loss: 0.028628546744585037, tv_loss: 0.03371870890259743\n",
      "iteration 2933, dc_loss: 0.02862362004816532, tv_loss: 0.03372844308614731\n",
      "iteration 2934, dc_loss: 0.028614113107323647, tv_loss: 0.03372999280691147\n",
      "iteration 2935, dc_loss: 0.028617870062589645, tv_loss: 0.03371741622686386\n",
      "iteration 2936, dc_loss: 0.02861800789833069, tv_loss: 0.033711936324834824\n",
      "iteration 2937, dc_loss: 0.028609300032258034, tv_loss: 0.033718228340148926\n",
      "iteration 2938, dc_loss: 0.028600914403796196, tv_loss: 0.033714793622493744\n",
      "iteration 2939, dc_loss: 0.028603551909327507, tv_loss: 0.033710777759552\n",
      "iteration 2940, dc_loss: 0.028600094839930534, tv_loss: 0.03371185064315796\n",
      "iteration 2941, dc_loss: 0.02859416976571083, tv_loss: 0.03372202068567276\n",
      "iteration 2942, dc_loss: 0.02859421819448471, tv_loss: 0.03373538330197334\n",
      "iteration 2943, dc_loss: 0.028582673519849777, tv_loss: 0.03374023362994194\n",
      "iteration 2944, dc_loss: 0.028583500534296036, tv_loss: 0.0337214432656765\n",
      "iteration 2945, dc_loss: 0.028583014383912086, tv_loss: 0.033715248107910156\n",
      "iteration 2946, dc_loss: 0.028575854375958443, tv_loss: 0.03372877091169357\n",
      "iteration 2947, dc_loss: 0.028575580567121506, tv_loss: 0.033725012093782425\n",
      "iteration 2948, dc_loss: 0.0285621490329504, tv_loss: 0.033739227801561356\n",
      "iteration 2949, dc_loss: 0.028564225882291794, tv_loss: 0.03372516483068466\n",
      "iteration 2950, dc_loss: 0.028565803542733192, tv_loss: 0.03372209146618843\n",
      "iteration 2951, dc_loss: 0.02856280282139778, tv_loss: 0.03371875733137131\n",
      "iteration 2952, dc_loss: 0.028550218790769577, tv_loss: 0.03372695669531822\n",
      "iteration 2953, dc_loss: 0.028547147288918495, tv_loss: 0.033730048686265945\n",
      "iteration 2954, dc_loss: 0.028551267459988594, tv_loss: 0.0337207093834877\n",
      "iteration 2955, dc_loss: 0.02854391373693943, tv_loss: 0.033724214881658554\n",
      "iteration 2956, dc_loss: 0.02853807993233204, tv_loss: 0.03372574597597122\n",
      "iteration 2957, dc_loss: 0.028536701574921608, tv_loss: 0.03372359275817871\n",
      "iteration 2958, dc_loss: 0.02853192202746868, tv_loss: 0.033725570887327194\n",
      "iteration 2959, dc_loss: 0.02852637507021427, tv_loss: 0.033727142959833145\n",
      "iteration 2960, dc_loss: 0.028529338538646698, tv_loss: 0.03371788188815117\n",
      "iteration 2961, dc_loss: 0.028527431190013885, tv_loss: 0.03371870890259743\n",
      "iteration 2962, dc_loss: 0.028514038771390915, tv_loss: 0.033731162548065186\n",
      "iteration 2963, dc_loss: 0.028510745614767075, tv_loss: 0.033728402107954025\n",
      "iteration 2964, dc_loss: 0.028507616370916367, tv_loss: 0.03373075649142265\n",
      "iteration 2965, dc_loss: 0.028510356321930885, tv_loss: 0.03373052924871445\n",
      "iteration 2966, dc_loss: 0.02851138263940811, tv_loss: 0.03372286632657051\n",
      "iteration 2967, dc_loss: 0.02849966660141945, tv_loss: 0.03372536227107048\n",
      "iteration 2968, dc_loss: 0.028493721038103104, tv_loss: 0.03372780606150627\n",
      "iteration 2969, dc_loss: 0.028494959697127342, tv_loss: 0.03372413292527199\n",
      "iteration 2970, dc_loss: 0.028488636016845703, tv_loss: 0.033727411180734634\n",
      "iteration 2971, dc_loss: 0.028482487425208092, tv_loss: 0.03373919799923897\n",
      "iteration 2972, dc_loss: 0.028481438755989075, tv_loss: 0.03373110294342041\n",
      "iteration 2973, dc_loss: 0.028486158698797226, tv_loss: 0.03371911868453026\n",
      "iteration 2974, dc_loss: 0.028467640280723572, tv_loss: 0.03374010697007179\n",
      "iteration 2975, dc_loss: 0.028476012870669365, tv_loss: 0.0337216779589653\n",
      "iteration 2976, dc_loss: 0.028471166267991066, tv_loss: 0.033724330365657806\n",
      "iteration 2977, dc_loss: 0.028456352651119232, tv_loss: 0.03373531624674797\n",
      "iteration 2978, dc_loss: 0.028465893119573593, tv_loss: 0.03371701017022133\n",
      "iteration 2979, dc_loss: 0.028452368453145027, tv_loss: 0.03372513875365257\n",
      "iteration 2980, dc_loss: 0.028459737077355385, tv_loss: 0.033718183636665344\n",
      "iteration 2981, dc_loss: 0.028450481593608856, tv_loss: 0.0337214320898056\n",
      "iteration 2982, dc_loss: 0.028444522991776466, tv_loss: 0.03372548148036003\n",
      "iteration 2983, dc_loss: 0.028442436829209328, tv_loss: 0.033735331147909164\n",
      "iteration 2984, dc_loss: 0.028443796560168266, tv_loss: 0.03373367339372635\n",
      "iteration 2985, dc_loss: 0.028439577668905258, tv_loss: 0.033729348331689835\n",
      "iteration 2986, dc_loss: 0.0284188911318779, tv_loss: 0.03373827785253525\n",
      "iteration 2987, dc_loss: 0.028436312451958656, tv_loss: 0.03371819853782654\n",
      "iteration 2988, dc_loss: 0.028426222503185272, tv_loss: 0.03373061865568161\n",
      "iteration 2989, dc_loss: 0.02842508815228939, tv_loss: 0.0337357297539711\n",
      "iteration 2990, dc_loss: 0.028414269909262657, tv_loss: 0.033737942576408386\n",
      "iteration 2991, dc_loss: 0.02840639464557171, tv_loss: 0.03373550623655319\n",
      "iteration 2992, dc_loss: 0.028414897620677948, tv_loss: 0.033725522458553314\n",
      "iteration 2993, dc_loss: 0.028408648446202278, tv_loss: 0.03373194485902786\n",
      "iteration 2994, dc_loss: 0.0284110177308321, tv_loss: 0.03373033553361893\n",
      "iteration 2995, dc_loss: 0.028392774984240532, tv_loss: 0.033748067915439606\n",
      "iteration 2996, dc_loss: 0.02840585634112358, tv_loss: 0.03372601047158241\n",
      "iteration 2997, dc_loss: 0.028405869379639626, tv_loss: 0.03371795266866684\n",
      "iteration 2998, dc_loss: 0.028385933488607407, tv_loss: 0.03374454006552696\n",
      "iteration 2999, dc_loss: 0.028390871360898018, tv_loss: 0.03374636173248291\n",
      "iteration 3000, dc_loss: 0.02840282954275608, tv_loss: 0.03372824192047119\n",
      "iteration 3001, dc_loss: 0.028389889746904373, tv_loss: 0.03373672068119049\n",
      "iteration 3002, dc_loss: 0.028393952175974846, tv_loss: 0.03373175114393234\n",
      "iteration 3003, dc_loss: 0.028401926159858704, tv_loss: 0.033725276589393616\n",
      "iteration 3004, dc_loss: 0.028395814821124077, tv_loss: 0.033724792301654816\n",
      "iteration 3005, dc_loss: 0.02836606092751026, tv_loss: 0.03374600037932396\n",
      "iteration 3006, dc_loss: 0.028374772518873215, tv_loss: 0.03372307866811752\n",
      "iteration 3007, dc_loss: 0.02837139368057251, tv_loss: 0.03371576592326164\n",
      "iteration 3008, dc_loss: 0.028354918584227562, tv_loss: 0.033729929476976395\n",
      "iteration 3009, dc_loss: 0.028349384665489197, tv_loss: 0.03372436761856079\n",
      "iteration 3010, dc_loss: 0.0283500999212265, tv_loss: 0.033722907304763794\n",
      "iteration 3011, dc_loss: 0.028346164152026176, tv_loss: 0.03372349590063095\n",
      "iteration 3012, dc_loss: 0.02834002859890461, tv_loss: 0.03373491391539574\n",
      "iteration 3013, dc_loss: 0.028338471427559853, tv_loss: 0.033739980310201645\n",
      "iteration 3014, dc_loss: 0.02833425998687744, tv_loss: 0.03373786807060242\n",
      "iteration 3015, dc_loss: 0.028332889080047607, tv_loss: 0.03373026102781296\n",
      "iteration 3016, dc_loss: 0.02832837775349617, tv_loss: 0.033727627247571945\n",
      "iteration 3017, dc_loss: 0.0283297561109066, tv_loss: 0.03372202068567276\n",
      "iteration 3018, dc_loss: 0.02832525596022606, tv_loss: 0.03371943160891533\n",
      "iteration 3019, dc_loss: 0.02831808663904667, tv_loss: 0.033729035407304764\n",
      "iteration 3020, dc_loss: 0.028312252834439278, tv_loss: 0.0337333008646965\n",
      "iteration 3021, dc_loss: 0.028309615328907967, tv_loss: 0.03372889757156372\n",
      "iteration 3022, dc_loss: 0.028311820700764656, tv_loss: 0.03373377025127411\n",
      "iteration 3023, dc_loss: 0.02829737216234207, tv_loss: 0.03373569995164871\n",
      "iteration 3024, dc_loss: 0.02829653024673462, tv_loss: 0.033733952790498734\n",
      "iteration 3025, dc_loss: 0.028296733275055885, tv_loss: 0.033728692680597305\n",
      "iteration 3026, dc_loss: 0.02829359658062458, tv_loss: 0.03372399881482124\n",
      "iteration 3027, dc_loss: 0.02828754298388958, tv_loss: 0.0337265282869339\n",
      "iteration 3028, dc_loss: 0.02828294038772583, tv_loss: 0.033735621720552444\n",
      "iteration 3029, dc_loss: 0.02827543206512928, tv_loss: 0.03374774754047394\n",
      "iteration 3030, dc_loss: 0.028277283534407616, tv_loss: 0.03373205289244652\n",
      "iteration 3031, dc_loss: 0.02827819623053074, tv_loss: 0.03372707590460777\n",
      "iteration 3032, dc_loss: 0.02826530486345291, tv_loss: 0.03373801335692406\n",
      "iteration 3033, dc_loss: 0.028268776834011078, tv_loss: 0.0337299108505249\n",
      "iteration 3034, dc_loss: 0.028265954926609993, tv_loss: 0.03374631330370903\n",
      "iteration 3035, dc_loss: 0.028259310871362686, tv_loss: 0.033737663179636\n",
      "iteration 3036, dc_loss: 0.02826097048819065, tv_loss: 0.03372862562537193\n",
      "iteration 3037, dc_loss: 0.028258105739951134, tv_loss: 0.03372909128665924\n",
      "iteration 3038, dc_loss: 0.028252704069018364, tv_loss: 0.03374023735523224\n",
      "iteration 3039, dc_loss: 0.028242478147149086, tv_loss: 0.033744193613529205\n",
      "iteration 3040, dc_loss: 0.02825186401605606, tv_loss: 0.033733878284692764\n",
      "iteration 3041, dc_loss: 0.02824539691209793, tv_loss: 0.0337313637137413\n",
      "iteration 3042, dc_loss: 0.02824423462152481, tv_loss: 0.033726342022418976\n",
      "iteration 3043, dc_loss: 0.028238382190465927, tv_loss: 0.03374449908733368\n",
      "iteration 3044, dc_loss: 0.028239019215106964, tv_loss: 0.03374356031417847\n",
      "iteration 3045, dc_loss: 0.028242871165275574, tv_loss: 0.03374126926064491\n",
      "iteration 3046, dc_loss: 0.02825062908232212, tv_loss: 0.033722732216119766\n",
      "iteration 3047, dc_loss: 0.028228670358657837, tv_loss: 0.03374539688229561\n",
      "iteration 3048, dc_loss: 0.02823502942919731, tv_loss: 0.03372938930988312\n",
      "iteration 3049, dc_loss: 0.028223685920238495, tv_loss: 0.0337403304874897\n",
      "iteration 3050, dc_loss: 0.02821356989443302, tv_loss: 0.03373676538467407\n",
      "iteration 3051, dc_loss: 0.028215494006872177, tv_loss: 0.033728860318660736\n",
      "iteration 3052, dc_loss: 0.02820931188762188, tv_loss: 0.03372830152511597\n",
      "iteration 3053, dc_loss: 0.028204016387462616, tv_loss: 0.03372878581285477\n",
      "iteration 3054, dc_loss: 0.02817915566265583, tv_loss: 0.03374716639518738\n",
      "iteration 3055, dc_loss: 0.028213994577527046, tv_loss: 0.03371448069810867\n",
      "iteration 3056, dc_loss: 0.028181929141283035, tv_loss: 0.03374962881207466\n",
      "iteration 3057, dc_loss: 0.0281901266425848, tv_loss: 0.03373328223824501\n",
      "iteration 3058, dc_loss: 0.028181536123156548, tv_loss: 0.03374340012669563\n",
      "iteration 3059, dc_loss: 0.028193317353725433, tv_loss: 0.033725712448358536\n",
      "iteration 3060, dc_loss: 0.028169289231300354, tv_loss: 0.03373817354440689\n",
      "iteration 3061, dc_loss: 0.0281657874584198, tv_loss: 0.033737633377313614\n",
      "iteration 3062, dc_loss: 0.02818266861140728, tv_loss: 0.0337209478020668\n",
      "iteration 3063, dc_loss: 0.028158744797110558, tv_loss: 0.03373829647898674\n",
      "iteration 3064, dc_loss: 0.028166813775897026, tv_loss: 0.03373141586780548\n",
      "iteration 3065, dc_loss: 0.02814660593867302, tv_loss: 0.03374346345663071\n",
      "iteration 3066, dc_loss: 0.028170239180326462, tv_loss: 0.0337170735001564\n",
      "iteration 3067, dc_loss: 0.028139373287558556, tv_loss: 0.033741071820259094\n",
      "iteration 3068, dc_loss: 0.028163515031337738, tv_loss: 0.0337141714990139\n",
      "iteration 3069, dc_loss: 0.028131039813160896, tv_loss: 0.03375494107604027\n",
      "iteration 3070, dc_loss: 0.028152678161859512, tv_loss: 0.03372405096888542\n",
      "iteration 3071, dc_loss: 0.028128691017627716, tv_loss: 0.0337492898106575\n",
      "iteration 3072, dc_loss: 0.028140151873230934, tv_loss: 0.033735889941453934\n",
      "iteration 3073, dc_loss: 0.028120435774326324, tv_loss: 0.03374534100294113\n",
      "iteration 3074, dc_loss: 0.028131812810897827, tv_loss: 0.033733416348695755\n",
      "iteration 3075, dc_loss: 0.028120309114456177, tv_loss: 0.03373967111110687\n",
      "iteration 3076, dc_loss: 0.02811865136027336, tv_loss: 0.03373830392956734\n",
      "iteration 3077, dc_loss: 0.028124254196882248, tv_loss: 0.03373164311051369\n",
      "iteration 3078, dc_loss: 0.028118649497628212, tv_loss: 0.03373527154326439\n",
      "iteration 3079, dc_loss: 0.028119206428527832, tv_loss: 0.03373578563332558\n",
      "iteration 3080, dc_loss: 0.028109457343816757, tv_loss: 0.03374270349740982\n",
      "iteration 3081, dc_loss: 0.02811477892100811, tv_loss: 0.033740028738975525\n",
      "iteration 3082, dc_loss: 0.028103485703468323, tv_loss: 0.03374287113547325\n",
      "iteration 3083, dc_loss: 0.028108755126595497, tv_loss: 0.03373841941356659\n",
      "iteration 3084, dc_loss: 0.02811373956501484, tv_loss: 0.03372536972165108\n",
      "iteration 3085, dc_loss: 0.028092827647924423, tv_loss: 0.03374524787068367\n",
      "iteration 3086, dc_loss: 0.02810606174170971, tv_loss: 0.03372509032487869\n",
      "iteration 3087, dc_loss: 0.028095604851841927, tv_loss: 0.03373738378286362\n",
      "iteration 3088, dc_loss: 0.028106776997447014, tv_loss: 0.033722128719091415\n",
      "iteration 3089, dc_loss: 0.02808247320353985, tv_loss: 0.03373606130480766\n",
      "iteration 3090, dc_loss: 0.028081217780709267, tv_loss: 0.033729176968336105\n",
      "iteration 3091, dc_loss: 0.028069090098142624, tv_loss: 0.033737633377313614\n",
      "iteration 3092, dc_loss: 0.028075940907001495, tv_loss: 0.03372795879840851\n",
      "iteration 3093, dc_loss: 0.028057822957634926, tv_loss: 0.0337376669049263\n",
      "iteration 3094, dc_loss: 0.02805599570274353, tv_loss: 0.033746156841516495\n",
      "iteration 3095, dc_loss: 0.028068266808986664, tv_loss: 0.033736374229192734\n",
      "iteration 3096, dc_loss: 0.028050683438777924, tv_loss: 0.03374197706580162\n",
      "iteration 3097, dc_loss: 0.02804596722126007, tv_loss: 0.03374435007572174\n",
      "iteration 3098, dc_loss: 0.028053632006049156, tv_loss: 0.033730264753103256\n",
      "iteration 3099, dc_loss: 0.0280466265976429, tv_loss: 0.03373512998223305\n",
      "iteration 3100, dc_loss: 0.028034338727593422, tv_loss: 0.03374384343624115\n",
      "iteration 3101, dc_loss: 0.028042875230312347, tv_loss: 0.03374410793185234\n",
      "iteration 3102, dc_loss: 0.028045538812875748, tv_loss: 0.033740751445293427\n",
      "iteration 3103, dc_loss: 0.028025615960359573, tv_loss: 0.0337568074464798\n",
      "iteration 3104, dc_loss: 0.028041774407029152, tv_loss: 0.03373068571090698\n",
      "iteration 3105, dc_loss: 0.028016669675707817, tv_loss: 0.033747296780347824\n",
      "iteration 3106, dc_loss: 0.02803930640220642, tv_loss: 0.0337277390062809\n",
      "iteration 3107, dc_loss: 0.028007952496409416, tv_loss: 0.033767879009246826\n",
      "iteration 3108, dc_loss: 0.02803840860724449, tv_loss: 0.03373431786894798\n",
      "iteration 3109, dc_loss: 0.02799382247030735, tv_loss: 0.033765677362680435\n",
      "iteration 3110, dc_loss: 0.028032442554831505, tv_loss: 0.0337151363492012\n",
      "iteration 3111, dc_loss: 0.028005631640553474, tv_loss: 0.033745963126420975\n",
      "iteration 3112, dc_loss: 0.028010504320263863, tv_loss: 0.03373774141073227\n",
      "iteration 3113, dc_loss: 0.02798258140683174, tv_loss: 0.03376443684101105\n",
      "iteration 3114, dc_loss: 0.02800765633583069, tv_loss: 0.03373111039400101\n",
      "iteration 3115, dc_loss: 0.027980130165815353, tv_loss: 0.0337488055229187\n",
      "iteration 3116, dc_loss: 0.027996785938739777, tv_loss: 0.03372833877801895\n",
      "iteration 3117, dc_loss: 0.027981845661997795, tv_loss: 0.033738646656274796\n",
      "iteration 3118, dc_loss: 0.027979418635368347, tv_loss: 0.03373332694172859\n",
      "iteration 3119, dc_loss: 0.02796812355518341, tv_loss: 0.03374439477920532\n",
      "iteration 3120, dc_loss: 0.02797437272965908, tv_loss: 0.03373710438609123\n",
      "iteration 3121, dc_loss: 0.027972126379609108, tv_loss: 0.03374658152461052\n",
      "iteration 3122, dc_loss: 0.02795991860330105, tv_loss: 0.033759064972400665\n",
      "iteration 3123, dc_loss: 0.02796916477382183, tv_loss: 0.033744923770427704\n",
      "iteration 3124, dc_loss: 0.02796032279729843, tv_loss: 0.033737875521183014\n",
      "iteration 3125, dc_loss: 0.02796199917793274, tv_loss: 0.03373285382986069\n",
      "iteration 3126, dc_loss: 0.027945784851908684, tv_loss: 0.03374746814370155\n",
      "iteration 3127, dc_loss: 0.027966516092419624, tv_loss: 0.033725082874298096\n",
      "iteration 3128, dc_loss: 0.027943221852183342, tv_loss: 0.03374733775854111\n",
      "iteration 3129, dc_loss: 0.02796310931444168, tv_loss: 0.03373577073216438\n",
      "iteration 3130, dc_loss: 0.027954012155532837, tv_loss: 0.033752746880054474\n",
      "iteration 3131, dc_loss: 0.02797010727226734, tv_loss: 0.033747926354408264\n",
      "iteration 3132, dc_loss: 0.02795455977320671, tv_loss: 0.03375411033630371\n",
      "iteration 3133, dc_loss: 0.027971800416707993, tv_loss: 0.03373229131102562\n",
      "iteration 3134, dc_loss: 0.027945691719651222, tv_loss: 0.03374693915247917\n",
      "iteration 3135, dc_loss: 0.02794540300965309, tv_loss: 0.03373834863305092\n",
      "iteration 3136, dc_loss: 0.02792593091726303, tv_loss: 0.03374521806836128\n",
      "iteration 3137, dc_loss: 0.02792477235198021, tv_loss: 0.03373892605304718\n",
      "iteration 3138, dc_loss: 0.02790733613073826, tv_loss: 0.0337483286857605\n",
      "iteration 3139, dc_loss: 0.027912316843867302, tv_loss: 0.0337376743555069\n",
      "iteration 3140, dc_loss: 0.02790135331451893, tv_loss: 0.03374236449599266\n",
      "iteration 3141, dc_loss: 0.027907954528927803, tv_loss: 0.03374564275145531\n",
      "iteration 3142, dc_loss: 0.027898399159312248, tv_loss: 0.033750541508197784\n",
      "iteration 3143, dc_loss: 0.027899272739887238, tv_loss: 0.033744264394044876\n",
      "iteration 3144, dc_loss: 0.027906382456421852, tv_loss: 0.03373606503009796\n",
      "iteration 3145, dc_loss: 0.027903560549020767, tv_loss: 0.033733218908309937\n",
      "iteration 3146, dc_loss: 0.02788429521024227, tv_loss: 0.03374534472823143\n",
      "iteration 3147, dc_loss: 0.0278773196041584, tv_loss: 0.033743616193532944\n",
      "iteration 3148, dc_loss: 0.027882328256964684, tv_loss: 0.03373481333255768\n",
      "iteration 3149, dc_loss: 0.027877293527126312, tv_loss: 0.03374712914228439\n",
      "iteration 3150, dc_loss: 0.027873288840055466, tv_loss: 0.03376041725277901\n",
      "iteration 3151, dc_loss: 0.02786839008331299, tv_loss: 0.03376204892992973\n",
      "iteration 3152, dc_loss: 0.02786898799240589, tv_loss: 0.03374553471803665\n",
      "iteration 3153, dc_loss: 0.02787291258573532, tv_loss: 0.03374086692929268\n",
      "iteration 3154, dc_loss: 0.02786346711218357, tv_loss: 0.033750105649232864\n",
      "iteration 3155, dc_loss: 0.027866121381521225, tv_loss: 0.03374389186501503\n",
      "iteration 3156, dc_loss: 0.02786034718155861, tv_loss: 0.03374391794204712\n",
      "iteration 3157, dc_loss: 0.027868056669831276, tv_loss: 0.033735137432813644\n",
      "iteration 3158, dc_loss: 0.027842622250318527, tv_loss: 0.03375120460987091\n",
      "iteration 3159, dc_loss: 0.027841689065098763, tv_loss: 0.033746350556612015\n",
      "iteration 3160, dc_loss: 0.02783893048763275, tv_loss: 0.033743612468242645\n",
      "iteration 3161, dc_loss: 0.027844391763210297, tv_loss: 0.033738330006599426\n",
      "iteration 3162, dc_loss: 0.027826571837067604, tv_loss: 0.033757325261831284\n",
      "iteration 3163, dc_loss: 0.027845239266753197, tv_loss: 0.033741310238838196\n",
      "iteration 3164, dc_loss: 0.027819374576210976, tv_loss: 0.03376312181353569\n",
      "iteration 3165, dc_loss: 0.02783820405602455, tv_loss: 0.03373691812157631\n",
      "iteration 3166, dc_loss: 0.02781418338418007, tv_loss: 0.033756665885448456\n",
      "iteration 3167, dc_loss: 0.027840962633490562, tv_loss: 0.03372785449028015\n",
      "iteration 3168, dc_loss: 0.027801604941487312, tv_loss: 0.03377411514520645\n",
      "iteration 3169, dc_loss: 0.02784331515431404, tv_loss: 0.03372832015156746\n",
      "iteration 3170, dc_loss: 0.027795592322945595, tv_loss: 0.0337747223675251\n",
      "iteration 3171, dc_loss: 0.027829676866531372, tv_loss: 0.03372933715581894\n",
      "iteration 3172, dc_loss: 0.0277923084795475, tv_loss: 0.033770907670259476\n",
      "iteration 3173, dc_loss: 0.027820654213428497, tv_loss: 0.03372865170240402\n",
      "iteration 3174, dc_loss: 0.0277754794806242, tv_loss: 0.03378009423613548\n",
      "iteration 3175, dc_loss: 0.027823232114315033, tv_loss: 0.033723361790180206\n",
      "iteration 3176, dc_loss: 0.027775360271334648, tv_loss: 0.03376203030347824\n",
      "iteration 3177, dc_loss: 0.027811512351036072, tv_loss: 0.03372760862112045\n",
      "iteration 3178, dc_loss: 0.027766456827521324, tv_loss: 0.03377249464392662\n",
      "iteration 3179, dc_loss: 0.027813658118247986, tv_loss: 0.03372596949338913\n",
      "iteration 3180, dc_loss: 0.02775384671986103, tv_loss: 0.03378157317638397\n",
      "iteration 3181, dc_loss: 0.02780037187039852, tv_loss: 0.03373071551322937\n",
      "iteration 3182, dc_loss: 0.027768902480602264, tv_loss: 0.033753179013729095\n",
      "iteration 3183, dc_loss: 0.027788963168859482, tv_loss: 0.033729325979948044\n",
      "iteration 3184, dc_loss: 0.027748707681894302, tv_loss: 0.03377034515142441\n",
      "iteration 3185, dc_loss: 0.027782471850514412, tv_loss: 0.03373108059167862\n",
      "iteration 3186, dc_loss: 0.027753755450248718, tv_loss: 0.03376447781920433\n",
      "iteration 3187, dc_loss: 0.027756555005908012, tv_loss: 0.03374384716153145\n",
      "iteration 3188, dc_loss: 0.027743693441152573, tv_loss: 0.03375735506415367\n",
      "iteration 3189, dc_loss: 0.027758195996284485, tv_loss: 0.03373235836625099\n",
      "iteration 3190, dc_loss: 0.02773856744170189, tv_loss: 0.03375278040766716\n",
      "iteration 3191, dc_loss: 0.027738770470023155, tv_loss: 0.033744316548109055\n",
      "iteration 3192, dc_loss: 0.027744170278310776, tv_loss: 0.03374423086643219\n",
      "iteration 3193, dc_loss: 0.027733217924833298, tv_loss: 0.03374691680073738\n",
      "iteration 3194, dc_loss: 0.027732286602258682, tv_loss: 0.03375890105962753\n",
      "iteration 3195, dc_loss: 0.027727345004677773, tv_loss: 0.03375905007123947\n",
      "iteration 3196, dc_loss: 0.02774101309478283, tv_loss: 0.03374695032835007\n",
      "iteration 3197, dc_loss: 0.027725884690880775, tv_loss: 0.033754438161849976\n",
      "iteration 3198, dc_loss: 0.027750717476010323, tv_loss: 0.033732857555150986\n",
      "iteration 3199, dc_loss: 0.027727434411644936, tv_loss: 0.03375902399420738\n",
      "iteration 3200, dc_loss: 0.027756547555327415, tv_loss: 0.03373827040195465\n",
      "iteration 3201, dc_loss: 0.027729863300919533, tv_loss: 0.03376759588718414\n",
      "iteration 3202, dc_loss: 0.027715589851140976, tv_loss: 0.033745940774679184\n",
      "iteration 3203, dc_loss: 0.02770036831498146, tv_loss: 0.03374462574720383\n",
      "iteration 3204, dc_loss: 0.027711551636457443, tv_loss: 0.033756665885448456\n",
      "iteration 3205, dc_loss: 0.027721352875232697, tv_loss: 0.03374366834759712\n",
      "iteration 3206, dc_loss: 0.0276961512863636, tv_loss: 0.03374008461833\n",
      "iteration 3207, dc_loss: 0.027703523635864258, tv_loss: 0.033745065331459045\n",
      "iteration 3208, dc_loss: 0.027716519311070442, tv_loss: 0.033745866268873215\n",
      "iteration 3209, dc_loss: 0.027683939784765244, tv_loss: 0.03375246748328209\n",
      "iteration 3210, dc_loss: 0.027696749195456505, tv_loss: 0.033746328204870224\n",
      "iteration 3211, dc_loss: 0.027707725763320923, tv_loss: 0.03374181315302849\n",
      "iteration 3212, dc_loss: 0.02768114022910595, tv_loss: 0.03374113887548447\n",
      "iteration 3213, dc_loss: 0.027703646570444107, tv_loss: 0.03373761475086212\n",
      "iteration 3214, dc_loss: 0.02770046889781952, tv_loss: 0.033753931522369385\n",
      "iteration 3215, dc_loss: 0.027668168768286705, tv_loss: 0.03374665603041649\n",
      "iteration 3216, dc_loss: 0.02770336903631687, tv_loss: 0.03372709080576897\n",
      "iteration 3217, dc_loss: 0.027701109647750854, tv_loss: 0.03374442458152771\n",
      "iteration 3218, dc_loss: 0.027666544541716576, tv_loss: 0.03374573960900307\n",
      "iteration 3219, dc_loss: 0.027713783085346222, tv_loss: 0.033733390271663666\n",
      "iteration 3220, dc_loss: 0.02770056575536728, tv_loss: 0.03376296907663345\n",
      "iteration 3221, dc_loss: 0.02766156941652298, tv_loss: 0.0337509922683239\n",
      "iteration 3222, dc_loss: 0.02775566279888153, tv_loss: 0.03371458128094673\n",
      "iteration 3223, dc_loss: 0.02774226665496826, tv_loss: 0.033771999180316925\n",
      "iteration 3224, dc_loss: 0.02773945964872837, tv_loss: 0.03376631811261177\n",
      "iteration 3225, dc_loss: 0.027737382799386978, tv_loss: 0.03372913971543312\n",
      "iteration 3226, dc_loss: 0.02775588259100914, tv_loss: 0.03374065086245537\n",
      "iteration 3227, dc_loss: 0.027658503502607346, tv_loss: 0.03377566859126091\n",
      "iteration 3228, dc_loss: 0.027746612206101418, tv_loss: 0.03373732790350914\n",
      "iteration 3229, dc_loss: 0.02768031693994999, tv_loss: 0.03374246880412102\n",
      "iteration 3230, dc_loss: 0.02771696075797081, tv_loss: 0.0337483175098896\n",
      "iteration 3231, dc_loss: 0.027712829411029816, tv_loss: 0.033753156661987305\n",
      "iteration 3232, dc_loss: 0.02776123769581318, tv_loss: 0.03373910114169121\n",
      "iteration 3233, dc_loss: 0.027718650177121162, tv_loss: 0.033747654408216476\n",
      "iteration 3234, dc_loss: 0.02766888029873371, tv_loss: 0.03375484421849251\n",
      "iteration 3235, dc_loss: 0.027759380638599396, tv_loss: 0.03374585136771202\n",
      "iteration 3236, dc_loss: 0.0276957880705595, tv_loss: 0.033755846321582794\n",
      "iteration 3237, dc_loss: 0.027751892805099487, tv_loss: 0.033740878105163574\n",
      "iteration 3238, dc_loss: 0.027742009609937668, tv_loss: 0.033745601773262024\n",
      "iteration 3239, dc_loss: 0.02771151065826416, tv_loss: 0.03375666216015816\n",
      "iteration 3240, dc_loss: 0.027659345418214798, tv_loss: 0.0337485745549202\n",
      "iteration 3241, dc_loss: 0.0277034230530262, tv_loss: 0.03373212367296219\n",
      "iteration 3242, dc_loss: 0.027634190395474434, tv_loss: 0.03374766185879707\n",
      "iteration 3243, dc_loss: 0.027642758563160896, tv_loss: 0.03375127166509628\n",
      "iteration 3244, dc_loss: 0.02764732390642166, tv_loss: 0.03374137356877327\n",
      "iteration 3245, dc_loss: 0.02764423005282879, tv_loss: 0.033723946660757065\n",
      "iteration 3246, dc_loss: 0.027637014165520668, tv_loss: 0.03373037651181221\n",
      "iteration 3247, dc_loss: 0.027607915922999382, tv_loss: 0.03375578299164772\n",
      "iteration 3248, dc_loss: 0.02760048396885395, tv_loss: 0.033749792724847794\n",
      "iteration 3249, dc_loss: 0.027625080198049545, tv_loss: 0.033727142959833145\n",
      "iteration 3250, dc_loss: 0.027608569711446762, tv_loss: 0.0337446928024292\n",
      "iteration 3251, dc_loss: 0.027594903483986855, tv_loss: 0.03375654295086861\n",
      "iteration 3252, dc_loss: 0.027598462998867035, tv_loss: 0.03374631702899933\n",
      "iteration 3253, dc_loss: 0.027586566284298897, tv_loss: 0.03374314308166504\n",
      "iteration 3254, dc_loss: 0.02759469673037529, tv_loss: 0.03374342992901802\n",
      "iteration 3255, dc_loss: 0.02758176438510418, tv_loss: 0.03374943137168884\n",
      "iteration 3256, dc_loss: 0.02758036181330681, tv_loss: 0.033749695867300034\n",
      "iteration 3257, dc_loss: 0.027580291032791138, tv_loss: 0.033740751445293427\n",
      "iteration 3258, dc_loss: 0.02756846882402897, tv_loss: 0.033746387809515\n",
      "iteration 3259, dc_loss: 0.027562757954001427, tv_loss: 0.03375019505620003\n",
      "iteration 3260, dc_loss: 0.027575530111789703, tv_loss: 0.0337413027882576\n",
      "iteration 3261, dc_loss: 0.027562763541936874, tv_loss: 0.03374900668859482\n",
      "iteration 3262, dc_loss: 0.027557387948036194, tv_loss: 0.033749137073755264\n",
      "iteration 3263, dc_loss: 0.027551760897040367, tv_loss: 0.03374563157558441\n",
      "iteration 3264, dc_loss: 0.027552984654903412, tv_loss: 0.03374232351779938\n",
      "iteration 3265, dc_loss: 0.027556994929909706, tv_loss: 0.03373938053846359\n",
      "iteration 3266, dc_loss: 0.02754523977637291, tv_loss: 0.03375447541475296\n",
      "iteration 3267, dc_loss: 0.027536673471331596, tv_loss: 0.03375336900353432\n",
      "iteration 3268, dc_loss: 0.02754938416182995, tv_loss: 0.0337376594543457\n",
      "iteration 3269, dc_loss: 0.027545548975467682, tv_loss: 0.033739831298589706\n",
      "iteration 3270, dc_loss: 0.02752642333507538, tv_loss: 0.03375348821282387\n",
      "iteration 3271, dc_loss: 0.027525871992111206, tv_loss: 0.033756401389837265\n",
      "iteration 3272, dc_loss: 0.027533764019608498, tv_loss: 0.033746443688869476\n",
      "iteration 3273, dc_loss: 0.027529671788215637, tv_loss: 0.033742863684892654\n",
      "iteration 3274, dc_loss: 0.02751988172531128, tv_loss: 0.03374442830681801\n",
      "iteration 3275, dc_loss: 0.027519894763827324, tv_loss: 0.033747296780347824\n",
      "iteration 3276, dc_loss: 0.02751929499208927, tv_loss: 0.033750373870134354\n",
      "iteration 3277, dc_loss: 0.02752244845032692, tv_loss: 0.03374749422073364\n",
      "iteration 3278, dc_loss: 0.027508415281772614, tv_loss: 0.03375309333205223\n",
      "iteration 3279, dc_loss: 0.02750815451145172, tv_loss: 0.033745381981134415\n",
      "iteration 3280, dc_loss: 0.027508389204740524, tv_loss: 0.0337446928024292\n",
      "iteration 3281, dc_loss: 0.027503343299031258, tv_loss: 0.03374727442860603\n",
      "iteration 3282, dc_loss: 0.027502793818712234, tv_loss: 0.033747848123311996\n",
      "iteration 3283, dc_loss: 0.0275011844933033, tv_loss: 0.03374886140227318\n",
      "iteration 3284, dc_loss: 0.027497045695781708, tv_loss: 0.03374586999416351\n",
      "iteration 3285, dc_loss: 0.027495140209794044, tv_loss: 0.033746495842933655\n",
      "iteration 3286, dc_loss: 0.02748863585293293, tv_loss: 0.03374648839235306\n",
      "iteration 3287, dc_loss: 0.027489731088280678, tv_loss: 0.033740222454071045\n",
      "iteration 3288, dc_loss: 0.027491210028529167, tv_loss: 0.03373890742659569\n",
      "iteration 3289, dc_loss: 0.027483517304062843, tv_loss: 0.03374389186501503\n",
      "iteration 3290, dc_loss: 0.027478190138936043, tv_loss: 0.03374546021223068\n",
      "iteration 3291, dc_loss: 0.027479909360408783, tv_loss: 0.03374413028359413\n",
      "iteration 3292, dc_loss: 0.027476325631141663, tv_loss: 0.033743880689144135\n",
      "iteration 3293, dc_loss: 0.027473853901028633, tv_loss: 0.03374262526631355\n",
      "iteration 3294, dc_loss: 0.02747466415166855, tv_loss: 0.033742573112249374\n",
      "iteration 3295, dc_loss: 0.027469415217638016, tv_loss: 0.03373836725950241\n",
      "iteration 3296, dc_loss: 0.02746417000889778, tv_loss: 0.033746588975191116\n",
      "iteration 3297, dc_loss: 0.027465203776955605, tv_loss: 0.033740948885679245\n",
      "iteration 3298, dc_loss: 0.027460196986794472, tv_loss: 0.033741746097803116\n",
      "iteration 3299, dc_loss: 0.02745973691344261, tv_loss: 0.033744435757398605\n",
      "iteration 3300, dc_loss: 0.027454063296318054, tv_loss: 0.033747654408216476\n",
      "iteration 3301, dc_loss: 0.027452535927295685, tv_loss: 0.03375115245580673\n",
      "iteration 3302, dc_loss: 0.02744879573583603, tv_loss: 0.03375248610973358\n",
      "iteration 3303, dc_loss: 0.027453487738966942, tv_loss: 0.033744461834430695\n",
      "iteration 3304, dc_loss: 0.02745174989104271, tv_loss: 0.03373982012271881\n",
      "iteration 3305, dc_loss: 0.027439825236797333, tv_loss: 0.033750057220458984\n",
      "iteration 3306, dc_loss: 0.027437610551714897, tv_loss: 0.03374754637479782\n",
      "iteration 3307, dc_loss: 0.02744314819574356, tv_loss: 0.03374630957841873\n",
      "iteration 3308, dc_loss: 0.027431821450591087, tv_loss: 0.03374573588371277\n",
      "iteration 3309, dc_loss: 0.027432601898908615, tv_loss: 0.033750880509614944\n",
      "iteration 3310, dc_loss: 0.027436478063464165, tv_loss: 0.03374393656849861\n",
      "iteration 3311, dc_loss: 0.027430010959506035, tv_loss: 0.033744361251592636\n",
      "iteration 3312, dc_loss: 0.027425942942500114, tv_loss: 0.033746931701898575\n",
      "iteration 3313, dc_loss: 0.027422910556197166, tv_loss: 0.03374746814370155\n",
      "iteration 3314, dc_loss: 0.027417782694101334, tv_loss: 0.033748019486665726\n",
      "iteration 3315, dc_loss: 0.027422908693552017, tv_loss: 0.0337451696395874\n",
      "iteration 3316, dc_loss: 0.027418211102485657, tv_loss: 0.03374936804175377\n",
      "iteration 3317, dc_loss: 0.027413589879870415, tv_loss: 0.03375503048300743\n",
      "iteration 3318, dc_loss: 0.02741159312427044, tv_loss: 0.03375062346458435\n",
      "iteration 3319, dc_loss: 0.027406517416238785, tv_loss: 0.033753830939531326\n",
      "iteration 3320, dc_loss: 0.02740861475467682, tv_loss: 0.03374701738357544\n",
      "iteration 3321, dc_loss: 0.027401059865951538, tv_loss: 0.033749110996723175\n",
      "iteration 3322, dc_loss: 0.02740131877362728, tv_loss: 0.033748559653759\n",
      "iteration 3323, dc_loss: 0.02740326337516308, tv_loss: 0.033749498426914215\n",
      "iteration 3324, dc_loss: 0.02739385887980461, tv_loss: 0.033754654228687286\n",
      "iteration 3325, dc_loss: 0.027391860261559486, tv_loss: 0.03375375643372536\n",
      "iteration 3326, dc_loss: 0.02740170992910862, tv_loss: 0.03373897448182106\n",
      "iteration 3327, dc_loss: 0.0273862574249506, tv_loss: 0.03375321999192238\n",
      "iteration 3328, dc_loss: 0.027379343286156654, tv_loss: 0.033759936690330505\n",
      "iteration 3329, dc_loss: 0.027388691902160645, tv_loss: 0.033743783831596375\n",
      "iteration 3330, dc_loss: 0.027387801557779312, tv_loss: 0.033751003444194794\n",
      "iteration 3331, dc_loss: 0.027381762862205505, tv_loss: 0.03375587984919548\n",
      "iteration 3332, dc_loss: 0.02736830897629261, tv_loss: 0.0337616428732872\n",
      "iteration 3333, dc_loss: 0.027376070618629456, tv_loss: 0.033747632056474686\n",
      "iteration 3334, dc_loss: 0.027379197999835014, tv_loss: 0.033742379397153854\n",
      "iteration 3335, dc_loss: 0.027369273826479912, tv_loss: 0.03375060111284256\n",
      "iteration 3336, dc_loss: 0.027364859357476234, tv_loss: 0.033751316368579865\n",
      "iteration 3337, dc_loss: 0.02736247517168522, tv_loss: 0.033750079572200775\n",
      "iteration 3338, dc_loss: 0.02736450918018818, tv_loss: 0.03374379128217697\n",
      "iteration 3339, dc_loss: 0.02736278437077999, tv_loss: 0.033746976405382156\n",
      "iteration 3340, dc_loss: 0.027354665100574493, tv_loss: 0.033754922449588776\n",
      "iteration 3341, dc_loss: 0.02735571190714836, tv_loss: 0.03374822065234184\n",
      "iteration 3342, dc_loss: 0.02735738456249237, tv_loss: 0.0337463840842247\n",
      "iteration 3343, dc_loss: 0.027344675734639168, tv_loss: 0.03375725448131561\n",
      "iteration 3344, dc_loss: 0.027347881346940994, tv_loss: 0.03376074135303497\n",
      "iteration 3345, dc_loss: 0.027350936084985733, tv_loss: 0.033745814114809036\n",
      "iteration 3346, dc_loss: 0.027341851964592934, tv_loss: 0.03375202417373657\n",
      "iteration 3347, dc_loss: 0.02734079584479332, tv_loss: 0.03375132381916046\n",
      "iteration 3348, dc_loss: 0.027337227016687393, tv_loss: 0.03374810516834259\n",
      "iteration 3349, dc_loss: 0.02733517251908779, tv_loss: 0.03375079482793808\n",
      "iteration 3350, dc_loss: 0.02733408287167549, tv_loss: 0.033747151494026184\n",
      "iteration 3351, dc_loss: 0.027331925928592682, tv_loss: 0.03374573960900307\n",
      "iteration 3352, dc_loss: 0.027331097051501274, tv_loss: 0.0337434820830822\n",
      "iteration 3353, dc_loss: 0.02732480876147747, tv_loss: 0.03375141695141792\n",
      "iteration 3354, dc_loss: 0.027321601286530495, tv_loss: 0.033754609525203705\n",
      "iteration 3355, dc_loss: 0.027322182431817055, tv_loss: 0.033757951110601425\n",
      "iteration 3356, dc_loss: 0.027316251769661903, tv_loss: 0.03375827893614769\n",
      "iteration 3357, dc_loss: 0.027324549853801727, tv_loss: 0.03374449163675308\n",
      "iteration 3358, dc_loss: 0.02731272205710411, tv_loss: 0.03374995291233063\n",
      "iteration 3359, dc_loss: 0.027303842827677727, tv_loss: 0.033763542771339417\n",
      "iteration 3360, dc_loss: 0.02731470763683319, tv_loss: 0.03374851867556572\n",
      "iteration 3361, dc_loss: 0.02730974368751049, tv_loss: 0.03375433385372162\n",
      "iteration 3362, dc_loss: 0.027297984808683395, tv_loss: 0.033762410283088684\n",
      "iteration 3363, dc_loss: 0.027301767840981483, tv_loss: 0.033748164772987366\n",
      "iteration 3364, dc_loss: 0.027304289862513542, tv_loss: 0.03375348821282387\n",
      "iteration 3365, dc_loss: 0.02729269117116928, tv_loss: 0.03375938907265663\n",
      "iteration 3366, dc_loss: 0.027292096987366676, tv_loss: 0.03375962749123573\n",
      "iteration 3367, dc_loss: 0.027301177382469177, tv_loss: 0.03374544531106949\n",
      "iteration 3368, dc_loss: 0.02729177102446556, tv_loss: 0.03374727442860603\n",
      "iteration 3369, dc_loss: 0.02727770432829857, tv_loss: 0.03376394882798195\n",
      "iteration 3370, dc_loss: 0.02728380635380745, tv_loss: 0.03376297280192375\n",
      "iteration 3371, dc_loss: 0.027287041768431664, tv_loss: 0.03375645726919174\n",
      "iteration 3372, dc_loss: 0.027281736955046654, tv_loss: 0.03375408053398132\n",
      "iteration 3373, dc_loss: 0.0272754468023777, tv_loss: 0.03375519812107086\n",
      "iteration 3374, dc_loss: 0.027273600921034813, tv_loss: 0.033761344850063324\n",
      "iteration 3375, dc_loss: 0.0272735096514225, tv_loss: 0.03376258537173271\n",
      "iteration 3376, dc_loss: 0.027269553393125534, tv_loss: 0.033759139478206635\n",
      "iteration 3377, dc_loss: 0.027266327291727066, tv_loss: 0.03375471383333206\n",
      "iteration 3378, dc_loss: 0.027267973870038986, tv_loss: 0.0337480865418911\n",
      "iteration 3379, dc_loss: 0.02726750075817108, tv_loss: 0.03375460207462311\n",
      "iteration 3380, dc_loss: 0.02725915051996708, tv_loss: 0.03376433253288269\n",
      "iteration 3381, dc_loss: 0.027253415435552597, tv_loss: 0.03375961259007454\n",
      "iteration 3382, dc_loss: 0.027259573340415955, tv_loss: 0.03375033661723137\n",
      "iteration 3383, dc_loss: 0.027255715802311897, tv_loss: 0.03374933823943138\n",
      "iteration 3384, dc_loss: 0.02725064754486084, tv_loss: 0.033761121332645416\n",
      "iteration 3385, dc_loss: 0.027247726917266846, tv_loss: 0.03376220539212227\n",
      "iteration 3386, dc_loss: 0.027243150398135185, tv_loss: 0.0337589830160141\n",
      "iteration 3387, dc_loss: 0.027244295924901962, tv_loss: 0.03375707566738129\n",
      "iteration 3388, dc_loss: 0.027246011421084404, tv_loss: 0.033751752227544785\n",
      "iteration 3389, dc_loss: 0.0272419024258852, tv_loss: 0.03374900668859482\n",
      "iteration 3390, dc_loss: 0.027233073487877846, tv_loss: 0.033754076808691025\n",
      "iteration 3391, dc_loss: 0.027230678126215935, tv_loss: 0.03376179561018944\n",
      "iteration 3392, dc_loss: 0.027240542694926262, tv_loss: 0.03374888747930527\n",
      "iteration 3393, dc_loss: 0.027226276695728302, tv_loss: 0.03376052901148796\n",
      "iteration 3394, dc_loss: 0.027226194739341736, tv_loss: 0.033762697130441666\n",
      "iteration 3395, dc_loss: 0.027227504178881645, tv_loss: 0.03375474363565445\n",
      "iteration 3396, dc_loss: 0.027223868295550346, tv_loss: 0.03375614434480667\n",
      "iteration 3397, dc_loss: 0.027219701558351517, tv_loss: 0.033754613250494\n",
      "iteration 3398, dc_loss: 0.027209272608160973, tv_loss: 0.03376472741365433\n",
      "iteration 3399, dc_loss: 0.02721725031733513, tv_loss: 0.033758293837308884\n",
      "iteration 3400, dc_loss: 0.027220821008086205, tv_loss: 0.03375249356031418\n",
      "iteration 3401, dc_loss: 0.027207151055336, tv_loss: 0.03375795856118202\n",
      "iteration 3402, dc_loss: 0.02720818482339382, tv_loss: 0.03375592455267906\n",
      "iteration 3403, dc_loss: 0.027207273989915848, tv_loss: 0.033752698451280594\n",
      "iteration 3404, dc_loss: 0.02720394916832447, tv_loss: 0.03375893458724022\n",
      "iteration 3405, dc_loss: 0.027200324460864067, tv_loss: 0.033756859600543976\n",
      "iteration 3406, dc_loss: 0.027197159826755524, tv_loss: 0.03376021608710289\n",
      "iteration 3407, dc_loss: 0.027198171243071556, tv_loss: 0.03375278785824776\n",
      "iteration 3408, dc_loss: 0.02719329670071602, tv_loss: 0.033751633018255234\n",
      "iteration 3409, dc_loss: 0.027191773056983948, tv_loss: 0.03375369310379028\n",
      "iteration 3410, dc_loss: 0.027191614732146263, tv_loss: 0.033747922629117966\n",
      "iteration 3411, dc_loss: 0.027186455205082893, tv_loss: 0.03374874219298363\n",
      "iteration 3412, dc_loss: 0.0271846242249012, tv_loss: 0.03375418111681938\n",
      "iteration 3413, dc_loss: 0.027177054435014725, tv_loss: 0.03375713899731636\n",
      "iteration 3414, dc_loss: 0.027181601151823997, tv_loss: 0.033757928758859634\n",
      "iteration 3415, dc_loss: 0.0271832924336195, tv_loss: 0.03376316651701927\n",
      "iteration 3416, dc_loss: 0.027170535176992416, tv_loss: 0.03376903012394905\n",
      "iteration 3417, dc_loss: 0.027169879525899887, tv_loss: 0.03375968337059021\n",
      "iteration 3418, dc_loss: 0.027175119146704674, tv_loss: 0.03374693915247917\n",
      "iteration 3419, dc_loss: 0.02716800570487976, tv_loss: 0.03375929966568947\n",
      "iteration 3420, dc_loss: 0.0271612536162138, tv_loss: 0.0337672159075737\n",
      "iteration 3421, dc_loss: 0.02716093137860298, tv_loss: 0.0337614007294178\n",
      "iteration 3422, dc_loss: 0.02716713584959507, tv_loss: 0.033750101923942566\n",
      "iteration 3423, dc_loss: 0.027155255898833275, tv_loss: 0.03375934809446335\n",
      "iteration 3424, dc_loss: 0.027156304568052292, tv_loss: 0.033751726150512695\n",
      "iteration 3425, dc_loss: 0.027155423536896706, tv_loss: 0.03375120088458061\n",
      "iteration 3426, dc_loss: 0.027149120345711708, tv_loss: 0.03375746309757233\n",
      "iteration 3427, dc_loss: 0.027147600427269936, tv_loss: 0.033755335956811905\n",
      "iteration 3428, dc_loss: 0.027143707498908043, tv_loss: 0.033752549439668655\n",
      "iteration 3429, dc_loss: 0.027148567140102386, tv_loss: 0.033752262592315674\n",
      "iteration 3430, dc_loss: 0.027141615748405457, tv_loss: 0.03375924006104469\n",
      "iteration 3431, dc_loss: 0.027134384959936142, tv_loss: 0.03376258909702301\n",
      "iteration 3432, dc_loss: 0.027138471603393555, tv_loss: 0.03375331684947014\n",
      "iteration 3433, dc_loss: 0.027138857170939445, tv_loss: 0.0337541364133358\n",
      "iteration 3434, dc_loss: 0.02713102288544178, tv_loss: 0.03376420959830284\n",
      "iteration 3435, dc_loss: 0.027128951624035835, tv_loss: 0.033760614693164825\n",
      "iteration 3436, dc_loss: 0.027128688991069794, tv_loss: 0.03375465050339699\n",
      "iteration 3437, dc_loss: 0.027122965082526207, tv_loss: 0.03375578299164772\n",
      "iteration 3438, dc_loss: 0.02712181769311428, tv_loss: 0.03375644609332085\n",
      "iteration 3439, dc_loss: 0.02712605707347393, tv_loss: 0.033751461654901505\n",
      "iteration 3440, dc_loss: 0.027116574347019196, tv_loss: 0.03375767171382904\n",
      "iteration 3441, dc_loss: 0.02711130678653717, tv_loss: 0.033760711550712585\n",
      "iteration 3442, dc_loss: 0.027114592492580414, tv_loss: 0.03375433012843132\n",
      "iteration 3443, dc_loss: 0.02711600810289383, tv_loss: 0.033749256283044815\n",
      "iteration 3444, dc_loss: 0.027104906737804413, tv_loss: 0.03375881537795067\n",
      "iteration 3445, dc_loss: 0.027106797322630882, tv_loss: 0.033755894750356674\n",
      "iteration 3446, dc_loss: 0.02710541896522045, tv_loss: 0.033754464238882065\n",
      "iteration 3447, dc_loss: 0.027101876214146614, tv_loss: 0.033755380660295486\n",
      "iteration 3448, dc_loss: 0.027098821476101875, tv_loss: 0.03375740721821785\n",
      "iteration 3449, dc_loss: 0.02709263563156128, tv_loss: 0.03375816345214844\n",
      "iteration 3450, dc_loss: 0.027099017053842545, tv_loss: 0.03375696390867233\n",
      "iteration 3451, dc_loss: 0.027095986530184746, tv_loss: 0.03375691920518875\n",
      "iteration 3452, dc_loss: 0.027087576687335968, tv_loss: 0.033762600272893906\n",
      "iteration 3453, dc_loss: 0.027086185291409492, tv_loss: 0.03376401215791702\n",
      "iteration 3454, dc_loss: 0.027080291882157326, tv_loss: 0.03377175331115723\n",
      "iteration 3455, dc_loss: 0.02708912082016468, tv_loss: 0.033760618418455124\n",
      "iteration 3456, dc_loss: 0.027078736573457718, tv_loss: 0.03376510739326477\n",
      "iteration 3457, dc_loss: 0.02708062343299389, tv_loss: 0.03375622630119324\n",
      "iteration 3458, dc_loss: 0.02707553468644619, tv_loss: 0.03375713899731636\n",
      "iteration 3459, dc_loss: 0.027068883180618286, tv_loss: 0.03377119079232216\n",
      "iteration 3460, dc_loss: 0.02707434445619583, tv_loss: 0.033763039857149124\n",
      "iteration 3461, dc_loss: 0.027065003290772438, tv_loss: 0.03377183899283409\n",
      "iteration 3462, dc_loss: 0.027073176577687263, tv_loss: 0.0337563119828701\n",
      "iteration 3463, dc_loss: 0.027063166722655296, tv_loss: 0.03375884145498276\n",
      "iteration 3464, dc_loss: 0.027057791128754616, tv_loss: 0.03376128897070885\n",
      "iteration 3465, dc_loss: 0.02706250362098217, tv_loss: 0.03375504910945892\n",
      "iteration 3466, dc_loss: 0.027057338505983353, tv_loss: 0.03376454859972\n",
      "iteration 3467, dc_loss: 0.027058878913521767, tv_loss: 0.033767666667699814\n",
      "iteration 3468, dc_loss: 0.027046529576182365, tv_loss: 0.0337749607861042\n",
      "iteration 3469, dc_loss: 0.027053995057940483, tv_loss: 0.03375884145498276\n",
      "iteration 3470, dc_loss: 0.02705015055835247, tv_loss: 0.033756427466869354\n",
      "iteration 3471, dc_loss: 0.027038034051656723, tv_loss: 0.03376759588718414\n",
      "iteration 3472, dc_loss: 0.02704090252518654, tv_loss: 0.033763252198696136\n",
      "iteration 3473, dc_loss: 0.027043601498007774, tv_loss: 0.03375968709588051\n",
      "iteration 3474, dc_loss: 0.027044720947742462, tv_loss: 0.033754970878362656\n",
      "iteration 3475, dc_loss: 0.027034997940063477, tv_loss: 0.033763591200113297\n",
      "iteration 3476, dc_loss: 0.02703385427594185, tv_loss: 0.03376690670847893\n",
      "iteration 3477, dc_loss: 0.027029499411582947, tv_loss: 0.03377049043774605\n",
      "iteration 3478, dc_loss: 0.02702738344669342, tv_loss: 0.03376801684498787\n",
      "iteration 3479, dc_loss: 0.027026502415537834, tv_loss: 0.033764101564884186\n",
      "iteration 3480, dc_loss: 0.027026159688830376, tv_loss: 0.03376239538192749\n",
      "iteration 3481, dc_loss: 0.027033606544137, tv_loss: 0.033749185502529144\n",
      "iteration 3482, dc_loss: 0.027017096057534218, tv_loss: 0.033765655010938644\n",
      "iteration 3483, dc_loss: 0.02701331116259098, tv_loss: 0.03376656025648117\n",
      "iteration 3484, dc_loss: 0.027016475796699524, tv_loss: 0.03375724330544472\n",
      "iteration 3485, dc_loss: 0.02701127529144287, tv_loss: 0.03376227989792824\n",
      "iteration 3486, dc_loss: 0.02700740098953247, tv_loss: 0.033763229846954346\n",
      "iteration 3487, dc_loss: 0.027010232210159302, tv_loss: 0.03375442326068878\n",
      "iteration 3488, dc_loss: 0.027011925354599953, tv_loss: 0.033760178834199905\n",
      "iteration 3489, dc_loss: 0.027006618678569794, tv_loss: 0.03376171737909317\n",
      "iteration 3490, dc_loss: 0.026995941996574402, tv_loss: 0.033774230629205704\n",
      "iteration 3491, dc_loss: 0.027000365778803825, tv_loss: 0.033769335597753525\n",
      "iteration 3492, dc_loss: 0.026997970417141914, tv_loss: 0.03376346081495285\n",
      "iteration 3493, dc_loss: 0.02699331007897854, tv_loss: 0.033763330429792404\n",
      "iteration 3494, dc_loss: 0.026989882811903954, tv_loss: 0.03376942500472069\n",
      "iteration 3495, dc_loss: 0.026996774598956108, tv_loss: 0.03376348689198494\n",
      "iteration 3496, dc_loss: 0.02699345350265503, tv_loss: 0.03376093879342079\n",
      "iteration 3497, dc_loss: 0.026980804279446602, tv_loss: 0.033768150955438614\n",
      "iteration 3498, dc_loss: 0.02698570489883423, tv_loss: 0.03375878557562828\n",
      "iteration 3499, dc_loss: 0.02697882056236267, tv_loss: 0.03376418724656105\n",
      "iteration 3500, dc_loss: 0.02697858214378357, tv_loss: 0.03376543149352074\n",
      "iteration 3501, dc_loss: 0.02697485312819481, tv_loss: 0.03376253694295883\n",
      "iteration 3502, dc_loss: 0.026978015899658203, tv_loss: 0.033758316189050674\n",
      "iteration 3503, dc_loss: 0.02697109803557396, tv_loss: 0.03376931697130203\n",
      "iteration 3504, dc_loss: 0.026965592056512833, tv_loss: 0.033772218972444534\n",
      "iteration 3505, dc_loss: 0.026969727128744125, tv_loss: 0.03376086801290512\n",
      "iteration 3506, dc_loss: 0.02696496993303299, tv_loss: 0.03376585990190506\n",
      "iteration 3507, dc_loss: 0.026964865624904633, tv_loss: 0.03376088663935661\n",
      "iteration 3508, dc_loss: 0.026959018781781197, tv_loss: 0.03376278653740883\n",
      "iteration 3509, dc_loss: 0.026960456743836403, tv_loss: 0.033769264817237854\n",
      "iteration 3510, dc_loss: 0.026954442262649536, tv_loss: 0.033777233213186264\n",
      "iteration 3511, dc_loss: 0.02695288136601448, tv_loss: 0.03377114608883858\n",
      "iteration 3512, dc_loss: 0.026954561471939087, tv_loss: 0.03375828266143799\n",
      "iteration 3513, dc_loss: 0.026950063183903694, tv_loss: 0.03376816213130951\n",
      "iteration 3514, dc_loss: 0.02694351226091385, tv_loss: 0.033771075308322906\n",
      "iteration 3515, dc_loss: 0.026938576251268387, tv_loss: 0.0337708555161953\n",
      "iteration 3516, dc_loss: 0.026946255937218666, tv_loss: 0.03376311436295509\n",
      "iteration 3517, dc_loss: 0.02694014273583889, tv_loss: 0.033765800297260284\n",
      "iteration 3518, dc_loss: 0.026936952024698257, tv_loss: 0.03376523032784462\n",
      "iteration 3519, dc_loss: 0.026938656345009804, tv_loss: 0.03375891223549843\n",
      "iteration 3520, dc_loss: 0.026933597400784492, tv_loss: 0.03376070410013199\n",
      "iteration 3521, dc_loss: 0.026928212493658066, tv_loss: 0.033762361854314804\n",
      "iteration 3522, dc_loss: 0.02692406065762043, tv_loss: 0.03376496583223343\n",
      "iteration 3523, dc_loss: 0.026929140090942383, tv_loss: 0.03375701233744621\n",
      "iteration 3524, dc_loss: 0.026927400380373, tv_loss: 0.03375871106982231\n",
      "iteration 3525, dc_loss: 0.026917386800050735, tv_loss: 0.03376650810241699\n",
      "iteration 3526, dc_loss: 0.02691708132624626, tv_loss: 0.03377183899283409\n",
      "iteration 3527, dc_loss: 0.026923006400465965, tv_loss: 0.03377462923526764\n",
      "iteration 3528, dc_loss: 0.026915349066257477, tv_loss: 0.03377526253461838\n",
      "iteration 3529, dc_loss: 0.026905469596385956, tv_loss: 0.03377090394496918\n",
      "iteration 3530, dc_loss: 0.026916369795799255, tv_loss: 0.0337694026529789\n",
      "iteration 3531, dc_loss: 0.02690432034432888, tv_loss: 0.033787861466407776\n",
      "iteration 3532, dc_loss: 0.02690243162214756, tv_loss: 0.0337730310857296\n",
      "iteration 3533, dc_loss: 0.026904018595814705, tv_loss: 0.03376927599310875\n",
      "iteration 3534, dc_loss: 0.026902902871370316, tv_loss: 0.03377571329474449\n",
      "iteration 3535, dc_loss: 0.02690458670258522, tv_loss: 0.033765919506549835\n",
      "iteration 3536, dc_loss: 0.02688860520720482, tv_loss: 0.033775851130485535\n",
      "iteration 3537, dc_loss: 0.026888258755207062, tv_loss: 0.03377602994441986\n",
      "iteration 3538, dc_loss: 0.026903562247753143, tv_loss: 0.03376718610525131\n",
      "iteration 3539, dc_loss: 0.026890261098742485, tv_loss: 0.03377116098999977\n",
      "iteration 3540, dc_loss: 0.026883261278271675, tv_loss: 0.03377461060881615\n",
      "iteration 3541, dc_loss: 0.02688109129667282, tv_loss: 0.033768463879823685\n",
      "iteration 3542, dc_loss: 0.026888445019721985, tv_loss: 0.0337689183652401\n",
      "iteration 3543, dc_loss: 0.02687905542552471, tv_loss: 0.033780358731746674\n",
      "iteration 3544, dc_loss: 0.02687862515449524, tv_loss: 0.03376879170536995\n",
      "iteration 3545, dc_loss: 0.026871539652347565, tv_loss: 0.03376983478665352\n",
      "iteration 3546, dc_loss: 0.026882000267505646, tv_loss: 0.03376642242074013\n",
      "iteration 3547, dc_loss: 0.02686982788145542, tv_loss: 0.03377976641058922\n",
      "iteration 3548, dc_loss: 0.026869844645261765, tv_loss: 0.03376908600330353\n",
      "iteration 3549, dc_loss: 0.026872385293245316, tv_loss: 0.033765826374292374\n",
      "iteration 3550, dc_loss: 0.026858309283852577, tv_loss: 0.03378064185380936\n",
      "iteration 3551, dc_loss: 0.02686256170272827, tv_loss: 0.03377458453178406\n",
      "iteration 3552, dc_loss: 0.02686203084886074, tv_loss: 0.03376837074756622\n",
      "iteration 3553, dc_loss: 0.026867877691984177, tv_loss: 0.033762574195861816\n",
      "iteration 3554, dc_loss: 0.02684868313372135, tv_loss: 0.033778611570596695\n",
      "iteration 3555, dc_loss: 0.02684575505554676, tv_loss: 0.03378022462129593\n",
      "iteration 3556, dc_loss: 0.0268680602312088, tv_loss: 0.033755991607904434\n",
      "iteration 3557, dc_loss: 0.026845911517739296, tv_loss: 0.03377401456236839\n",
      "iteration 3558, dc_loss: 0.026843473315238953, tv_loss: 0.033770736306905746\n",
      "iteration 3559, dc_loss: 0.026841051876544952, tv_loss: 0.03377017378807068\n",
      "iteration 3560, dc_loss: 0.026860849931836128, tv_loss: 0.03376117721199989\n",
      "iteration 3561, dc_loss: 0.026835152879357338, tv_loss: 0.03378172218799591\n",
      "iteration 3562, dc_loss: 0.026847494766116142, tv_loss: 0.033771954476833344\n",
      "iteration 3563, dc_loss: 0.026848873123526573, tv_loss: 0.03376426547765732\n",
      "iteration 3564, dc_loss: 0.0268473569303751, tv_loss: 0.033767737448215485\n",
      "iteration 3565, dc_loss: 0.026827214285731316, tv_loss: 0.03379112482070923\n",
      "iteration 3566, dc_loss: 0.026839686557650566, tv_loss: 0.03377864137291908\n",
      "iteration 3567, dc_loss: 0.02684778906404972, tv_loss: 0.03376120701432228\n",
      "iteration 3568, dc_loss: 0.026841823011636734, tv_loss: 0.03376713767647743\n",
      "iteration 3569, dc_loss: 0.02682678960263729, tv_loss: 0.03377935662865639\n",
      "iteration 3570, dc_loss: 0.026832064613699913, tv_loss: 0.03376081585884094\n",
      "iteration 3571, dc_loss: 0.026821402832865715, tv_loss: 0.033770713955163956\n",
      "iteration 3572, dc_loss: 0.02681655064225197, tv_loss: 0.0337674543261528\n",
      "iteration 3573, dc_loss: 0.026806868612766266, tv_loss: 0.03377709910273552\n",
      "iteration 3574, dc_loss: 0.02681676298379898, tv_loss: 0.03376338258385658\n",
      "iteration 3575, dc_loss: 0.02681647054851055, tv_loss: 0.033758584409952164\n",
      "iteration 3576, dc_loss: 0.026807526126503944, tv_loss: 0.033770378679037094\n",
      "iteration 3577, dc_loss: 0.026804737746715546, tv_loss: 0.03376956284046173\n",
      "iteration 3578, dc_loss: 0.02680346742272377, tv_loss: 0.033771123737096786\n",
      "iteration 3579, dc_loss: 0.026809649541974068, tv_loss: 0.03376463055610657\n",
      "iteration 3580, dc_loss: 0.026798153296113014, tv_loss: 0.033773377537727356\n",
      "iteration 3581, dc_loss: 0.026802221313118935, tv_loss: 0.03376462310552597\n",
      "iteration 3582, dc_loss: 0.026796480640769005, tv_loss: 0.033765945583581924\n",
      "iteration 3583, dc_loss: 0.02678903564810753, tv_loss: 0.03376973792910576\n",
      "iteration 3584, dc_loss: 0.026781437918543816, tv_loss: 0.03377603739500046\n",
      "iteration 3585, dc_loss: 0.026792239397764206, tv_loss: 0.03376009687781334\n",
      "iteration 3586, dc_loss: 0.026790428906679153, tv_loss: 0.033767543733119965\n",
      "iteration 3587, dc_loss: 0.026779673993587494, tv_loss: 0.033772196620702744\n",
      "iteration 3588, dc_loss: 0.026780489832162857, tv_loss: 0.03377797082066536\n",
      "iteration 3589, dc_loss: 0.02677340991795063, tv_loss: 0.03377563878893852\n",
      "iteration 3590, dc_loss: 0.026783030480146408, tv_loss: 0.03376651927828789\n",
      "iteration 3591, dc_loss: 0.026777470484375954, tv_loss: 0.03376718610525131\n",
      "iteration 3592, dc_loss: 0.026769235730171204, tv_loss: 0.03377049043774605\n",
      "iteration 3593, dc_loss: 0.026761163026094437, tv_loss: 0.033779505640268326\n",
      "iteration 3594, dc_loss: 0.026782196015119553, tv_loss: 0.033757228404283524\n",
      "iteration 3595, dc_loss: 0.02675727568566799, tv_loss: 0.03378299996256828\n",
      "iteration 3596, dc_loss: 0.02675551176071167, tv_loss: 0.03378162160515785\n",
      "iteration 3597, dc_loss: 0.026769433170557022, tv_loss: 0.033762965351343155\n",
      "iteration 3598, dc_loss: 0.026762237772345543, tv_loss: 0.033764783293008804\n",
      "iteration 3599, dc_loss: 0.026746856048703194, tv_loss: 0.03377661854028702\n",
      "iteration 3600, dc_loss: 0.026751689612865448, tv_loss: 0.03377315402030945\n",
      "iteration 3601, dc_loss: 0.02675982564687729, tv_loss: 0.033773940056562424\n",
      "iteration 3602, dc_loss: 0.026742329820990562, tv_loss: 0.03377518430352211\n",
      "iteration 3603, dc_loss: 0.026740195229649544, tv_loss: 0.03377458080649376\n",
      "iteration 3604, dc_loss: 0.026754962280392647, tv_loss: 0.033761944621801376\n",
      "iteration 3605, dc_loss: 0.02674981951713562, tv_loss: 0.0337652824819088\n",
      "iteration 3606, dc_loss: 0.02673518843948841, tv_loss: 0.03377093747258186\n",
      "iteration 3607, dc_loss: 0.02673693746328354, tv_loss: 0.03377184644341469\n",
      "iteration 3608, dc_loss: 0.02674066461622715, tv_loss: 0.03376167640089989\n",
      "iteration 3609, dc_loss: 0.026738692075014114, tv_loss: 0.03376815840601921\n",
      "iteration 3610, dc_loss: 0.02673206850886345, tv_loss: 0.03376491367816925\n",
      "iteration 3611, dc_loss: 0.026732614263892174, tv_loss: 0.03376574441790581\n",
      "iteration 3612, dc_loss: 0.02673209272325039, tv_loss: 0.03376656770706177\n",
      "iteration 3613, dc_loss: 0.0267298873513937, tv_loss: 0.033770862966775894\n",
      "iteration 3614, dc_loss: 0.02672659419476986, tv_loss: 0.03376251086592674\n",
      "iteration 3615, dc_loss: 0.02672749198973179, tv_loss: 0.03376045078039169\n",
      "iteration 3616, dc_loss: 0.02672567218542099, tv_loss: 0.033766694366931915\n",
      "iteration 3617, dc_loss: 0.026723386719822884, tv_loss: 0.033769119530916214\n",
      "iteration 3618, dc_loss: 0.026717236265540123, tv_loss: 0.03377184271812439\n",
      "iteration 3619, dc_loss: 0.026714671403169632, tv_loss: 0.03376414626836777\n",
      "iteration 3620, dc_loss: 0.02671717293560505, tv_loss: 0.03376581892371178\n",
      "iteration 3621, dc_loss: 0.026719579473137856, tv_loss: 0.03376379609107971\n",
      "iteration 3622, dc_loss: 0.026713106781244278, tv_loss: 0.03376714885234833\n",
      "iteration 3623, dc_loss: 0.02670644409954548, tv_loss: 0.0337706096470356\n",
      "iteration 3624, dc_loss: 0.026710649952292442, tv_loss: 0.033760443329811096\n",
      "iteration 3625, dc_loss: 0.026712242513895035, tv_loss: 0.03376556932926178\n",
      "iteration 3626, dc_loss: 0.026705576106905937, tv_loss: 0.03376760333776474\n",
      "iteration 3627, dc_loss: 0.02670341730117798, tv_loss: 0.03376644477248192\n",
      "iteration 3628, dc_loss: 0.026703348383307457, tv_loss: 0.033763933926820755\n",
      "iteration 3629, dc_loss: 0.026700278744101524, tv_loss: 0.03376093506813049\n",
      "iteration 3630, dc_loss: 0.026696419343352318, tv_loss: 0.033774640411138535\n",
      "iteration 3631, dc_loss: 0.026697205379605293, tv_loss: 0.03376585245132446\n",
      "iteration 3632, dc_loss: 0.026702171191573143, tv_loss: 0.033757973462343216\n",
      "iteration 3633, dc_loss: 0.02669227309525013, tv_loss: 0.033766746520996094\n",
      "iteration 3634, dc_loss: 0.026684390380978584, tv_loss: 0.03377468138933182\n",
      "iteration 3635, dc_loss: 0.02669263817369938, tv_loss: 0.03376079350709915\n",
      "iteration 3636, dc_loss: 0.026696089655160904, tv_loss: 0.033755362033843994\n",
      "iteration 3637, dc_loss: 0.02668476291000843, tv_loss: 0.03376271575689316\n",
      "iteration 3638, dc_loss: 0.026680681854486465, tv_loss: 0.033769648522138596\n",
      "iteration 3639, dc_loss: 0.02668275125324726, tv_loss: 0.03376556187868118\n",
      "iteration 3640, dc_loss: 0.026687854900956154, tv_loss: 0.03376467525959015\n",
      "iteration 3641, dc_loss: 0.0266824159771204, tv_loss: 0.03375984728336334\n",
      "iteration 3642, dc_loss: 0.026673773303627968, tv_loss: 0.033771153539419174\n",
      "iteration 3643, dc_loss: 0.026671836152672768, tv_loss: 0.03376969322562218\n",
      "iteration 3644, dc_loss: 0.02667979709804058, tv_loss: 0.03376515209674835\n",
      "iteration 3645, dc_loss: 0.026678774505853653, tv_loss: 0.033763907849788666\n",
      "iteration 3646, dc_loss: 0.02666369266808033, tv_loss: 0.0337713323533535\n",
      "iteration 3647, dc_loss: 0.02666558139026165, tv_loss: 0.03376736119389534\n",
      "iteration 3648, dc_loss: 0.026675811037421227, tv_loss: 0.03375692665576935\n",
      "iteration 3649, dc_loss: 0.02666626311838627, tv_loss: 0.033762287348508835\n",
      "iteration 3650, dc_loss: 0.026657307520508766, tv_loss: 0.033772993832826614\n",
      "iteration 3651, dc_loss: 0.026664596050977707, tv_loss: 0.03375851735472679\n",
      "iteration 3652, dc_loss: 0.026661857962608337, tv_loss: 0.03375917673110962\n",
      "iteration 3653, dc_loss: 0.02665778063237667, tv_loss: 0.033766020089387894\n",
      "iteration 3654, dc_loss: 0.026657508686184883, tv_loss: 0.033761125057935715\n",
      "iteration 3655, dc_loss: 0.02665563113987446, tv_loss: 0.03376815468072891\n",
      "iteration 3656, dc_loss: 0.02665363997220993, tv_loss: 0.03376206383109093\n",
      "iteration 3657, dc_loss: 0.026651041582226753, tv_loss: 0.033766720443964005\n",
      "iteration 3658, dc_loss: 0.026650413870811462, tv_loss: 0.03376423940062523\n",
      "iteration 3659, dc_loss: 0.02664889581501484, tv_loss: 0.03376419097185135\n",
      "iteration 3660, dc_loss: 0.02664310112595558, tv_loss: 0.03376908227801323\n",
      "iteration 3661, dc_loss: 0.026644188910722733, tv_loss: 0.03376280516386032\n",
      "iteration 3662, dc_loss: 0.02664734609425068, tv_loss: 0.033758874982595444\n",
      "iteration 3663, dc_loss: 0.02664303407073021, tv_loss: 0.03376081958413124\n",
      "iteration 3664, dc_loss: 0.026640407741069794, tv_loss: 0.03376034274697304\n",
      "iteration 3665, dc_loss: 0.02663777954876423, tv_loss: 0.03376546502113342\n",
      "iteration 3666, dc_loss: 0.026632023975253105, tv_loss: 0.0337696447968483\n",
      "iteration 3667, dc_loss: 0.026637017726898193, tv_loss: 0.033768363296985626\n",
      "iteration 3668, dc_loss: 0.02663560025393963, tv_loss: 0.03375772759318352\n",
      "iteration 3669, dc_loss: 0.026630667969584465, tv_loss: 0.033763568848371506\n",
      "iteration 3670, dc_loss: 0.026629969477653503, tv_loss: 0.033765826374292374\n",
      "iteration 3671, dc_loss: 0.026625007390975952, tv_loss: 0.03376694768667221\n",
      "iteration 3672, dc_loss: 0.026624280959367752, tv_loss: 0.03377411887049675\n",
      "iteration 3673, dc_loss: 0.02662617526948452, tv_loss: 0.0337597094476223\n",
      "iteration 3674, dc_loss: 0.026622353121638298, tv_loss: 0.033764779567718506\n",
      "iteration 3675, dc_loss: 0.026619266718626022, tv_loss: 0.03377307578921318\n",
      "iteration 3676, dc_loss: 0.026619942858815193, tv_loss: 0.03376748785376549\n",
      "iteration 3677, dc_loss: 0.026620885357260704, tv_loss: 0.03376739099621773\n",
      "iteration 3678, dc_loss: 0.02661256492137909, tv_loss: 0.033765774220228195\n",
      "iteration 3679, dc_loss: 0.02661062404513359, tv_loss: 0.03376976400613785\n",
      "iteration 3680, dc_loss: 0.026616346091032028, tv_loss: 0.03376742824912071\n",
      "iteration 3681, dc_loss: 0.026608912274241447, tv_loss: 0.033776503056287766\n",
      "iteration 3682, dc_loss: 0.02660568803548813, tv_loss: 0.033768005669116974\n",
      "iteration 3683, dc_loss: 0.026608018204569817, tv_loss: 0.03376556932926178\n",
      "iteration 3684, dc_loss: 0.026604538783431053, tv_loss: 0.0337691493332386\n",
      "iteration 3685, dc_loss: 0.026603683829307556, tv_loss: 0.033776018768548965\n",
      "iteration 3686, dc_loss: 0.0265997052192688, tv_loss: 0.03377528116106987\n",
      "iteration 3687, dc_loss: 0.026603087782859802, tv_loss: 0.03376173600554466\n",
      "iteration 3688, dc_loss: 0.026599418371915817, tv_loss: 0.033772062510252\n",
      "iteration 3689, dc_loss: 0.02659394033253193, tv_loss: 0.03378250449895859\n",
      "iteration 3690, dc_loss: 0.02659478969871998, tv_loss: 0.033771809190511703\n",
      "iteration 3691, dc_loss: 0.026591520756483078, tv_loss: 0.033777642995119095\n",
      "iteration 3692, dc_loss: 0.026587748900055885, tv_loss: 0.03377946838736534\n",
      "iteration 3693, dc_loss: 0.026593569666147232, tv_loss: 0.03377262130379677\n",
      "iteration 3694, dc_loss: 0.02658919431269169, tv_loss: 0.03376755490899086\n",
      "iteration 3695, dc_loss: 0.026581332087516785, tv_loss: 0.03378063067793846\n",
      "iteration 3696, dc_loss: 0.02658224292099476, tv_loss: 0.03378068655729294\n",
      "iteration 3697, dc_loss: 0.026585135608911514, tv_loss: 0.033767927438020706\n",
      "iteration 3698, dc_loss: 0.026585569605231285, tv_loss: 0.033763837069272995\n",
      "iteration 3699, dc_loss: 0.026579076424241066, tv_loss: 0.03377731144428253\n",
      "iteration 3700, dc_loss: 0.026572223752737045, tv_loss: 0.03377911075949669\n",
      "iteration 3701, dc_loss: 0.026575647294521332, tv_loss: 0.033767666667699814\n",
      "iteration 3702, dc_loss: 0.026577623561024666, tv_loss: 0.033764660358428955\n",
      "iteration 3703, dc_loss: 0.026570124551653862, tv_loss: 0.033774588257074356\n",
      "iteration 3704, dc_loss: 0.026566417887806892, tv_loss: 0.033775437623262405\n",
      "iteration 3705, dc_loss: 0.026570802554488182, tv_loss: 0.03376075625419617\n",
      "iteration 3706, dc_loss: 0.026572076603770256, tv_loss: 0.03376368060708046\n",
      "iteration 3707, dc_loss: 0.02656189352273941, tv_loss: 0.033772312104701996\n",
      "iteration 3708, dc_loss: 0.02655806578695774, tv_loss: 0.03377719596028328\n",
      "iteration 3709, dc_loss: 0.026564735919237137, tv_loss: 0.03376823663711548\n",
      "iteration 3710, dc_loss: 0.026564301922917366, tv_loss: 0.033759839832782745\n",
      "iteration 3711, dc_loss: 0.02655668929219246, tv_loss: 0.03376831114292145\n",
      "iteration 3712, dc_loss: 0.02655070833861828, tv_loss: 0.03377494588494301\n",
      "iteration 3713, dc_loss: 0.026555592194199562, tv_loss: 0.033763643354177475\n",
      "iteration 3714, dc_loss: 0.026558004319667816, tv_loss: 0.03376429155468941\n",
      "iteration 3715, dc_loss: 0.0265493281185627, tv_loss: 0.03376971557736397\n",
      "iteration 3716, dc_loss: 0.026548141613602638, tv_loss: 0.03376500681042671\n",
      "iteration 3717, dc_loss: 0.026549162343144417, tv_loss: 0.0337660051882267\n",
      "iteration 3718, dc_loss: 0.02654864266514778, tv_loss: 0.03376438468694687\n",
      "iteration 3719, dc_loss: 0.026542650535702705, tv_loss: 0.03376775607466698\n",
      "iteration 3720, dc_loss: 0.026537401601672173, tv_loss: 0.0337749682366848\n",
      "iteration 3721, dc_loss: 0.02654230408370495, tv_loss: 0.03376994654536247\n",
      "iteration 3722, dc_loss: 0.02654154971241951, tv_loss: 0.03376525267958641\n",
      "iteration 3723, dc_loss: 0.026532717049121857, tv_loss: 0.03377234935760498\n",
      "iteration 3724, dc_loss: 0.026535257697105408, tv_loss: 0.033768199384212494\n",
      "iteration 3725, dc_loss: 0.026537159457802773, tv_loss: 0.033763669431209564\n",
      "iteration 3726, dc_loss: 0.026533083990216255, tv_loss: 0.0337679348886013\n",
      "iteration 3727, dc_loss: 0.026527319103479385, tv_loss: 0.03377167135477066\n",
      "iteration 3728, dc_loss: 0.026531759649515152, tv_loss: 0.03376098722219467\n",
      "iteration 3729, dc_loss: 0.026527466252446175, tv_loss: 0.03376027196645737\n",
      "iteration 3730, dc_loss: 0.026523267850279808, tv_loss: 0.03376990929245949\n",
      "iteration 3731, dc_loss: 0.026524489745497704, tv_loss: 0.033769503235816956\n",
      "iteration 3732, dc_loss: 0.02652243711054325, tv_loss: 0.0337715819478035\n",
      "iteration 3733, dc_loss: 0.026519915089011192, tv_loss: 0.033776748925447464\n",
      "iteration 3734, dc_loss: 0.026516018435359, tv_loss: 0.033772584050893784\n",
      "iteration 3735, dc_loss: 0.026519466191530228, tv_loss: 0.03376263007521629\n",
      "iteration 3736, dc_loss: 0.02651466429233551, tv_loss: 0.033768195658922195\n",
      "iteration 3737, dc_loss: 0.026507699862122536, tv_loss: 0.03377542272210121\n",
      "iteration 3738, dc_loss: 0.026513494551181793, tv_loss: 0.033771272748708725\n",
      "iteration 3739, dc_loss: 0.02651371620595455, tv_loss: 0.03376496583223343\n",
      "iteration 3740, dc_loss: 0.026508016511797905, tv_loss: 0.033767811954021454\n",
      "iteration 3741, dc_loss: 0.02650264836847782, tv_loss: 0.03377329930663109\n",
      "iteration 3742, dc_loss: 0.026505010202527046, tv_loss: 0.03377363085746765\n",
      "iteration 3743, dc_loss: 0.026505663990974426, tv_loss: 0.0337660126388073\n",
      "iteration 3744, dc_loss: 0.026499878615140915, tv_loss: 0.03377137705683708\n",
      "iteration 3745, dc_loss: 0.026500936597585678, tv_loss: 0.03376840427517891\n",
      "iteration 3746, dc_loss: 0.0264954324811697, tv_loss: 0.033770885318517685\n",
      "iteration 3747, dc_loss: 0.02649456448853016, tv_loss: 0.03377261757850647\n",
      "iteration 3748, dc_loss: 0.026498539373278618, tv_loss: 0.033777084201574326\n",
      "iteration 3749, dc_loss: 0.026491178199648857, tv_loss: 0.03377383574843407\n",
      "iteration 3750, dc_loss: 0.026487823575735092, tv_loss: 0.03377580642700195\n",
      "iteration 3751, dc_loss: 0.026490774005651474, tv_loss: 0.03377262130379677\n",
      "iteration 3752, dc_loss: 0.02648702822625637, tv_loss: 0.03378436341881752\n",
      "iteration 3753, dc_loss: 0.026485111564397812, tv_loss: 0.03377193957567215\n",
      "iteration 3754, dc_loss: 0.026485275477170944, tv_loss: 0.03377189487218857\n",
      "iteration 3755, dc_loss: 0.026481525972485542, tv_loss: 0.03377879410982132\n",
      "iteration 3756, dc_loss: 0.026480309665203094, tv_loss: 0.03377467766404152\n",
      "iteration 3757, dc_loss: 0.026476413011550903, tv_loss: 0.03377656266093254\n",
      "iteration 3758, dc_loss: 0.026478320360183716, tv_loss: 0.03377334028482437\n",
      "iteration 3759, dc_loss: 0.026479367166757584, tv_loss: 0.033771924674510956\n",
      "iteration 3760, dc_loss: 0.02647130936384201, tv_loss: 0.03378073498606682\n",
      "iteration 3761, dc_loss: 0.02647075615823269, tv_loss: 0.03377188742160797\n",
      "iteration 3762, dc_loss: 0.026473654434084892, tv_loss: 0.033768534660339355\n",
      "iteration 3763, dc_loss: 0.02647050842642784, tv_loss: 0.0337742380797863\n",
      "iteration 3764, dc_loss: 0.026464054360985756, tv_loss: 0.03377397358417511\n",
      "iteration 3765, dc_loss: 0.02646511048078537, tv_loss: 0.033771973103284836\n",
      "iteration 3766, dc_loss: 0.026464808732271194, tv_loss: 0.03376907855272293\n",
      "iteration 3767, dc_loss: 0.026460127905011177, tv_loss: 0.03377113118767738\n",
      "iteration 3768, dc_loss: 0.026460349559783936, tv_loss: 0.033775217831134796\n",
      "iteration 3769, dc_loss: 0.026461821049451828, tv_loss: 0.033765945583581924\n",
      "iteration 3770, dc_loss: 0.026454469189047813, tv_loss: 0.033770762383937836\n",
      "iteration 3771, dc_loss: 0.026455506682395935, tv_loss: 0.033773716539144516\n",
      "iteration 3772, dc_loss: 0.026456492021679878, tv_loss: 0.0337645560503006\n",
      "iteration 3773, dc_loss: 0.02644987590610981, tv_loss: 0.03376930207014084\n",
      "iteration 3774, dc_loss: 0.026448315009474754, tv_loss: 0.03377695754170418\n",
      "iteration 3775, dc_loss: 0.026447031646966934, tv_loss: 0.03377152234315872\n",
      "iteration 3776, dc_loss: 0.026449520140886307, tv_loss: 0.033768560737371445\n",
      "iteration 3777, dc_loss: 0.0264446921646595, tv_loss: 0.03377305716276169\n",
      "iteration 3778, dc_loss: 0.026440566405653954, tv_loss: 0.0337701141834259\n",
      "iteration 3779, dc_loss: 0.026445019990205765, tv_loss: 0.03376693278551102\n",
      "iteration 3780, dc_loss: 0.02643837220966816, tv_loss: 0.03377550467848778\n",
      "iteration 3781, dc_loss: 0.026436276733875275, tv_loss: 0.03376728296279907\n",
      "iteration 3782, dc_loss: 0.026441847905516624, tv_loss: 0.03376644477248192\n",
      "iteration 3783, dc_loss: 0.026437288150191307, tv_loss: 0.033771805465221405\n",
      "iteration 3784, dc_loss: 0.026429282501339912, tv_loss: 0.03377068042755127\n",
      "iteration 3785, dc_loss: 0.026429658755660057, tv_loss: 0.03377145156264305\n",
      "iteration 3786, dc_loss: 0.026428941637277603, tv_loss: 0.033771224319934845\n",
      "iteration 3787, dc_loss: 0.026427818462252617, tv_loss: 0.03377891331911087\n",
      "iteration 3788, dc_loss: 0.02643078751862049, tv_loss: 0.03376865014433861\n",
      "iteration 3789, dc_loss: 0.026422856375575066, tv_loss: 0.03377385437488556\n",
      "iteration 3790, dc_loss: 0.026418475434184074, tv_loss: 0.03377640247344971\n",
      "iteration 3791, dc_loss: 0.026422781869769096, tv_loss: 0.033769749104976654\n",
      "iteration 3792, dc_loss: 0.026424728333950043, tv_loss: 0.03377319127321243\n",
      "iteration 3793, dc_loss: 0.026416946202516556, tv_loss: 0.03377268835902214\n",
      "iteration 3794, dc_loss: 0.026413723826408386, tv_loss: 0.03377443552017212\n",
      "iteration 3795, dc_loss: 0.026417650282382965, tv_loss: 0.033768098801374435\n",
      "iteration 3796, dc_loss: 0.02641213871538639, tv_loss: 0.03378089517354965\n",
      "iteration 3797, dc_loss: 0.026408342644572258, tv_loss: 0.03378230333328247\n",
      "iteration 3798, dc_loss: 0.026410358026623726, tv_loss: 0.03377168998122215\n",
      "iteration 3799, dc_loss: 0.026409637182950974, tv_loss: 0.033771563321352005\n",
      "iteration 3800, dc_loss: 0.02640443481504917, tv_loss: 0.03377824276685715\n",
      "iteration 3801, dc_loss: 0.026406561955809593, tv_loss: 0.03378107398748398\n",
      "iteration 3802, dc_loss: 0.02640562877058983, tv_loss: 0.0337732657790184\n",
      "iteration 3803, dc_loss: 0.0263963770121336, tv_loss: 0.03378203883767128\n",
      "iteration 3804, dc_loss: 0.02639298513531685, tv_loss: 0.03378578647971153\n",
      "iteration 3805, dc_loss: 0.026400770992040634, tv_loss: 0.03377643600106239\n",
      "iteration 3806, dc_loss: 0.026401618495583534, tv_loss: 0.03376965597271919\n",
      "iteration 3807, dc_loss: 0.026392409577965736, tv_loss: 0.03377334773540497\n",
      "iteration 3808, dc_loss: 0.026386979967355728, tv_loss: 0.033781588077545166\n",
      "iteration 3809, dc_loss: 0.02639451064169407, tv_loss: 0.033771365880966187\n",
      "iteration 3810, dc_loss: 0.026396047323942184, tv_loss: 0.03377125412225723\n",
      "iteration 3811, dc_loss: 0.02638668566942215, tv_loss: 0.03377106040716171\n",
      "iteration 3812, dc_loss: 0.02637750469148159, tv_loss: 0.03377741202712059\n",
      "iteration 3813, dc_loss: 0.026384077966213226, tv_loss: 0.033770766109228134\n",
      "iteration 3814, dc_loss: 0.02638549730181694, tv_loss: 0.03376724570989609\n",
      "iteration 3815, dc_loss: 0.026378275826573372, tv_loss: 0.03377695381641388\n",
      "iteration 3816, dc_loss: 0.026381414383649826, tv_loss: 0.03377191349864006\n",
      "iteration 3817, dc_loss: 0.026377808302640915, tv_loss: 0.03377380222082138\n",
      "iteration 3818, dc_loss: 0.02637815847992897, tv_loss: 0.03377002477645874\n",
      "iteration 3819, dc_loss: 0.026371710002422333, tv_loss: 0.033773090690374374\n",
      "iteration 3820, dc_loss: 0.026370834559202194, tv_loss: 0.033772047609090805\n",
      "iteration 3821, dc_loss: 0.026374222710728645, tv_loss: 0.03376619145274162\n",
      "iteration 3822, dc_loss: 0.026372091844677925, tv_loss: 0.03376772999763489\n",
      "iteration 3823, dc_loss: 0.026361564174294472, tv_loss: 0.033777039498090744\n",
      "iteration 3824, dc_loss: 0.02636200189590454, tv_loss: 0.03377223759889603\n",
      "iteration 3825, dc_loss: 0.026364939287304878, tv_loss: 0.03376852348446846\n",
      "iteration 3826, dc_loss: 0.02636944130063057, tv_loss: 0.03376192972064018\n",
      "iteration 3827, dc_loss: 0.026357734575867653, tv_loss: 0.033774808049201965\n",
      "iteration 3828, dc_loss: 0.026355665177106857, tv_loss: 0.03377498686313629\n",
      "iteration 3829, dc_loss: 0.02636202983558178, tv_loss: 0.03376300260424614\n",
      "iteration 3830, dc_loss: 0.026355892419815063, tv_loss: 0.03376689925789833\n",
      "iteration 3831, dc_loss: 0.026350872591137886, tv_loss: 0.03377654030919075\n",
      "iteration 3832, dc_loss: 0.026348644867539406, tv_loss: 0.03377449885010719\n",
      "iteration 3833, dc_loss: 0.026353605091571808, tv_loss: 0.03377004712820053\n",
      "iteration 3834, dc_loss: 0.026352351531386375, tv_loss: 0.03376665338873863\n",
      "iteration 3835, dc_loss: 0.026344893500208855, tv_loss: 0.03377757593989372\n",
      "iteration 3836, dc_loss: 0.026342591270804405, tv_loss: 0.033779021352529526\n",
      "iteration 3837, dc_loss: 0.02634395845234394, tv_loss: 0.033771343529224396\n",
      "iteration 3838, dc_loss: 0.02634301409125328, tv_loss: 0.03377140685915947\n",
      "iteration 3839, dc_loss: 0.02634373866021633, tv_loss: 0.03376997634768486\n",
      "iteration 3840, dc_loss: 0.02633804827928543, tv_loss: 0.033773571252822876\n",
      "iteration 3841, dc_loss: 0.026334216818213463, tv_loss: 0.03377286717295647\n",
      "iteration 3842, dc_loss: 0.02633501961827278, tv_loss: 0.03377772122621536\n",
      "iteration 3843, dc_loss: 0.026333920657634735, tv_loss: 0.033777572214603424\n",
      "iteration 3844, dc_loss: 0.026336779817938805, tv_loss: 0.03377091512084007\n",
      "iteration 3845, dc_loss: 0.026323705911636353, tv_loss: 0.0337810218334198\n",
      "iteration 3846, dc_loss: 0.026332233101129532, tv_loss: 0.03376953303813934\n",
      "iteration 3847, dc_loss: 0.026331055909395218, tv_loss: 0.033769574016332626\n",
      "iteration 3848, dc_loss: 0.02631979249417782, tv_loss: 0.033786699175834656\n",
      "iteration 3849, dc_loss: 0.026320680975914, tv_loss: 0.03378383815288544\n",
      "iteration 3850, dc_loss: 0.026325881481170654, tv_loss: 0.03376820310950279\n",
      "iteration 3851, dc_loss: 0.026320770382881165, tv_loss: 0.033772993832826614\n",
      "iteration 3852, dc_loss: 0.026319414377212524, tv_loss: 0.03378168120980263\n",
      "iteration 3853, dc_loss: 0.026314040645956993, tv_loss: 0.0337836891412735\n",
      "iteration 3854, dc_loss: 0.026316655799746513, tv_loss: 0.03377210348844528\n",
      "iteration 3855, dc_loss: 0.02631491981446743, tv_loss: 0.03377364203333855\n",
      "iteration 3856, dc_loss: 0.02630949392914772, tv_loss: 0.03377909958362579\n",
      "iteration 3857, dc_loss: 0.02631068415939808, tv_loss: 0.03377141058444977\n",
      "iteration 3858, dc_loss: 0.026309261098504066, tv_loss: 0.03377343714237213\n",
      "iteration 3859, dc_loss: 0.026305969804525375, tv_loss: 0.03377337008714676\n",
      "iteration 3860, dc_loss: 0.02630433812737465, tv_loss: 0.03377312421798706\n",
      "iteration 3861, dc_loss: 0.026307832449674606, tv_loss: 0.033769454807043076\n",
      "iteration 3862, dc_loss: 0.02630133181810379, tv_loss: 0.033770132809877396\n",
      "iteration 3863, dc_loss: 0.026297107338905334, tv_loss: 0.03377450630068779\n",
      "iteration 3864, dc_loss: 0.02630295790731907, tv_loss: 0.03376537561416626\n",
      "iteration 3865, dc_loss: 0.0262969508767128, tv_loss: 0.03376814350485802\n",
      "iteration 3866, dc_loss: 0.02629459649324417, tv_loss: 0.03377065435051918\n",
      "iteration 3867, dc_loss: 0.026293179020285606, tv_loss: 0.0337713360786438\n",
      "iteration 3868, dc_loss: 0.026291238144040108, tv_loss: 0.0337698869407177\n",
      "iteration 3869, dc_loss: 0.026295268908143044, tv_loss: 0.03376489505171776\n",
      "iteration 3870, dc_loss: 0.026287177577614784, tv_loss: 0.03377365693449974\n",
      "iteration 3871, dc_loss: 0.026283638551831245, tv_loss: 0.03378294035792351\n",
      "iteration 3872, dc_loss: 0.026285216212272644, tv_loss: 0.03378131240606308\n",
      "iteration 3873, dc_loss: 0.026287920773029327, tv_loss: 0.03377560153603554\n",
      "iteration 3874, dc_loss: 0.026282140985131264, tv_loss: 0.03377413749694824\n",
      "iteration 3875, dc_loss: 0.026278305798768997, tv_loss: 0.03377428278326988\n",
      "iteration 3876, dc_loss: 0.026280609890818596, tv_loss: 0.03377504274249077\n",
      "iteration 3877, dc_loss: 0.02627730369567871, tv_loss: 0.033782780170440674\n",
      "iteration 3878, dc_loss: 0.026271913200616837, tv_loss: 0.03378310427069664\n",
      "iteration 3879, dc_loss: 0.0262764785438776, tv_loss: 0.03377538546919823\n",
      "iteration 3880, dc_loss: 0.02627405896782875, tv_loss: 0.03377785533666611\n",
      "iteration 3881, dc_loss: 0.02626713365316391, tv_loss: 0.033782150596380234\n",
      "iteration 3882, dc_loss: 0.026270847767591476, tv_loss: 0.033777255564928055\n",
      "iteration 3883, dc_loss: 0.026271646842360497, tv_loss: 0.033774059265851974\n",
      "iteration 3884, dc_loss: 0.026262085884809494, tv_loss: 0.03378422558307648\n",
      "iteration 3885, dc_loss: 0.026257866993546486, tv_loss: 0.03378630802035332\n",
      "iteration 3886, dc_loss: 0.02626684121787548, tv_loss: 0.03377343714237213\n",
      "iteration 3887, dc_loss: 0.02626267820596695, tv_loss: 0.033781748265028\n",
      "iteration 3888, dc_loss: 0.026254119351506233, tv_loss: 0.03378009423613548\n",
      "iteration 3889, dc_loss: 0.026262162253260612, tv_loss: 0.033774688839912415\n",
      "iteration 3890, dc_loss: 0.026257110759615898, tv_loss: 0.033783722668886185\n",
      "iteration 3891, dc_loss: 0.02624882198870182, tv_loss: 0.0337807834148407\n",
      "iteration 3892, dc_loss: 0.026255620643496513, tv_loss: 0.03377693518996239\n",
      "iteration 3893, dc_loss: 0.02625374123454094, tv_loss: 0.033777590841054916\n",
      "iteration 3894, dc_loss: 0.02624817192554474, tv_loss: 0.03377581387758255\n",
      "iteration 3895, dc_loss: 0.02624676190316677, tv_loss: 0.0337812677025795\n",
      "iteration 3896, dc_loss: 0.026243846863508224, tv_loss: 0.03378070890903473\n",
      "iteration 3897, dc_loss: 0.026248516514897346, tv_loss: 0.03377380594611168\n",
      "iteration 3898, dc_loss: 0.026239857077598572, tv_loss: 0.033780913800001144\n",
      "iteration 3899, dc_loss: 0.026235200464725494, tv_loss: 0.0337795689702034\n",
      "iteration 3900, dc_loss: 0.026247352361679077, tv_loss: 0.0337715782225132\n",
      "iteration 3901, dc_loss: 0.026237519457936287, tv_loss: 0.0337788350880146\n",
      "iteration 3902, dc_loss: 0.026230329647660255, tv_loss: 0.03377701714634895\n",
      "iteration 3903, dc_loss: 0.026236699894070625, tv_loss: 0.03377235308289528\n",
      "iteration 3904, dc_loss: 0.026238825172185898, tv_loss: 0.03377126529812813\n",
      "iteration 3905, dc_loss: 0.026232212781906128, tv_loss: 0.03377682715654373\n",
      "iteration 3906, dc_loss: 0.026222169399261475, tv_loss: 0.033790893852710724\n",
      "iteration 3907, dc_loss: 0.026227563619613647, tv_loss: 0.03377868980169296\n",
      "iteration 3908, dc_loss: 0.026230521500110626, tv_loss: 0.033771906048059464\n",
      "iteration 3909, dc_loss: 0.02622448280453682, tv_loss: 0.033773548901081085\n",
      "iteration 3910, dc_loss: 0.026222804561257362, tv_loss: 0.033778171986341476\n",
      "iteration 3911, dc_loss: 0.026219941675662994, tv_loss: 0.03378395736217499\n",
      "iteration 3912, dc_loss: 0.026219964027404785, tv_loss: 0.03378044813871384\n",
      "iteration 3913, dc_loss: 0.026220830157399178, tv_loss: 0.03376886621117592\n",
      "iteration 3914, dc_loss: 0.026212330907583237, tv_loss: 0.03378620743751526\n",
      "iteration 3915, dc_loss: 0.026212602853775024, tv_loss: 0.03378161042928696\n",
      "iteration 3916, dc_loss: 0.026215611025691032, tv_loss: 0.03377684950828552\n",
      "iteration 3917, dc_loss: 0.026215625926852226, tv_loss: 0.03377428650856018\n",
      "iteration 3918, dc_loss: 0.026204735040664673, tv_loss: 0.033780839294195175\n",
      "iteration 3919, dc_loss: 0.02620766870677471, tv_loss: 0.03378484398126602\n",
      "iteration 3920, dc_loss: 0.0262125376611948, tv_loss: 0.03377958759665489\n",
      "iteration 3921, dc_loss: 0.026199042797088623, tv_loss: 0.033784057945013046\n",
      "iteration 3922, dc_loss: 0.026204435154795647, tv_loss: 0.033776961266994476\n",
      "iteration 3923, dc_loss: 0.02620496042072773, tv_loss: 0.0337776243686676\n",
      "iteration 3924, dc_loss: 0.026200255379080772, tv_loss: 0.033786285668611526\n",
      "iteration 3925, dc_loss: 0.026196938008069992, tv_loss: 0.03378034383058548\n",
      "iteration 3926, dc_loss: 0.026203572750091553, tv_loss: 0.03377244994044304\n",
      "iteration 3927, dc_loss: 0.026190534234046936, tv_loss: 0.03379261493682861\n",
      "iteration 3928, dc_loss: 0.026190854609012604, tv_loss: 0.033780504018068314\n",
      "iteration 3929, dc_loss: 0.026194218546152115, tv_loss: 0.03377869352698326\n",
      "iteration 3930, dc_loss: 0.026192156597971916, tv_loss: 0.03378244861960411\n",
      "iteration 3931, dc_loss: 0.026185182854533195, tv_loss: 0.033783987164497375\n",
      "iteration 3932, dc_loss: 0.026188910007476807, tv_loss: 0.03377921134233475\n",
      "iteration 3933, dc_loss: 0.02618728205561638, tv_loss: 0.03377586603164673\n",
      "iteration 3934, dc_loss: 0.026184218004345894, tv_loss: 0.03377755731344223\n",
      "iteration 3935, dc_loss: 0.026178516447544098, tv_loss: 0.03378289192914963\n",
      "iteration 3936, dc_loss: 0.02618379332125187, tv_loss: 0.033770669251680374\n",
      "iteration 3937, dc_loss: 0.02617751434445381, tv_loss: 0.03378354385495186\n",
      "iteration 3938, dc_loss: 0.026172811165452003, tv_loss: 0.03378399461507797\n",
      "iteration 3939, dc_loss: 0.026178857311606407, tv_loss: 0.0337790809571743\n",
      "iteration 3940, dc_loss: 0.02617599070072174, tv_loss: 0.03378216177225113\n",
      "iteration 3941, dc_loss: 0.026171086356043816, tv_loss: 0.033778708428144455\n",
      "iteration 3942, dc_loss: 0.0261723343282938, tv_loss: 0.03378072753548622\n",
      "iteration 3943, dc_loss: 0.026164522394537926, tv_loss: 0.03379480168223381\n",
      "iteration 3944, dc_loss: 0.026169560849666595, tv_loss: 0.03378356248140335\n",
      "iteration 3945, dc_loss: 0.026169724762439728, tv_loss: 0.03377324342727661\n",
      "iteration 3946, dc_loss: 0.02616492472589016, tv_loss: 0.03378253057599068\n",
      "iteration 3947, dc_loss: 0.026155222207307816, tv_loss: 0.03379274159669876\n",
      "iteration 3948, dc_loss: 0.02616141363978386, tv_loss: 0.03378725051879883\n",
      "iteration 3949, dc_loss: 0.026166649535298347, tv_loss: 0.033774297684431076\n",
      "iteration 3950, dc_loss: 0.026156602427363396, tv_loss: 0.03378532454371452\n",
      "iteration 3951, dc_loss: 0.026150109246373177, tv_loss: 0.03379669412970543\n",
      "iteration 3952, dc_loss: 0.026162702590227127, tv_loss: 0.033774930983781815\n",
      "iteration 3953, dc_loss: 0.026158243417739868, tv_loss: 0.033772505819797516\n",
      "iteration 3954, dc_loss: 0.026138516142964363, tv_loss: 0.033790476620197296\n",
      "iteration 3955, dc_loss: 0.026152851060032845, tv_loss: 0.03377743065357208\n",
      "iteration 3956, dc_loss: 0.02615487016737461, tv_loss: 0.033777251839637756\n",
      "iteration 3957, dc_loss: 0.02614094875752926, tv_loss: 0.03378336504101753\n",
      "iteration 3958, dc_loss: 0.026143355295062065, tv_loss: 0.03378170356154442\n",
      "iteration 3959, dc_loss: 0.026150383055210114, tv_loss: 0.033770907670259476\n",
      "iteration 3960, dc_loss: 0.026148932054638863, tv_loss: 0.033769745379686356\n",
      "iteration 3961, dc_loss: 0.02613016404211521, tv_loss: 0.033790431916713715\n",
      "iteration 3962, dc_loss: 0.02614816091954708, tv_loss: 0.033771105110645294\n",
      "iteration 3963, dc_loss: 0.026143506169319153, tv_loss: 0.03378012031316757\n",
      "iteration 3964, dc_loss: 0.02613689750432968, tv_loss: 0.033783312886953354\n",
      "iteration 3965, dc_loss: 0.026135053485631943, tv_loss: 0.03378161042928696\n",
      "iteration 3966, dc_loss: 0.02614631876349449, tv_loss: 0.03376735374331474\n",
      "iteration 3967, dc_loss: 0.026135481894016266, tv_loss: 0.03377984091639519\n",
      "iteration 3968, dc_loss: 0.02614407241344452, tv_loss: 0.03376910462975502\n",
      "iteration 3969, dc_loss: 0.026132868602871895, tv_loss: 0.03378157690167427\n",
      "iteration 3970, dc_loss: 0.026139618828892708, tv_loss: 0.03377422317862511\n",
      "iteration 3971, dc_loss: 0.026122931391000748, tv_loss: 0.03378559648990631\n",
      "iteration 3972, dc_loss: 0.026124827563762665, tv_loss: 0.03377986699342728\n",
      "iteration 3973, dc_loss: 0.026122063398361206, tv_loss: 0.03377674147486687\n",
      "iteration 3974, dc_loss: 0.026124335825443268, tv_loss: 0.03377170115709305\n",
      "iteration 3975, dc_loss: 0.026118582114577293, tv_loss: 0.03377294912934303\n",
      "iteration 3976, dc_loss: 0.026111522689461708, tv_loss: 0.033784378319978714\n",
      "iteration 3977, dc_loss: 0.02611653506755829, tv_loss: 0.03377602621912956\n",
      "iteration 3978, dc_loss: 0.026115018874406815, tv_loss: 0.03377637639641762\n",
      "iteration 3979, dc_loss: 0.026116754859685898, tv_loss: 0.033777885138988495\n",
      "iteration 3980, dc_loss: 0.02610909566283226, tv_loss: 0.03378070145845413\n",
      "iteration 3981, dc_loss: 0.026113737374544144, tv_loss: 0.03377484157681465\n",
      "iteration 3982, dc_loss: 0.026109294965863228, tv_loss: 0.03377436473965645\n",
      "iteration 3983, dc_loss: 0.026101665571331978, tv_loss: 0.033777739852666855\n",
      "iteration 3984, dc_loss: 0.0261024571955204, tv_loss: 0.03377474844455719\n",
      "iteration 3985, dc_loss: 0.02610321156680584, tv_loss: 0.0337747298181057\n",
      "iteration 3986, dc_loss: 0.02609788440167904, tv_loss: 0.033784713596105576\n",
      "iteration 3987, dc_loss: 0.026097921654582024, tv_loss: 0.03378479927778244\n",
      "iteration 3988, dc_loss: 0.02610332891345024, tv_loss: 0.03377171978354454\n",
      "iteration 3989, dc_loss: 0.026093890890479088, tv_loss: 0.03377857059240341\n",
      "iteration 3990, dc_loss: 0.026091318577528, tv_loss: 0.033778078854084015\n",
      "iteration 3991, dc_loss: 0.02609226293861866, tv_loss: 0.03377709537744522\n",
      "iteration 3992, dc_loss: 0.02609960548579693, tv_loss: 0.033770885318517685\n",
      "iteration 3993, dc_loss: 0.026082970201969147, tv_loss: 0.03378771245479584\n",
      "iteration 3994, dc_loss: 0.026082023978233337, tv_loss: 0.03378727659583092\n",
      "iteration 3995, dc_loss: 0.02608773671090603, tv_loss: 0.03377868980169296\n",
      "iteration 3996, dc_loss: 0.026083867996931076, tv_loss: 0.03377420827746391\n",
      "iteration 3997, dc_loss: 0.026081692427396774, tv_loss: 0.03377935662865639\n",
      "iteration 3998, dc_loss: 0.026079310104250908, tv_loss: 0.03378056362271309\n",
      "iteration 3999, dc_loss: 0.026084188371896744, tv_loss: 0.03377297893166542\n",
      "iteration 4000, dc_loss: 0.02607300505042076, tv_loss: 0.033779531717300415\n",
      "PSNR Value mt1: 35.77876077524474\n",
      "SSIM Value mt1: 0.7715722252164936\n",
      "tensor([[[6.0409e-04, 7.4484e-05]]]) tensor([[[1.2870, 1.2171]]])\n",
      "torch.Size([1, 1, 2]) torch.Size([1, 1, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| params = dict(fi: dict_keys(['net.0.linear.weight', 'net.0.linear.bias', 'net.1.linear.weight', 'net.1.linear.bias', 'net.2.linear.weight', 'net.2.linear.bias', 'net.3.linear.weight', 'net.3.linear.bias', 'net.4.linear.weight', 'net.4.linear.bias', 'net.5.weight', 'net.5.bias'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "iteration 1, dc_loss: 2.935042142868042, tv_loss: 0.0008581512374803424\n",
      "iteration 2, dc_loss: 2.813906192779541, tv_loss: 0.004346647299826145\n",
      "iteration 3, dc_loss: 2.748802423477173, tv_loss: 0.0064378357492387295\n",
      "iteration 4, dc_loss: 2.7061820030212402, tv_loss: 0.007817002013325691\n",
      "iteration 5, dc_loss: 2.6745200157165527, tv_loss: 0.008727215230464935\n",
      "iteration 6, dc_loss: 2.6500349044799805, tv_loss: 0.009370515123009682\n",
      "iteration 7, dc_loss: 2.6297852993011475, tv_loss: 0.009868411347270012\n",
      "iteration 8, dc_loss: 2.611405849456787, tv_loss: 0.010255226865410805\n",
      "iteration 9, dc_loss: 2.594622850418091, tv_loss: 0.010551880113780499\n",
      "iteration 10, dc_loss: 2.5781402587890625, tv_loss: 0.010800882242619991\n",
      "iteration 11, dc_loss: 2.5617458820343018, tv_loss: 0.010989588685333729\n",
      "iteration 12, dc_loss: 2.5462396144866943, tv_loss: 0.011076439172029495\n",
      "iteration 13, dc_loss: 2.531658172607422, tv_loss: 0.01109855342656374\n",
      "iteration 14, dc_loss: 2.517054796218872, tv_loss: 0.01116863265633583\n",
      "iteration 15, dc_loss: 2.502668857574463, tv_loss: 0.011266229674220085\n",
      "iteration 16, dc_loss: 2.4888088703155518, tv_loss: 0.011269018054008484\n",
      "iteration 17, dc_loss: 2.4751460552215576, tv_loss: 0.011155589483678341\n",
      "iteration 18, dc_loss: 2.4615657329559326, tv_loss: 0.011036506853997707\n",
      "iteration 19, dc_loss: 2.4481449127197266, tv_loss: 0.010928362607955933\n",
      "iteration 20, dc_loss: 2.4349217414855957, tv_loss: 0.010742252692580223\n",
      "iteration 21, dc_loss: 2.4220285415649414, tv_loss: 0.010493257082998753\n",
      "iteration 22, dc_loss: 2.4092087745666504, tv_loss: 0.010173100046813488\n",
      "iteration 23, dc_loss: 2.396359443664551, tv_loss: 0.009708648547530174\n",
      "iteration 24, dc_loss: 2.383430004119873, tv_loss: 0.009269512258470058\n",
      "iteration 25, dc_loss: 2.37048602104187, tv_loss: 0.008882624097168446\n",
      "iteration 26, dc_loss: 2.357898473739624, tv_loss: 0.008371677249670029\n",
      "iteration 27, dc_loss: 2.345548152923584, tv_loss: 0.007998956367373466\n",
      "iteration 28, dc_loss: 2.3332881927490234, tv_loss: 0.0077134487219154835\n",
      "iteration 29, dc_loss: 2.3210158348083496, tv_loss: 0.007435501087456942\n",
      "iteration 30, dc_loss: 2.308638095855713, tv_loss: 0.007380526512861252\n",
      "iteration 31, dc_loss: 2.2965729236602783, tv_loss: 0.0073352050967514515\n",
      "iteration 32, dc_loss: 2.2846381664276123, tv_loss: 0.007513796910643578\n",
      "iteration 33, dc_loss: 2.273057699203491, tv_loss: 0.007582914549857378\n",
      "iteration 34, dc_loss: 2.261791467666626, tv_loss: 0.007820215076208115\n",
      "iteration 35, dc_loss: 2.2501800060272217, tv_loss: 0.007840747945010662\n",
      "iteration 36, dc_loss: 2.237851142883301, tv_loss: 0.008013046346604824\n",
      "iteration 37, dc_loss: 2.225429058074951, tv_loss: 0.008038497529923916\n",
      "iteration 38, dc_loss: 2.2146713733673096, tv_loss: 0.00800212100148201\n",
      "iteration 39, dc_loss: 2.203653573989868, tv_loss: 0.008070992305874825\n",
      "iteration 40, dc_loss: 2.1913740634918213, tv_loss: 0.008010054007172585\n",
      "iteration 41, dc_loss: 2.1805877685546875, tv_loss: 0.008050150237977505\n",
      "iteration 42, dc_loss: 2.1699681282043457, tv_loss: 0.008266543969511986\n",
      "iteration 43, dc_loss: 2.1580233573913574, tv_loss: 0.008329886943101883\n",
      "iteration 44, dc_loss: 2.147406816482544, tv_loss: 0.008443048223853111\n",
      "iteration 45, dc_loss: 2.1368508338928223, tv_loss: 0.008667045272886753\n",
      "iteration 46, dc_loss: 2.125201940536499, tv_loss: 0.008733219467103481\n",
      "iteration 47, dc_loss: 2.1149799823760986, tv_loss: 0.00881839357316494\n",
      "iteration 48, dc_loss: 2.104484796524048, tv_loss: 0.009042047895491123\n",
      "iteration 49, dc_loss: 2.093158006668091, tv_loss: 0.009121677838265896\n",
      "iteration 50, dc_loss: 2.083294153213501, tv_loss: 0.009175378829240799\n",
      "iteration 51, dc_loss: 2.0728585720062256, tv_loss: 0.009385653771460056\n",
      "iteration 52, dc_loss: 2.061903476715088, tv_loss: 0.009465817362070084\n",
      "iteration 53, dc_loss: 2.0522215366363525, tv_loss: 0.009514004923403263\n",
      "iteration 54, dc_loss: 2.0418570041656494, tv_loss: 0.009734655730426311\n",
      "iteration 55, dc_loss: 2.0312790870666504, tv_loss: 0.009822849184274673\n",
      "iteration 56, dc_loss: 2.021744728088379, tv_loss: 0.009858423843979836\n",
      "iteration 57, dc_loss: 2.0115468502044678, tv_loss: 0.010075741447508335\n",
      "iteration 58, dc_loss: 2.001242160797119, tv_loss: 0.010170924477279186\n",
      "iteration 59, dc_loss: 1.9917986392974854, tv_loss: 0.010206427425146103\n",
      "iteration 60, dc_loss: 1.9818650484085083, tv_loss: 0.010405845008790493\n",
      "iteration 61, dc_loss: 1.9718133211135864, tv_loss: 0.010500254109501839\n",
      "iteration 62, dc_loss: 1.96242094039917, tv_loss: 0.010549466125667095\n",
      "iteration 63, dc_loss: 1.9527220726013184, tv_loss: 0.010745364241302013\n",
      "iteration 64, dc_loss: 1.9429221153259277, tv_loss: 0.010842064395546913\n",
      "iteration 65, dc_loss: 1.9335927963256836, tv_loss: 0.010901452042162418\n",
      "iteration 66, dc_loss: 1.9241076707839966, tv_loss: 0.011077705770730972\n",
      "iteration 67, dc_loss: 1.9145539999008179, tv_loss: 0.01115917693823576\n",
      "iteration 68, dc_loss: 1.9053200483322144, tv_loss: 0.011229629628360271\n",
      "iteration 69, dc_loss: 1.8960086107254028, tv_loss: 0.011408320628106594\n",
      "iteration 70, dc_loss: 1.88668692111969, tv_loss: 0.01148979738354683\n",
      "iteration 71, dc_loss: 1.8775511980056763, tv_loss: 0.011581465601921082\n",
      "iteration 72, dc_loss: 1.8683875799179077, tv_loss: 0.011747310869395733\n",
      "iteration 73, dc_loss: 1.8592829704284668, tv_loss: 0.011819526553153992\n",
      "iteration 74, dc_loss: 1.8502824306488037, tv_loss: 0.011937695555388927\n",
      "iteration 75, dc_loss: 1.841280221939087, tv_loss: 0.012077311053872108\n",
      "iteration 76, dc_loss: 1.8323451280593872, tv_loss: 0.012137374840676785\n",
      "iteration 77, dc_loss: 1.8234741687774658, tv_loss: 0.012270993553102016\n",
      "iteration 78, dc_loss: 1.8147058486938477, tv_loss: 0.012391733936965466\n",
      "iteration 79, dc_loss: 1.805937647819519, tv_loss: 0.012492126785218716\n",
      "iteration 80, dc_loss: 1.7971508502960205, tv_loss: 0.01261052954941988\n",
      "iteration 81, dc_loss: 1.788522720336914, tv_loss: 0.01268870197236538\n",
      "iteration 82, dc_loss: 1.779945731163025, tv_loss: 0.01278824545443058\n",
      "iteration 83, dc_loss: 1.771353840827942, tv_loss: 0.012898554094135761\n",
      "iteration 84, dc_loss: 1.7628326416015625, tv_loss: 0.012999948114156723\n",
      "iteration 85, dc_loss: 1.7544023990631104, tv_loss: 0.013152052648365498\n",
      "iteration 86, dc_loss: 1.7460546493530273, tv_loss: 0.013277552090585232\n",
      "iteration 87, dc_loss: 1.7377192974090576, tv_loss: 0.013339167460799217\n",
      "iteration 88, dc_loss: 1.7293906211853027, tv_loss: 0.013442798517644405\n",
      "iteration 89, dc_loss: 1.7212001085281372, tv_loss: 0.013539237901568413\n",
      "iteration 90, dc_loss: 1.7131397724151611, tv_loss: 0.01367997657507658\n",
      "iteration 91, dc_loss: 1.7053278684616089, tv_loss: 0.01371720153838396\n",
      "iteration 92, dc_loss: 1.6977877616882324, tv_loss: 0.013902447186410427\n",
      "iteration 93, dc_loss: 1.6901236772537231, tv_loss: 0.013873429968953133\n",
      "iteration 94, dc_loss: 1.682025671005249, tv_loss: 0.01409157644957304\n",
      "iteration 95, dc_loss: 1.673101544380188, tv_loss: 0.014044278301298618\n",
      "iteration 96, dc_loss: 1.6651244163513184, tv_loss: 0.01408371515572071\n",
      "iteration 97, dc_loss: 1.6577262878417969, tv_loss: 0.014247618615627289\n",
      "iteration 98, dc_loss: 1.6496878862380981, tv_loss: 0.01429404690861702\n",
      "iteration 99, dc_loss: 1.6414331197738647, tv_loss: 0.014537873677909374\n",
      "iteration 100, dc_loss: 1.6337954998016357, tv_loss: 0.01455637812614441\n",
      "iteration 101, dc_loss: 1.6264523267745972, tv_loss: 0.014512206427752972\n",
      "iteration 102, dc_loss: 1.618512749671936, tv_loss: 0.014731224626302719\n",
      "iteration 103, dc_loss: 1.6106505393981934, tv_loss: 0.01484970934689045\n",
      "iteration 104, dc_loss: 1.6032452583312988, tv_loss: 0.01486106589436531\n",
      "iteration 105, dc_loss: 1.595840334892273, tv_loss: 0.015000221319496632\n",
      "iteration 106, dc_loss: 1.58823823928833, tv_loss: 0.015023910440504551\n",
      "iteration 107, dc_loss: 1.5806437730789185, tv_loss: 0.015156558714807034\n",
      "iteration 108, dc_loss: 1.5732914209365845, tv_loss: 0.015283815562725067\n",
      "iteration 109, dc_loss: 1.565980076789856, tv_loss: 0.015257217921316624\n",
      "iteration 110, dc_loss: 1.5584163665771484, tv_loss: 0.015396158210933208\n",
      "iteration 111, dc_loss: 1.5511186122894287, tv_loss: 0.01543427538126707\n",
      "iteration 112, dc_loss: 1.5439248085021973, tv_loss: 0.015511082485318184\n",
      "iteration 113, dc_loss: 1.53672456741333, tv_loss: 0.01566700078547001\n",
      "iteration 114, dc_loss: 1.5296350717544556, tv_loss: 0.01573939621448517\n",
      "iteration 115, dc_loss: 1.5224854946136475, tv_loss: 0.01580565981566906\n",
      "iteration 116, dc_loss: 1.5152491331100464, tv_loss: 0.015880340710282326\n",
      "iteration 117, dc_loss: 1.508165717124939, tv_loss: 0.015927685424685478\n",
      "iteration 118, dc_loss: 1.5010851621627808, tv_loss: 0.01605679653584957\n",
      "iteration 119, dc_loss: 1.494096279144287, tv_loss: 0.016050733625888824\n",
      "iteration 120, dc_loss: 1.4870364665985107, tv_loss: 0.016142576932907104\n",
      "iteration 121, dc_loss: 1.4801450967788696, tv_loss: 0.016204211860895157\n",
      "iteration 122, dc_loss: 1.4733185768127441, tv_loss: 0.01632785052061081\n",
      "iteration 123, dc_loss: 1.4665697813034058, tv_loss: 0.016470719128847122\n",
      "iteration 124, dc_loss: 1.4598608016967773, tv_loss: 0.016464941203594208\n",
      "iteration 125, dc_loss: 1.4530431032180786, tv_loss: 0.01657894253730774\n",
      "iteration 126, dc_loss: 1.4462482929229736, tv_loss: 0.016616908833384514\n",
      "iteration 127, dc_loss: 1.4394505023956299, tv_loss: 0.016693202778697014\n",
      "iteration 128, dc_loss: 1.4327235221862793, tv_loss: 0.01674642413854599\n",
      "iteration 129, dc_loss: 1.426100492477417, tv_loss: 0.016788864508271217\n",
      "iteration 130, dc_loss: 1.4195185899734497, tv_loss: 0.016921954229474068\n",
      "iteration 131, dc_loss: 1.4131759405136108, tv_loss: 0.01693829707801342\n",
      "iteration 132, dc_loss: 1.4068350791931152, tv_loss: 0.017099393531680107\n",
      "iteration 133, dc_loss: 1.4006630182266235, tv_loss: 0.0170203298330307\n",
      "iteration 134, dc_loss: 1.3944023847579956, tv_loss: 0.01718701422214508\n",
      "iteration 135, dc_loss: 1.3879505395889282, tv_loss: 0.017107119783759117\n",
      "iteration 136, dc_loss: 1.3811593055725098, tv_loss: 0.017194826155900955\n",
      "iteration 137, dc_loss: 1.3742636442184448, tv_loss: 0.017430009320378304\n",
      "iteration 138, dc_loss: 1.3679616451263428, tv_loss: 0.017532259225845337\n",
      "iteration 139, dc_loss: 1.3617323637008667, tv_loss: 0.017445189878344536\n",
      "iteration 140, dc_loss: 1.355499505996704, tv_loss: 0.017445102334022522\n",
      "iteration 141, dc_loss: 1.3491487503051758, tv_loss: 0.017566794529557228\n",
      "iteration 142, dc_loss: 1.3428162336349487, tv_loss: 0.01790025644004345\n",
      "iteration 143, dc_loss: 1.3367021083831787, tv_loss: 0.01768689788877964\n",
      "iteration 144, dc_loss: 1.3304885625839233, tv_loss: 0.01778229884803295\n",
      "iteration 145, dc_loss: 1.3243144750595093, tv_loss: 0.017889264971017838\n",
      "iteration 146, dc_loss: 1.3182438611984253, tv_loss: 0.018027514219284058\n",
      "iteration 147, dc_loss: 1.3121623992919922, tv_loss: 0.017991259694099426\n",
      "iteration 148, dc_loss: 1.3062152862548828, tv_loss: 0.017983894795179367\n",
      "iteration 149, dc_loss: 1.3002897500991821, tv_loss: 0.018210120499134064\n",
      "iteration 150, dc_loss: 1.2944227457046509, tv_loss: 0.018295634537935257\n",
      "iteration 151, dc_loss: 1.2885396480560303, tv_loss: 0.018234161660075188\n",
      "iteration 152, dc_loss: 1.2824136018753052, tv_loss: 0.01831180602312088\n",
      "iteration 153, dc_loss: 1.2763029336929321, tv_loss: 0.01847633719444275\n",
      "iteration 154, dc_loss: 1.270389199256897, tv_loss: 0.018409043550491333\n",
      "iteration 155, dc_loss: 1.2645126581192017, tv_loss: 0.01847761869430542\n",
      "iteration 156, dc_loss: 1.2587335109710693, tv_loss: 0.018525755032896996\n",
      "iteration 157, dc_loss: 1.2529857158660889, tv_loss: 0.018651172518730164\n",
      "iteration 158, dc_loss: 1.2472214698791504, tv_loss: 0.018756629899144173\n",
      "iteration 159, dc_loss: 1.2415839433670044, tv_loss: 0.018710391595959663\n",
      "iteration 160, dc_loss: 1.2357579469680786, tv_loss: 0.0188994649797678\n",
      "iteration 161, dc_loss: 1.2302074432373047, tv_loss: 0.01880117692053318\n",
      "iteration 162, dc_loss: 1.224461317062378, tv_loss: 0.018955809995532036\n",
      "iteration 163, dc_loss: 1.2189228534698486, tv_loss: 0.018947042524814606\n",
      "iteration 164, dc_loss: 1.2133615016937256, tv_loss: 0.019036708399653435\n",
      "iteration 165, dc_loss: 1.2078505754470825, tv_loss: 0.01912054233253002\n",
      "iteration 166, dc_loss: 1.2023983001708984, tv_loss: 0.01915598288178444\n",
      "iteration 167, dc_loss: 1.1968108415603638, tv_loss: 0.0192511435598135\n",
      "iteration 168, dc_loss: 1.1912692785263062, tv_loss: 0.019255520775914192\n",
      "iteration 169, dc_loss: 1.1856749057769775, tv_loss: 0.019349854439496994\n",
      "iteration 170, dc_loss: 1.1802842617034912, tv_loss: 0.01933758705854416\n",
      "iteration 171, dc_loss: 1.1749439239501953, tv_loss: 0.01943429931998253\n",
      "iteration 172, dc_loss: 1.169875144958496, tv_loss: 0.0194168109446764\n",
      "iteration 173, dc_loss: 1.1649627685546875, tv_loss: 0.019585130736231804\n",
      "iteration 174, dc_loss: 1.1600719690322876, tv_loss: 0.019489675760269165\n",
      "iteration 175, dc_loss: 1.154651403427124, tv_loss: 0.01969234272837639\n",
      "iteration 176, dc_loss: 1.1487481594085693, tv_loss: 0.019594918936491013\n",
      "iteration 177, dc_loss: 1.1431468725204468, tv_loss: 0.019672613590955734\n",
      "iteration 178, dc_loss: 1.13817298412323, tv_loss: 0.019892636686563492\n",
      "iteration 179, dc_loss: 1.1333776712417603, tv_loss: 0.019749341532588005\n",
      "iteration 180, dc_loss: 1.127756953239441, tv_loss: 0.01998668909072876\n",
      "iteration 181, dc_loss: 1.122347116470337, tv_loss: 0.019859962165355682\n",
      "iteration 182, dc_loss: 1.1171208620071411, tv_loss: 0.019999206066131592\n",
      "iteration 183, dc_loss: 1.1121389865875244, tv_loss: 0.020109813660383224\n",
      "iteration 184, dc_loss: 1.1070865392684937, tv_loss: 0.019994430243968964\n",
      "iteration 185, dc_loss: 1.1016813516616821, tv_loss: 0.020133372396230698\n",
      "iteration 186, dc_loss: 1.0965958833694458, tv_loss: 0.020083488896489143\n",
      "iteration 187, dc_loss: 1.0916467905044556, tv_loss: 0.02022836171090603\n",
      "iteration 188, dc_loss: 1.0868128538131714, tv_loss: 0.020355258136987686\n",
      "iteration 189, dc_loss: 1.0817880630493164, tv_loss: 0.02030343748629093\n",
      "iteration 190, dc_loss: 1.076560139656067, tv_loss: 0.020429544150829315\n",
      "iteration 191, dc_loss: 1.0716643333435059, tv_loss: 0.020364388823509216\n",
      "iteration 192, dc_loss: 1.0666568279266357, tv_loss: 0.02056148275732994\n",
      "iteration 193, dc_loss: 1.0619789361953735, tv_loss: 0.02038857713341713\n",
      "iteration 194, dc_loss: 1.0570541620254517, tv_loss: 0.020668040961027145\n",
      "iteration 195, dc_loss: 1.0528262853622437, tv_loss: 0.02044059894979\n",
      "iteration 196, dc_loss: 1.0482004880905151, tv_loss: 0.020939666777849197\n",
      "iteration 197, dc_loss: 1.04339599609375, tv_loss: 0.02044672891497612\n",
      "iteration 198, dc_loss: 1.0380618572235107, tv_loss: 0.020793749019503593\n",
      "iteration 199, dc_loss: 1.0330607891082764, tv_loss: 0.020730094984173775\n",
      "iteration 200, dc_loss: 1.0291105508804321, tv_loss: 0.020629102364182472\n",
      "iteration 201, dc_loss: 1.0243475437164307, tv_loss: 0.02113056182861328\n",
      "iteration 202, dc_loss: 1.019270658493042, tv_loss: 0.02091730758547783\n",
      "iteration 203, dc_loss: 1.015006184577942, tv_loss: 0.020889732986688614\n",
      "iteration 204, dc_loss: 1.0098921060562134, tv_loss: 0.021143082529306412\n",
      "iteration 205, dc_loss: 1.0058997869491577, tv_loss: 0.02075701579451561\n",
      "iteration 206, dc_loss: 1.000871181488037, tv_loss: 0.021249400451779366\n",
      "iteration 207, dc_loss: 0.9959497451782227, tv_loss: 0.02111741341650486\n",
      "iteration 208, dc_loss: 0.9920324683189392, tv_loss: 0.02099403366446495\n",
      "iteration 209, dc_loss: 0.9869171977043152, tv_loss: 0.02140714041888714\n",
      "iteration 210, dc_loss: 0.9824144244194031, tv_loss: 0.02132805995643139\n",
      "iteration 211, dc_loss: 0.9777811765670776, tv_loss: 0.021179456263780594\n",
      "iteration 212, dc_loss: 0.9729010462760925, tv_loss: 0.021282481029629707\n",
      "iteration 213, dc_loss: 0.9684903621673584, tv_loss: 0.021320993080735207\n",
      "iteration 214, dc_loss: 0.9638854265213013, tv_loss: 0.021439094096422195\n",
      "iteration 215, dc_loss: 0.9595968127250671, tv_loss: 0.021496035158634186\n",
      "iteration 216, dc_loss: 0.9548926949501038, tv_loss: 0.021508174017071724\n",
      "iteration 217, dc_loss: 0.9507360458374023, tv_loss: 0.02139509841799736\n",
      "iteration 218, dc_loss: 0.9458784461021423, tv_loss: 0.02173648029565811\n",
      "iteration 219, dc_loss: 0.9418804049491882, tv_loss: 0.021474013105034828\n",
      "iteration 220, dc_loss: 0.9378833174705505, tv_loss: 0.021751446649432182\n",
      "iteration 221, dc_loss: 0.933602511882782, tv_loss: 0.021637214347720146\n",
      "iteration 222, dc_loss: 0.9295021891593933, tv_loss: 0.021687528118491173\n",
      "iteration 223, dc_loss: 0.9250213503837585, tv_loss: 0.02188577875494957\n",
      "iteration 224, dc_loss: 0.9208976030349731, tv_loss: 0.0215549748390913\n",
      "iteration 225, dc_loss: 0.9163246154785156, tv_loss: 0.021993739530444145\n",
      "iteration 226, dc_loss: 0.9121382832527161, tv_loss: 0.021836616098880768\n",
      "iteration 227, dc_loss: 0.9078700542449951, tv_loss: 0.021810809150338173\n",
      "iteration 228, dc_loss: 0.9033103585243225, tv_loss: 0.02214706875383854\n",
      "iteration 229, dc_loss: 0.8988894820213318, tv_loss: 0.021877773106098175\n",
      "iteration 230, dc_loss: 0.8949419856071472, tv_loss: 0.02195640839636326\n",
      "iteration 231, dc_loss: 0.8904241323471069, tv_loss: 0.022153569385409355\n",
      "iteration 232, dc_loss: 0.8864822387695312, tv_loss: 0.02216400019824505\n",
      "iteration 233, dc_loss: 0.8825796842575073, tv_loss: 0.022101514041423798\n",
      "iteration 234, dc_loss: 0.8786178827285767, tv_loss: 0.02229716256260872\n",
      "iteration 235, dc_loss: 0.8743868470191956, tv_loss: 0.022053781896829605\n",
      "iteration 236, dc_loss: 0.8702950477600098, tv_loss: 0.02231941558420658\n",
      "iteration 237, dc_loss: 0.866208016872406, tv_loss: 0.02209528721868992\n",
      "iteration 238, dc_loss: 0.8616970777511597, tv_loss: 0.022486723959445953\n",
      "iteration 239, dc_loss: 0.8576297163963318, tv_loss: 0.02235044352710247\n",
      "iteration 240, dc_loss: 0.8535963892936707, tv_loss: 0.022390464320778847\n",
      "iteration 241, dc_loss: 0.8496198654174805, tv_loss: 0.022464986890554428\n",
      "iteration 242, dc_loss: 0.8456031084060669, tv_loss: 0.022383952513337135\n",
      "iteration 243, dc_loss: 0.8419910073280334, tv_loss: 0.02277442440390587\n",
      "iteration 244, dc_loss: 0.8377484679222107, tv_loss: 0.02243926003575325\n",
      "iteration 245, dc_loss: 0.8336479067802429, tv_loss: 0.022599944844841957\n",
      "iteration 246, dc_loss: 0.829777717590332, tv_loss: 0.02272048406302929\n",
      "iteration 247, dc_loss: 0.8263360261917114, tv_loss: 0.022565359249711037\n",
      "iteration 248, dc_loss: 0.8224180936813354, tv_loss: 0.022930707782506943\n",
      "iteration 249, dc_loss: 0.8188925385475159, tv_loss: 0.022546866908669472\n",
      "iteration 250, dc_loss: 0.8154228329658508, tv_loss: 0.022932568565011024\n",
      "iteration 251, dc_loss: 0.8111844062805176, tv_loss: 0.022760096937417984\n",
      "iteration 252, dc_loss: 0.8070796132087708, tv_loss: 0.022778881713747978\n",
      "iteration 253, dc_loss: 0.8036841750144958, tv_loss: 0.023137573152780533\n",
      "iteration 254, dc_loss: 0.8007376194000244, tv_loss: 0.022725483402609825\n",
      "iteration 255, dc_loss: 0.7965889573097229, tv_loss: 0.023239651694893837\n",
      "iteration 256, dc_loss: 0.7922319769859314, tv_loss: 0.02294202335178852\n",
      "iteration 257, dc_loss: 0.7887230515480042, tv_loss: 0.02289765328168869\n",
      "iteration 258, dc_loss: 0.7853130102157593, tv_loss: 0.02332933060824871\n",
      "iteration 259, dc_loss: 0.7815579175949097, tv_loss: 0.02287515066564083\n",
      "iteration 260, dc_loss: 0.777833878993988, tv_loss: 0.023213645443320274\n",
      "iteration 261, dc_loss: 0.773635983467102, tv_loss: 0.02330293320119381\n",
      "iteration 262, dc_loss: 0.7708386182785034, tv_loss: 0.0230390764772892\n",
      "iteration 263, dc_loss: 0.7670855522155762, tv_loss: 0.023515619337558746\n",
      "iteration 264, dc_loss: 0.7631739974021912, tv_loss: 0.023115728050470352\n",
      "iteration 265, dc_loss: 0.7596444487571716, tv_loss: 0.023308780044317245\n",
      "iteration 266, dc_loss: 0.7557445764541626, tv_loss: 0.023587385192513466\n",
      "iteration 267, dc_loss: 0.7529644966125488, tv_loss: 0.023114480078220367\n",
      "iteration 268, dc_loss: 0.74880051612854, tv_loss: 0.023509124293923378\n",
      "iteration 269, dc_loss: 0.7451977729797363, tv_loss: 0.023494571447372437\n",
      "iteration 270, dc_loss: 0.742107629776001, tv_loss: 0.023405499756336212\n",
      "iteration 271, dc_loss: 0.7380845546722412, tv_loss: 0.02365492470562458\n",
      "iteration 272, dc_loss: 0.735008180141449, tv_loss: 0.023443147540092468\n",
      "iteration 273, dc_loss: 0.7313302159309387, tv_loss: 0.0235883891582489\n",
      "iteration 274, dc_loss: 0.7277599573135376, tv_loss: 0.0236782506108284\n",
      "iteration 275, dc_loss: 0.7243296504020691, tv_loss: 0.02365845814347267\n",
      "iteration 276, dc_loss: 0.7207774519920349, tv_loss: 0.0236455537378788\n",
      "iteration 277, dc_loss: 0.7176376581192017, tv_loss: 0.023562844842672348\n",
      "iteration 278, dc_loss: 0.7140089273452759, tv_loss: 0.02388572134077549\n",
      "iteration 279, dc_loss: 0.7108177542686462, tv_loss: 0.023812653496861458\n",
      "iteration 280, dc_loss: 0.7074459195137024, tv_loss: 0.02380640245974064\n",
      "iteration 281, dc_loss: 0.7040117383003235, tv_loss: 0.023877128958702087\n",
      "iteration 282, dc_loss: 0.7009527087211609, tv_loss: 0.023846229538321495\n",
      "iteration 283, dc_loss: 0.6975639462471008, tv_loss: 0.024061139672994614\n",
      "iteration 284, dc_loss: 0.6943122744560242, tv_loss: 0.023909995332360268\n",
      "iteration 285, dc_loss: 0.6910018920898438, tv_loss: 0.023991333320736885\n",
      "iteration 286, dc_loss: 0.6877984404563904, tv_loss: 0.024128755554556847\n",
      "iteration 287, dc_loss: 0.6848485469818115, tv_loss: 0.023934602737426758\n",
      "iteration 288, dc_loss: 0.681577205657959, tv_loss: 0.02426229976117611\n",
      "iteration 289, dc_loss: 0.6786374449729919, tv_loss: 0.023997778072953224\n",
      "iteration 290, dc_loss: 0.6754151582717896, tv_loss: 0.02425171062350273\n",
      "iteration 291, dc_loss: 0.67194002866745, tv_loss: 0.024111321195960045\n",
      "iteration 292, dc_loss: 0.6685925722122192, tv_loss: 0.02422078512609005\n",
      "iteration 293, dc_loss: 0.6653910875320435, tv_loss: 0.02433888427913189\n",
      "iteration 294, dc_loss: 0.6626280546188354, tv_loss: 0.02418055199086666\n",
      "iteration 295, dc_loss: 0.6595061421394348, tv_loss: 0.024578802287578583\n",
      "iteration 296, dc_loss: 0.6568442583084106, tv_loss: 0.024167411029338837\n",
      "iteration 297, dc_loss: 0.653594970703125, tv_loss: 0.02455269731581211\n",
      "iteration 298, dc_loss: 0.6503866910934448, tv_loss: 0.02432229183614254\n",
      "iteration 299, dc_loss: 0.6470314860343933, tv_loss: 0.02446158044040203\n",
      "iteration 300, dc_loss: 0.6437257528305054, tv_loss: 0.024566326290369034\n",
      "iteration 301, dc_loss: 0.6408844590187073, tv_loss: 0.02436874620616436\n",
      "iteration 302, dc_loss: 0.6376586556434631, tv_loss: 0.024680685251951218\n",
      "iteration 303, dc_loss: 0.634972095489502, tv_loss: 0.02443067729473114\n",
      "iteration 304, dc_loss: 0.6318610310554504, tv_loss: 0.024726351723074913\n",
      "iteration 305, dc_loss: 0.6290431022644043, tv_loss: 0.02457687072455883\n",
      "iteration 306, dc_loss: 0.6261909008026123, tv_loss: 0.024643167853355408\n",
      "iteration 307, dc_loss: 0.623172402381897, tv_loss: 0.02469860017299652\n",
      "iteration 308, dc_loss: 0.6202839016914368, tv_loss: 0.024647844955325127\n",
      "iteration 309, dc_loss: 0.6170459389686584, tv_loss: 0.02487283945083618\n",
      "iteration 310, dc_loss: 0.6141642928123474, tv_loss: 0.024669280275702477\n",
      "iteration 311, dc_loss: 0.6107833385467529, tv_loss: 0.02492128312587738\n",
      "iteration 312, dc_loss: 0.6080245971679688, tv_loss: 0.024692445993423462\n",
      "iteration 313, dc_loss: 0.6051616072654724, tv_loss: 0.025043215602636337\n",
      "iteration 314, dc_loss: 0.603415310382843, tv_loss: 0.024690035730600357\n",
      "iteration 315, dc_loss: 0.6029053330421448, tv_loss: 0.025304371491074562\n",
      "iteration 316, dc_loss: 0.5987134575843811, tv_loss: 0.024618634954094887\n",
      "iteration 317, dc_loss: 0.5941416025161743, tv_loss: 0.025041023269295692\n",
      "iteration 318, dc_loss: 0.5912278294563293, tv_loss: 0.024928437545895576\n",
      "iteration 319, dc_loss: 0.5894008874893188, tv_loss: 0.024826206266880035\n",
      "iteration 320, dc_loss: 0.5868229866027832, tv_loss: 0.025212198495864868\n",
      "iteration 321, dc_loss: 0.5831171274185181, tv_loss: 0.024766061455011368\n",
      "iteration 322, dc_loss: 0.5800870060920715, tv_loss: 0.02513939142227173\n",
      "iteration 323, dc_loss: 0.577662467956543, tv_loss: 0.02516220510005951\n",
      "iteration 324, dc_loss: 0.5743680000305176, tv_loss: 0.02525685913860798\n",
      "iteration 325, dc_loss: 0.5719180107116699, tv_loss: 0.025065775960683823\n",
      "iteration 326, dc_loss: 0.5685783624649048, tv_loss: 0.025256404653191566\n",
      "iteration 327, dc_loss: 0.5660232901573181, tv_loss: 0.025255434215068817\n",
      "iteration 328, dc_loss: 0.5632567405700684, tv_loss: 0.025357553735375404\n",
      "iteration 329, dc_loss: 0.560562252998352, tv_loss: 0.025272062048316002\n",
      "iteration 330, dc_loss: 0.5577758550643921, tv_loss: 0.025313152000308037\n",
      "iteration 331, dc_loss: 0.5548850893974304, tv_loss: 0.025200417265295982\n",
      "iteration 332, dc_loss: 0.552211344242096, tv_loss: 0.025609206408262253\n",
      "iteration 333, dc_loss: 0.5500575304031372, tv_loss: 0.025306330993771553\n",
      "iteration 334, dc_loss: 0.5468906760215759, tv_loss: 0.025616994127631187\n",
      "iteration 335, dc_loss: 0.5445259809494019, tv_loss: 0.02535516396164894\n",
      "iteration 336, dc_loss: 0.5413734316825867, tv_loss: 0.02566247433423996\n",
      "iteration 337, dc_loss: 0.5391618013381958, tv_loss: 0.025423331186175346\n",
      "iteration 338, dc_loss: 0.5367874503135681, tv_loss: 0.02569049596786499\n",
      "iteration 339, dc_loss: 0.5341144800186157, tv_loss: 0.02551122196018696\n",
      "iteration 340, dc_loss: 0.5313422679901123, tv_loss: 0.025779809802770615\n",
      "iteration 341, dc_loss: 0.5289884805679321, tv_loss: 0.02555190771818161\n",
      "iteration 342, dc_loss: 0.5260231494903564, tv_loss: 0.025824205949902534\n",
      "iteration 343, dc_loss: 0.5238606333732605, tv_loss: 0.0255160853266716\n",
      "iteration 344, dc_loss: 0.5210358500480652, tv_loss: 0.025951765477657318\n",
      "iteration 345, dc_loss: 0.5186848044395447, tv_loss: 0.02572222799062729\n",
      "iteration 346, dc_loss: 0.5159453749656677, tv_loss: 0.025844566524028778\n",
      "iteration 347, dc_loss: 0.5134092569351196, tv_loss: 0.025851568207144737\n",
      "iteration 348, dc_loss: 0.511067807674408, tv_loss: 0.025866568088531494\n",
      "iteration 349, dc_loss: 0.5086192488670349, tv_loss: 0.02600819058716297\n",
      "iteration 350, dc_loss: 0.5064300298690796, tv_loss: 0.02580082044005394\n",
      "iteration 351, dc_loss: 0.5038843750953674, tv_loss: 0.026167087256908417\n",
      "iteration 352, dc_loss: 0.5024433135986328, tv_loss: 0.025718633085489273\n",
      "iteration 353, dc_loss: 0.5014222264289856, tv_loss: 0.026478784158825874\n",
      "iteration 354, dc_loss: 0.5002858638763428, tv_loss: 0.025586720556020737\n",
      "iteration 355, dc_loss: 0.4991951584815979, tv_loss: 0.026515765115618706\n",
      "iteration 356, dc_loss: 0.4941466152667999, tv_loss: 0.02587296813726425\n",
      "iteration 357, dc_loss: 0.491768479347229, tv_loss: 0.025876779109239578\n",
      "iteration 358, dc_loss: 0.4897741675376892, tv_loss: 0.02650783769786358\n",
      "iteration 359, dc_loss: 0.48724469542503357, tv_loss: 0.025884751230478287\n",
      "iteration 360, dc_loss: 0.48443129658699036, tv_loss: 0.02621382847428322\n",
      "iteration 361, dc_loss: 0.4814881682395935, tv_loss: 0.02623654156923294\n",
      "iteration 362, dc_loss: 0.47991788387298584, tv_loss: 0.025976095348596573\n",
      "iteration 363, dc_loss: 0.4772808253765106, tv_loss: 0.026392290368676186\n",
      "iteration 364, dc_loss: 0.4750344455242157, tv_loss: 0.026153361424803734\n",
      "iteration 365, dc_loss: 0.4721287786960602, tv_loss: 0.02635045535862446\n",
      "iteration 366, dc_loss: 0.4701409339904785, tv_loss: 0.026552636176347733\n",
      "iteration 367, dc_loss: 0.4681127071380615, tv_loss: 0.026165936142206192\n",
      "iteration 368, dc_loss: 0.4653831124305725, tv_loss: 0.026357999071478844\n",
      "iteration 369, dc_loss: 0.4632754623889923, tv_loss: 0.0263900775462389\n",
      "iteration 370, dc_loss: 0.46102049946784973, tv_loss: 0.026356007903814316\n",
      "iteration 371, dc_loss: 0.4587186574935913, tv_loss: 0.02652263641357422\n",
      "iteration 372, dc_loss: 0.4566301703453064, tv_loss: 0.026589414104819298\n",
      "iteration 373, dc_loss: 0.45432212948799133, tv_loss: 0.02654661424458027\n",
      "iteration 374, dc_loss: 0.45223191380500793, tv_loss: 0.02648123726248741\n",
      "iteration 375, dc_loss: 0.44993841648101807, tv_loss: 0.02650163322687149\n",
      "iteration 376, dc_loss: 0.447900652885437, tv_loss: 0.026502937078475952\n",
      "iteration 377, dc_loss: 0.44563764333724976, tv_loss: 0.02665133960545063\n",
      "iteration 378, dc_loss: 0.4435710906982422, tv_loss: 0.026789376512169838\n",
      "iteration 379, dc_loss: 0.44145023822784424, tv_loss: 0.026682430878281593\n",
      "iteration 380, dc_loss: 0.43939366936683655, tv_loss: 0.026588723063468933\n",
      "iteration 381, dc_loss: 0.4371996223926544, tv_loss: 0.026677237823605537\n",
      "iteration 382, dc_loss: 0.43520063161849976, tv_loss: 0.026622368022799492\n",
      "iteration 383, dc_loss: 0.4330201745033264, tv_loss: 0.026883680373430252\n",
      "iteration 384, dc_loss: 0.4311799705028534, tv_loss: 0.026836756616830826\n",
      "iteration 385, dc_loss: 0.4290812313556671, tv_loss: 0.026895374059677124\n",
      "iteration 386, dc_loss: 0.42766812443733215, tv_loss: 0.026668058708310127\n",
      "iteration 387, dc_loss: 0.42702531814575195, tv_loss: 0.02713610976934433\n",
      "iteration 388, dc_loss: 0.4255448281764984, tv_loss: 0.026570379734039307\n",
      "iteration 389, dc_loss: 0.4230387806892395, tv_loss: 0.027201242744922638\n",
      "iteration 390, dc_loss: 0.4207304120063782, tv_loss: 0.026730036363005638\n",
      "iteration 391, dc_loss: 0.41798967123031616, tv_loss: 0.027274353429675102\n",
      "iteration 392, dc_loss: 0.4164462089538574, tv_loss: 0.027029071003198624\n",
      "iteration 393, dc_loss: 0.41456782817840576, tv_loss: 0.026967724785208702\n",
      "iteration 394, dc_loss: 0.41273146867752075, tv_loss: 0.02730889804661274\n",
      "iteration 395, dc_loss: 0.41066667437553406, tv_loss: 0.027049042284488678\n",
      "iteration 396, dc_loss: 0.408380925655365, tv_loss: 0.027265749871730804\n",
      "iteration 397, dc_loss: 0.4066452383995056, tv_loss: 0.027126261964440346\n",
      "iteration 398, dc_loss: 0.4045916795730591, tv_loss: 0.027423633262515068\n",
      "iteration 399, dc_loss: 0.40269941091537476, tv_loss: 0.02712981402873993\n",
      "iteration 400, dc_loss: 0.4007347524166107, tv_loss: 0.02729102596640587\n",
      "iteration 401, dc_loss: 0.39879724383354187, tv_loss: 0.027272123843431473\n",
      "iteration 402, dc_loss: 0.3969990611076355, tv_loss: 0.027198990806937218\n",
      "iteration 403, dc_loss: 0.39537984132766724, tv_loss: 0.02746276557445526\n",
      "iteration 404, dc_loss: 0.3939387798309326, tv_loss: 0.027239181101322174\n",
      "iteration 405, dc_loss: 0.3924204707145691, tv_loss: 0.027163490653038025\n",
      "iteration 406, dc_loss: 0.3908466100692749, tv_loss: 0.027449624612927437\n",
      "iteration 407, dc_loss: 0.38930222392082214, tv_loss: 0.027322061359882355\n",
      "iteration 408, dc_loss: 0.387977659702301, tv_loss: 0.027229709550738335\n",
      "iteration 409, dc_loss: 0.38629186153411865, tv_loss: 0.027527043595910072\n",
      "iteration 410, dc_loss: 0.3848350942134857, tv_loss: 0.027379021048545837\n",
      "iteration 411, dc_loss: 0.383537620306015, tv_loss: 0.027333557605743408\n",
      "iteration 412, dc_loss: 0.38188862800598145, tv_loss: 0.027614101767539978\n",
      "iteration 413, dc_loss: 0.3805030584335327, tv_loss: 0.02740195393562317\n",
      "iteration 414, dc_loss: 0.3791000247001648, tv_loss: 0.027486125007271767\n",
      "iteration 415, dc_loss: 0.37755563855171204, tv_loss: 0.02759524993598461\n",
      "iteration 416, dc_loss: 0.37620851397514343, tv_loss: 0.027428247034549713\n",
      "iteration 417, dc_loss: 0.37479284405708313, tv_loss: 0.027535703033208847\n",
      "iteration 418, dc_loss: 0.37331023812294006, tv_loss: 0.027618946507573128\n",
      "iteration 419, dc_loss: 0.37191304564476013, tv_loss: 0.027540260925889015\n",
      "iteration 420, dc_loss: 0.3705388605594635, tv_loss: 0.027552004903554916\n",
      "iteration 421, dc_loss: 0.36915427446365356, tv_loss: 0.027706915512681007\n",
      "iteration 422, dc_loss: 0.3677242398262024, tv_loss: 0.027646921575069427\n",
      "iteration 423, dc_loss: 0.3664253056049347, tv_loss: 0.027686510235071182\n",
      "iteration 424, dc_loss: 0.3649924695491791, tv_loss: 0.027687864378094673\n",
      "iteration 425, dc_loss: 0.3635922372341156, tv_loss: 0.027689959853887558\n",
      "iteration 426, dc_loss: 0.36228448152542114, tv_loss: 0.02771044708788395\n",
      "iteration 427, dc_loss: 0.36091500520706177, tv_loss: 0.027758382260799408\n",
      "iteration 428, dc_loss: 0.3595725893974304, tv_loss: 0.027712415903806686\n",
      "iteration 429, dc_loss: 0.3582397699356079, tv_loss: 0.02775273099541664\n",
      "iteration 430, dc_loss: 0.3568866550922394, tv_loss: 0.027802782133221626\n",
      "iteration 431, dc_loss: 0.3555074632167816, tv_loss: 0.027805790305137634\n",
      "iteration 432, dc_loss: 0.3542691469192505, tv_loss: 0.02772599086165428\n",
      "iteration 433, dc_loss: 0.35288962721824646, tv_loss: 0.02785753458738327\n",
      "iteration 434, dc_loss: 0.3516444265842438, tv_loss: 0.027773462235927582\n",
      "iteration 435, dc_loss: 0.35026925802230835, tv_loss: 0.02787439338862896\n",
      "iteration 436, dc_loss: 0.349059522151947, tv_loss: 0.027828948572278023\n",
      "iteration 437, dc_loss: 0.34767329692840576, tv_loss: 0.027952095493674278\n",
      "iteration 438, dc_loss: 0.346658855676651, tv_loss: 0.02778339758515358\n",
      "iteration 439, dc_loss: 0.34531867504119873, tv_loss: 0.02806832455098629\n",
      "iteration 440, dc_loss: 0.3443891704082489, tv_loss: 0.02776077389717102\n",
      "iteration 441, dc_loss: 0.34300607442855835, tv_loss: 0.028207892552018166\n",
      "iteration 442, dc_loss: 0.3420819044113159, tv_loss: 0.027704685926437378\n",
      "iteration 443, dc_loss: 0.3405490517616272, tv_loss: 0.02817360870540142\n",
      "iteration 444, dc_loss: 0.33923768997192383, tv_loss: 0.0278268251568079\n",
      "iteration 445, dc_loss: 0.33752092719078064, tv_loss: 0.0281857680529356\n",
      "iteration 446, dc_loss: 0.33622851967811584, tv_loss: 0.028093060478568077\n",
      "iteration 447, dc_loss: 0.33520838618278503, tv_loss: 0.027931207790970802\n",
      "iteration 448, dc_loss: 0.3339252769947052, tv_loss: 0.02814938686788082\n",
      "iteration 449, dc_loss: 0.33294814825057983, tv_loss: 0.027928855270147324\n",
      "iteration 450, dc_loss: 0.33143773674964905, tv_loss: 0.028377246111631393\n",
      "iteration 451, dc_loss: 0.33032068610191345, tv_loss: 0.028042856603860855\n",
      "iteration 452, dc_loss: 0.3287809193134308, tv_loss: 0.02822604402899742\n",
      "iteration 453, dc_loss: 0.3276815116405487, tv_loss: 0.028165774419903755\n",
      "iteration 454, dc_loss: 0.32643234729766846, tv_loss: 0.02825489267706871\n",
      "iteration 455, dc_loss: 0.32519063353538513, tv_loss: 0.028289297595620155\n",
      "iteration 456, dc_loss: 0.32439544796943665, tv_loss: 0.0280902162194252\n",
      "iteration 457, dc_loss: 0.32292571663856506, tv_loss: 0.028496582061052322\n",
      "iteration 458, dc_loss: 0.3219242990016937, tv_loss: 0.02807566337287426\n",
      "iteration 459, dc_loss: 0.3204649090766907, tv_loss: 0.028406785801053047\n",
      "iteration 460, dc_loss: 0.31932324171066284, tv_loss: 0.0284134354442358\n",
      "iteration 461, dc_loss: 0.31807348132133484, tv_loss: 0.028282692655920982\n",
      "iteration 462, dc_loss: 0.3168502449989319, tv_loss: 0.028490180149674416\n",
      "iteration 463, dc_loss: 0.3156064748764038, tv_loss: 0.028414864093065262\n",
      "iteration 464, dc_loss: 0.31446099281311035, tv_loss: 0.02855444885790348\n",
      "iteration 465, dc_loss: 0.3133915960788727, tv_loss: 0.028389232233166695\n",
      "iteration 466, dc_loss: 0.3122110366821289, tv_loss: 0.028401030227541924\n",
      "iteration 467, dc_loss: 0.3111184239387512, tv_loss: 0.028400901705026627\n",
      "iteration 468, dc_loss: 0.30991294980049133, tv_loss: 0.028654402121901512\n",
      "iteration 469, dc_loss: 0.3090173006057739, tv_loss: 0.028331760317087173\n",
      "iteration 470, dc_loss: 0.30781498551368713, tv_loss: 0.028751341626048088\n",
      "iteration 471, dc_loss: 0.3071542978286743, tv_loss: 0.028317783027887344\n",
      "iteration 472, dc_loss: 0.30574825406074524, tv_loss: 0.028772976249456406\n",
      "iteration 473, dc_loss: 0.30497780442237854, tv_loss: 0.028320981189608574\n",
      "iteration 474, dc_loss: 0.3036469519138336, tv_loss: 0.02892947755753994\n",
      "iteration 475, dc_loss: 0.30246543884277344, tv_loss: 0.02849014662206173\n",
      "iteration 476, dc_loss: 0.3009684383869171, tv_loss: 0.028869768604636192\n",
      "iteration 477, dc_loss: 0.2996898293495178, tv_loss: 0.028595516458153725\n",
      "iteration 478, dc_loss: 0.29867830872535706, tv_loss: 0.028746310621500015\n",
      "iteration 479, dc_loss: 0.29753148555755615, tv_loss: 0.028652559965848923\n",
      "iteration 480, dc_loss: 0.29667773842811584, tv_loss: 0.028761766850948334\n",
      "iteration 481, dc_loss: 0.2955876588821411, tv_loss: 0.028791552409529686\n",
      "iteration 482, dc_loss: 0.2946043610572815, tv_loss: 0.028635691851377487\n",
      "iteration 483, dc_loss: 0.29318559169769287, tv_loss: 0.02893662080168724\n",
      "iteration 484, dc_loss: 0.2921414077281952, tv_loss: 0.028641352429986\n",
      "iteration 485, dc_loss: 0.29089245200157166, tv_loss: 0.0288790762424469\n",
      "iteration 486, dc_loss: 0.2898767292499542, tv_loss: 0.028709840029478073\n",
      "iteration 487, dc_loss: 0.2887681722640991, tv_loss: 0.028769109398126602\n",
      "iteration 488, dc_loss: 0.2877262830734253, tv_loss: 0.028833508491516113\n",
      "iteration 489, dc_loss: 0.2867477834224701, tv_loss: 0.02873784862458706\n",
      "iteration 490, dc_loss: 0.2856113910675049, tv_loss: 0.028876161202788353\n",
      "iteration 491, dc_loss: 0.28486907482147217, tv_loss: 0.028626350685954094\n",
      "iteration 492, dc_loss: 0.28361740708351135, tv_loss: 0.02911105751991272\n",
      "iteration 493, dc_loss: 0.2828975021839142, tv_loss: 0.028599578887224197\n",
      "iteration 494, dc_loss: 0.2814377546310425, tv_loss: 0.029018549248576164\n",
      "iteration 495, dc_loss: 0.28064340353012085, tv_loss: 0.028705209493637085\n",
      "iteration 496, dc_loss: 0.27929019927978516, tv_loss: 0.029085665941238403\n",
      "iteration 497, dc_loss: 0.2783435881137848, tv_loss: 0.028860190883278847\n",
      "iteration 498, dc_loss: 0.2771790027618408, tv_loss: 0.028930528089404106\n",
      "iteration 499, dc_loss: 0.2761365473270416, tv_loss: 0.02903180569410324\n",
      "iteration 500, dc_loss: 0.2751164734363556, tv_loss: 0.028973035514354706\n",
      "iteration 501, dc_loss: 0.27405625581741333, tv_loss: 0.028961583971977234\n",
      "iteration 502, dc_loss: 0.2730605900287628, tv_loss: 0.029015839099884033\n",
      "iteration 503, dc_loss: 0.2719501554965973, tv_loss: 0.02906801924109459\n",
      "iteration 504, dc_loss: 0.2710111439228058, tv_loss: 0.0289500392973423\n",
      "iteration 505, dc_loss: 0.2699468731880188, tv_loss: 0.02907564304769039\n",
      "iteration 506, dc_loss: 0.26898112893104553, tv_loss: 0.029087094590067863\n",
      "iteration 507, dc_loss: 0.2680249512195587, tv_loss: 0.029044542461633682\n",
      "iteration 508, dc_loss: 0.26715418696403503, tv_loss: 0.029008567333221436\n",
      "iteration 509, dc_loss: 0.26616498827934265, tv_loss: 0.029254881665110588\n",
      "iteration 510, dc_loss: 0.2657129168510437, tv_loss: 0.028990089893341064\n",
      "iteration 511, dc_loss: 0.2650442123413086, tv_loss: 0.02935614250600338\n",
      "iteration 512, dc_loss: 0.2651473879814148, tv_loss: 0.028768662363290787\n",
      "iteration 513, dc_loss: 0.2648038864135742, tv_loss: 0.029659630730748177\n",
      "iteration 514, dc_loss: 0.2627306580543518, tv_loss: 0.02891283668577671\n",
      "iteration 515, dc_loss: 0.26021748781204224, tv_loss: 0.02926315739750862\n",
      "iteration 516, dc_loss: 0.25949397683143616, tv_loss: 0.029287947341799736\n",
      "iteration 517, dc_loss: 0.2596152722835541, tv_loss: 0.02902342937886715\n",
      "iteration 518, dc_loss: 0.2583412528038025, tv_loss: 0.02948753349483013\n",
      "iteration 519, dc_loss: 0.25674712657928467, tv_loss: 0.029033256694674492\n",
      "iteration 520, dc_loss: 0.2557486593723297, tv_loss: 0.029283670708537102\n",
      "iteration 521, dc_loss: 0.25520509481430054, tv_loss: 0.029530316591262817\n",
      "iteration 522, dc_loss: 0.25439396500587463, tv_loss: 0.029046522453427315\n",
      "iteration 523, dc_loss: 0.2527596652507782, tv_loss: 0.029514048248529434\n",
      "iteration 524, dc_loss: 0.25174009799957275, tv_loss: 0.029354676604270935\n",
      "iteration 525, dc_loss: 0.2514922618865967, tv_loss: 0.029128629714250565\n",
      "iteration 526, dc_loss: 0.2503586709499359, tv_loss: 0.02960820682346821\n",
      "iteration 527, dc_loss: 0.2493274062871933, tv_loss: 0.029153738170862198\n",
      "iteration 528, dc_loss: 0.24826006591320038, tv_loss: 0.029318664222955704\n",
      "iteration 529, dc_loss: 0.24727047979831696, tv_loss: 0.02957308478653431\n",
      "iteration 530, dc_loss: 0.2469077706336975, tv_loss: 0.02908623218536377\n",
      "iteration 531, dc_loss: 0.24544011056423187, tv_loss: 0.029570352286100388\n",
      "iteration 532, dc_loss: 0.24432618916034698, tv_loss: 0.02944043092429638\n",
      "iteration 533, dc_loss: 0.24380819499492645, tv_loss: 0.029247891157865524\n",
      "iteration 534, dc_loss: 0.24274109303951263, tv_loss: 0.029632428660988808\n",
      "iteration 535, dc_loss: 0.24214377999305725, tv_loss: 0.02923237718641758\n",
      "iteration 536, dc_loss: 0.2407430112361908, tv_loss: 0.029573801904916763\n",
      "iteration 537, dc_loss: 0.23993226885795593, tv_loss: 0.029608260840177536\n",
      "iteration 538, dc_loss: 0.23927944898605347, tv_loss: 0.02940857969224453\n",
      "iteration 539, dc_loss: 0.2381858080625534, tv_loss: 0.02972118742763996\n",
      "iteration 540, dc_loss: 0.23742812871932983, tv_loss: 0.0295240581035614\n",
      "iteration 541, dc_loss: 0.23622120916843414, tv_loss: 0.02959781512618065\n",
      "iteration 542, dc_loss: 0.23545019328594208, tv_loss: 0.029682451859116554\n",
      "iteration 543, dc_loss: 0.2348605841398239, tv_loss: 0.02950240485370159\n",
      "iteration 544, dc_loss: 0.2337774783372879, tv_loss: 0.029658427461981773\n",
      "iteration 545, dc_loss: 0.232847660779953, tv_loss: 0.029514603316783905\n",
      "iteration 546, dc_loss: 0.2319084256887436, tv_loss: 0.029701782390475273\n",
      "iteration 547, dc_loss: 0.23114292323589325, tv_loss: 0.029645778238773346\n",
      "iteration 548, dc_loss: 0.23036430776119232, tv_loss: 0.029568737372756004\n",
      "iteration 549, dc_loss: 0.22943921387195587, tv_loss: 0.029829582199454308\n",
      "iteration 550, dc_loss: 0.22865816950798035, tv_loss: 0.029591605067253113\n",
      "iteration 551, dc_loss: 0.22763977944850922, tv_loss: 0.029717838391661644\n",
      "iteration 552, dc_loss: 0.2267325222492218, tv_loss: 0.029741624370217323\n",
      "iteration 553, dc_loss: 0.22626523673534393, tv_loss: 0.02963113784790039\n",
      "iteration 554, dc_loss: 0.2252192199230194, tv_loss: 0.029927276074886322\n",
      "iteration 555, dc_loss: 0.22442743182182312, tv_loss: 0.02964869700372219\n",
      "iteration 556, dc_loss: 0.22358962893486023, tv_loss: 0.02970081754028797\n",
      "iteration 557, dc_loss: 0.22254705429077148, tv_loss: 0.029896868392825127\n",
      "iteration 558, dc_loss: 0.22193382680416107, tv_loss: 0.029722213745117188\n",
      "iteration 559, dc_loss: 0.22103510797023773, tv_loss: 0.029848730191588402\n",
      "iteration 560, dc_loss: 0.2203962802886963, tv_loss: 0.029801761731505394\n",
      "iteration 561, dc_loss: 0.21959203481674194, tv_loss: 0.02995821088552475\n",
      "iteration 562, dc_loss: 0.21863320469856262, tv_loss: 0.029765920713543892\n",
      "iteration 563, dc_loss: 0.21779409050941467, tv_loss: 0.02984810061752796\n",
      "iteration 564, dc_loss: 0.21690873801708221, tv_loss: 0.029986971989274025\n",
      "iteration 565, dc_loss: 0.2162555605173111, tv_loss: 0.02982817403972149\n",
      "iteration 566, dc_loss: 0.21536143124103546, tv_loss: 0.029988739639520645\n",
      "iteration 567, dc_loss: 0.2147430032491684, tv_loss: 0.029811017215251923\n",
      "iteration 568, dc_loss: 0.2137714922428131, tv_loss: 0.030067743733525276\n",
      "iteration 569, dc_loss: 0.21308305859565735, tv_loss: 0.029824642464518547\n",
      "iteration 570, dc_loss: 0.21218514442443848, tv_loss: 0.029936164617538452\n",
      "iteration 571, dc_loss: 0.21140256524085999, tv_loss: 0.029968494549393654\n",
      "iteration 572, dc_loss: 0.21074733138084412, tv_loss: 0.02985425852239132\n",
      "iteration 573, dc_loss: 0.20982269942760468, tv_loss: 0.030050022527575493\n",
      "iteration 574, dc_loss: 0.20918291807174683, tv_loss: 0.029859181493520737\n",
      "iteration 575, dc_loss: 0.2082453966140747, tv_loss: 0.030028555542230606\n",
      "iteration 576, dc_loss: 0.20757099986076355, tv_loss: 0.029924284666776657\n",
      "iteration 577, dc_loss: 0.20667795836925507, tv_loss: 0.03008299693465233\n",
      "iteration 578, dc_loss: 0.20608939230442047, tv_loss: 0.030051955953240395\n",
      "iteration 579, dc_loss: 0.2051975280046463, tv_loss: 0.030118653550744057\n",
      "iteration 580, dc_loss: 0.20462599396705627, tv_loss: 0.02993449568748474\n",
      "iteration 581, dc_loss: 0.20374268293380737, tv_loss: 0.030157050117850304\n",
      "iteration 582, dc_loss: 0.20330774784088135, tv_loss: 0.029887797310948372\n",
      "iteration 583, dc_loss: 0.20228378474712372, tv_loss: 0.030280176550149918\n",
      "iteration 584, dc_loss: 0.2019132822751999, tv_loss: 0.029954180121421814\n",
      "iteration 585, dc_loss: 0.20083196461200714, tv_loss: 0.030301310122013092\n",
      "iteration 586, dc_loss: 0.2002718299627304, tv_loss: 0.029959598556160927\n",
      "iteration 587, dc_loss: 0.19922871887683868, tv_loss: 0.030213935300707817\n",
      "iteration 588, dc_loss: 0.19876247644424438, tv_loss: 0.03001369908452034\n",
      "iteration 589, dc_loss: 0.19815683364868164, tv_loss: 0.030244505032896996\n",
      "iteration 590, dc_loss: 0.19804178178310394, tv_loss: 0.03026161529123783\n",
      "iteration 591, dc_loss: 0.19846133887767792, tv_loss: 0.03026994690299034\n",
      "iteration 592, dc_loss: 0.1986878663301468, tv_loss: 0.03010823018848896\n",
      "iteration 593, dc_loss: 0.1986345797777176, tv_loss: 0.030380627140402794\n",
      "iteration 594, dc_loss: 0.19596664607524872, tv_loss: 0.030123889446258545\n",
      "iteration 595, dc_loss: 0.19378456473350525, tv_loss: 0.030445588752627373\n",
      "iteration 596, dc_loss: 0.19408352673053741, tv_loss: 0.03021148405969143\n",
      "iteration 597, dc_loss: 0.19392754137516022, tv_loss: 0.03032779134809971\n",
      "iteration 598, dc_loss: 0.1923043131828308, tv_loss: 0.030448688194155693\n",
      "iteration 599, dc_loss: 0.19122137129306793, tv_loss: 0.03016599826514721\n",
      "iteration 600, dc_loss: 0.19119110703468323, tv_loss: 0.03045961819589138\n",
      "iteration 601, dc_loss: 0.1905553787946701, tv_loss: 0.030380327254533768\n",
      "iteration 602, dc_loss: 0.18906815350055695, tv_loss: 0.03028799220919609\n",
      "iteration 603, dc_loss: 0.18852251768112183, tv_loss: 0.030467890202999115\n",
      "iteration 604, dc_loss: 0.1885187178850174, tv_loss: 0.03029032237827778\n",
      "iteration 605, dc_loss: 0.18703266978263855, tv_loss: 0.03041882999241352\n",
      "iteration 606, dc_loss: 0.18630002439022064, tv_loss: 0.03034363128244877\n",
      "iteration 607, dc_loss: 0.18615075945854187, tv_loss: 0.030398037284612656\n",
      "iteration 608, dc_loss: 0.18497806787490845, tv_loss: 0.030493328347802162\n",
      "iteration 609, dc_loss: 0.18425872921943665, tv_loss: 0.030336951836943626\n",
      "iteration 610, dc_loss: 0.183831587433815, tv_loss: 0.03041759505867958\n",
      "iteration 611, dc_loss: 0.18308976292610168, tv_loss: 0.0304563008248806\n",
      "iteration 612, dc_loss: 0.182235985994339, tv_loss: 0.03052108734846115\n",
      "iteration 613, dc_loss: 0.18169119954109192, tv_loss: 0.03038186766207218\n",
      "iteration 614, dc_loss: 0.18093827366828918, tv_loss: 0.03055875562131405\n",
      "iteration 615, dc_loss: 0.18031956255435944, tv_loss: 0.03052329272031784\n",
      "iteration 616, dc_loss: 0.1796492487192154, tv_loss: 0.030431648716330528\n",
      "iteration 617, dc_loss: 0.1789228767156601, tv_loss: 0.030626017600297928\n",
      "iteration 618, dc_loss: 0.1784520298242569, tv_loss: 0.030441032722592354\n",
      "iteration 619, dc_loss: 0.17757929861545563, tv_loss: 0.030674997717142105\n",
      "iteration 620, dc_loss: 0.17704196274280548, tv_loss: 0.030578002333641052\n",
      "iteration 621, dc_loss: 0.17641891539096832, tv_loss: 0.030505232512950897\n",
      "iteration 622, dc_loss: 0.17569763958454132, tv_loss: 0.030715737491846085\n",
      "iteration 623, dc_loss: 0.1750643402338028, tv_loss: 0.030623717233538628\n",
      "iteration 624, dc_loss: 0.17451371252536774, tv_loss: 0.030488645657896996\n",
      "iteration 625, dc_loss: 0.17376984655857086, tv_loss: 0.03068295121192932\n",
      "iteration 626, dc_loss: 0.17322075366973877, tv_loss: 0.030622150748968124\n",
      "iteration 627, dc_loss: 0.1726427972316742, tv_loss: 0.030562711879611015\n",
      "iteration 628, dc_loss: 0.1718687117099762, tv_loss: 0.030650729313492775\n",
      "iteration 629, dc_loss: 0.17137162387371063, tv_loss: 0.030570324510335922\n",
      "iteration 630, dc_loss: 0.17073485255241394, tv_loss: 0.03071446903049946\n",
      "iteration 631, dc_loss: 0.17018985748291016, tv_loss: 0.030622221529483795\n",
      "iteration 632, dc_loss: 0.16946208477020264, tv_loss: 0.030707087367773056\n",
      "iteration 633, dc_loss: 0.16907048225402832, tv_loss: 0.030542759224772453\n",
      "iteration 634, dc_loss: 0.16834509372711182, tv_loss: 0.030797414481639862\n",
      "iteration 635, dc_loss: 0.1681397706270218, tv_loss: 0.03061184287071228\n",
      "iteration 636, dc_loss: 0.1675288826227188, tv_loss: 0.031011654064059258\n",
      "iteration 637, dc_loss: 0.16786478459835052, tv_loss: 0.030285438522696495\n",
      "iteration 638, dc_loss: 0.16717776656150818, tv_loss: 0.031220652163028717\n",
      "iteration 639, dc_loss: 0.16680772602558136, tv_loss: 0.030320551246404648\n",
      "iteration 640, dc_loss: 0.16511137783527374, tv_loss: 0.030931569635868073\n",
      "iteration 641, dc_loss: 0.1643524020910263, tv_loss: 0.030733155086636543\n",
      "iteration 642, dc_loss: 0.16405636072158813, tv_loss: 0.03069852851331234\n",
      "iteration 643, dc_loss: 0.16331924498081207, tv_loss: 0.031010916456580162\n",
      "iteration 644, dc_loss: 0.16309413313865662, tv_loss: 0.03066410683095455\n",
      "iteration 645, dc_loss: 0.16226917505264282, tv_loss: 0.030990295112133026\n",
      "iteration 646, dc_loss: 0.16144908964633942, tv_loss: 0.03082374297082424\n",
      "iteration 647, dc_loss: 0.1610720157623291, tv_loss: 0.030836062505841255\n",
      "iteration 648, dc_loss: 0.16042065620422363, tv_loss: 0.030994266271591187\n",
      "iteration 649, dc_loss: 0.16008366644382477, tv_loss: 0.030709994956851006\n",
      "iteration 650, dc_loss: 0.15934225916862488, tv_loss: 0.030942825600504875\n",
      "iteration 651, dc_loss: 0.15863406658172607, tv_loss: 0.030933057889342308\n",
      "iteration 652, dc_loss: 0.15812629461288452, tv_loss: 0.030814750120043755\n",
      "iteration 653, dc_loss: 0.15762917697429657, tv_loss: 0.030976073816418648\n",
      "iteration 654, dc_loss: 0.15730945765972137, tv_loss: 0.030820097774267197\n",
      "iteration 655, dc_loss: 0.15644364058971405, tv_loss: 0.031001338735222816\n",
      "iteration 656, dc_loss: 0.15587671101093292, tv_loss: 0.030879346653819084\n",
      "iteration 657, dc_loss: 0.15533792972564697, tv_loss: 0.030945727601647377\n",
      "iteration 658, dc_loss: 0.15476416051387787, tv_loss: 0.031002864241600037\n",
      "iteration 659, dc_loss: 0.15441104769706726, tv_loss: 0.030843382701277733\n",
      "iteration 660, dc_loss: 0.1537691205739975, tv_loss: 0.030962642282247543\n",
      "iteration 661, dc_loss: 0.1532927304506302, tv_loss: 0.030876033008098602\n",
      "iteration 662, dc_loss: 0.1525721549987793, tv_loss: 0.031111760064959526\n",
      "iteration 663, dc_loss: 0.15217390656471252, tv_loss: 0.030927633866667747\n",
      "iteration 664, dc_loss: 0.15155287086963654, tv_loss: 0.030994242057204247\n",
      "iteration 665, dc_loss: 0.15106289088726044, tv_loss: 0.031003383919596672\n",
      "iteration 666, dc_loss: 0.15060989558696747, tv_loss: 0.031035827472805977\n",
      "iteration 667, dc_loss: 0.14997214078903198, tv_loss: 0.03113316185772419\n",
      "iteration 668, dc_loss: 0.14975036680698395, tv_loss: 0.030840223655104637\n",
      "iteration 669, dc_loss: 0.14897692203521729, tv_loss: 0.031199119985103607\n",
      "iteration 670, dc_loss: 0.1487082988023758, tv_loss: 0.030962761491537094\n",
      "iteration 671, dc_loss: 0.1479130983352661, tv_loss: 0.031144144013524055\n",
      "iteration 672, dc_loss: 0.14748342335224152, tv_loss: 0.03102875128388405\n",
      "iteration 673, dc_loss: 0.14703978598117828, tv_loss: 0.031115524470806122\n",
      "iteration 674, dc_loss: 0.1464652121067047, tv_loss: 0.031166071072220802\n",
      "iteration 675, dc_loss: 0.1459442526102066, tv_loss: 0.031102556735277176\n",
      "iteration 676, dc_loss: 0.14546436071395874, tv_loss: 0.03109654411673546\n",
      "iteration 677, dc_loss: 0.1449323445558548, tv_loss: 0.0311561431735754\n",
      "iteration 678, dc_loss: 0.14448121190071106, tv_loss: 0.031126227229833603\n",
      "iteration 679, dc_loss: 0.14395134150981903, tv_loss: 0.031135061755776405\n",
      "iteration 680, dc_loss: 0.14348863065242767, tv_loss: 0.031104888767004013\n",
      "iteration 681, dc_loss: 0.14303140342235565, tv_loss: 0.031108178198337555\n",
      "iteration 682, dc_loss: 0.14247651398181915, tv_loss: 0.031255610287189484\n",
      "iteration 683, dc_loss: 0.142129048705101, tv_loss: 0.03112269565463066\n",
      "iteration 684, dc_loss: 0.14152748882770538, tv_loss: 0.031247446313500404\n",
      "iteration 685, dc_loss: 0.14134614169597626, tv_loss: 0.03111935220658779\n",
      "iteration 686, dc_loss: 0.14077357947826385, tv_loss: 0.03147399052977562\n",
      "iteration 687, dc_loss: 0.1410606950521469, tv_loss: 0.030953427776694298\n",
      "iteration 688, dc_loss: 0.14072199165821075, tv_loss: 0.03161657601594925\n",
      "iteration 689, dc_loss: 0.14113686978816986, tv_loss: 0.030807485803961754\n",
      "iteration 690, dc_loss: 0.14020128548145294, tv_loss: 0.03166018798947334\n",
      "iteration 691, dc_loss: 0.13943490386009216, tv_loss: 0.030978741124272346\n",
      "iteration 692, dc_loss: 0.13806487619876862, tv_loss: 0.03128890320658684\n",
      "iteration 693, dc_loss: 0.13742145895957947, tv_loss: 0.03143341466784477\n",
      "iteration 694, dc_loss: 0.1377529799938202, tv_loss: 0.030963024124503136\n",
      "iteration 695, dc_loss: 0.13704895973205566, tv_loss: 0.031495802104473114\n",
      "iteration 696, dc_loss: 0.13654226064682007, tv_loss: 0.031139910221099854\n",
      "iteration 697, dc_loss: 0.13585850596427917, tv_loss: 0.031147867441177368\n",
      "iteration 698, dc_loss: 0.13519811630249023, tv_loss: 0.03146705776453018\n",
      "iteration 699, dc_loss: 0.1352291852235794, tv_loss: 0.031048348173499107\n",
      "iteration 700, dc_loss: 0.13443580269813538, tv_loss: 0.03147608041763306\n",
      "iteration 701, dc_loss: 0.1339392215013504, tv_loss: 0.03139492869377136\n",
      "iteration 702, dc_loss: 0.13352741301059723, tv_loss: 0.031210487708449364\n",
      "iteration 703, dc_loss: 0.1328948587179184, tv_loss: 0.03143376111984253\n",
      "iteration 704, dc_loss: 0.1327444463968277, tv_loss: 0.031287916004657745\n",
      "iteration 705, dc_loss: 0.13215753436088562, tv_loss: 0.03152138367295265\n",
      "iteration 706, dc_loss: 0.13169196248054504, tv_loss: 0.03134169802069664\n",
      "iteration 707, dc_loss: 0.131204292178154, tv_loss: 0.03135973960161209\n",
      "iteration 708, dc_loss: 0.13066674768924713, tv_loss: 0.03153619170188904\n",
      "iteration 709, dc_loss: 0.1304706186056137, tv_loss: 0.03134014084935188\n",
      "iteration 710, dc_loss: 0.12991003692150116, tv_loss: 0.03148498758673668\n",
      "iteration 711, dc_loss: 0.12954504787921906, tv_loss: 0.03137552738189697\n",
      "iteration 712, dc_loss: 0.12905538082122803, tv_loss: 0.03149070963263512\n",
      "iteration 713, dc_loss: 0.1285756528377533, tv_loss: 0.03150556981563568\n",
      "iteration 714, dc_loss: 0.12827394902706146, tv_loss: 0.03137373551726341\n",
      "iteration 715, dc_loss: 0.1277577430009842, tv_loss: 0.03157338872551918\n",
      "iteration 716, dc_loss: 0.12750107049942017, tv_loss: 0.03141697868704796\n",
      "iteration 717, dc_loss: 0.12693333625793457, tv_loss: 0.03152036666870117\n",
      "iteration 718, dc_loss: 0.12651538848876953, tv_loss: 0.031472980976104736\n",
      "iteration 719, dc_loss: 0.12614966928958893, tv_loss: 0.03144620731472969\n",
      "iteration 720, dc_loss: 0.12562747299671173, tv_loss: 0.03157820925116539\n",
      "iteration 721, dc_loss: 0.1253480762243271, tv_loss: 0.031442344188690186\n",
      "iteration 722, dc_loss: 0.12488548457622528, tv_loss: 0.03152065724134445\n",
      "iteration 723, dc_loss: 0.124514140188694, tv_loss: 0.031495604664087296\n",
      "iteration 724, dc_loss: 0.12412126362323761, tv_loss: 0.03150181844830513\n",
      "iteration 725, dc_loss: 0.12368835508823395, tv_loss: 0.0315321646630764\n",
      "iteration 726, dc_loss: 0.12323510646820068, tv_loss: 0.0315437875688076\n",
      "iteration 727, dc_loss: 0.1228729784488678, tv_loss: 0.03149312734603882\n",
      "iteration 728, dc_loss: 0.12241005152463913, tv_loss: 0.031571757048368454\n",
      "iteration 729, dc_loss: 0.12214777618646622, tv_loss: 0.03146896883845329\n",
      "iteration 730, dc_loss: 0.12164221704006195, tv_loss: 0.03158475086092949\n",
      "iteration 731, dc_loss: 0.1213487833738327, tv_loss: 0.03147825598716736\n",
      "iteration 732, dc_loss: 0.12088900804519653, tv_loss: 0.03154783323407173\n",
      "iteration 733, dc_loss: 0.12050127983093262, tv_loss: 0.03157784789800644\n",
      "iteration 734, dc_loss: 0.12014862149953842, tv_loss: 0.03167722746729851\n",
      "iteration 735, dc_loss: 0.11972752958536148, tv_loss: 0.03167462348937988\n",
      "iteration 736, dc_loss: 0.1194051206111908, tv_loss: 0.031562454998493195\n",
      "iteration 737, dc_loss: 0.11898038536310196, tv_loss: 0.03160935640335083\n",
      "iteration 738, dc_loss: 0.1186646893620491, tv_loss: 0.031738247722387314\n",
      "iteration 739, dc_loss: 0.11821384727954865, tv_loss: 0.0317765511572361\n",
      "iteration 740, dc_loss: 0.11817005276679993, tv_loss: 0.03150397911667824\n",
      "iteration 741, dc_loss: 0.11762730032205582, tv_loss: 0.0319327749311924\n",
      "iteration 742, dc_loss: 0.11776602268218994, tv_loss: 0.03145702928304672\n",
      "iteration 743, dc_loss: 0.11727529764175415, tv_loss: 0.03193308413028717\n",
      "iteration 744, dc_loss: 0.11762924492359161, tv_loss: 0.03142658248543739\n",
      "iteration 745, dc_loss: 0.11698669195175171, tv_loss: 0.032076988369226456\n",
      "iteration 746, dc_loss: 0.11692649126052856, tv_loss: 0.0314139723777771\n",
      "iteration 747, dc_loss: 0.11579691618680954, tv_loss: 0.03183508291840553\n",
      "iteration 748, dc_loss: 0.11513317376375198, tv_loss: 0.03164256736636162\n",
      "iteration 749, dc_loss: 0.1148330494761467, tv_loss: 0.03157447651028633\n",
      "iteration 750, dc_loss: 0.1144445389509201, tv_loss: 0.03199242427945137\n",
      "iteration 751, dc_loss: 0.1145937368273735, tv_loss: 0.03154793381690979\n",
      "iteration 752, dc_loss: 0.11372731626033783, tv_loss: 0.031926751136779785\n",
      "iteration 753, dc_loss: 0.1134175956249237, tv_loss: 0.03162028267979622\n",
      "iteration 754, dc_loss: 0.11295340955257416, tv_loss: 0.03182607889175415\n",
      "iteration 755, dc_loss: 0.11248879879713058, tv_loss: 0.03195304423570633\n",
      "iteration 756, dc_loss: 0.11257896572351456, tv_loss: 0.03153659403324127\n",
      "iteration 757, dc_loss: 0.11183790862560272, tv_loss: 0.03201865777373314\n",
      "iteration 758, dc_loss: 0.11165270954370499, tv_loss: 0.03172827512025833\n",
      "iteration 759, dc_loss: 0.11121043562889099, tv_loss: 0.03173332288861275\n",
      "iteration 760, dc_loss: 0.11072832345962524, tv_loss: 0.031884048134088516\n",
      "iteration 761, dc_loss: 0.11075281351804733, tv_loss: 0.031598303467035294\n",
      "iteration 762, dc_loss: 0.11005785316228867, tv_loss: 0.032021742314100266\n",
      "iteration 763, dc_loss: 0.1098993718624115, tv_loss: 0.031717658042907715\n",
      "iteration 764, dc_loss: 0.109463170170784, tv_loss: 0.03177782893180847\n",
      "iteration 765, dc_loss: 0.1090887114405632, tv_loss: 0.03189771994948387\n",
      "iteration 766, dc_loss: 0.10910748690366745, tv_loss: 0.031607527285814285\n",
      "iteration 767, dc_loss: 0.10840419679880142, tv_loss: 0.03208034485578537\n",
      "iteration 768, dc_loss: 0.10828465968370438, tv_loss: 0.03184361010789871\n",
      "iteration 769, dc_loss: 0.10772660374641418, tv_loss: 0.03188794106245041\n",
      "iteration 770, dc_loss: 0.10735797137022018, tv_loss: 0.031894486397504807\n",
      "iteration 771, dc_loss: 0.10719472169876099, tv_loss: 0.03190929442644119\n",
      "iteration 772, dc_loss: 0.10674022138118744, tv_loss: 0.032024577260017395\n",
      "iteration 773, dc_loss: 0.10670439153909683, tv_loss: 0.03173123300075531\n",
      "iteration 774, dc_loss: 0.10608955472707748, tv_loss: 0.0320294052362442\n",
      "iteration 775, dc_loss: 0.10585296899080276, tv_loss: 0.03189896419644356\n",
      "iteration 776, dc_loss: 0.10551279783248901, tv_loss: 0.031883761286735535\n",
      "iteration 777, dc_loss: 0.10510117560625076, tv_loss: 0.031988270580768585\n",
      "iteration 778, dc_loss: 0.10509960353374481, tv_loss: 0.031740594655275345\n",
      "iteration 779, dc_loss: 0.10448584705591202, tv_loss: 0.032111264765262604\n",
      "iteration 780, dc_loss: 0.1044037938117981, tv_loss: 0.03182273358106613\n",
      "iteration 781, dc_loss: 0.10385406762361526, tv_loss: 0.03196040168404579\n",
      "iteration 782, dc_loss: 0.10356467217206955, tv_loss: 0.03194945305585861\n",
      "iteration 783, dc_loss: 0.10344383120536804, tv_loss: 0.031860604882240295\n",
      "iteration 784, dc_loss: 0.10305429995059967, tv_loss: 0.03207114711403847\n",
      "iteration 785, dc_loss: 0.10299240052700043, tv_loss: 0.031961191445589066\n",
      "iteration 786, dc_loss: 0.10251441597938538, tv_loss: 0.03210453316569328\n",
      "iteration 787, dc_loss: 0.10237392783164978, tv_loss: 0.03192427009344101\n",
      "iteration 788, dc_loss: 0.10211331397294998, tv_loss: 0.03191376104950905\n",
      "iteration 789, dc_loss: 0.10174641758203506, tv_loss: 0.03219227492809296\n",
      "iteration 790, dc_loss: 0.10180513560771942, tv_loss: 0.031856145709753036\n",
      "iteration 791, dc_loss: 0.10118399560451508, tv_loss: 0.03227218613028526\n",
      "iteration 792, dc_loss: 0.10130723565816879, tv_loss: 0.031697873026132584\n",
      "iteration 793, dc_loss: 0.10034432262182236, tv_loss: 0.032316919416189194\n",
      "iteration 794, dc_loss: 0.10018501430749893, tv_loss: 0.03193182498216629\n",
      "iteration 795, dc_loss: 0.0997510626912117, tv_loss: 0.03198917582631111\n",
      "iteration 796, dc_loss: 0.09936907142400742, tv_loss: 0.0321732759475708\n",
      "iteration 797, dc_loss: 0.09956622123718262, tv_loss: 0.03180825337767601\n",
      "iteration 798, dc_loss: 0.09884078800678253, tv_loss: 0.032306235283613205\n",
      "iteration 799, dc_loss: 0.09878213703632355, tv_loss: 0.0319388248026371\n",
      "iteration 800, dc_loss: 0.09832732379436493, tv_loss: 0.032040346413850784\n",
      "iteration 801, dc_loss: 0.09804341197013855, tv_loss: 0.03212182596325874\n",
      "iteration 802, dc_loss: 0.09793845564126968, tv_loss: 0.03190672770142555\n",
      "iteration 803, dc_loss: 0.09747020155191422, tv_loss: 0.03207552433013916\n",
      "iteration 804, dc_loss: 0.09727517515420914, tv_loss: 0.032181527465581894\n",
      "iteration 805, dc_loss: 0.09713730216026306, tv_loss: 0.03198137879371643\n",
      "iteration 806, dc_loss: 0.09683367609977722, tv_loss: 0.03200841695070267\n",
      "iteration 807, dc_loss: 0.09657885134220123, tv_loss: 0.03215157613158226\n",
      "iteration 808, dc_loss: 0.09639991074800491, tv_loss: 0.03208639845252037\n",
      "iteration 809, dc_loss: 0.0961873009800911, tv_loss: 0.03203457593917847\n",
      "iteration 810, dc_loss: 0.09591009467840195, tv_loss: 0.03214877471327782\n",
      "iteration 811, dc_loss: 0.09575290977954865, tv_loss: 0.032022688537836075\n",
      "iteration 812, dc_loss: 0.09550545364618301, tv_loss: 0.03204701468348503\n",
      "iteration 813, dc_loss: 0.0952015295624733, tv_loss: 0.032202836126089096\n",
      "iteration 814, dc_loss: 0.09506479650735855, tv_loss: 0.03211860731244087\n",
      "iteration 815, dc_loss: 0.09488873183727264, tv_loss: 0.03205075114965439\n",
      "iteration 816, dc_loss: 0.09453809261322021, tv_loss: 0.03218157961964607\n",
      "iteration 817, dc_loss: 0.0944080576300621, tv_loss: 0.03206689655780792\n",
      "iteration 818, dc_loss: 0.09421846270561218, tv_loss: 0.03210262209177017\n",
      "iteration 819, dc_loss: 0.09390345960855484, tv_loss: 0.03227473795413971\n",
      "iteration 820, dc_loss: 0.09374547749757767, tv_loss: 0.0321163684129715\n",
      "iteration 821, dc_loss: 0.09357486665248871, tv_loss: 0.03214864432811737\n",
      "iteration 822, dc_loss: 0.09325911104679108, tv_loss: 0.03228134289383888\n",
      "iteration 823, dc_loss: 0.09309054911136627, tv_loss: 0.03214050084352493\n",
      "iteration 824, dc_loss: 0.09293302893638611, tv_loss: 0.03216991573572159\n",
      "iteration 825, dc_loss: 0.09264575690031052, tv_loss: 0.03224693611264229\n",
      "iteration 826, dc_loss: 0.09247726947069168, tv_loss: 0.03214891254901886\n",
      "iteration 827, dc_loss: 0.09227888286113739, tv_loss: 0.03222489356994629\n",
      "iteration 828, dc_loss: 0.09202786535024643, tv_loss: 0.032241735607385635\n",
      "iteration 829, dc_loss: 0.09186744689941406, tv_loss: 0.03215137869119644\n",
      "iteration 830, dc_loss: 0.09167739748954773, tv_loss: 0.0322139598429203\n",
      "iteration 831, dc_loss: 0.09141132235527039, tv_loss: 0.03227444738149643\n",
      "iteration 832, dc_loss: 0.0912090465426445, tv_loss: 0.03221869468688965\n",
      "iteration 833, dc_loss: 0.09107229858636856, tv_loss: 0.03218057379126549\n",
      "iteration 834, dc_loss: 0.09082384407520294, tv_loss: 0.03229063004255295\n",
      "iteration 835, dc_loss: 0.09059973806142807, tv_loss: 0.03225192427635193\n",
      "iteration 836, dc_loss: 0.09048034995794296, tv_loss: 0.03219189867377281\n",
      "iteration 837, dc_loss: 0.09021002054214478, tv_loss: 0.032328467816114426\n",
      "iteration 838, dc_loss: 0.0899830013513565, tv_loss: 0.03228132799267769\n",
      "iteration 839, dc_loss: 0.08990651369094849, tv_loss: 0.0321960523724556\n",
      "iteration 840, dc_loss: 0.08961267024278641, tv_loss: 0.03231358155608177\n",
      "iteration 841, dc_loss: 0.08939962834119797, tv_loss: 0.032272618263959885\n",
      "iteration 842, dc_loss: 0.08928756415843964, tv_loss: 0.03224845230579376\n",
      "iteration 843, dc_loss: 0.08901304751634598, tv_loss: 0.03232339769601822\n",
      "iteration 844, dc_loss: 0.08882458508014679, tv_loss: 0.03227265179157257\n",
      "iteration 845, dc_loss: 0.08869414776563644, tv_loss: 0.03230361267924309\n",
      "iteration 846, dc_loss: 0.08843083679676056, tv_loss: 0.032326266169548035\n",
      "iteration 847, dc_loss: 0.08824802935123444, tv_loss: 0.03232506662607193\n",
      "iteration 848, dc_loss: 0.08808550983667374, tv_loss: 0.03236433118581772\n",
      "iteration 849, dc_loss: 0.08786418288946152, tv_loss: 0.03230965510010719\n",
      "iteration 850, dc_loss: 0.08765871077775955, tv_loss: 0.032374948263168335\n",
      "iteration 851, dc_loss: 0.0874786227941513, tv_loss: 0.03235394507646561\n",
      "iteration 852, dc_loss: 0.08730919659137726, tv_loss: 0.0322788767516613\n",
      "iteration 853, dc_loss: 0.08710391074419022, tv_loss: 0.03235854208469391\n",
      "iteration 854, dc_loss: 0.08690992742776871, tv_loss: 0.03235375136137009\n",
      "iteration 855, dc_loss: 0.0867357924580574, tv_loss: 0.032300375401973724\n",
      "iteration 856, dc_loss: 0.08654314279556274, tv_loss: 0.03236064687371254\n",
      "iteration 857, dc_loss: 0.0863441675901413, tv_loss: 0.03237884119153023\n",
      "iteration 858, dc_loss: 0.0861690565943718, tv_loss: 0.03231527656316757\n",
      "iteration 859, dc_loss: 0.08594958484172821, tv_loss: 0.03235763683915138\n",
      "iteration 860, dc_loss: 0.08580280095338821, tv_loss: 0.03235229104757309\n",
      "iteration 861, dc_loss: 0.08563520759344101, tv_loss: 0.03231394290924072\n",
      "iteration 862, dc_loss: 0.08539331704378128, tv_loss: 0.032369211316108704\n",
      "iteration 863, dc_loss: 0.08528592437505722, tv_loss: 0.03229337930679321\n",
      "iteration 864, dc_loss: 0.08503753691911697, tv_loss: 0.032329924404621124\n",
      "iteration 865, dc_loss: 0.08485573530197144, tv_loss: 0.032360177487134933\n",
      "iteration 866, dc_loss: 0.0847250372171402, tv_loss: 0.03233290836215019\n",
      "iteration 867, dc_loss: 0.08449675142765045, tv_loss: 0.03239478915929794\n",
      "iteration 868, dc_loss: 0.08429770171642303, tv_loss: 0.03240150213241577\n",
      "iteration 869, dc_loss: 0.08418243378400803, tv_loss: 0.0322997123003006\n",
      "iteration 870, dc_loss: 0.08395490050315857, tv_loss: 0.03236909210681915\n",
      "iteration 871, dc_loss: 0.0837894007563591, tv_loss: 0.0323546938598156\n",
      "iteration 872, dc_loss: 0.08360577374696732, tv_loss: 0.032412540167570114\n",
      "iteration 873, dc_loss: 0.08341645449399948, tv_loss: 0.032450754195451736\n",
      "iteration 874, dc_loss: 0.08332318067550659, tv_loss: 0.03230958804488182\n",
      "iteration 875, dc_loss: 0.08303304016590118, tv_loss: 0.03241322189569473\n",
      "iteration 876, dc_loss: 0.08288723975419998, tv_loss: 0.032374776899814606\n",
      "iteration 877, dc_loss: 0.08276136219501495, tv_loss: 0.03234776109457016\n",
      "iteration 878, dc_loss: 0.08253099769353867, tv_loss: 0.03243676945567131\n",
      "iteration 879, dc_loss: 0.082427479326725, tv_loss: 0.03243233636021614\n",
      "iteration 880, dc_loss: 0.08217219263315201, tv_loss: 0.032490842044353485\n",
      "iteration 881, dc_loss: 0.08203617483377457, tv_loss: 0.03238464891910553\n",
      "iteration 882, dc_loss: 0.08188486099243164, tv_loss: 0.03244968503713608\n",
      "iteration 883, dc_loss: 0.08167052268981934, tv_loss: 0.03252352774143219\n",
      "iteration 884, dc_loss: 0.08157019317150116, tv_loss: 0.03240136057138443\n",
      "iteration 885, dc_loss: 0.0813145786523819, tv_loss: 0.03249206393957138\n",
      "iteration 886, dc_loss: 0.08116070926189423, tv_loss: 0.032494429498910904\n",
      "iteration 887, dc_loss: 0.08107352256774902, tv_loss: 0.03239154443144798\n",
      "iteration 888, dc_loss: 0.0808306336402893, tv_loss: 0.032478976994752884\n",
      "iteration 889, dc_loss: 0.08070559799671173, tv_loss: 0.03242442384362221\n",
      "iteration 890, dc_loss: 0.08051615208387375, tv_loss: 0.032427456229925156\n",
      "iteration 891, dc_loss: 0.0803130641579628, tv_loss: 0.03248083218932152\n",
      "iteration 892, dc_loss: 0.08025716990232468, tv_loss: 0.03238821029663086\n",
      "iteration 893, dc_loss: 0.07997474074363708, tv_loss: 0.032553836703300476\n",
      "iteration 894, dc_loss: 0.07993265986442566, tv_loss: 0.032442606985569\n",
      "iteration 895, dc_loss: 0.0796569436788559, tv_loss: 0.03253462538123131\n",
      "iteration 896, dc_loss: 0.07959996163845062, tv_loss: 0.03241981565952301\n",
      "iteration 897, dc_loss: 0.07943447679281235, tv_loss: 0.03247600421309471\n",
      "iteration 898, dc_loss: 0.07936687022447586, tv_loss: 0.03244224563241005\n",
      "iteration 899, dc_loss: 0.07917789369821548, tv_loss: 0.03255553916096687\n",
      "iteration 900, dc_loss: 0.07923436909914017, tv_loss: 0.03239220380783081\n",
      "iteration 901, dc_loss: 0.07889524102210999, tv_loss: 0.03263336047530174\n",
      "iteration 902, dc_loss: 0.07888218760490417, tv_loss: 0.03243472799658775\n",
      "iteration 903, dc_loss: 0.07847405225038528, tv_loss: 0.03256748244166374\n",
      "iteration 904, dc_loss: 0.07833150774240494, tv_loss: 0.032441895455121994\n",
      "iteration 905, dc_loss: 0.07806161046028137, tv_loss: 0.032501187175512314\n",
      "iteration 906, dc_loss: 0.07787685096263885, tv_loss: 0.03253167122602463\n",
      "iteration 907, dc_loss: 0.07787974923849106, tv_loss: 0.032466672360897064\n",
      "iteration 908, dc_loss: 0.0776568278670311, tv_loss: 0.03262903541326523\n",
      "iteration 909, dc_loss: 0.07759872823953629, tv_loss: 0.03249898552894592\n",
      "iteration 910, dc_loss: 0.07736079394817352, tv_loss: 0.03251585736870766\n",
      "iteration 911, dc_loss: 0.07715669274330139, tv_loss: 0.032496970146894455\n",
      "iteration 912, dc_loss: 0.07693309336900711, tv_loss: 0.03259473666548729\n",
      "iteration 913, dc_loss: 0.07686915993690491, tv_loss: 0.03257245942950249\n",
      "iteration 914, dc_loss: 0.07668427377939224, tv_loss: 0.03258734941482544\n",
      "iteration 915, dc_loss: 0.07658357918262482, tv_loss: 0.03250870108604431\n",
      "iteration 916, dc_loss: 0.07642408460378647, tv_loss: 0.03252191096544266\n",
      "iteration 917, dc_loss: 0.07619874179363251, tv_loss: 0.032638829201459885\n",
      "iteration 918, dc_loss: 0.07617628574371338, tv_loss: 0.0325227826833725\n",
      "iteration 919, dc_loss: 0.07583726942539215, tv_loss: 0.032672781497240067\n",
      "iteration 920, dc_loss: 0.07585413753986359, tv_loss: 0.03247919678688049\n",
      "iteration 921, dc_loss: 0.07556555420160294, tv_loss: 0.03265110030770302\n",
      "iteration 922, dc_loss: 0.0754394382238388, tv_loss: 0.0326303206384182\n",
      "iteration 923, dc_loss: 0.07539936155080795, tv_loss: 0.03249436989426613\n",
      "iteration 924, dc_loss: 0.07510484755039215, tv_loss: 0.03267011418938637\n",
      "iteration 925, dc_loss: 0.07513546943664551, tv_loss: 0.032497987151145935\n",
      "iteration 926, dc_loss: 0.07481968402862549, tv_loss: 0.0326591394841671\n",
      "iteration 927, dc_loss: 0.07473708689212799, tv_loss: 0.032609906047582626\n",
      "iteration 928, dc_loss: 0.07464336603879929, tv_loss: 0.0325629897415638\n",
      "iteration 929, dc_loss: 0.07440687716007233, tv_loss: 0.03263797610998154\n",
      "iteration 930, dc_loss: 0.07433117181062698, tv_loss: 0.032548412680625916\n",
      "iteration 931, dc_loss: 0.07409002631902695, tv_loss: 0.03264651820063591\n",
      "iteration 932, dc_loss: 0.07396697252988815, tv_loss: 0.03265603631734848\n",
      "iteration 933, dc_loss: 0.07384752482175827, tv_loss: 0.032617442309856415\n",
      "iteration 934, dc_loss: 0.07369916886091232, tv_loss: 0.03258338198065758\n",
      "iteration 935, dc_loss: 0.07353615015745163, tv_loss: 0.03260735049843788\n",
      "iteration 936, dc_loss: 0.07343927770853043, tv_loss: 0.032574329525232315\n",
      "iteration 937, dc_loss: 0.07320315390825272, tv_loss: 0.032726772129535675\n",
      "iteration 938, dc_loss: 0.07317241281270981, tv_loss: 0.03261284530162811\n",
      "iteration 939, dc_loss: 0.07295620441436768, tv_loss: 0.032649021595716476\n",
      "iteration 940, dc_loss: 0.07284705340862274, tv_loss: 0.03259849175810814\n",
      "iteration 941, dc_loss: 0.0727289468050003, tv_loss: 0.032607678323984146\n",
      "iteration 942, dc_loss: 0.07260044664144516, tv_loss: 0.03263266384601593\n",
      "iteration 943, dc_loss: 0.07249664515256882, tv_loss: 0.03262235224246979\n",
      "iteration 944, dc_loss: 0.07236205041408539, tv_loss: 0.032651763409376144\n",
      "iteration 945, dc_loss: 0.07223149389028549, tv_loss: 0.0327301025390625\n",
      "iteration 946, dc_loss: 0.07221151888370514, tv_loss: 0.03259647265076637\n",
      "iteration 947, dc_loss: 0.07189241051673889, tv_loss: 0.03277742490172386\n",
      "iteration 948, dc_loss: 0.07201559841632843, tv_loss: 0.032501962035894394\n",
      "iteration 949, dc_loss: 0.07166461646556854, tv_loss: 0.032797396183013916\n",
      "iteration 950, dc_loss: 0.07187467068433762, tv_loss: 0.032498154789209366\n",
      "iteration 951, dc_loss: 0.07148760557174683, tv_loss: 0.03282763808965683\n",
      "iteration 952, dc_loss: 0.07156746089458466, tv_loss: 0.03254534304141998\n",
      "iteration 953, dc_loss: 0.07107140868902206, tv_loss: 0.032810747623443604\n",
      "iteration 954, dc_loss: 0.07097324728965759, tv_loss: 0.032655637711286545\n",
      "iteration 955, dc_loss: 0.07080861926078796, tv_loss: 0.032640255987644196\n",
      "iteration 956, dc_loss: 0.07063441723585129, tv_loss: 0.0327472947537899\n",
      "iteration 957, dc_loss: 0.07069098204374313, tv_loss: 0.03258063271641731\n",
      "iteration 958, dc_loss: 0.07033057510852814, tv_loss: 0.03280433639883995\n",
      "iteration 959, dc_loss: 0.07040884345769882, tv_loss: 0.03258302062749863\n",
      "iteration 960, dc_loss: 0.0700824186205864, tv_loss: 0.03281944990158081\n",
      "iteration 961, dc_loss: 0.07007329910993576, tv_loss: 0.03272460028529167\n",
      "iteration 962, dc_loss: 0.06990102678537369, tv_loss: 0.03270350769162178\n",
      "iteration 963, dc_loss: 0.06975050270557404, tv_loss: 0.032751813530921936\n",
      "iteration 964, dc_loss: 0.06967297196388245, tv_loss: 0.032678090035915375\n",
      "iteration 965, dc_loss: 0.0694451704621315, tv_loss: 0.032784998416900635\n",
      "iteration 966, dc_loss: 0.06933002173900604, tv_loss: 0.03270365670323372\n",
      "iteration 967, dc_loss: 0.069188691675663, tv_loss: 0.03272005170583725\n",
      "iteration 968, dc_loss: 0.0690140575170517, tv_loss: 0.03273482620716095\n",
      "iteration 969, dc_loss: 0.06896232068538666, tv_loss: 0.03273291140794754\n",
      "iteration 970, dc_loss: 0.06874722242355347, tv_loss: 0.03284475952386856\n",
      "iteration 971, dc_loss: 0.06875475496053696, tv_loss: 0.03266673535108566\n",
      "iteration 972, dc_loss: 0.06852762401103973, tv_loss: 0.032769281417131424\n",
      "iteration 973, dc_loss: 0.06843966990709305, tv_loss: 0.032808270305395126\n",
      "iteration 974, dc_loss: 0.0683455839753151, tv_loss: 0.03273490071296692\n",
      "iteration 975, dc_loss: 0.0681113600730896, tv_loss: 0.03285422548651695\n",
      "iteration 976, dc_loss: 0.06823962181806564, tv_loss: 0.03263816982507706\n",
      "iteration 977, dc_loss: 0.0679149478673935, tv_loss: 0.03290495648980141\n",
      "iteration 978, dc_loss: 0.06801006197929382, tv_loss: 0.03267017379403114\n",
      "iteration 979, dc_loss: 0.06773404777050018, tv_loss: 0.032838981598615646\n",
      "iteration 980, dc_loss: 0.06777925789356232, tv_loss: 0.03268156573176384\n",
      "iteration 981, dc_loss: 0.06759355962276459, tv_loss: 0.0328056700527668\n",
      "iteration 982, dc_loss: 0.06760383397340775, tv_loss: 0.03269629552960396\n",
      "iteration 983, dc_loss: 0.06741180270910263, tv_loss: 0.032850492745637894\n",
      "iteration 984, dc_loss: 0.06739117205142975, tv_loss: 0.03273772820830345\n",
      "iteration 985, dc_loss: 0.06702900677919388, tv_loss: 0.03290500119328499\n",
      "iteration 986, dc_loss: 0.06702331453561783, tv_loss: 0.032649651169776917\n",
      "iteration 987, dc_loss: 0.06664430350065231, tv_loss: 0.03284701332449913\n",
      "iteration 988, dc_loss: 0.06664496660232544, tv_loss: 0.03269581124186516\n",
      "iteration 989, dc_loss: 0.06642239540815353, tv_loss: 0.03281119093298912\n",
      "iteration 990, dc_loss: 0.06641405820846558, tv_loss: 0.03272289037704468\n",
      "iteration 991, dc_loss: 0.06624172627925873, tv_loss: 0.0327807292342186\n",
      "iteration 992, dc_loss: 0.06608021259307861, tv_loss: 0.03290024399757385\n",
      "iteration 993, dc_loss: 0.06607749313116074, tv_loss: 0.03278370574116707\n",
      "iteration 994, dc_loss: 0.06580797582864761, tv_loss: 0.03290267288684845\n",
      "iteration 995, dc_loss: 0.06596731394529343, tv_loss: 0.032723307609558105\n",
      "iteration 996, dc_loss: 0.065595842897892, tv_loss: 0.03300033137202263\n",
      "iteration 997, dc_loss: 0.06564082205295563, tv_loss: 0.032724328339099884\n",
      "iteration 998, dc_loss: 0.06531336158514023, tv_loss: 0.032919496297836304\n",
      "iteration 999, dc_loss: 0.06528797745704651, tv_loss: 0.03283701464533806\n",
      "iteration 1000, dc_loss: 0.0650951936841011, tv_loss: 0.03286159411072731\n",
      "iteration 1001, dc_loss: 0.0649743303656578, tv_loss: 0.03287293389439583\n",
      "iteration 1002, dc_loss: 0.06498201191425323, tv_loss: 0.032775577157735825\n",
      "iteration 1003, dc_loss: 0.06471478939056396, tv_loss: 0.03295348212122917\n",
      "iteration 1004, dc_loss: 0.0647684782743454, tv_loss: 0.03277437016367912\n",
      "iteration 1005, dc_loss: 0.0645155981183052, tv_loss: 0.03291313722729683\n",
      "iteration 1006, dc_loss: 0.0645236149430275, tv_loss: 0.0327659510076046\n",
      "iteration 1007, dc_loss: 0.0642724484205246, tv_loss: 0.032884154468774796\n",
      "iteration 1008, dc_loss: 0.06424970179796219, tv_loss: 0.03279561549425125\n",
      "iteration 1009, dc_loss: 0.06410367786884308, tv_loss: 0.03287035971879959\n",
      "iteration 1010, dc_loss: 0.06404800713062286, tv_loss: 0.032853711396455765\n",
      "iteration 1011, dc_loss: 0.06399104744195938, tv_loss: 0.032875511795282364\n",
      "iteration 1012, dc_loss: 0.06385224312543869, tv_loss: 0.0329110324382782\n",
      "iteration 1013, dc_loss: 0.06380818039178848, tv_loss: 0.03286406770348549\n",
      "iteration 1014, dc_loss: 0.06375154852867126, tv_loss: 0.032823190093040466\n",
      "iteration 1015, dc_loss: 0.06361038982868195, tv_loss: 0.032915856689214706\n",
      "iteration 1016, dc_loss: 0.06365541368722916, tv_loss: 0.03278373181819916\n",
      "iteration 1017, dc_loss: 0.06338822096586227, tv_loss: 0.03301561623811722\n",
      "iteration 1018, dc_loss: 0.06355517357587814, tv_loss: 0.03274533525109291\n",
      "iteration 1019, dc_loss: 0.0631740391254425, tv_loss: 0.033055663108825684\n",
      "iteration 1020, dc_loss: 0.06333722174167633, tv_loss: 0.03270293027162552\n",
      "iteration 1021, dc_loss: 0.06284722685813904, tv_loss: 0.03305163234472275\n",
      "iteration 1022, dc_loss: 0.06295623630285263, tv_loss: 0.03273366764187813\n",
      "iteration 1023, dc_loss: 0.062498804181814194, tv_loss: 0.033060744404792786\n",
      "iteration 1024, dc_loss: 0.06254105269908905, tv_loss: 0.03284848481416702\n",
      "iteration 1025, dc_loss: 0.0623517706990242, tv_loss: 0.03289297968149185\n",
      "iteration 1026, dc_loss: 0.06222778558731079, tv_loss: 0.03293873742222786\n",
      "iteration 1027, dc_loss: 0.06231638789176941, tv_loss: 0.032793078571558\n",
      "iteration 1028, dc_loss: 0.062048424035310745, tv_loss: 0.03305867686867714\n",
      "iteration 1029, dc_loss: 0.062209602445364, tv_loss: 0.03278299421072006\n",
      "iteration 1030, dc_loss: 0.0617690347135067, tv_loss: 0.033087752759456635\n",
      "iteration 1031, dc_loss: 0.06182484328746796, tv_loss: 0.03282564878463745\n",
      "iteration 1032, dc_loss: 0.06156815215945244, tv_loss: 0.032911427319049835\n",
      "iteration 1033, dc_loss: 0.06144716590642929, tv_loss: 0.03294152766466141\n",
      "iteration 1034, dc_loss: 0.0614507757127285, tv_loss: 0.032832179218530655\n",
      "iteration 1035, dc_loss: 0.06124347820878029, tv_loss: 0.03296361863613129\n",
      "iteration 1036, dc_loss: 0.061311621218919754, tv_loss: 0.03284354507923126\n",
      "iteration 1037, dc_loss: 0.0610683336853981, tv_loss: 0.033053942024707794\n",
      "iteration 1038, dc_loss: 0.06105919927358627, tv_loss: 0.032926660031080246\n",
      "iteration 1039, dc_loss: 0.06089584156870842, tv_loss: 0.03292522579431534\n",
      "iteration 1040, dc_loss: 0.060804583132267, tv_loss: 0.03291795030236244\n",
      "iteration 1041, dc_loss: 0.06070544570684433, tv_loss: 0.03296280652284622\n",
      "iteration 1042, dc_loss: 0.060561422258615494, tv_loss: 0.03303306922316551\n",
      "iteration 1043, dc_loss: 0.060447756201028824, tv_loss: 0.0329548642039299\n",
      "iteration 1044, dc_loss: 0.06036372110247612, tv_loss: 0.03293471410870552\n",
      "iteration 1045, dc_loss: 0.060199227184057236, tv_loss: 0.03310168534517288\n",
      "iteration 1046, dc_loss: 0.06015755981206894, tv_loss: 0.032958563417196274\n",
      "iteration 1047, dc_loss: 0.0600433275103569, tv_loss: 0.033023953437805176\n",
      "iteration 1048, dc_loss: 0.059936828911304474, tv_loss: 0.033020444214344025\n",
      "iteration 1049, dc_loss: 0.059834904968738556, tv_loss: 0.0329643152654171\n",
      "iteration 1050, dc_loss: 0.05975307151675224, tv_loss: 0.0329517163336277\n",
      "iteration 1051, dc_loss: 0.05970326066017151, tv_loss: 0.032935746014118195\n",
      "iteration 1052, dc_loss: 0.059526897966861725, tv_loss: 0.03306368365883827\n",
      "iteration 1053, dc_loss: 0.059574294835329056, tv_loss: 0.03291799873113632\n",
      "iteration 1054, dc_loss: 0.05934639647603035, tv_loss: 0.03306332603096962\n",
      "iteration 1055, dc_loss: 0.05955807492136955, tv_loss: 0.032802727073431015\n",
      "iteration 1056, dc_loss: 0.05924020707607269, tv_loss: 0.033162280917167664\n",
      "iteration 1057, dc_loss: 0.05954645201563835, tv_loss: 0.03281480818986893\n",
      "iteration 1058, dc_loss: 0.05915720760822296, tv_loss: 0.033213648945093155\n",
      "iteration 1059, dc_loss: 0.05949041619896889, tv_loss: 0.03276529163122177\n",
      "iteration 1060, dc_loss: 0.05909045413136482, tv_loss: 0.03315190598368645\n",
      "iteration 1061, dc_loss: 0.059180568903684616, tv_loss: 0.03293429687619209\n",
      "iteration 1062, dc_loss: 0.05886134132742882, tv_loss: 0.033099256455898285\n",
      "iteration 1063, dc_loss: 0.05869586020708084, tv_loss: 0.03294425085186958\n",
      "iteration 1064, dc_loss: 0.0584583654999733, tv_loss: 0.03298773989081383\n",
      "iteration 1065, dc_loss: 0.05822987109422684, tv_loss: 0.03315511718392372\n",
      "iteration 1066, dc_loss: 0.0583818145096302, tv_loss: 0.032928142696619034\n",
      "iteration 1067, dc_loss: 0.05819542706012726, tv_loss: 0.0331515409052372\n",
      "iteration 1068, dc_loss: 0.05832380801439285, tv_loss: 0.03296501934528351\n",
      "iteration 1069, dc_loss: 0.05801776424050331, tv_loss: 0.03306937590241432\n",
      "iteration 1070, dc_loss: 0.05794435366988182, tv_loss: 0.03301184996962547\n",
      "iteration 1071, dc_loss: 0.05785086378455162, tv_loss: 0.0330234058201313\n",
      "iteration 1072, dc_loss: 0.057646509259939194, tv_loss: 0.03308738023042679\n",
      "iteration 1073, dc_loss: 0.05765033885836601, tv_loss: 0.03293831646442413\n",
      "iteration 1074, dc_loss: 0.05754449963569641, tv_loss: 0.03307799622416496\n",
      "iteration 1075, dc_loss: 0.057638563215732574, tv_loss: 0.033007677644491196\n",
      "iteration 1076, dc_loss: 0.05736945569515228, tv_loss: 0.03308584541082382\n",
      "iteration 1077, dc_loss: 0.05729059875011444, tv_loss: 0.03297637775540352\n",
      "iteration 1078, dc_loss: 0.057224683463573456, tv_loss: 0.03304420784115791\n",
      "iteration 1079, dc_loss: 0.0571100190281868, tv_loss: 0.033003635704517365\n",
      "iteration 1080, dc_loss: 0.05696583539247513, tv_loss: 0.03306173160672188\n",
      "iteration 1081, dc_loss: 0.05689554661512375, tv_loss: 0.033059261739254\n",
      "iteration 1082, dc_loss: 0.056858256459236145, tv_loss: 0.03300051763653755\n",
      "iteration 1083, dc_loss: 0.05663943663239479, tv_loss: 0.03310500457882881\n",
      "iteration 1084, dc_loss: 0.056685205549001694, tv_loss: 0.03294651210308075\n",
      "iteration 1085, dc_loss: 0.05643254518508911, tv_loss: 0.03311756253242493\n",
      "iteration 1086, dc_loss: 0.05648453161120415, tv_loss: 0.03296010568737984\n",
      "iteration 1087, dc_loss: 0.0562954917550087, tv_loss: 0.0330611951649189\n",
      "iteration 1088, dc_loss: 0.05625046789646149, tv_loss: 0.033053744584321976\n",
      "iteration 1089, dc_loss: 0.056121088564395905, tv_loss: 0.03314577415585518\n",
      "iteration 1090, dc_loss: 0.056063197553157806, tv_loss: 0.03305013105273247\n",
      "iteration 1091, dc_loss: 0.05596292391419411, tv_loss: 0.033043257892131805\n",
      "iteration 1092, dc_loss: 0.05586061254143715, tv_loss: 0.03307311609387398\n",
      "iteration 1093, dc_loss: 0.05578957870602608, tv_loss: 0.03310127183794975\n",
      "iteration 1094, dc_loss: 0.05565078184008598, tv_loss: 0.03312530741095543\n",
      "iteration 1095, dc_loss: 0.05570222809910774, tv_loss: 0.03298352286219597\n",
      "iteration 1096, dc_loss: 0.0554632842540741, tv_loss: 0.033141497522592545\n",
      "iteration 1097, dc_loss: 0.055542200803756714, tv_loss: 0.03300684317946434\n",
      "iteration 1098, dc_loss: 0.05529084429144859, tv_loss: 0.03322162106633186\n",
      "iteration 1099, dc_loss: 0.05539793521165848, tv_loss: 0.032993804663419724\n",
      "iteration 1100, dc_loss: 0.05513114854693413, tv_loss: 0.03313037008047104\n",
      "iteration 1101, dc_loss: 0.05514099821448326, tv_loss: 0.03303959593176842\n",
      "iteration 1102, dc_loss: 0.054952848702669144, tv_loss: 0.033221956342458725\n",
      "iteration 1103, dc_loss: 0.055023133754730225, tv_loss: 0.033038023859262466\n",
      "iteration 1104, dc_loss: 0.05481233075261116, tv_loss: 0.03315145894885063\n",
      "iteration 1105, dc_loss: 0.05487776920199394, tv_loss: 0.033016834408044815\n",
      "iteration 1106, dc_loss: 0.05462581664323807, tv_loss: 0.033224619925022125\n",
      "iteration 1107, dc_loss: 0.054768647998571396, tv_loss: 0.03301924467086792\n",
      "iteration 1108, dc_loss: 0.054576147347688675, tv_loss: 0.033157266676425934\n",
      "iteration 1109, dc_loss: 0.05467969551682472, tv_loss: 0.03300277888774872\n",
      "iteration 1110, dc_loss: 0.0544402115046978, tv_loss: 0.03320952132344246\n",
      "iteration 1111, dc_loss: 0.05468039587140083, tv_loss: 0.03294761851429939\n",
      "iteration 1112, dc_loss: 0.05442342162132263, tv_loss: 0.03327818587422371\n",
      "iteration 1113, dc_loss: 0.054674264043569565, tv_loss: 0.03294763341546059\n",
      "iteration 1114, dc_loss: 0.05421113595366478, tv_loss: 0.03333607316017151\n",
      "iteration 1115, dc_loss: 0.054401155561208725, tv_loss: 0.0329132080078125\n",
      "iteration 1116, dc_loss: 0.05385845899581909, tv_loss: 0.03329680487513542\n",
      "iteration 1117, dc_loss: 0.053965114057064056, tv_loss: 0.03299284726381302\n",
      "iteration 1118, dc_loss: 0.053647927939891815, tv_loss: 0.033186085522174835\n",
      "iteration 1119, dc_loss: 0.05368681997060776, tv_loss: 0.03308846428990364\n",
      "iteration 1120, dc_loss: 0.053658198565244675, tv_loss: 0.03311767801642418\n",
      "iteration 1121, dc_loss: 0.05357012897729874, tv_loss: 0.03321203961968422\n",
      "iteration 1122, dc_loss: 0.053597863763570786, tv_loss: 0.033064186573028564\n",
      "iteration 1123, dc_loss: 0.05334247648715973, tv_loss: 0.033197030425071716\n",
      "iteration 1124, dc_loss: 0.05335238575935364, tv_loss: 0.033042505383491516\n",
      "iteration 1125, dc_loss: 0.05310143157839775, tv_loss: 0.033176448196172714\n",
      "iteration 1126, dc_loss: 0.05310596898198128, tv_loss: 0.03306720778346062\n",
      "iteration 1127, dc_loss: 0.05299048870801926, tv_loss: 0.03315024450421333\n",
      "iteration 1128, dc_loss: 0.052941471338272095, tv_loss: 0.03318844735622406\n",
      "iteration 1129, dc_loss: 0.05294763296842575, tv_loss: 0.03313165903091431\n",
      "iteration 1130, dc_loss: 0.0528196282684803, tv_loss: 0.03315481171011925\n",
      "iteration 1131, dc_loss: 0.05277206376194954, tv_loss: 0.033089715987443924\n",
      "iteration 1132, dc_loss: 0.05261244252324104, tv_loss: 0.033132873475551605\n",
      "iteration 1133, dc_loss: 0.05253998935222626, tv_loss: 0.033102620393037796\n",
      "iteration 1134, dc_loss: 0.05241158604621887, tv_loss: 0.0331314317882061\n",
      "iteration 1135, dc_loss: 0.05236257240176201, tv_loss: 0.033172860741615295\n",
      "iteration 1136, dc_loss: 0.0522836409509182, tv_loss: 0.03322548791766167\n",
      "iteration 1137, dc_loss: 0.052195094525814056, tv_loss: 0.033169012516736984\n",
      "iteration 1138, dc_loss: 0.05217951536178589, tv_loss: 0.03310331702232361\n",
      "iteration 1139, dc_loss: 0.05203435197472572, tv_loss: 0.0332607701420784\n",
      "iteration 1140, dc_loss: 0.05203502997756004, tv_loss: 0.033162739127874374\n",
      "iteration 1141, dc_loss: 0.05186982452869415, tv_loss: 0.03321373090147972\n",
      "iteration 1142, dc_loss: 0.05189187452197075, tv_loss: 0.03314751386642456\n",
      "iteration 1143, dc_loss: 0.051696404814720154, tv_loss: 0.033324480056762695\n",
      "iteration 1144, dc_loss: 0.051814425736665726, tv_loss: 0.03308861330151558\n",
      "iteration 1145, dc_loss: 0.05154537409543991, tv_loss: 0.03329507261514664\n",
      "iteration 1146, dc_loss: 0.05171307176351547, tv_loss: 0.033086761832237244\n",
      "iteration 1147, dc_loss: 0.05142553150653839, tv_loss: 0.03333152458071709\n",
      "iteration 1148, dc_loss: 0.05157895013689995, tv_loss: 0.03306443244218826\n",
      "iteration 1149, dc_loss: 0.051281362771987915, tv_loss: 0.03332442790269852\n",
      "iteration 1150, dc_loss: 0.05146472156047821, tv_loss: 0.03309072554111481\n",
      "iteration 1151, dc_loss: 0.051130518317222595, tv_loss: 0.03336593881249428\n",
      "iteration 1152, dc_loss: 0.05132804438471794, tv_loss: 0.033038925379514694\n",
      "iteration 1153, dc_loss: 0.05098723620176315, tv_loss: 0.03331459313631058\n",
      "iteration 1154, dc_loss: 0.05111795663833618, tv_loss: 0.03313151374459267\n",
      "iteration 1155, dc_loss: 0.05082475394010544, tv_loss: 0.03330657631158829\n",
      "iteration 1156, dc_loss: 0.050932448357343674, tv_loss: 0.03311408683657646\n",
      "iteration 1157, dc_loss: 0.050780389457941055, tv_loss: 0.03323552384972572\n",
      "iteration 1158, dc_loss: 0.05080033838748932, tv_loss: 0.03320416063070297\n",
      "iteration 1159, dc_loss: 0.05072556063532829, tv_loss: 0.033235158771276474\n",
      "iteration 1160, dc_loss: 0.050704121589660645, tv_loss: 0.03317762538790703\n",
      "iteration 1161, dc_loss: 0.050670791417360306, tv_loss: 0.03322131186723709\n",
      "iteration 1162, dc_loss: 0.05066441372036934, tv_loss: 0.033193230628967285\n",
      "iteration 1163, dc_loss: 0.05055608972907066, tv_loss: 0.03328844532370567\n",
      "iteration 1164, dc_loss: 0.050605013966560364, tv_loss: 0.03313956782221794\n",
      "iteration 1165, dc_loss: 0.05041885003447533, tv_loss: 0.03326483070850372\n",
      "iteration 1166, dc_loss: 0.05041709169745445, tv_loss: 0.03311047703027725\n",
      "iteration 1167, dc_loss: 0.05012628436088562, tv_loss: 0.03328133746981621\n",
      "iteration 1168, dc_loss: 0.05021314695477486, tv_loss: 0.03307335078716278\n",
      "iteration 1169, dc_loss: 0.04986374080181122, tv_loss: 0.033364053815603256\n",
      "iteration 1170, dc_loss: 0.05015959218144417, tv_loss: 0.03300682455301285\n",
      "iteration 1171, dc_loss: 0.04976791888475418, tv_loss: 0.03340618684887886\n",
      "iteration 1172, dc_loss: 0.05005117878317833, tv_loss: 0.03312111645936966\n",
      "iteration 1173, dc_loss: 0.04963552951812744, tv_loss: 0.03340538591146469\n",
      "iteration 1174, dc_loss: 0.04974909871816635, tv_loss: 0.033111944794654846\n",
      "iteration 1175, dc_loss: 0.04947230964899063, tv_loss: 0.03336884826421738\n",
      "iteration 1176, dc_loss: 0.049424104392528534, tv_loss: 0.0332733616232872\n",
      "iteration 1177, dc_loss: 0.04947569593787193, tv_loss: 0.033140189945697784\n",
      "iteration 1178, dc_loss: 0.049267031252384186, tv_loss: 0.03337669372558594\n",
      "iteration 1179, dc_loss: 0.04944188892841339, tv_loss: 0.033182669430971146\n",
      "iteration 1180, dc_loss: 0.049141738563776016, tv_loss: 0.03334442153573036\n",
      "iteration 1181, dc_loss: 0.04924502968788147, tv_loss: 0.033179860562086105\n",
      "iteration 1182, dc_loss: 0.049015022814273834, tv_loss: 0.033343371003866196\n",
      "iteration 1183, dc_loss: 0.04901403933763504, tv_loss: 0.03321828693151474\n",
      "iteration 1184, dc_loss: 0.048921044915914536, tv_loss: 0.03322761133313179\n",
      "iteration 1185, dc_loss: 0.0488322488963604, tv_loss: 0.03328453376889229\n",
      "iteration 1186, dc_loss: 0.04881209507584572, tv_loss: 0.03326481580734253\n",
      "iteration 1187, dc_loss: 0.048713233321905136, tv_loss: 0.03326791897416115\n",
      "iteration 1188, dc_loss: 0.048691075295209885, tv_loss: 0.033220984041690826\n",
      "iteration 1189, dc_loss: 0.04860306531190872, tv_loss: 0.03323369100689888\n",
      "iteration 1190, dc_loss: 0.04853927344083786, tv_loss: 0.033227160573005676\n",
      "iteration 1191, dc_loss: 0.04846222326159477, tv_loss: 0.0332372710108757\n",
      "iteration 1192, dc_loss: 0.048406511545181274, tv_loss: 0.033248718827962875\n",
      "iteration 1193, dc_loss: 0.04835585132241249, tv_loss: 0.0332520455121994\n",
      "iteration 1194, dc_loss: 0.048236116766929626, tv_loss: 0.033330053091049194\n",
      "iteration 1195, dc_loss: 0.0482734851539135, tv_loss: 0.033194445073604584\n",
      "iteration 1196, dc_loss: 0.048075005412101746, tv_loss: 0.033332519233226776\n",
      "iteration 1197, dc_loss: 0.0481748953461647, tv_loss: 0.033160217106342316\n",
      "iteration 1198, dc_loss: 0.04796949028968811, tv_loss: 0.03332459181547165\n",
      "iteration 1199, dc_loss: 0.04811463505029678, tv_loss: 0.03315240889787674\n",
      "iteration 1200, dc_loss: 0.04787224903702736, tv_loss: 0.033405330032110214\n",
      "iteration 1201, dc_loss: 0.04809148237109184, tv_loss: 0.033135440200567245\n",
      "iteration 1202, dc_loss: 0.047764722257852554, tv_loss: 0.03334818407893181\n",
      "iteration 1203, dc_loss: 0.0477171428501606, tv_loss: 0.03323788195848465\n",
      "iteration 1204, dc_loss: 0.04781411960721016, tv_loss: 0.033178746700286865\n",
      "iteration 1205, dc_loss: 0.04761641472578049, tv_loss: 0.03335030376911163\n",
      "iteration 1206, dc_loss: 0.04764346405863762, tv_loss: 0.03320308029651642\n",
      "iteration 1207, dc_loss: 0.04759787395596504, tv_loss: 0.033218979835510254\n",
      "iteration 1208, dc_loss: 0.04744580015540123, tv_loss: 0.03333407640457153\n",
      "iteration 1209, dc_loss: 0.047537725418806076, tv_loss: 0.033169329166412354\n",
      "iteration 1210, dc_loss: 0.047414399683475494, tv_loss: 0.03325467184185982\n",
      "iteration 1211, dc_loss: 0.04730387032032013, tv_loss: 0.03331505134701729\n",
      "iteration 1212, dc_loss: 0.04739508777856827, tv_loss: 0.033209528774023056\n",
      "iteration 1213, dc_loss: 0.04726765304803848, tv_loss: 0.0332871749997139\n",
      "iteration 1214, dc_loss: 0.04716663807630539, tv_loss: 0.033304400742053986\n",
      "iteration 1215, dc_loss: 0.047251779586076736, tv_loss: 0.03320099413394928\n",
      "iteration 1216, dc_loss: 0.047125931829214096, tv_loss: 0.033347003161907196\n",
      "iteration 1217, dc_loss: 0.04706985503435135, tv_loss: 0.03327513858675957\n",
      "iteration 1218, dc_loss: 0.047125332057476044, tv_loss: 0.03321259096264839\n",
      "iteration 1219, dc_loss: 0.04695410653948784, tv_loss: 0.03332702815532684\n",
      "iteration 1220, dc_loss: 0.0469646081328392, tv_loss: 0.03322884067893028\n",
      "iteration 1221, dc_loss: 0.046974342316389084, tv_loss: 0.03323002904653549\n",
      "iteration 1222, dc_loss: 0.046772003173828125, tv_loss: 0.033353157341480255\n",
      "iteration 1223, dc_loss: 0.0468515045940876, tv_loss: 0.033240724354982376\n",
      "iteration 1224, dc_loss: 0.04676143825054169, tv_loss: 0.033259227871894836\n",
      "iteration 1225, dc_loss: 0.046627238392829895, tv_loss: 0.03331970050930977\n",
      "iteration 1226, dc_loss: 0.04670584946870804, tv_loss: 0.03323245048522949\n",
      "iteration 1227, dc_loss: 0.04657316580414772, tv_loss: 0.0332830548286438\n",
      "iteration 1228, dc_loss: 0.046510715037584305, tv_loss: 0.03331659734249115\n",
      "iteration 1229, dc_loss: 0.04651989787817001, tv_loss: 0.033238817006349564\n",
      "iteration 1230, dc_loss: 0.04641653597354889, tv_loss: 0.033298954367637634\n",
      "iteration 1231, dc_loss: 0.04642174392938614, tv_loss: 0.03331291303038597\n",
      "iteration 1232, dc_loss: 0.0463433563709259, tv_loss: 0.03330826386809349\n",
      "iteration 1233, dc_loss: 0.0462968684732914, tv_loss: 0.033290937542915344\n",
      "iteration 1234, dc_loss: 0.04627075791358948, tv_loss: 0.033307384699583054\n",
      "iteration 1235, dc_loss: 0.04618643596768379, tv_loss: 0.03335481137037277\n",
      "iteration 1236, dc_loss: 0.046186696738004684, tv_loss: 0.03325515612959862\n",
      "iteration 1237, dc_loss: 0.046115756034851074, tv_loss: 0.03331649675965309\n",
      "iteration 1238, dc_loss: 0.04604000970721245, tv_loss: 0.03331783786416054\n",
      "iteration 1239, dc_loss: 0.046051979064941406, tv_loss: 0.03329591825604439\n",
      "iteration 1240, dc_loss: 0.045943424105644226, tv_loss: 0.03331573307514191\n",
      "iteration 1241, dc_loss: 0.04593276605010033, tv_loss: 0.03332975134253502\n",
      "iteration 1242, dc_loss: 0.045903805643320084, tv_loss: 0.03326354920864105\n",
      "iteration 1243, dc_loss: 0.04579683020710945, tv_loss: 0.033375151455402374\n",
      "iteration 1244, dc_loss: 0.045819900929927826, tv_loss: 0.03325536847114563\n",
      "iteration 1245, dc_loss: 0.04574670270085335, tv_loss: 0.03333815559744835\n",
      "iteration 1246, dc_loss: 0.04565194621682167, tv_loss: 0.03333015739917755\n",
      "iteration 1247, dc_loss: 0.04568704217672348, tv_loss: 0.03331330046057701\n",
      "iteration 1248, dc_loss: 0.04558074474334717, tv_loss: 0.03334176912903786\n",
      "iteration 1249, dc_loss: 0.0455472394824028, tv_loss: 0.03332719951868057\n",
      "iteration 1250, dc_loss: 0.045546650886535645, tv_loss: 0.033294256776571274\n",
      "iteration 1251, dc_loss: 0.045446764677762985, tv_loss: 0.0333370566368103\n",
      "iteration 1252, dc_loss: 0.04543366655707359, tv_loss: 0.033298641443252563\n",
      "iteration 1253, dc_loss: 0.04537886753678322, tv_loss: 0.033305034041404724\n",
      "iteration 1254, dc_loss: 0.04531651735305786, tv_loss: 0.0333760529756546\n",
      "iteration 1255, dc_loss: 0.04529218748211861, tv_loss: 0.033319104462862015\n",
      "iteration 1256, dc_loss: 0.04525632783770561, tv_loss: 0.03333134204149246\n",
      "iteration 1257, dc_loss: 0.04518549144268036, tv_loss: 0.033365171402692795\n",
      "iteration 1258, dc_loss: 0.045156080275774, tv_loss: 0.033325277268886566\n",
      "iteration 1259, dc_loss: 0.045124392956495285, tv_loss: 0.03329053893685341\n",
      "iteration 1260, dc_loss: 0.04506727680563927, tv_loss: 0.033329155296087265\n",
      "iteration 1261, dc_loss: 0.045015688985586166, tv_loss: 0.03333122655749321\n",
      "iteration 1262, dc_loss: 0.0449824295938015, tv_loss: 0.03333291783928871\n",
      "iteration 1263, dc_loss: 0.04493187740445137, tv_loss: 0.03332841768860817\n",
      "iteration 1264, dc_loss: 0.04489518702030182, tv_loss: 0.033336494117975235\n",
      "iteration 1265, dc_loss: 0.04484681040048599, tv_loss: 0.033348605036735535\n",
      "iteration 1266, dc_loss: 0.04480213299393654, tv_loss: 0.03332103416323662\n",
      "iteration 1267, dc_loss: 0.04478250816464424, tv_loss: 0.03332669287919998\n",
      "iteration 1268, dc_loss: 0.04471328482031822, tv_loss: 0.03334883600473404\n",
      "iteration 1269, dc_loss: 0.04465871304273605, tv_loss: 0.03336401283740997\n",
      "iteration 1270, dc_loss: 0.044671181589365005, tv_loss: 0.03328089043498039\n",
      "iteration 1271, dc_loss: 0.04458768665790558, tv_loss: 0.03335054591298103\n",
      "iteration 1272, dc_loss: 0.044530805200338364, tv_loss: 0.03334544226527214\n",
      "iteration 1273, dc_loss: 0.04451342672109604, tv_loss: 0.033331647515296936\n",
      "iteration 1274, dc_loss: 0.044462502002716064, tv_loss: 0.03332291916012764\n",
      "iteration 1275, dc_loss: 0.04444250464439392, tv_loss: 0.03333067521452904\n",
      "iteration 1276, dc_loss: 0.0443655364215374, tv_loss: 0.033372215926647186\n",
      "iteration 1277, dc_loss: 0.04432590305805206, tv_loss: 0.033353447914123535\n",
      "iteration 1278, dc_loss: 0.0443330854177475, tv_loss: 0.033293213695287704\n",
      "iteration 1279, dc_loss: 0.044234465807676315, tv_loss: 0.03334847092628479\n",
      "iteration 1280, dc_loss: 0.044215817004442215, tv_loss: 0.03336716815829277\n",
      "iteration 1281, dc_loss: 0.04417264834046364, tv_loss: 0.0333394818007946\n",
      "iteration 1282, dc_loss: 0.0441158190369606, tv_loss: 0.03334910422563553\n",
      "iteration 1283, dc_loss: 0.04411425068974495, tv_loss: 0.03330279886722565\n",
      "iteration 1284, dc_loss: 0.04403191804885864, tv_loss: 0.033389244228601456\n",
      "iteration 1285, dc_loss: 0.043991997838020325, tv_loss: 0.033375486731529236\n",
      "iteration 1286, dc_loss: 0.043981894850730896, tv_loss: 0.03331008180975914\n",
      "iteration 1287, dc_loss: 0.04389560967683792, tv_loss: 0.03337431326508522\n",
      "iteration 1288, dc_loss: 0.04390177130699158, tv_loss: 0.033360570669174194\n",
      "iteration 1289, dc_loss: 0.043852947652339935, tv_loss: 0.03334379568696022\n",
      "iteration 1290, dc_loss: 0.043770696967840195, tv_loss: 0.033365894109010696\n",
      "iteration 1291, dc_loss: 0.04377666860818863, tv_loss: 0.0333065390586853\n",
      "iteration 1292, dc_loss: 0.04371317848563194, tv_loss: 0.03333461284637451\n",
      "iteration 1293, dc_loss: 0.043663837015628815, tv_loss: 0.033367808908224106\n",
      "iteration 1294, dc_loss: 0.04364686459302902, tv_loss: 0.03338060900568962\n",
      "iteration 1295, dc_loss: 0.043573424220085144, tv_loss: 0.033381298184394836\n",
      "iteration 1296, dc_loss: 0.04358783736824989, tv_loss: 0.03330449387431145\n",
      "iteration 1297, dc_loss: 0.0434986837208271, tv_loss: 0.033342719078063965\n",
      "iteration 1298, dc_loss: 0.04346944019198418, tv_loss: 0.03334934636950493\n",
      "iteration 1299, dc_loss: 0.043444495648145676, tv_loss: 0.03337359428405762\n",
      "iteration 1300, dc_loss: 0.04338498041033745, tv_loss: 0.03338884934782982\n",
      "iteration 1301, dc_loss: 0.0433686263859272, tv_loss: 0.03332863375544548\n",
      "iteration 1302, dc_loss: 0.043289463967084885, tv_loss: 0.03335491940379143\n",
      "iteration 1303, dc_loss: 0.043290071189403534, tv_loss: 0.03331213444471359\n",
      "iteration 1304, dc_loss: 0.043221380561590195, tv_loss: 0.03333760052919388\n",
      "iteration 1305, dc_loss: 0.04320153966546059, tv_loss: 0.03331974893808365\n",
      "iteration 1306, dc_loss: 0.04314667731523514, tv_loss: 0.03334250673651695\n",
      "iteration 1307, dc_loss: 0.043114736676216125, tv_loss: 0.03336804732680321\n",
      "iteration 1308, dc_loss: 0.04306741803884506, tv_loss: 0.03340402618050575\n",
      "iteration 1309, dc_loss: 0.0430455282330513, tv_loss: 0.03335539624094963\n",
      "iteration 1310, dc_loss: 0.04295865818858147, tv_loss: 0.033378731459379196\n",
      "iteration 1311, dc_loss: 0.04297107458114624, tv_loss: 0.03331576660275459\n",
      "iteration 1312, dc_loss: 0.042898111045360565, tv_loss: 0.033354751765728\n",
      "iteration 1313, dc_loss: 0.042890798300504684, tv_loss: 0.03334534913301468\n",
      "iteration 1314, dc_loss: 0.042838335037231445, tv_loss: 0.033413343131542206\n",
      "iteration 1315, dc_loss: 0.04278542101383209, tv_loss: 0.03339952230453491\n",
      "iteration 1316, dc_loss: 0.04277471825480461, tv_loss: 0.03333587944507599\n",
      "iteration 1317, dc_loss: 0.04270090535283089, tv_loss: 0.03337051719427109\n",
      "iteration 1318, dc_loss: 0.04267379641532898, tv_loss: 0.033440180122852325\n",
      "iteration 1319, dc_loss: 0.0426531657576561, tv_loss: 0.03337956964969635\n",
      "iteration 1320, dc_loss: 0.04258687049150467, tv_loss: 0.03338043391704559\n",
      "iteration 1321, dc_loss: 0.04258761182427406, tv_loss: 0.03342575579881668\n",
      "iteration 1322, dc_loss: 0.042523354291915894, tv_loss: 0.03340282663702965\n",
      "iteration 1323, dc_loss: 0.042459581047296524, tv_loss: 0.033419545739889145\n",
      "iteration 1324, dc_loss: 0.042477428913116455, tv_loss: 0.033429160714149475\n",
      "iteration 1325, dc_loss: 0.04240098223090172, tv_loss: 0.03339211270213127\n",
      "iteration 1326, dc_loss: 0.04237717017531395, tv_loss: 0.03341984748840332\n",
      "iteration 1327, dc_loss: 0.04233282431960106, tv_loss: 0.03345372900366783\n",
      "iteration 1328, dc_loss: 0.04230250045657158, tv_loss: 0.03338317200541496\n",
      "iteration 1329, dc_loss: 0.0422559529542923, tv_loss: 0.03344309329986572\n",
      "iteration 1330, dc_loss: 0.042195264250040054, tv_loss: 0.03344743326306343\n",
      "iteration 1331, dc_loss: 0.04220544546842575, tv_loss: 0.033378906548023224\n",
      "iteration 1332, dc_loss: 0.04215385764837265, tv_loss: 0.03346268832683563\n",
      "iteration 1333, dc_loss: 0.04208648204803467, tv_loss: 0.033429261296987534\n",
      "iteration 1334, dc_loss: 0.042092565447092056, tv_loss: 0.033440716564655304\n",
      "iteration 1335, dc_loss: 0.04202481731772423, tv_loss: 0.033438991755247116\n",
      "iteration 1336, dc_loss: 0.04198106378316879, tv_loss: 0.033440232276916504\n",
      "iteration 1337, dc_loss: 0.041969213634729385, tv_loss: 0.03345330432057381\n",
      "iteration 1338, dc_loss: 0.041917022317647934, tv_loss: 0.0334191657602787\n",
      "iteration 1339, dc_loss: 0.04187662526965141, tv_loss: 0.03346488997340202\n",
      "iteration 1340, dc_loss: 0.04183686524629593, tv_loss: 0.03342569246888161\n",
      "iteration 1341, dc_loss: 0.041849203407764435, tv_loss: 0.033370014280080795\n",
      "iteration 1342, dc_loss: 0.041742559522390366, tv_loss: 0.03349791467189789\n",
      "iteration 1343, dc_loss: 0.04175698757171631, tv_loss: 0.03339265286922455\n",
      "iteration 1344, dc_loss: 0.0417100228369236, tv_loss: 0.033440664410591125\n",
      "iteration 1345, dc_loss: 0.04175283759832382, tv_loss: 0.033395156264305115\n",
      "iteration 1346, dc_loss: 0.04162914305925369, tv_loss: 0.033488981425762177\n",
      "iteration 1347, dc_loss: 0.04188651591539383, tv_loss: 0.033298999071121216\n",
      "iteration 1348, dc_loss: 0.04175253584980965, tv_loss: 0.03360480070114136\n",
      "iteration 1349, dc_loss: 0.042177312076091766, tv_loss: 0.0331961028277874\n",
      "iteration 1350, dc_loss: 0.04194742068648338, tv_loss: 0.033644042909145355\n",
      "iteration 1351, dc_loss: 0.04221729934215546, tv_loss: 0.033193979412317276\n",
      "iteration 1352, dc_loss: 0.04164190590381622, tv_loss: 0.03357213735580444\n",
      "iteration 1353, dc_loss: 0.04161687195301056, tv_loss: 0.033299852162599564\n",
      "iteration 1354, dc_loss: 0.04145180433988571, tv_loss: 0.03338921070098877\n",
      "iteration 1355, dc_loss: 0.04134254902601242, tv_loss: 0.03350578993558884\n",
      "iteration 1356, dc_loss: 0.04155740141868591, tv_loss: 0.03328341618180275\n",
      "iteration 1357, dc_loss: 0.041326556354761124, tv_loss: 0.033528897911310196\n",
      "iteration 1358, dc_loss: 0.04148438200354576, tv_loss: 0.03331189230084419\n",
      "iteration 1359, dc_loss: 0.041308291256427765, tv_loss: 0.03342069685459137\n",
      "iteration 1360, dc_loss: 0.04123419150710106, tv_loss: 0.03343137353658676\n",
      "iteration 1361, dc_loss: 0.04123024642467499, tv_loss: 0.03333186358213425\n",
      "iteration 1362, dc_loss: 0.0411573089659214, tv_loss: 0.03346308693289757\n",
      "iteration 1363, dc_loss: 0.04135223478078842, tv_loss: 0.03331702575087547\n",
      "iteration 1364, dc_loss: 0.04102591797709465, tv_loss: 0.03345226123929024\n",
      "iteration 1365, dc_loss: 0.04105377569794655, tv_loss: 0.03338531777262688\n",
      "iteration 1366, dc_loss: 0.041078321635723114, tv_loss: 0.03336469456553459\n",
      "iteration 1367, dc_loss: 0.04092356562614441, tv_loss: 0.03344139829277992\n",
      "iteration 1368, dc_loss: 0.041064921766519547, tv_loss: 0.033362723886966705\n",
      "iteration 1369, dc_loss: 0.04084716737270355, tv_loss: 0.03345499560236931\n",
      "iteration 1370, dc_loss: 0.04089396819472313, tv_loss: 0.03335009142756462\n",
      "iteration 1371, dc_loss: 0.04085936397314072, tv_loss: 0.03341372311115265\n",
      "iteration 1372, dc_loss: 0.04070102795958519, tv_loss: 0.033423323184251785\n",
      "iteration 1373, dc_loss: 0.040815576910972595, tv_loss: 0.03337453305721283\n",
      "iteration 1374, dc_loss: 0.040662623941898346, tv_loss: 0.0334668830037117\n",
      "iteration 1375, dc_loss: 0.04067542031407356, tv_loss: 0.033374935388565063\n",
      "iteration 1376, dc_loss: 0.04064277186989784, tv_loss: 0.03340071067214012\n",
      "iteration 1377, dc_loss: 0.04052552580833435, tv_loss: 0.03344586119055748\n",
      "iteration 1378, dc_loss: 0.040570735931396484, tv_loss: 0.03340214490890503\n",
      "iteration 1379, dc_loss: 0.040509216487407684, tv_loss: 0.03342566266655922\n",
      "iteration 1380, dc_loss: 0.04050545394420624, tv_loss: 0.03336511179804802\n",
      "iteration 1381, dc_loss: 0.040430549532175064, tv_loss: 0.033446330577135086\n",
      "iteration 1382, dc_loss: 0.040367722511291504, tv_loss: 0.033433154225349426\n",
      "iteration 1383, dc_loss: 0.04039817303419113, tv_loss: 0.03338795155286789\n",
      "iteration 1384, dc_loss: 0.040335770696401596, tv_loss: 0.03344671055674553\n",
      "iteration 1385, dc_loss: 0.040292445570230484, tv_loss: 0.033415842801332474\n",
      "iteration 1386, dc_loss: 0.04025783762335777, tv_loss: 0.03340170532464981\n",
      "iteration 1387, dc_loss: 0.04021785408258438, tv_loss: 0.03340904042124748\n",
      "iteration 1388, dc_loss: 0.040158290416002274, tv_loss: 0.03341684862971306\n",
      "iteration 1389, dc_loss: 0.0401604026556015, tv_loss: 0.033389996737241745\n",
      "iteration 1390, dc_loss: 0.04011838883161545, tv_loss: 0.033418770879507065\n",
      "iteration 1391, dc_loss: 0.04005345702171326, tv_loss: 0.03344571217894554\n",
      "iteration 1392, dc_loss: 0.040048375725746155, tv_loss: 0.03341936692595482\n",
      "iteration 1393, dc_loss: 0.03998655453324318, tv_loss: 0.033419396728277206\n",
      "iteration 1394, dc_loss: 0.039951957762241364, tv_loss: 0.033400796353816986\n",
      "iteration 1395, dc_loss: 0.039924781769514084, tv_loss: 0.03342307358980179\n",
      "iteration 1396, dc_loss: 0.03989534080028534, tv_loss: 0.03343704342842102\n",
      "iteration 1397, dc_loss: 0.039868392050266266, tv_loss: 0.03345304727554321\n",
      "iteration 1398, dc_loss: 0.039812978357076645, tv_loss: 0.03343648836016655\n",
      "iteration 1399, dc_loss: 0.03979415073990822, tv_loss: 0.03340836986899376\n",
      "iteration 1400, dc_loss: 0.0397629477083683, tv_loss: 0.033413153141736984\n",
      "iteration 1401, dc_loss: 0.03971124812960625, tv_loss: 0.033458590507507324\n",
      "iteration 1402, dc_loss: 0.039669763296842575, tv_loss: 0.03347548097372055\n",
      "iteration 1403, dc_loss: 0.03968507796525955, tv_loss: 0.0333985760807991\n",
      "iteration 1404, dc_loss: 0.03961583971977234, tv_loss: 0.033447664231061935\n",
      "iteration 1405, dc_loss: 0.0396001823246479, tv_loss: 0.03342583402991295\n",
      "iteration 1406, dc_loss: 0.039548471570014954, tv_loss: 0.03345293551683426\n",
      "iteration 1407, dc_loss: 0.03951118513941765, tv_loss: 0.033450160175561905\n",
      "iteration 1408, dc_loss: 0.03952173516154289, tv_loss: 0.03339175507426262\n",
      "iteration 1409, dc_loss: 0.03942100331187248, tv_loss: 0.033470917493104935\n",
      "iteration 1410, dc_loss: 0.039481498301029205, tv_loss: 0.033366426825523376\n",
      "iteration 1411, dc_loss: 0.039362356066703796, tv_loss: 0.03346895053982735\n",
      "iteration 1412, dc_loss: 0.03936704620718956, tv_loss: 0.03342001512646675\n",
      "iteration 1413, dc_loss: 0.03932943940162659, tv_loss: 0.03343454375863075\n",
      "iteration 1414, dc_loss: 0.03928570821881294, tv_loss: 0.03344103321433067\n",
      "iteration 1415, dc_loss: 0.039299093186855316, tv_loss: 0.03340506926178932\n",
      "iteration 1416, dc_loss: 0.03920910507440567, tv_loss: 0.03346980735659599\n",
      "iteration 1417, dc_loss: 0.03923089802265167, tv_loss: 0.03341227397322655\n",
      "iteration 1418, dc_loss: 0.03914763033390045, tv_loss: 0.03346681967377663\n",
      "iteration 1419, dc_loss: 0.039149992167949677, tv_loss: 0.03342519327998161\n",
      "iteration 1420, dc_loss: 0.039083853363990784, tv_loss: 0.03346184268593788\n",
      "iteration 1421, dc_loss: 0.03907504305243492, tv_loss: 0.03343457356095314\n",
      "iteration 1422, dc_loss: 0.0390433631837368, tv_loss: 0.033416036516427994\n",
      "iteration 1423, dc_loss: 0.03901359438896179, tv_loss: 0.03343707323074341\n",
      "iteration 1424, dc_loss: 0.0389779657125473, tv_loss: 0.03342803940176964\n",
      "iteration 1425, dc_loss: 0.03893740102648735, tv_loss: 0.03344588354229927\n",
      "iteration 1426, dc_loss: 0.03892046585679054, tv_loss: 0.033430855721235275\n",
      "iteration 1427, dc_loss: 0.0388985201716423, tv_loss: 0.03343365341424942\n",
      "iteration 1428, dc_loss: 0.03885100781917572, tv_loss: 0.033475447446107864\n",
      "iteration 1429, dc_loss: 0.038832325488328934, tv_loss: 0.033457811921834946\n",
      "iteration 1430, dc_loss: 0.038789212703704834, tv_loss: 0.03346198797225952\n",
      "iteration 1431, dc_loss: 0.03881487250328064, tv_loss: 0.033386874943971634\n",
      "iteration 1432, dc_loss: 0.03870148956775665, tv_loss: 0.033505238592624664\n",
      "iteration 1433, dc_loss: 0.03882588446140289, tv_loss: 0.033380597829818726\n",
      "iteration 1434, dc_loss: 0.038667336106300354, tv_loss: 0.03358866646885872\n",
      "iteration 1435, dc_loss: 0.03898270055651665, tv_loss: 0.033274948596954346\n",
      "iteration 1436, dc_loss: 0.03870958089828491, tv_loss: 0.033680181950330734\n",
      "iteration 1437, dc_loss: 0.03921079263091087, tv_loss: 0.03320131078362465\n",
      "iteration 1438, dc_loss: 0.03882177919149399, tv_loss: 0.03370491787791252\n",
      "iteration 1439, dc_loss: 0.039161015301942825, tv_loss: 0.03324900567531586\n",
      "iteration 1440, dc_loss: 0.03868266940116882, tv_loss: 0.033626437187194824\n",
      "iteration 1441, dc_loss: 0.03876866400241852, tv_loss: 0.03335314244031906\n",
      "iteration 1442, dc_loss: 0.03855004906654358, tv_loss: 0.03343600779771805\n",
      "iteration 1443, dc_loss: 0.03837083280086517, tv_loss: 0.03353124111890793\n",
      "iteration 1444, dc_loss: 0.03855776786804199, tv_loss: 0.03334542736411095\n",
      "iteration 1445, dc_loss: 0.03833969682455063, tv_loss: 0.033604931086301804\n",
      "iteration 1446, dc_loss: 0.038585927337408066, tv_loss: 0.03332770988345146\n",
      "iteration 1447, dc_loss: 0.03836199268698692, tv_loss: 0.0335281603038311\n",
      "iteration 1448, dc_loss: 0.038384415209293365, tv_loss: 0.03341003879904747\n",
      "iteration 1449, dc_loss: 0.038310207426548004, tv_loss: 0.033419784158468246\n",
      "iteration 1450, dc_loss: 0.038152627646923065, tv_loss: 0.03354542702436447\n",
      "iteration 1451, dc_loss: 0.03829555585980415, tv_loss: 0.03338753432035446\n",
      "iteration 1452, dc_loss: 0.038148801773786545, tv_loss: 0.033547304570674896\n",
      "iteration 1453, dc_loss: 0.03826281428337097, tv_loss: 0.033384062349796295\n",
      "iteration 1454, dc_loss: 0.03808962553739548, tv_loss: 0.033508289605379105\n",
      "iteration 1455, dc_loss: 0.03809601813554764, tv_loss: 0.03344803303480148\n",
      "iteration 1456, dc_loss: 0.03803270310163498, tv_loss: 0.03347576782107353\n",
      "iteration 1457, dc_loss: 0.03796372562646866, tv_loss: 0.0335082970559597\n",
      "iteration 1458, dc_loss: 0.03805080056190491, tv_loss: 0.03338250145316124\n",
      "iteration 1459, dc_loss: 0.037911418825387955, tv_loss: 0.03351568803191185\n",
      "iteration 1460, dc_loss: 0.037988077849149704, tv_loss: 0.03341027721762657\n",
      "iteration 1461, dc_loss: 0.03785361722111702, tv_loss: 0.03351026028394699\n",
      "iteration 1462, dc_loss: 0.03788337484002113, tv_loss: 0.033435363322496414\n",
      "iteration 1463, dc_loss: 0.03781381621956825, tv_loss: 0.033471446484327316\n",
      "iteration 1464, dc_loss: 0.03775882348418236, tv_loss: 0.03348791226744652\n",
      "iteration 1465, dc_loss: 0.03779402747750282, tv_loss: 0.033419568091630936\n",
      "iteration 1466, dc_loss: 0.03769111633300781, tv_loss: 0.033489424735307693\n",
      "iteration 1467, dc_loss: 0.037767037749290466, tv_loss: 0.033405669033527374\n",
      "iteration 1468, dc_loss: 0.03762038052082062, tv_loss: 0.03351952135562897\n",
      "iteration 1469, dc_loss: 0.0377236045897007, tv_loss: 0.03339764475822449\n",
      "iteration 1470, dc_loss: 0.037581343203783035, tv_loss: 0.03351974114775658\n",
      "iteration 1471, dc_loss: 0.03764137998223305, tv_loss: 0.03343711048364639\n",
      "iteration 1472, dc_loss: 0.03751387819647789, tv_loss: 0.033528123050928116\n",
      "iteration 1473, dc_loss: 0.03757501766085625, tv_loss: 0.03342418372631073\n",
      "iteration 1474, dc_loss: 0.037458617240190506, tv_loss: 0.033489201217889786\n",
      "iteration 1475, dc_loss: 0.03747370094060898, tv_loss: 0.033456701785326004\n",
      "iteration 1476, dc_loss: 0.037424392998218536, tv_loss: 0.033487387001514435\n",
      "iteration 1477, dc_loss: 0.03744269907474518, tv_loss: 0.03344925493001938\n",
      "iteration 1478, dc_loss: 0.037379294633865356, tv_loss: 0.03349020704627037\n",
      "iteration 1479, dc_loss: 0.03733697161078453, tv_loss: 0.0334959551692009\n",
      "iteration 1480, dc_loss: 0.03732997924089432, tv_loss: 0.033460699021816254\n",
      "iteration 1481, dc_loss: 0.03727749362587929, tv_loss: 0.0334855355322361\n",
      "iteration 1482, dc_loss: 0.037320949137210846, tv_loss: 0.033416975289583206\n",
      "iteration 1483, dc_loss: 0.03720853850245476, tv_loss: 0.03350432217121124\n",
      "iteration 1484, dc_loss: 0.037304528057575226, tv_loss: 0.03338775411248207\n",
      "iteration 1485, dc_loss: 0.03713317587971687, tv_loss: 0.0335627943277359\n",
      "iteration 1486, dc_loss: 0.03730710968375206, tv_loss: 0.03338761627674103\n",
      "iteration 1487, dc_loss: 0.03708355873823166, tv_loss: 0.03364885225892067\n",
      "iteration 1488, dc_loss: 0.03732864931225777, tv_loss: 0.03334661200642586\n",
      "iteration 1489, dc_loss: 0.03704507276415825, tv_loss: 0.03360149264335632\n",
      "iteration 1490, dc_loss: 0.03728507459163666, tv_loss: 0.03334914147853851\n",
      "iteration 1491, dc_loss: 0.037019938230514526, tv_loss: 0.03362453728914261\n",
      "iteration 1492, dc_loss: 0.03725077211856842, tv_loss: 0.033372458070516586\n",
      "iteration 1493, dc_loss: 0.03704381734132767, tv_loss: 0.03356426581740379\n",
      "iteration 1494, dc_loss: 0.037154920399188995, tv_loss: 0.033413469791412354\n",
      "iteration 1495, dc_loss: 0.03702317178249359, tv_loss: 0.033523090183734894\n",
      "iteration 1496, dc_loss: 0.03704926371574402, tv_loss: 0.03345201164484024\n",
      "iteration 1497, dc_loss: 0.03696123883128166, tv_loss: 0.033504944294691086\n",
      "iteration 1498, dc_loss: 0.03689337894320488, tv_loss: 0.03349664434790611\n",
      "iteration 1499, dc_loss: 0.03680998086929321, tv_loss: 0.03349214047193527\n",
      "iteration 1500, dc_loss: 0.036797840148210526, tv_loss: 0.03345465287566185\n",
      "iteration 1501, dc_loss: 0.036736927926540375, tv_loss: 0.033479586243629456\n",
      "iteration 1502, dc_loss: 0.03675777092576027, tv_loss: 0.03345029056072235\n",
      "iteration 1503, dc_loss: 0.03670284524559975, tv_loss: 0.03348672762513161\n",
      "iteration 1504, dc_loss: 0.03672608733177185, tv_loss: 0.033434852957725525\n",
      "iteration 1505, dc_loss: 0.03667447715997696, tv_loss: 0.033457133919000626\n",
      "iteration 1506, dc_loss: 0.03661603480577469, tv_loss: 0.03348121419548988\n",
      "iteration 1507, dc_loss: 0.03662487491965294, tv_loss: 0.03344380855560303\n",
      "iteration 1508, dc_loss: 0.03653937950730324, tv_loss: 0.03352881968021393\n",
      "iteration 1509, dc_loss: 0.03665298596024513, tv_loss: 0.03341333195567131\n",
      "iteration 1510, dc_loss: 0.036452967673540115, tv_loss: 0.03360695019364357\n",
      "iteration 1511, dc_loss: 0.03664021193981171, tv_loss: 0.033390577882528305\n",
      "iteration 1512, dc_loss: 0.03639953210949898, tv_loss: 0.03358563035726547\n",
      "iteration 1513, dc_loss: 0.03655507415533066, tv_loss: 0.03338538110256195\n",
      "iteration 1514, dc_loss: 0.0363614596426487, tv_loss: 0.033557333052158356\n",
      "iteration 1515, dc_loss: 0.036497216671705246, tv_loss: 0.033385373651981354\n",
      "iteration 1516, dc_loss: 0.03632371872663498, tv_loss: 0.03354860469698906\n",
      "iteration 1517, dc_loss: 0.03646484762430191, tv_loss: 0.03338763490319252\n",
      "iteration 1518, dc_loss: 0.036309827119112015, tv_loss: 0.0335562564432621\n",
      "iteration 1519, dc_loss: 0.03642752394080162, tv_loss: 0.03347324952483177\n",
      "iteration 1520, dc_loss: 0.036265578120946884, tv_loss: 0.03360632061958313\n",
      "iteration 1521, dc_loss: 0.03638036921620369, tv_loss: 0.03340750187635422\n",
      "iteration 1522, dc_loss: 0.03622167184948921, tv_loss: 0.033557843416929245\n",
      "iteration 1523, dc_loss: 0.03633112460374832, tv_loss: 0.033462755382061005\n",
      "iteration 1524, dc_loss: 0.03613129258155823, tv_loss: 0.03361600264906883\n",
      "iteration 1525, dc_loss: 0.03627565875649452, tv_loss: 0.03339440003037453\n",
      "iteration 1526, dc_loss: 0.036050714552402496, tv_loss: 0.033635303378105164\n",
      "iteration 1527, dc_loss: 0.03622838482260704, tv_loss: 0.033432915806770325\n",
      "iteration 1528, dc_loss: 0.035989001393318176, tv_loss: 0.03359127789735794\n",
      "iteration 1529, dc_loss: 0.036118123680353165, tv_loss: 0.033431027084589005\n",
      "iteration 1530, dc_loss: 0.03594544902443886, tv_loss: 0.0335906557738781\n",
      "iteration 1531, dc_loss: 0.036020807921886444, tv_loss: 0.03347742184996605\n",
      "iteration 1532, dc_loss: 0.035924628376960754, tv_loss: 0.03353201225399971\n",
      "iteration 1533, dc_loss: 0.035940833389759064, tv_loss: 0.03346746042370796\n",
      "iteration 1534, dc_loss: 0.03587455302476883, tv_loss: 0.033522117882966995\n",
      "iteration 1535, dc_loss: 0.03589276969432831, tv_loss: 0.033469174057245255\n",
      "iteration 1536, dc_loss: 0.03582387417554855, tv_loss: 0.03350260481238365\n",
      "iteration 1537, dc_loss: 0.03583190590143204, tv_loss: 0.033463578671216965\n",
      "iteration 1538, dc_loss: 0.03577783703804016, tv_loss: 0.03349728137254715\n",
      "iteration 1539, dc_loss: 0.03576795384287834, tv_loss: 0.03349126875400543\n",
      "iteration 1540, dc_loss: 0.03573647513985634, tv_loss: 0.033542245626449585\n",
      "iteration 1541, dc_loss: 0.035729676485061646, tv_loss: 0.033506978303194046\n",
      "iteration 1542, dc_loss: 0.035667531192302704, tv_loss: 0.03351730853319168\n",
      "iteration 1543, dc_loss: 0.035707321017980576, tv_loss: 0.033456794917583466\n",
      "iteration 1544, dc_loss: 0.035604629665613174, tv_loss: 0.03359369561076164\n",
      "iteration 1545, dc_loss: 0.03568914532661438, tv_loss: 0.0334748774766922\n",
      "iteration 1546, dc_loss: 0.035580676048994064, tv_loss: 0.03353758156299591\n",
      "iteration 1547, dc_loss: 0.03569534048438072, tv_loss: 0.0334404818713665\n",
      "iteration 1548, dc_loss: 0.03555018827319145, tv_loss: 0.03366722911596298\n",
      "iteration 1549, dc_loss: 0.03584974259138107, tv_loss: 0.03338165208697319\n",
      "iteration 1550, dc_loss: 0.035686127841472626, tv_loss: 0.03368594869971275\n",
      "iteration 1551, dc_loss: 0.03614573925733566, tv_loss: 0.03330602869391441\n",
      "iteration 1552, dc_loss: 0.03592076152563095, tv_loss: 0.033762868493795395\n",
      "iteration 1553, dc_loss: 0.036313753575086594, tv_loss: 0.03330173343420029\n",
      "iteration 1554, dc_loss: 0.03583564609289169, tv_loss: 0.03369346633553505\n",
      "iteration 1555, dc_loss: 0.0358063243329525, tv_loss: 0.033395517617464066\n",
      "iteration 1556, dc_loss: 0.03542738035321236, tv_loss: 0.033546846359968185\n",
      "iteration 1557, dc_loss: 0.03534874692559242, tv_loss: 0.0335334911942482\n",
      "iteration 1558, dc_loss: 0.035530973225831985, tv_loss: 0.03340104967355728\n",
      "iteration 1559, dc_loss: 0.03541604429483414, tv_loss: 0.03360747918486595\n",
      "iteration 1560, dc_loss: 0.03558969870209694, tv_loss: 0.0333995558321476\n",
      "iteration 1561, dc_loss: 0.035340238362550735, tv_loss: 0.03358856216073036\n",
      "iteration 1562, dc_loss: 0.035352371633052826, tv_loss: 0.03344971686601639\n",
      "iteration 1563, dc_loss: 0.03523855283856392, tv_loss: 0.03355023264884949\n",
      "iteration 1564, dc_loss: 0.03528212383389473, tv_loss: 0.033506568521261215\n",
      "iteration 1565, dc_loss: 0.03528019040822983, tv_loss: 0.03349527716636658\n",
      "iteration 1566, dc_loss: 0.03523889183998108, tv_loss: 0.03353621065616608\n",
      "iteration 1567, dc_loss: 0.03520321100950241, tv_loss: 0.033483706414699554\n",
      "iteration 1568, dc_loss: 0.035068877041339874, tv_loss: 0.03355851769447327\n",
      "iteration 1569, dc_loss: 0.035132408142089844, tv_loss: 0.03347131237387657\n",
      "iteration 1570, dc_loss: 0.03508609160780907, tv_loss: 0.033511560410261154\n",
      "iteration 1571, dc_loss: 0.03506944328546524, tv_loss: 0.033488865941762924\n",
      "iteration 1572, dc_loss: 0.035044264048337936, tv_loss: 0.03345778211951256\n",
      "iteration 1573, dc_loss: 0.03493919596076012, tv_loss: 0.03352491557598114\n",
      "iteration 1574, dc_loss: 0.035016871988773346, tv_loss: 0.033442579209804535\n",
      "iteration 1575, dc_loss: 0.03489457443356514, tv_loss: 0.03356456384062767\n",
      "iteration 1576, dc_loss: 0.034978292882442474, tv_loss: 0.03349566087126732\n",
      "iteration 1577, dc_loss: 0.03488663211464882, tv_loss: 0.03354071453213692\n",
      "iteration 1578, dc_loss: 0.034814439713954926, tv_loss: 0.03352554142475128\n",
      "iteration 1579, dc_loss: 0.034890685230493546, tv_loss: 0.03344006836414337\n",
      "iteration 1580, dc_loss: 0.03475979343056679, tv_loss: 0.03359609842300415\n",
      "iteration 1581, dc_loss: 0.03488466888666153, tv_loss: 0.03348185867071152\n",
      "iteration 1582, dc_loss: 0.034720126539468765, tv_loss: 0.033574189990758896\n",
      "iteration 1583, dc_loss: 0.034727826714515686, tv_loss: 0.0334957018494606\n",
      "iteration 1584, dc_loss: 0.03473449870944023, tv_loss: 0.033469829708337784\n",
      "iteration 1585, dc_loss: 0.034672223031520844, tv_loss: 0.033574655652046204\n",
      "iteration 1586, dc_loss: 0.03471776470541954, tv_loss: 0.03351525589823723\n",
      "iteration 1587, dc_loss: 0.034606512635946274, tv_loss: 0.033538129180669785\n",
      "iteration 1588, dc_loss: 0.034611161798238754, tv_loss: 0.0335090309381485\n",
      "iteration 1589, dc_loss: 0.03460052236914635, tv_loss: 0.033542878925800323\n",
      "iteration 1590, dc_loss: 0.03455132991075516, tv_loss: 0.03358535096049309\n",
      "iteration 1591, dc_loss: 0.034563951194286346, tv_loss: 0.03349321708083153\n",
      "iteration 1592, dc_loss: 0.03453376144170761, tv_loss: 0.0335313081741333\n",
      "iteration 1593, dc_loss: 0.03448767960071564, tv_loss: 0.03356320783495903\n",
      "iteration 1594, dc_loss: 0.03450435772538185, tv_loss: 0.03349534049630165\n",
      "iteration 1595, dc_loss: 0.034424182027578354, tv_loss: 0.0335400253534317\n",
      "iteration 1596, dc_loss: 0.034473732113838196, tv_loss: 0.033482082188129425\n",
      "iteration 1597, dc_loss: 0.03437238186597824, tv_loss: 0.033567819744348526\n",
      "iteration 1598, dc_loss: 0.03443801403045654, tv_loss: 0.033492643386125565\n",
      "iteration 1599, dc_loss: 0.03433363884687424, tv_loss: 0.033582575619220734\n",
      "iteration 1600, dc_loss: 0.034453585743904114, tv_loss: 0.0334344357252121\n",
      "iteration 1601, dc_loss: 0.034270573407411575, tv_loss: 0.03363358974456787\n",
      "iteration 1602, dc_loss: 0.03440804407000542, tv_loss: 0.033424898982048035\n",
      "iteration 1603, dc_loss: 0.03430139273405075, tv_loss: 0.03348603472113609\n",
      "iteration 1604, dc_loss: 0.03420756757259369, tv_loss: 0.03358706459403038\n",
      "iteration 1605, dc_loss: 0.03433986380696297, tv_loss: 0.033428262919187546\n",
      "iteration 1606, dc_loss: 0.03421685844659805, tv_loss: 0.0335078202188015\n",
      "iteration 1607, dc_loss: 0.03417490795254707, tv_loss: 0.033544715493917465\n",
      "iteration 1608, dc_loss: 0.034286074340343475, tv_loss: 0.03343188390135765\n",
      "iteration 1609, dc_loss: 0.03414159268140793, tv_loss: 0.033550456166267395\n",
      "iteration 1610, dc_loss: 0.03415708616375923, tv_loss: 0.03351019322872162\n",
      "iteration 1611, dc_loss: 0.03419508785009384, tv_loss: 0.03346269205212593\n",
      "iteration 1612, dc_loss: 0.03408786654472351, tv_loss: 0.03355932608246803\n",
      "iteration 1613, dc_loss: 0.03411911427974701, tv_loss: 0.03348779305815697\n",
      "iteration 1614, dc_loss: 0.03411395102739334, tv_loss: 0.03346700221300125\n",
      "iteration 1615, dc_loss: 0.03404488041996956, tv_loss: 0.03352820873260498\n",
      "iteration 1616, dc_loss: 0.034086450934410095, tv_loss: 0.03346369415521622\n",
      "iteration 1617, dc_loss: 0.034035179764032364, tv_loss: 0.033502403646707535\n",
      "iteration 1618, dc_loss: 0.033998843282461166, tv_loss: 0.03353526443243027\n",
      "iteration 1619, dc_loss: 0.034042224287986755, tv_loss: 0.033490728586912155\n",
      "iteration 1620, dc_loss: 0.033959295600652695, tv_loss: 0.03353886678814888\n",
      "iteration 1621, dc_loss: 0.03397151455283165, tv_loss: 0.033492691814899445\n",
      "iteration 1622, dc_loss: 0.03398102521896362, tv_loss: 0.033468883484601974\n",
      "iteration 1623, dc_loss: 0.033912744373083115, tv_loss: 0.0335177406668663\n",
      "iteration 1624, dc_loss: 0.033929578959941864, tv_loss: 0.03348233178257942\n",
      "iteration 1625, dc_loss: 0.03390321135520935, tv_loss: 0.03348696976900101\n",
      "iteration 1626, dc_loss: 0.03386509418487549, tv_loss: 0.03351613134145737\n",
      "iteration 1627, dc_loss: 0.03389626741409302, tv_loss: 0.03349334001541138\n",
      "iteration 1628, dc_loss: 0.03383670374751091, tv_loss: 0.03355216979980469\n",
      "iteration 1629, dc_loss: 0.03383263573050499, tv_loss: 0.03350651636719704\n",
      "iteration 1630, dc_loss: 0.03382284566760063, tv_loss: 0.033490777015686035\n",
      "iteration 1631, dc_loss: 0.03377585858106613, tv_loss: 0.03351876884698868\n",
      "iteration 1632, dc_loss: 0.03380382061004639, tv_loss: 0.03348010778427124\n",
      "iteration 1633, dc_loss: 0.033767636865377426, tv_loss: 0.03350343555212021\n",
      "iteration 1634, dc_loss: 0.03374211862683296, tv_loss: 0.03353384509682655\n",
      "iteration 1635, dc_loss: 0.033738307654857635, tv_loss: 0.033518075942993164\n",
      "iteration 1636, dc_loss: 0.033710092306137085, tv_loss: 0.033511631190776825\n",
      "iteration 1637, dc_loss: 0.03368956223130226, tv_loss: 0.03350738808512688\n",
      "iteration 1638, dc_loss: 0.033688776195049286, tv_loss: 0.03349069133400917\n",
      "iteration 1639, dc_loss: 0.03365730121731758, tv_loss: 0.033511288464069366\n",
      "iteration 1640, dc_loss: 0.03364973142743111, tv_loss: 0.03351225703954697\n",
      "iteration 1641, dc_loss: 0.03364258259534836, tv_loss: 0.03352190554141998\n",
      "iteration 1642, dc_loss: 0.03359747305512428, tv_loss: 0.033529605716466904\n",
      "iteration 1643, dc_loss: 0.033607084304094315, tv_loss: 0.033493395894765854\n",
      "iteration 1644, dc_loss: 0.03358529880642891, tv_loss: 0.0334944985806942\n",
      "iteration 1645, dc_loss: 0.03355320170521736, tv_loss: 0.03351027891039848\n",
      "iteration 1646, dc_loss: 0.033551886677742004, tv_loss: 0.03350304439663887\n",
      "iteration 1647, dc_loss: 0.03352677449584007, tv_loss: 0.033530570566654205\n",
      "iteration 1648, dc_loss: 0.03352503478527069, tv_loss: 0.03352715075016022\n",
      "iteration 1649, dc_loss: 0.03348971903324127, tv_loss: 0.033524394035339355\n",
      "iteration 1650, dc_loss: 0.03348517045378685, tv_loss: 0.03350105509161949\n",
      "iteration 1651, dc_loss: 0.03346661105751991, tv_loss: 0.033496614545583725\n",
      "iteration 1652, dc_loss: 0.033443257212638855, tv_loss: 0.03350849449634552\n",
      "iteration 1653, dc_loss: 0.03344191610813141, tv_loss: 0.033500585705041885\n",
      "iteration 1654, dc_loss: 0.03340228274464607, tv_loss: 0.03355540335178375\n",
      "iteration 1655, dc_loss: 0.03340571001172066, tv_loss: 0.033512942492961884\n",
      "iteration 1656, dc_loss: 0.033379968255758286, tv_loss: 0.03351840004324913\n",
      "iteration 1657, dc_loss: 0.03337937965989113, tv_loss: 0.033488474786281586\n",
      "iteration 1658, dc_loss: 0.0333591066300869, tv_loss: 0.03348805010318756\n",
      "iteration 1659, dc_loss: 0.033320918679237366, tv_loss: 0.033517833799123764\n",
      "iteration 1660, dc_loss: 0.033326178789138794, tv_loss: 0.033498313277959824\n",
      "iteration 1661, dc_loss: 0.033296138048172, tv_loss: 0.033537138253450394\n",
      "iteration 1662, dc_loss: 0.03328842669725418, tv_loss: 0.03354937583208084\n",
      "iteration 1663, dc_loss: 0.033261511474847794, tv_loss: 0.03352813795208931\n",
      "iteration 1664, dc_loss: 0.03325694799423218, tv_loss: 0.03349699825048447\n",
      "iteration 1665, dc_loss: 0.03324663266539574, tv_loss: 0.03350793197751045\n",
      "iteration 1666, dc_loss: 0.03322237357497215, tv_loss: 0.03354952856898308\n",
      "iteration 1667, dc_loss: 0.033211834728717804, tv_loss: 0.03352656960487366\n",
      "iteration 1668, dc_loss: 0.033176593482494354, tv_loss: 0.033526308834552765\n",
      "iteration 1669, dc_loss: 0.03318425267934799, tv_loss: 0.03349635377526283\n",
      "iteration 1670, dc_loss: 0.033165473490953445, tv_loss: 0.03350549936294556\n",
      "iteration 1671, dc_loss: 0.03313048556447029, tv_loss: 0.03353229537606239\n",
      "iteration 1672, dc_loss: 0.03313891589641571, tv_loss: 0.03352135419845581\n",
      "iteration 1673, dc_loss: 0.03310662880539894, tv_loss: 0.0335225947201252\n",
      "iteration 1674, dc_loss: 0.03309947997331619, tv_loss: 0.03350232541561127\n",
      "iteration 1675, dc_loss: 0.03307100385427475, tv_loss: 0.033515699207782745\n",
      "iteration 1676, dc_loss: 0.03305928036570549, tv_loss: 0.03350841999053955\n",
      "iteration 1677, dc_loss: 0.03305717557668686, tv_loss: 0.03349333256483078\n",
      "iteration 1678, dc_loss: 0.033028654754161835, tv_loss: 0.03350900113582611\n",
      "iteration 1679, dc_loss: 0.03302018716931343, tv_loss: 0.033516738563776016\n",
      "iteration 1680, dc_loss: 0.03299623355269432, tv_loss: 0.03355831652879715\n",
      "iteration 1681, dc_loss: 0.03299514949321747, tv_loss: 0.03352411836385727\n",
      "iteration 1682, dc_loss: 0.03295483812689781, tv_loss: 0.03352038189768791\n",
      "iteration 1683, dc_loss: 0.032963503152132034, tv_loss: 0.03350294008851051\n",
      "iteration 1684, dc_loss: 0.032935917377471924, tv_loss: 0.03353066369891167\n",
      "iteration 1685, dc_loss: 0.03291526436805725, tv_loss: 0.03355361148715019\n",
      "iteration 1686, dc_loss: 0.03291856870055199, tv_loss: 0.03351322561502457\n",
      "iteration 1687, dc_loss: 0.03287428617477417, tv_loss: 0.0335255041718483\n",
      "iteration 1688, dc_loss: 0.03288906067609787, tv_loss: 0.033502597361803055\n",
      "iteration 1689, dc_loss: 0.032860975712537766, tv_loss: 0.03351714462041855\n",
      "iteration 1690, dc_loss: 0.03284180164337158, tv_loss: 0.033536043018102646\n",
      "iteration 1691, dc_loss: 0.0328296422958374, tv_loss: 0.03352653980255127\n",
      "iteration 1692, dc_loss: 0.032813239842653275, tv_loss: 0.033522725105285645\n",
      "iteration 1693, dc_loss: 0.03280538693070412, tv_loss: 0.03350707143545151\n",
      "iteration 1694, dc_loss: 0.03277666121721268, tv_loss: 0.03352319821715355\n",
      "iteration 1695, dc_loss: 0.032763510942459106, tv_loss: 0.03350912407040596\n",
      "iteration 1696, dc_loss: 0.032754454761743546, tv_loss: 0.033518269658088684\n",
      "iteration 1697, dc_loss: 0.03273941949009895, tv_loss: 0.03350410982966423\n",
      "iteration 1698, dc_loss: 0.032713476568460464, tv_loss: 0.03352053090929985\n",
      "iteration 1699, dc_loss: 0.0327148512005806, tv_loss: 0.03350022807717323\n",
      "iteration 1700, dc_loss: 0.03267965465784073, tv_loss: 0.0335262194275856\n",
      "iteration 1701, dc_loss: 0.03269611671566963, tv_loss: 0.03351178020238876\n",
      "iteration 1702, dc_loss: 0.03264796361327171, tv_loss: 0.03356662765145302\n",
      "iteration 1703, dc_loss: 0.0326516330242157, tv_loss: 0.03351258113980293\n",
      "iteration 1704, dc_loss: 0.03261827304959297, tv_loss: 0.03352302312850952\n",
      "iteration 1705, dc_loss: 0.03261800855398178, tv_loss: 0.033525560051202774\n",
      "iteration 1706, dc_loss: 0.0326090082526207, tv_loss: 0.033537548035383224\n",
      "iteration 1707, dc_loss: 0.03256623446941376, tv_loss: 0.03356461226940155\n",
      "iteration 1708, dc_loss: 0.0325855128467083, tv_loss: 0.033510804176330566\n",
      "iteration 1709, dc_loss: 0.03254341334104538, tv_loss: 0.033521123230457306\n",
      "iteration 1710, dc_loss: 0.03253776207566261, tv_loss: 0.03351619094610214\n",
      "iteration 1711, dc_loss: 0.03252962604165077, tv_loss: 0.03351292014122009\n",
      "iteration 1712, dc_loss: 0.032508108764886856, tv_loss: 0.03352835401892662\n",
      "iteration 1713, dc_loss: 0.03250492736697197, tv_loss: 0.03352709114551544\n",
      "iteration 1714, dc_loss: 0.03247113898396492, tv_loss: 0.03353797271847725\n",
      "iteration 1715, dc_loss: 0.03245750069618225, tv_loss: 0.033525194972753525\n",
      "iteration 1716, dc_loss: 0.03245362639427185, tv_loss: 0.03350915387272835\n",
      "iteration 1717, dc_loss: 0.03243263438344002, tv_loss: 0.033526644110679626\n",
      "iteration 1718, dc_loss: 0.03242909535765648, tv_loss: 0.03352796286344528\n",
      "iteration 1719, dc_loss: 0.032378602772951126, tv_loss: 0.03357227146625519\n",
      "iteration 1720, dc_loss: 0.03240225836634636, tv_loss: 0.03352353721857071\n",
      "iteration 1721, dc_loss: 0.032374151051044464, tv_loss: 0.03352529555559158\n",
      "iteration 1722, dc_loss: 0.03237206116318703, tv_loss: 0.03350913152098656\n",
      "iteration 1723, dc_loss: 0.03233732283115387, tv_loss: 0.033541325479745865\n",
      "iteration 1724, dc_loss: 0.032321829348802567, tv_loss: 0.033535122871398926\n",
      "iteration 1725, dc_loss: 0.03232696279883385, tv_loss: 0.03351849317550659\n",
      "iteration 1726, dc_loss: 0.032292407006025314, tv_loss: 0.03353169932961464\n",
      "iteration 1727, dc_loss: 0.03228329122066498, tv_loss: 0.033520910888910294\n",
      "iteration 1728, dc_loss: 0.032297033816576004, tv_loss: 0.03349318355321884\n",
      "iteration 1729, dc_loss: 0.03223496302962303, tv_loss: 0.0335366390645504\n",
      "iteration 1730, dc_loss: 0.03226570412516594, tv_loss: 0.033496033400297165\n",
      "iteration 1731, dc_loss: 0.03221248835325241, tv_loss: 0.03353102132678032\n",
      "iteration 1732, dc_loss: 0.03224731981754303, tv_loss: 0.03348703682422638\n",
      "iteration 1733, dc_loss: 0.03218385577201843, tv_loss: 0.033544957637786865\n",
      "iteration 1734, dc_loss: 0.03221878781914711, tv_loss: 0.033500127494335175\n",
      "iteration 1735, dc_loss: 0.032168518751859665, tv_loss: 0.03357117995619774\n",
      "iteration 1736, dc_loss: 0.03220939263701439, tv_loss: 0.03352028876543045\n",
      "iteration 1737, dc_loss: 0.032128237187862396, tv_loss: 0.03356248512864113\n",
      "iteration 1738, dc_loss: 0.03216233104467392, tv_loss: 0.033495139330625534\n",
      "iteration 1739, dc_loss: 0.0321076437830925, tv_loss: 0.03357527777552605\n",
      "iteration 1740, dc_loss: 0.0321599543094635, tv_loss: 0.03352285176515579\n",
      "iteration 1741, dc_loss: 0.032080113887786865, tv_loss: 0.03356687352061272\n",
      "iteration 1742, dc_loss: 0.03215300664305687, tv_loss: 0.033500492572784424\n",
      "iteration 1743, dc_loss: 0.0320885069668293, tv_loss: 0.03357686847448349\n",
      "iteration 1744, dc_loss: 0.032123763114213943, tv_loss: 0.033530570566654205\n",
      "iteration 1745, dc_loss: 0.0320623405277729, tv_loss: 0.033537667244672775\n",
      "iteration 1746, dc_loss: 0.03205886110663414, tv_loss: 0.03350602835416794\n",
      "iteration 1747, dc_loss: 0.03199175000190735, tv_loss: 0.03353689983487129\n",
      "iteration 1748, dc_loss: 0.03201581910252571, tv_loss: 0.03351207450032234\n",
      "iteration 1749, dc_loss: 0.03196059539914131, tv_loss: 0.03356501832604408\n",
      "iteration 1750, dc_loss: 0.03199734911322594, tv_loss: 0.033509913831949234\n",
      "iteration 1751, dc_loss: 0.03193476051092148, tv_loss: 0.033539436757564545\n",
      "iteration 1752, dc_loss: 0.03194255754351616, tv_loss: 0.03350824862718582\n",
      "iteration 1753, dc_loss: 0.03191681578755379, tv_loss: 0.03351660072803497\n",
      "iteration 1754, dc_loss: 0.031890831887722015, tv_loss: 0.03354349732398987\n",
      "iteration 1755, dc_loss: 0.031909022480249405, tv_loss: 0.03352117910981178\n",
      "iteration 1756, dc_loss: 0.03187508508563042, tv_loss: 0.03354552015662193\n",
      "iteration 1757, dc_loss: 0.03185659274458885, tv_loss: 0.03354250267148018\n",
      "iteration 1758, dc_loss: 0.031833257526159286, tv_loss: 0.03353700414299965\n",
      "iteration 1759, dc_loss: 0.03183848410844803, tv_loss: 0.033519964665174484\n",
      "iteration 1760, dc_loss: 0.031809836626052856, tv_loss: 0.03353557735681534\n",
      "iteration 1761, dc_loss: 0.031817108392715454, tv_loss: 0.03351924568414688\n",
      "iteration 1762, dc_loss: 0.0317641906440258, tv_loss: 0.03357245400547981\n",
      "iteration 1763, dc_loss: 0.03179251775145531, tv_loss: 0.033519916236400604\n",
      "iteration 1764, dc_loss: 0.03175395727157593, tv_loss: 0.0335296094417572\n",
      "iteration 1765, dc_loss: 0.031757794320583344, tv_loss: 0.03351464495062828\n",
      "iteration 1766, dc_loss: 0.031723253428936005, tv_loss: 0.033531758934259415\n",
      "iteration 1767, dc_loss: 0.03172506392002106, tv_loss: 0.0335070863366127\n",
      "iteration 1768, dc_loss: 0.03170323371887207, tv_loss: 0.033519353717565536\n",
      "iteration 1769, dc_loss: 0.03168140724301338, tv_loss: 0.033527594059705734\n",
      "iteration 1770, dc_loss: 0.03166632726788521, tv_loss: 0.033524930477142334\n",
      "iteration 1771, dc_loss: 0.03168439492583275, tv_loss: 0.033502645790576935\n",
      "iteration 1772, dc_loss: 0.03161988407373428, tv_loss: 0.0335657000541687\n",
      "iteration 1773, dc_loss: 0.031646665185689926, tv_loss: 0.03352772817015648\n",
      "iteration 1774, dc_loss: 0.03160883113741875, tv_loss: 0.033554594963788986\n",
      "iteration 1775, dc_loss: 0.03162263706326485, tv_loss: 0.033513229340314865\n",
      "iteration 1776, dc_loss: 0.03156978636980057, tv_loss: 0.03354473412036896\n",
      "iteration 1777, dc_loss: 0.03159783035516739, tv_loss: 0.03352158144116402\n",
      "iteration 1778, dc_loss: 0.0315437950193882, tv_loss: 0.0335700660943985\n",
      "iteration 1779, dc_loss: 0.031583189964294434, tv_loss: 0.03351694718003273\n",
      "iteration 1780, dc_loss: 0.03150869905948639, tv_loss: 0.03356083109974861\n",
      "iteration 1781, dc_loss: 0.031570229679346085, tv_loss: 0.033487718552351\n",
      "iteration 1782, dc_loss: 0.031488075852394104, tv_loss: 0.03356240317225456\n",
      "iteration 1783, dc_loss: 0.03155577927827835, tv_loss: 0.03347961604595184\n",
      "iteration 1784, dc_loss: 0.031453315168619156, tv_loss: 0.03358946740627289\n",
      "iteration 1785, dc_loss: 0.03153461590409279, tv_loss: 0.03351178020238876\n",
      "iteration 1786, dc_loss: 0.03145527467131615, tv_loss: 0.03357841446995735\n",
      "iteration 1787, dc_loss: 0.03151347488164902, tv_loss: 0.03349239379167557\n",
      "iteration 1788, dc_loss: 0.03141843155026436, tv_loss: 0.03357798978686333\n",
      "iteration 1789, dc_loss: 0.031499359756708145, tv_loss: 0.03347545117139816\n",
      "iteration 1790, dc_loss: 0.03139762580394745, tv_loss: 0.03357020393013954\n",
      "iteration 1791, dc_loss: 0.03146732971072197, tv_loss: 0.03348973020911217\n",
      "iteration 1792, dc_loss: 0.031362175941467285, tv_loss: 0.0335945300757885\n",
      "iteration 1793, dc_loss: 0.031468942761421204, tv_loss: 0.03347758948802948\n",
      "iteration 1794, dc_loss: 0.031321048736572266, tv_loss: 0.033626068383455276\n",
      "iteration 1795, dc_loss: 0.03142567723989487, tv_loss: 0.0334755964577198\n",
      "iteration 1796, dc_loss: 0.03128752484917641, tv_loss: 0.033589448779821396\n",
      "iteration 1797, dc_loss: 0.03138361871242523, tv_loss: 0.03346904367208481\n",
      "iteration 1798, dc_loss: 0.03127105534076691, tv_loss: 0.03355993703007698\n",
      "iteration 1799, dc_loss: 0.03129485994577408, tv_loss: 0.033537495881319046\n",
      "iteration 1800, dc_loss: 0.031263526529073715, tv_loss: 0.03356364741921425\n",
      "iteration 1801, dc_loss: 0.0312366746366024, tv_loss: 0.033553384244441986\n",
      "iteration 1802, dc_loss: 0.03127481788396835, tv_loss: 0.03348875045776367\n",
      "iteration 1803, dc_loss: 0.031200988218188286, tv_loss: 0.03357204422354698\n",
      "iteration 1804, dc_loss: 0.03125395625829697, tv_loss: 0.03351835161447525\n",
      "iteration 1805, dc_loss: 0.031191866844892502, tv_loss: 0.033573031425476074\n",
      "iteration 1806, dc_loss: 0.031219560652971268, tv_loss: 0.03350888937711716\n",
      "iteration 1807, dc_loss: 0.031158149242401123, tv_loss: 0.033564627170562744\n",
      "iteration 1808, dc_loss: 0.03119543194770813, tv_loss: 0.03351388871669769\n",
      "iteration 1809, dc_loss: 0.031147561967372894, tv_loss: 0.033544234931468964\n",
      "iteration 1810, dc_loss: 0.031178440898656845, tv_loss: 0.033490944653749466\n",
      "iteration 1811, dc_loss: 0.031097711995244026, tv_loss: 0.03356976434588432\n",
      "iteration 1812, dc_loss: 0.031165195629000664, tv_loss: 0.03347953036427498\n",
      "iteration 1813, dc_loss: 0.031069394201040268, tv_loss: 0.03355815261602402\n",
      "iteration 1814, dc_loss: 0.03112407959997654, tv_loss: 0.033480532467365265\n",
      "iteration 1815, dc_loss: 0.031044738367199898, tv_loss: 0.033548370003700256\n",
      "iteration 1816, dc_loss: 0.03108983300626278, tv_loss: 0.03351348266005516\n",
      "iteration 1817, dc_loss: 0.031016331166028976, tv_loss: 0.03361140191555023\n",
      "iteration 1818, dc_loss: 0.031079281121492386, tv_loss: 0.03351564705371857\n",
      "iteration 1819, dc_loss: 0.030969351530075073, tv_loss: 0.03357715904712677\n",
      "iteration 1820, dc_loss: 0.031051525846123695, tv_loss: 0.03349296748638153\n",
      "iteration 1821, dc_loss: 0.030979277566075325, tv_loss: 0.033568523824214935\n",
      "iteration 1822, dc_loss: 0.031003212556242943, tv_loss: 0.03355196490883827\n",
      "iteration 1823, dc_loss: 0.030958279967308044, tv_loss: 0.0335688479244709\n",
      "iteration 1824, dc_loss: 0.03099236451089382, tv_loss: 0.03350392356514931\n",
      "iteration 1825, dc_loss: 0.03095940127968788, tv_loss: 0.03354138135910034\n",
      "iteration 1826, dc_loss: 0.03097129799425602, tv_loss: 0.033526014536619186\n",
      "iteration 1827, dc_loss: 0.030917299911379814, tv_loss: 0.03356393426656723\n",
      "iteration 1828, dc_loss: 0.031013712286949158, tv_loss: 0.03347676619887352\n",
      "iteration 1829, dc_loss: 0.030862553045153618, tv_loss: 0.0336245633661747\n",
      "iteration 1830, dc_loss: 0.031021704897284508, tv_loss: 0.033467721194028854\n",
      "iteration 1831, dc_loss: 0.030841482803225517, tv_loss: 0.03364013880491257\n",
      "iteration 1832, dc_loss: 0.03100426495075226, tv_loss: 0.03345483914017677\n",
      "iteration 1833, dc_loss: 0.03082950972020626, tv_loss: 0.03363235294818878\n",
      "iteration 1834, dc_loss: 0.030953319743275642, tv_loss: 0.03345168009400368\n",
      "iteration 1835, dc_loss: 0.03079332783818245, tv_loss: 0.03360440954566002\n",
      "iteration 1836, dc_loss: 0.030859054997563362, tv_loss: 0.03350609913468361\n",
      "iteration 1837, dc_loss: 0.030792459845542908, tv_loss: 0.03355507552623749\n",
      "iteration 1838, dc_loss: 0.03077816776931286, tv_loss: 0.03355729579925537\n",
      "iteration 1839, dc_loss: 0.030793163925409317, tv_loss: 0.03352459892630577\n",
      "iteration 1840, dc_loss: 0.030729511752724648, tv_loss: 0.03357487544417381\n",
      "iteration 1841, dc_loss: 0.030782785266637802, tv_loss: 0.033497102558612823\n",
      "iteration 1842, dc_loss: 0.030683469027280807, tv_loss: 0.033602744340896606\n",
      "iteration 1843, dc_loss: 0.03076321817934513, tv_loss: 0.03350469842553139\n",
      "iteration 1844, dc_loss: 0.0306894239038229, tv_loss: 0.033572424203157425\n",
      "iteration 1845, dc_loss: 0.03073795698583126, tv_loss: 0.03350813686847687\n",
      "iteration 1846, dc_loss: 0.030661601573228836, tv_loss: 0.03357064351439476\n",
      "iteration 1847, dc_loss: 0.03070208430290222, tv_loss: 0.03351030871272087\n",
      "iteration 1848, dc_loss: 0.030642099678516388, tv_loss: 0.03356613218784332\n",
      "iteration 1849, dc_loss: 0.030687406659126282, tv_loss: 0.033496905118227005\n",
      "iteration 1850, dc_loss: 0.030601464211940765, tv_loss: 0.033585112541913986\n",
      "iteration 1851, dc_loss: 0.030643999576568604, tv_loss: 0.03352649509906769\n",
      "iteration 1852, dc_loss: 0.030582066625356674, tv_loss: 0.0335674062371254\n",
      "iteration 1853, dc_loss: 0.030596772208809853, tv_loss: 0.033522579818964005\n",
      "iteration 1854, dc_loss: 0.03056313470005989, tv_loss: 0.03354211524128914\n",
      "iteration 1855, dc_loss: 0.03056417778134346, tv_loss: 0.03353067860007286\n",
      "iteration 1856, dc_loss: 0.030545957386493683, tv_loss: 0.03355429694056511\n",
      "iteration 1857, dc_loss: 0.03054448962211609, tv_loss: 0.03355614095926285\n",
      "iteration 1858, dc_loss: 0.030506236478686333, tv_loss: 0.033565230667591095\n",
      "iteration 1859, dc_loss: 0.0305193904787302, tv_loss: 0.03352028876543045\n",
      "iteration 1860, dc_loss: 0.030493304133415222, tv_loss: 0.03355512395501137\n",
      "iteration 1861, dc_loss: 0.030479973182082176, tv_loss: 0.03355995938181877\n",
      "iteration 1862, dc_loss: 0.030480066314339638, tv_loss: 0.03354337066411972\n",
      "iteration 1863, dc_loss: 0.03044792450964451, tv_loss: 0.0335482656955719\n",
      "iteration 1864, dc_loss: 0.030477015301585197, tv_loss: 0.03350701555609703\n",
      "iteration 1865, dc_loss: 0.030426032841205597, tv_loss: 0.03355327993631363\n",
      "iteration 1866, dc_loss: 0.030428284779191017, tv_loss: 0.03354625776410103\n",
      "iteration 1867, dc_loss: 0.030405886471271515, tv_loss: 0.033567119389772415\n",
      "iteration 1868, dc_loss: 0.030402719974517822, tv_loss: 0.03355879336595535\n",
      "iteration 1869, dc_loss: 0.030415957793593407, tv_loss: 0.03351474180817604\n",
      "iteration 1870, dc_loss: 0.03037094697356224, tv_loss: 0.03356332331895828\n",
      "iteration 1871, dc_loss: 0.030407950282096863, tv_loss: 0.03351187705993652\n",
      "iteration 1872, dc_loss: 0.030368169769644737, tv_loss: 0.03356615826487541\n",
      "iteration 1873, dc_loss: 0.030466055497527122, tv_loss: 0.033481910824775696\n",
      "iteration 1874, dc_loss: 0.03038802742958069, tv_loss: 0.033604696393013\n",
      "iteration 1875, dc_loss: 0.030558917671442032, tv_loss: 0.033478032797575\n",
      "iteration 1876, dc_loss: 0.03041917085647583, tv_loss: 0.03368773311376572\n",
      "iteration 1877, dc_loss: 0.030621526762843132, tv_loss: 0.033426299691200256\n",
      "iteration 1878, dc_loss: 0.030346475541591644, tv_loss: 0.033691778779029846\n",
      "iteration 1879, dc_loss: 0.03058576211333275, tv_loss: 0.03340357914566994\n",
      "iteration 1880, dc_loss: 0.030336758121848106, tv_loss: 0.03370624780654907\n",
      "iteration 1881, dc_loss: 0.03048548847436905, tv_loss: 0.03347492963075638\n",
      "iteration 1882, dc_loss: 0.03032197616994381, tv_loss: 0.033607736229896545\n",
      "iteration 1883, dc_loss: 0.030328217893838882, tv_loss: 0.0335460901260376\n",
      "iteration 1884, dc_loss: 0.03028927370905876, tv_loss: 0.03354456275701523\n",
      "iteration 1885, dc_loss: 0.030185941606760025, tv_loss: 0.03360280767083168\n",
      "iteration 1886, dc_loss: 0.030275991186499596, tv_loss: 0.03349619358778\n",
      "iteration 1887, dc_loss: 0.03019425831735134, tv_loss: 0.033621128648519516\n",
      "iteration 1888, dc_loss: 0.03026508167386055, tv_loss: 0.03353169932961464\n",
      "iteration 1889, dc_loss: 0.03018663078546524, tv_loss: 0.03357669338583946\n",
      "iteration 1890, dc_loss: 0.030187703669071198, tv_loss: 0.03353523463010788\n",
      "iteration 1891, dc_loss: 0.030145032331347466, tv_loss: 0.03357313200831413\n",
      "iteration 1892, dc_loss: 0.030143139883875847, tv_loss: 0.033564887940883636\n",
      "iteration 1893, dc_loss: 0.030123760923743248, tv_loss: 0.0335472971200943\n",
      "iteration 1894, dc_loss: 0.030107969418168068, tv_loss: 0.03354763612151146\n",
      "iteration 1895, dc_loss: 0.03013298474252224, tv_loss: 0.033541273325681686\n",
      "iteration 1896, dc_loss: 0.030060354620218277, tv_loss: 0.03360765799880028\n",
      "iteration 1897, dc_loss: 0.03010687232017517, tv_loss: 0.033517297357320786\n",
      "iteration 1898, dc_loss: 0.03003658726811409, tv_loss: 0.03356194496154785\n",
      "iteration 1899, dc_loss: 0.03005896881222725, tv_loss: 0.03353717550635338\n",
      "iteration 1900, dc_loss: 0.030013877898454666, tv_loss: 0.033585600554943085\n",
      "iteration 1901, dc_loss: 0.030020644888281822, tv_loss: 0.033559806644916534\n",
      "iteration 1902, dc_loss: 0.03002890571951866, tv_loss: 0.033524591475725174\n",
      "iteration 1903, dc_loss: 0.02996564283967018, tv_loss: 0.03356945887207985\n",
      "iteration 1904, dc_loss: 0.030011622235178947, tv_loss: 0.03351987153291702\n",
      "iteration 1905, dc_loss: 0.02994828298687935, tv_loss: 0.0335811972618103\n",
      "iteration 1906, dc_loss: 0.0299812201410532, tv_loss: 0.03353578597307205\n",
      "iteration 1907, dc_loss: 0.02992120198905468, tv_loss: 0.03357933461666107\n",
      "iteration 1908, dc_loss: 0.02994587831199169, tv_loss: 0.033528804779052734\n",
      "iteration 1909, dc_loss: 0.029918856918811798, tv_loss: 0.03354211896657944\n",
      "iteration 1910, dc_loss: 0.029911762103438377, tv_loss: 0.033535122871398926\n",
      "iteration 1911, dc_loss: 0.02989468164741993, tv_loss: 0.03353529050946236\n",
      "iteration 1912, dc_loss: 0.02987806499004364, tv_loss: 0.033561766147613525\n",
      "iteration 1913, dc_loss: 0.0298873670399189, tv_loss: 0.03354291617870331\n",
      "iteration 1914, dc_loss: 0.02984902448952198, tv_loss: 0.03356488049030304\n",
      "iteration 1915, dc_loss: 0.02985009178519249, tv_loss: 0.033546701073646545\n",
      "iteration 1916, dc_loss: 0.02983819879591465, tv_loss: 0.03354030102491379\n",
      "iteration 1917, dc_loss: 0.029828237369656563, tv_loss: 0.03355449438095093\n",
      "iteration 1918, dc_loss: 0.029820449650287628, tv_loss: 0.03354768455028534\n",
      "iteration 1919, dc_loss: 0.029786746948957443, tv_loss: 0.03356638550758362\n",
      "iteration 1920, dc_loss: 0.02981141395866871, tv_loss: 0.033532727509737015\n",
      "iteration 1921, dc_loss: 0.029760250821709633, tv_loss: 0.03357881307601929\n",
      "iteration 1922, dc_loss: 0.029785549268126488, tv_loss: 0.03354358673095703\n",
      "iteration 1923, dc_loss: 0.029755355790257454, tv_loss: 0.03356371819972992\n",
      "iteration 1924, dc_loss: 0.029767414554953575, tv_loss: 0.03352265805006027\n",
      "iteration 1925, dc_loss: 0.029717881232500076, tv_loss: 0.03357400372624397\n",
      "iteration 1926, dc_loss: 0.029731011018157005, tv_loss: 0.03355201333761215\n",
      "iteration 1927, dc_loss: 0.029713477939367294, tv_loss: 0.033553265035152435\n",
      "iteration 1928, dc_loss: 0.029716065153479576, tv_loss: 0.03353697061538696\n",
      "iteration 1929, dc_loss: 0.029679851606488228, tv_loss: 0.03356463834643364\n",
      "iteration 1930, dc_loss: 0.02969438023865223, tv_loss: 0.03352866694331169\n",
      "iteration 1931, dc_loss: 0.02966243401169777, tv_loss: 0.03355874866247177\n",
      "iteration 1932, dc_loss: 0.029710307717323303, tv_loss: 0.03352050110697746\n",
      "iteration 1933, dc_loss: 0.029620354995131493, tv_loss: 0.03361855447292328\n",
      "iteration 1934, dc_loss: 0.029705436900258064, tv_loss: 0.033515289425849915\n",
      "iteration 1935, dc_loss: 0.029593247920274734, tv_loss: 0.03361711651086807\n",
      "iteration 1936, dc_loss: 0.02973352186381817, tv_loss: 0.0334773063659668\n",
      "iteration 1937, dc_loss: 0.029606366530060768, tv_loss: 0.03362366184592247\n",
      "iteration 1938, dc_loss: 0.029790125787258148, tv_loss: 0.03345254436135292\n",
      "iteration 1939, dc_loss: 0.029614565894007683, tv_loss: 0.03367139771580696\n",
      "iteration 1940, dc_loss: 0.02984420210123062, tv_loss: 0.033445805311203\n",
      "iteration 1941, dc_loss: 0.02967853844165802, tv_loss: 0.03365873172879219\n",
      "iteration 1942, dc_loss: 0.029879117384552956, tv_loss: 0.033433571457862854\n",
      "iteration 1943, dc_loss: 0.029643790796399117, tv_loss: 0.03366144746541977\n",
      "iteration 1944, dc_loss: 0.02977045625448227, tv_loss: 0.03346629440784454\n",
      "iteration 1945, dc_loss: 0.02955956757068634, tv_loss: 0.03362298011779785\n",
      "iteration 1946, dc_loss: 0.029623284935951233, tv_loss: 0.03348151221871376\n",
      "iteration 1947, dc_loss: 0.029484525322914124, tv_loss: 0.03358500823378563\n",
      "iteration 1948, dc_loss: 0.02950163744390011, tv_loss: 0.033560942858457565\n",
      "iteration 1949, dc_loss: 0.029535971581935883, tv_loss: 0.033536314964294434\n",
      "iteration 1950, dc_loss: 0.029451929032802582, tv_loss: 0.03362457454204559\n",
      "iteration 1951, dc_loss: 0.02959039993584156, tv_loss: 0.033480092883110046\n",
      "iteration 1952, dc_loss: 0.029430905357003212, tv_loss: 0.033631645143032074\n",
      "iteration 1953, dc_loss: 0.029578013345599174, tv_loss: 0.03344578668475151\n",
      "iteration 1954, dc_loss: 0.029379000887274742, tv_loss: 0.03361492231488228\n",
      "iteration 1955, dc_loss: 0.029444625601172447, tv_loss: 0.03354116901755333\n",
      "iteration 1956, dc_loss: 0.029416630044579506, tv_loss: 0.033571381121873856\n",
      "iteration 1957, dc_loss: 0.029371201992034912, tv_loss: 0.03359502553939819\n",
      "iteration 1958, dc_loss: 0.02948031947016716, tv_loss: 0.03347845375537872\n",
      "iteration 1959, dc_loss: 0.02931906096637249, tv_loss: 0.033638473600149155\n",
      "iteration 1960, dc_loss: 0.029474778100848198, tv_loss: 0.03348001092672348\n",
      "iteration 1961, dc_loss: 0.029317276552319527, tv_loss: 0.03361845761537552\n",
      "iteration 1962, dc_loss: 0.029374985024333, tv_loss: 0.03353148698806763\n",
      "iteration 1963, dc_loss: 0.029339902102947235, tv_loss: 0.03353935107588768\n",
      "iteration 1964, dc_loss: 0.029290396720170975, tv_loss: 0.033577967435121536\n",
      "iteration 1965, dc_loss: 0.029369553551077843, tv_loss: 0.03348453342914581\n",
      "iteration 1966, dc_loss: 0.029252205044031143, tv_loss: 0.03360159695148468\n",
      "iteration 1967, dc_loss: 0.02935512736439705, tv_loss: 0.033487718552351\n",
      "iteration 1968, dc_loss: 0.029262175783514977, tv_loss: 0.03356717526912689\n",
      "iteration 1969, dc_loss: 0.029298905283212662, tv_loss: 0.03352005407214165\n",
      "iteration 1970, dc_loss: 0.02925023064017296, tv_loss: 0.033560652285814285\n",
      "iteration 1971, dc_loss: 0.029262421652674675, tv_loss: 0.03354522958397865\n",
      "iteration 1972, dc_loss: 0.02923605591058731, tv_loss: 0.03357595205307007\n",
      "iteration 1973, dc_loss: 0.029215611517429352, tv_loss: 0.03356922045350075\n",
      "iteration 1974, dc_loss: 0.0292078647762537, tv_loss: 0.03355088084936142\n",
      "iteration 1975, dc_loss: 0.029223332181572914, tv_loss: 0.033528003841638565\n",
      "iteration 1976, dc_loss: 0.02918120287358761, tv_loss: 0.03356354311108589\n",
      "iteration 1977, dc_loss: 0.02917701005935669, tv_loss: 0.03358279541134834\n",
      "iteration 1978, dc_loss: 0.029179822653532028, tv_loss: 0.03356820344924927\n",
      "iteration 1979, dc_loss: 0.029151270166039467, tv_loss: 0.03356113284826279\n",
      "iteration 1980, dc_loss: 0.02917957492172718, tv_loss: 0.03352061286568642\n",
      "iteration 1981, dc_loss: 0.029117483645677567, tv_loss: 0.033578500151634216\n",
      "iteration 1982, dc_loss: 0.029168128967285156, tv_loss: 0.03351868689060211\n",
      "iteration 1983, dc_loss: 0.029101679101586342, tv_loss: 0.0335879884660244\n",
      "iteration 1984, dc_loss: 0.0291331447660923, tv_loss: 0.033559370785951614\n",
      "iteration 1985, dc_loss: 0.029088549315929413, tv_loss: 0.03358133137226105\n",
      "iteration 1986, dc_loss: 0.02910037338733673, tv_loss: 0.03354674577713013\n",
      "iteration 1987, dc_loss: 0.029092680662870407, tv_loss: 0.03354707360267639\n",
      "iteration 1988, dc_loss: 0.029072169214487076, tv_loss: 0.033596694469451904\n",
      "iteration 1989, dc_loss: 0.02904495596885681, tv_loss: 0.03359068185091019\n",
      "iteration 1990, dc_loss: 0.02907097525894642, tv_loss: 0.033535249531269073\n",
      "iteration 1991, dc_loss: 0.02900751121342182, tv_loss: 0.03360559046268463\n",
      "iteration 1992, dc_loss: 0.02906065806746483, tv_loss: 0.0335637666285038\n",
      "iteration 1993, dc_loss: 0.02900226041674614, tv_loss: 0.03359612822532654\n",
      "iteration 1994, dc_loss: 0.029042845591902733, tv_loss: 0.03351942077279091\n",
      "iteration 1995, dc_loss: 0.028969688341021538, tv_loss: 0.03361900895833969\n",
      "iteration 1996, dc_loss: 0.029027989134192467, tv_loss: 0.03355788812041283\n",
      "iteration 1997, dc_loss: 0.028954029083251953, tv_loss: 0.03359755501151085\n",
      "iteration 1998, dc_loss: 0.029003256931900978, tv_loss: 0.03353226184844971\n",
      "iteration 1999, dc_loss: 0.028951091691851616, tv_loss: 0.03361213952302933\n",
      "iteration 2000, dc_loss: 0.02899373322725296, tv_loss: 0.03356409817934036\n",
      "iteration 2001, dc_loss: 0.02894720435142517, tv_loss: 0.03359171748161316\n",
      "iteration 2002, dc_loss: 0.028966808691620827, tv_loss: 0.0335380882024765\n",
      "iteration 2003, dc_loss: 0.028921645134687424, tv_loss: 0.033551111817359924\n",
      "iteration 2004, dc_loss: 0.028914455324411392, tv_loss: 0.03356722369790077\n",
      "iteration 2005, dc_loss: 0.028943048790097237, tv_loss: 0.03352813422679901\n",
      "iteration 2006, dc_loss: 0.02888704650104046, tv_loss: 0.03355535492300987\n",
      "iteration 2007, dc_loss: 0.02888103760778904, tv_loss: 0.03354785963892937\n",
      "iteration 2008, dc_loss: 0.028903581202030182, tv_loss: 0.033531554043293\n",
      "iteration 2009, dc_loss: 0.02887478470802307, tv_loss: 0.03354133665561676\n",
      "iteration 2010, dc_loss: 0.028877541422843933, tv_loss: 0.033517029136419296\n",
      "iteration 2011, dc_loss: 0.028869029134511948, tv_loss: 0.03353119641542435\n",
      "iteration 2012, dc_loss: 0.028840938583016396, tv_loss: 0.03355154022574425\n",
      "iteration 2013, dc_loss: 0.02885427512228489, tv_loss: 0.03352515771985054\n",
      "iteration 2014, dc_loss: 0.028823575004935265, tv_loss: 0.033563509583473206\n",
      "iteration 2015, dc_loss: 0.028815982863307, tv_loss: 0.03356611356139183\n",
      "iteration 2016, dc_loss: 0.028825420886278152, tv_loss: 0.033525269478559494\n",
      "iteration 2017, dc_loss: 0.028798991814255714, tv_loss: 0.033542804419994354\n",
      "iteration 2018, dc_loss: 0.02880280651152134, tv_loss: 0.033536218106746674\n",
      "iteration 2019, dc_loss: 0.028808610513806343, tv_loss: 0.033534660935401917\n",
      "iteration 2020, dc_loss: 0.02877608872950077, tv_loss: 0.03357359394431114\n",
      "iteration 2021, dc_loss: 0.0287740807980299, tv_loss: 0.03354533016681671\n",
      "iteration 2022, dc_loss: 0.028780587017536163, tv_loss: 0.03352687880396843\n",
      "iteration 2023, dc_loss: 0.028744643554091454, tv_loss: 0.03355836123228073\n",
      "iteration 2024, dc_loss: 0.02875983715057373, tv_loss: 0.033530499786138535\n",
      "iteration 2025, dc_loss: 0.028752412647008896, tv_loss: 0.03352949768304825\n",
      "iteration 2026, dc_loss: 0.028718702495098114, tv_loss: 0.03356142342090607\n",
      "iteration 2027, dc_loss: 0.02874171920120716, tv_loss: 0.03352340683341026\n",
      "iteration 2028, dc_loss: 0.02871048077940941, tv_loss: 0.03353916481137276\n",
      "iteration 2029, dc_loss: 0.028696522116661072, tv_loss: 0.033557429909706116\n",
      "iteration 2030, dc_loss: 0.02873491682112217, tv_loss: 0.03350553661584854\n",
      "iteration 2031, dc_loss: 0.028691833838820457, tv_loss: 0.0335327610373497\n",
      "iteration 2032, dc_loss: 0.028675995767116547, tv_loss: 0.03354613855481148\n",
      "iteration 2033, dc_loss: 0.028698623180389404, tv_loss: 0.03352295979857445\n",
      "iteration 2034, dc_loss: 0.028661904856562614, tv_loss: 0.03354467824101448\n",
      "iteration 2035, dc_loss: 0.028667224571108818, tv_loss: 0.03354533761739731\n",
      "iteration 2036, dc_loss: 0.028662674129009247, tv_loss: 0.03355841711163521\n",
      "iteration 2037, dc_loss: 0.02864295057952404, tv_loss: 0.033552248030900955\n",
      "iteration 2038, dc_loss: 0.02864082157611847, tv_loss: 0.033550623804330826\n",
      "iteration 2039, dc_loss: 0.028639964759349823, tv_loss: 0.03353433683514595\n",
      "iteration 2040, dc_loss: 0.028622249141335487, tv_loss: 0.03355896472930908\n",
      "iteration 2041, dc_loss: 0.028614766895771027, tv_loss: 0.03355875983834267\n",
      "iteration 2042, dc_loss: 0.028620008379220963, tv_loss: 0.0335434190928936\n",
      "iteration 2043, dc_loss: 0.028607778251171112, tv_loss: 0.033541809767484665\n",
      "iteration 2044, dc_loss: 0.028591344133019447, tv_loss: 0.03353947028517723\n",
      "iteration 2045, dc_loss: 0.02858497016131878, tv_loss: 0.033560700714588165\n",
      "iteration 2046, dc_loss: 0.028579305857419968, tv_loss: 0.0335618332028389\n",
      "iteration 2047, dc_loss: 0.028580883517861366, tv_loss: 0.033542852848768234\n",
      "iteration 2048, dc_loss: 0.02855767123401165, tv_loss: 0.03354987874627113\n",
      "iteration 2049, dc_loss: 0.02855994924902916, tv_loss: 0.03354332596063614\n",
      "iteration 2050, dc_loss: 0.028558267280459404, tv_loss: 0.033543337136507034\n",
      "iteration 2051, dc_loss: 0.028537794947624207, tv_loss: 0.03356027975678444\n",
      "iteration 2052, dc_loss: 0.028540104627609253, tv_loss: 0.033539727330207825\n",
      "iteration 2053, dc_loss: 0.028523830696940422, tv_loss: 0.03354737535119057\n",
      "iteration 2054, dc_loss: 0.028523266315460205, tv_loss: 0.03353634849190712\n",
      "iteration 2055, dc_loss: 0.02851850353181362, tv_loss: 0.03353060036897659\n",
      "iteration 2056, dc_loss: 0.028498798608779907, tv_loss: 0.03354587405920029\n",
      "iteration 2057, dc_loss: 0.02849205583333969, tv_loss: 0.03354095667600632\n",
      "iteration 2058, dc_loss: 0.028486870229244232, tv_loss: 0.033545151352882385\n",
      "iteration 2059, dc_loss: 0.02849169634282589, tv_loss: 0.03353126347064972\n",
      "iteration 2060, dc_loss: 0.028478000313043594, tv_loss: 0.033551957458257675\n",
      "iteration 2061, dc_loss: 0.028470637276768684, tv_loss: 0.03354958817362785\n",
      "iteration 2062, dc_loss: 0.028449013829231262, tv_loss: 0.03356024622917175\n",
      "iteration 2063, dc_loss: 0.02845098450779915, tv_loss: 0.03354210406541824\n",
      "iteration 2064, dc_loss: 0.028449878096580505, tv_loss: 0.03353262320160866\n",
      "iteration 2065, dc_loss: 0.028436724096536636, tv_loss: 0.033538952469825745\n",
      "iteration 2066, dc_loss: 0.02842571586370468, tv_loss: 0.033540092408657074\n",
      "iteration 2067, dc_loss: 0.02842722460627556, tv_loss: 0.033527620136737823\n",
      "iteration 2068, dc_loss: 0.028413191437721252, tv_loss: 0.03353271260857582\n",
      "iteration 2069, dc_loss: 0.028405286371707916, tv_loss: 0.03353988006711006\n",
      "iteration 2070, dc_loss: 0.028411313891410828, tv_loss: 0.03352661803364754\n",
      "iteration 2071, dc_loss: 0.028387688100337982, tv_loss: 0.033568039536476135\n",
      "iteration 2072, dc_loss: 0.02838119864463806, tv_loss: 0.033570535480976105\n",
      "iteration 2073, dc_loss: 0.02837308496236801, tv_loss: 0.03355002403259277\n",
      "iteration 2074, dc_loss: 0.028379948809742928, tv_loss: 0.0335373692214489\n",
      "iteration 2075, dc_loss: 0.0283597931265831, tv_loss: 0.03355218842625618\n",
      "iteration 2076, dc_loss: 0.02834155783057213, tv_loss: 0.03357886150479317\n",
      "iteration 2077, dc_loss: 0.028366388753056526, tv_loss: 0.03353967145085335\n",
      "iteration 2078, dc_loss: 0.02834295481443405, tv_loss: 0.033542681485414505\n",
      "iteration 2079, dc_loss: 0.028332103043794632, tv_loss: 0.03354879841208458\n",
      "iteration 2080, dc_loss: 0.028329618275165558, tv_loss: 0.03353916481137276\n",
      "iteration 2081, dc_loss: 0.02830316312611103, tv_loss: 0.03357526287436485\n",
      "iteration 2082, dc_loss: 0.0283131692558527, tv_loss: 0.03356156498193741\n",
      "iteration 2083, dc_loss: 0.028307493776082993, tv_loss: 0.033552736043930054\n",
      "iteration 2084, dc_loss: 0.028306981548666954, tv_loss: 0.03353200480341911\n",
      "iteration 2085, dc_loss: 0.02829708717763424, tv_loss: 0.033537574112415314\n",
      "iteration 2086, dc_loss: 0.02827523835003376, tv_loss: 0.03355461731553078\n",
      "iteration 2087, dc_loss: 0.028278527781367302, tv_loss: 0.03354901075363159\n",
      "iteration 2088, dc_loss: 0.02825630083680153, tv_loss: 0.03356274589896202\n",
      "iteration 2089, dc_loss: 0.028259925544261932, tv_loss: 0.03354624658823013\n",
      "iteration 2090, dc_loss: 0.028255879878997803, tv_loss: 0.0335426926612854\n",
      "iteration 2091, dc_loss: 0.02824942208826542, tv_loss: 0.03353699669241905\n",
      "iteration 2092, dc_loss: 0.028239386156201363, tv_loss: 0.033547401428222656\n",
      "iteration 2093, dc_loss: 0.02823932282626629, tv_loss: 0.033540379256010056\n",
      "iteration 2094, dc_loss: 0.028219588100910187, tv_loss: 0.03355986624956131\n",
      "iteration 2095, dc_loss: 0.028209125623106956, tv_loss: 0.033554576337337494\n",
      "iteration 2096, dc_loss: 0.028201894834637642, tv_loss: 0.03355375677347183\n",
      "iteration 2097, dc_loss: 0.028209466487169266, tv_loss: 0.03353789076209068\n",
      "iteration 2098, dc_loss: 0.028199922293424606, tv_loss: 0.033538103103637695\n",
      "iteration 2099, dc_loss: 0.028183437883853912, tv_loss: 0.033545106649398804\n",
      "iteration 2100, dc_loss: 0.028181863948702812, tv_loss: 0.03353561460971832\n",
      "iteration 2101, dc_loss: 0.02817589044570923, tv_loss: 0.03353176638484001\n",
      "iteration 2102, dc_loss: 0.028165241703391075, tv_loss: 0.033541321754455566\n",
      "iteration 2103, dc_loss: 0.02816077321767807, tv_loss: 0.033544089645147324\n",
      "iteration 2104, dc_loss: 0.028146017342805862, tv_loss: 0.033578354865312576\n",
      "iteration 2105, dc_loss: 0.02815023623406887, tv_loss: 0.03356463462114334\n",
      "iteration 2106, dc_loss: 0.028137603774666786, tv_loss: 0.03354014456272125\n",
      "iteration 2107, dc_loss: 0.02812589704990387, tv_loss: 0.033570386469364166\n",
      "iteration 2108, dc_loss: 0.028126949444413185, tv_loss: 0.03358536958694458\n",
      "iteration 2109, dc_loss: 0.028103653341531754, tv_loss: 0.03357158973813057\n",
      "iteration 2110, dc_loss: 0.02811572328209877, tv_loss: 0.03354409709572792\n",
      "iteration 2111, dc_loss: 0.028108691796660423, tv_loss: 0.03357529267668724\n",
      "iteration 2112, dc_loss: 0.028086429461836815, tv_loss: 0.03358042240142822\n",
      "iteration 2113, dc_loss: 0.028094109147787094, tv_loss: 0.033547427505254745\n",
      "iteration 2114, dc_loss: 0.02807621844112873, tv_loss: 0.033585671335458755\n",
      "iteration 2115, dc_loss: 0.028061553835868835, tv_loss: 0.033577755093574524\n",
      "iteration 2116, dc_loss: 0.028075283393263817, tv_loss: 0.03353629261255264\n",
      "iteration 2117, dc_loss: 0.02805919200181961, tv_loss: 0.03357410430908203\n",
      "iteration 2118, dc_loss: 0.028052957728505135, tv_loss: 0.03357609361410141\n",
      "iteration 2119, dc_loss: 0.028049452230334282, tv_loss: 0.03354574367403984\n",
      "iteration 2120, dc_loss: 0.028032246977090836, tv_loss: 0.03357221186161041\n",
      "iteration 2121, dc_loss: 0.028032535687088966, tv_loss: 0.03357527405023575\n",
      "iteration 2122, dc_loss: 0.028023671358823776, tv_loss: 0.03355683386325836\n",
      "iteration 2123, dc_loss: 0.028007660061120987, tv_loss: 0.033559348434209824\n",
      "iteration 2124, dc_loss: 0.028012340888381004, tv_loss: 0.03356168046593666\n",
      "iteration 2125, dc_loss: 0.028009139001369476, tv_loss: 0.03354771062731743\n",
      "iteration 2126, dc_loss: 0.027990445494651794, tv_loss: 0.03354820981621742\n",
      "iteration 2127, dc_loss: 0.027985677123069763, tv_loss: 0.033549029380083084\n",
      "iteration 2128, dc_loss: 0.027982760220766068, tv_loss: 0.03354634717106819\n",
      "iteration 2129, dc_loss: 0.02797372080385685, tv_loss: 0.03354964777827263\n",
      "iteration 2130, dc_loss: 0.027974430471658707, tv_loss: 0.033541206270456314\n",
      "iteration 2131, dc_loss: 0.027950366958975792, tv_loss: 0.033551283180713654\n",
      "iteration 2132, dc_loss: 0.027948936447501183, tv_loss: 0.0335548110306263\n",
      "iteration 2133, dc_loss: 0.027952568605542183, tv_loss: 0.03353842347860336\n",
      "iteration 2134, dc_loss: 0.027940664440393448, tv_loss: 0.03354579210281372\n",
      "iteration 2135, dc_loss: 0.027918867766857147, tv_loss: 0.03355904668569565\n",
      "iteration 2136, dc_loss: 0.027933627367019653, tv_loss: 0.033529482781887054\n",
      "iteration 2137, dc_loss: 0.02792731486260891, tv_loss: 0.03352970629930496\n",
      "iteration 2138, dc_loss: 0.027901552617549896, tv_loss: 0.03354920819401741\n",
      "iteration 2139, dc_loss: 0.027901673689484596, tv_loss: 0.03354382887482643\n",
      "iteration 2140, dc_loss: 0.02789895236492157, tv_loss: 0.03353472799062729\n",
      "iteration 2141, dc_loss: 0.027888134121894836, tv_loss: 0.03353971242904663\n",
      "iteration 2142, dc_loss: 0.02788996510207653, tv_loss: 0.033536218106746674\n",
      "iteration 2143, dc_loss: 0.027865519747138023, tv_loss: 0.03355332836508751\n",
      "iteration 2144, dc_loss: 0.027876807376742363, tv_loss: 0.03354518488049507\n",
      "iteration 2145, dc_loss: 0.027859853580594063, tv_loss: 0.03357892110943794\n",
      "iteration 2146, dc_loss: 0.027853842824697495, tv_loss: 0.033561766147613525\n",
      "iteration 2147, dc_loss: 0.02784290537238121, tv_loss: 0.033554308116436005\n",
      "iteration 2148, dc_loss: 0.027850331738591194, tv_loss: 0.0335492268204689\n",
      "iteration 2149, dc_loss: 0.027836250141263008, tv_loss: 0.0335603803396225\n",
      "iteration 2150, dc_loss: 0.027825145050883293, tv_loss: 0.03356355056166649\n",
      "iteration 2151, dc_loss: 0.027827788144350052, tv_loss: 0.03353951871395111\n",
      "iteration 2152, dc_loss: 0.027800170704722404, tv_loss: 0.03355927765369415\n",
      "iteration 2153, dc_loss: 0.02780074253678322, tv_loss: 0.033565886318683624\n",
      "iteration 2154, dc_loss: 0.02782152220606804, tv_loss: 0.03354543447494507\n",
      "iteration 2155, dc_loss: 0.02778276428580284, tv_loss: 0.03357003256678581\n",
      "iteration 2156, dc_loss: 0.027795273810625076, tv_loss: 0.03354437276721001\n",
      "iteration 2157, dc_loss: 0.02778034470975399, tv_loss: 0.033542435616254807\n",
      "iteration 2158, dc_loss: 0.02777118980884552, tv_loss: 0.033555563539266586\n",
      "iteration 2159, dc_loss: 0.0277716051787138, tv_loss: 0.03354432061314583\n",
      "iteration 2160, dc_loss: 0.02775789424777031, tv_loss: 0.033548787236213684\n",
      "iteration 2161, dc_loss: 0.02775433659553528, tv_loss: 0.03354295715689659\n",
      "iteration 2162, dc_loss: 0.027744244784116745, tv_loss: 0.03354937583208084\n",
      "iteration 2163, dc_loss: 0.02774054743349552, tv_loss: 0.03354264423251152\n",
      "iteration 2164, dc_loss: 0.027743324637413025, tv_loss: 0.03354526311159134\n",
      "iteration 2165, dc_loss: 0.027731209993362427, tv_loss: 0.03356148675084114\n",
      "iteration 2166, dc_loss: 0.02770349569618702, tv_loss: 0.033575303852558136\n",
      "iteration 2167, dc_loss: 0.027718165889382362, tv_loss: 0.03354661166667938\n",
      "iteration 2168, dc_loss: 0.027708007022738457, tv_loss: 0.03354547545313835\n",
      "iteration 2169, dc_loss: 0.02769893780350685, tv_loss: 0.03356126323342323\n",
      "iteration 2170, dc_loss: 0.02769658900797367, tv_loss: 0.03355047479271889\n",
      "iteration 2171, dc_loss: 0.02768680639564991, tv_loss: 0.03355443477630615\n",
      "iteration 2172, dc_loss: 0.027681443840265274, tv_loss: 0.03354344889521599\n",
      "iteration 2173, dc_loss: 0.02767915092408657, tv_loss: 0.03354112058877945\n",
      "iteration 2174, dc_loss: 0.027664517983794212, tv_loss: 0.033548641949892044\n",
      "iteration 2175, dc_loss: 0.02766764722764492, tv_loss: 0.03354286774992943\n",
      "iteration 2176, dc_loss: 0.02765100821852684, tv_loss: 0.0335678905248642\n",
      "iteration 2177, dc_loss: 0.027681197971105576, tv_loss: 0.03354380279779434\n",
      "iteration 2178, dc_loss: 0.02765273116528988, tv_loss: 0.03357328101992607\n",
      "iteration 2179, dc_loss: 0.027694376185536385, tv_loss: 0.03352303430438042\n",
      "iteration 2180, dc_loss: 0.027650197967886925, tv_loss: 0.03357330337166786\n",
      "iteration 2181, dc_loss: 0.027695555239915848, tv_loss: 0.03351984918117523\n",
      "iteration 2182, dc_loss: 0.027638040482997894, tv_loss: 0.03356105834245682\n",
      "iteration 2183, dc_loss: 0.027648407965898514, tv_loss: 0.033524952828884125\n",
      "iteration 2184, dc_loss: 0.027588067576289177, tv_loss: 0.033562373369932175\n",
      "iteration 2185, dc_loss: 0.02760344371199608, tv_loss: 0.03356284275650978\n",
      "iteration 2186, dc_loss: 0.027593979611992836, tv_loss: 0.03358101844787598\n",
      "iteration 2187, dc_loss: 0.02759874239563942, tv_loss: 0.03355276957154274\n",
      "iteration 2188, dc_loss: 0.027590662240982056, tv_loss: 0.03354113921523094\n",
      "iteration 2189, dc_loss: 0.027546362951397896, tv_loss: 0.0335916168987751\n",
      "iteration 2190, dc_loss: 0.027587831020355225, tv_loss: 0.03353572636842728\n",
      "iteration 2191, dc_loss: 0.027549905702471733, tv_loss: 0.03356176242232323\n",
      "iteration 2192, dc_loss: 0.027555568143725395, tv_loss: 0.033545903861522675\n",
      "iteration 2193, dc_loss: 0.027550803497433662, tv_loss: 0.03353622928261757\n",
      "iteration 2194, dc_loss: 0.027519606053829193, tv_loss: 0.03356815502047539\n",
      "iteration 2195, dc_loss: 0.027550790458917618, tv_loss: 0.03354330360889435\n",
      "iteration 2196, dc_loss: 0.027503253892064095, tv_loss: 0.03358086198568344\n",
      "iteration 2197, dc_loss: 0.027523359283804893, tv_loss: 0.033543121069669724\n",
      "iteration 2198, dc_loss: 0.0275090504437685, tv_loss: 0.03354766219854355\n",
      "iteration 2199, dc_loss: 0.02749500796198845, tv_loss: 0.03355219215154648\n",
      "iteration 2200, dc_loss: 0.027510978281497955, tv_loss: 0.03353611379861832\n",
      "iteration 2201, dc_loss: 0.027474859729409218, tv_loss: 0.033565014600753784\n",
      "iteration 2202, dc_loss: 0.027491068467497826, tv_loss: 0.03355182707309723\n",
      "iteration 2203, dc_loss: 0.02746845968067646, tv_loss: 0.033574026077985764\n",
      "iteration 2204, dc_loss: 0.027466651052236557, tv_loss: 0.03356019780039787\n",
      "iteration 2205, dc_loss: 0.02746317908167839, tv_loss: 0.03355100005865097\n",
      "iteration 2206, dc_loss: 0.027458932250738144, tv_loss: 0.033542435616254807\n",
      "iteration 2207, dc_loss: 0.027445416897535324, tv_loss: 0.033554598689079285\n",
      "iteration 2208, dc_loss: 0.027437640354037285, tv_loss: 0.033559199422597885\n",
      "iteration 2209, dc_loss: 0.027446720749139786, tv_loss: 0.0335502065718174\n",
      "iteration 2210, dc_loss: 0.02741897664964199, tv_loss: 0.033570148050785065\n",
      "iteration 2211, dc_loss: 0.027426136657595634, tv_loss: 0.03355049714446068\n",
      "iteration 2212, dc_loss: 0.027410876005887985, tv_loss: 0.033555563539266586\n",
      "iteration 2213, dc_loss: 0.027419086545705795, tv_loss: 0.03354161977767944\n",
      "iteration 2214, dc_loss: 0.027398768812417984, tv_loss: 0.033561382442712784\n",
      "iteration 2215, dc_loss: 0.027395229786634445, tv_loss: 0.03356339409947395\n",
      "iteration 2216, dc_loss: 0.027394937351346016, tv_loss: 0.033554110676050186\n",
      "iteration 2217, dc_loss: 0.02738768607378006, tv_loss: 0.03355187922716141\n",
      "iteration 2218, dc_loss: 0.027376821264624596, tv_loss: 0.03354731202125549\n",
      "iteration 2219, dc_loss: 0.02736218273639679, tv_loss: 0.033557165414094925\n",
      "iteration 2220, dc_loss: 0.027371427044272423, tv_loss: 0.0335397869348526\n",
      "iteration 2221, dc_loss: 0.02735736407339573, tv_loss: 0.03354243189096451\n",
      "iteration 2222, dc_loss: 0.02735494263470173, tv_loss: 0.033544983714818954\n",
      "iteration 2223, dc_loss: 0.02733929082751274, tv_loss: 0.033569157123565674\n",
      "iteration 2224, dc_loss: 0.027344075962901115, tv_loss: 0.03356130048632622\n",
      "iteration 2225, dc_loss: 0.02733185514807701, tv_loss: 0.03355751559138298\n",
      "iteration 2226, dc_loss: 0.027327144518494606, tv_loss: 0.03354707360267639\n",
      "iteration 2227, dc_loss: 0.02731313370168209, tv_loss: 0.03354991227388382\n",
      "iteration 2228, dc_loss: 0.027317434549331665, tv_loss: 0.03353721648454666\n",
      "iteration 2229, dc_loss: 0.02730851247906685, tv_loss: 0.03353719785809517\n",
      "iteration 2230, dc_loss: 0.027295265346765518, tv_loss: 0.033541254699230194\n",
      "iteration 2231, dc_loss: 0.027298688888549805, tv_loss: 0.03353207930922508\n",
      "iteration 2232, dc_loss: 0.027286922559142113, tv_loss: 0.033535994589328766\n",
      "iteration 2233, dc_loss: 0.027278538793325424, tv_loss: 0.03353746235370636\n",
      "iteration 2234, dc_loss: 0.0272762980312109, tv_loss: 0.03353150933980942\n",
      "iteration 2235, dc_loss: 0.027265870943665504, tv_loss: 0.03354182094335556\n",
      "iteration 2236, dc_loss: 0.027262086048722267, tv_loss: 0.0335431843996048\n",
      "iteration 2237, dc_loss: 0.027256831526756287, tv_loss: 0.03354692459106445\n",
      "iteration 2238, dc_loss: 0.027258047834038734, tv_loss: 0.03356486186385155\n",
      "iteration 2239, dc_loss: 0.027229895815253258, tv_loss: 0.03358769416809082\n",
      "iteration 2240, dc_loss: 0.027248084545135498, tv_loss: 0.033548757433891296\n",
      "iteration 2241, dc_loss: 0.027227558195590973, tv_loss: 0.033553291112184525\n",
      "iteration 2242, dc_loss: 0.02721981890499592, tv_loss: 0.03355502709746361\n",
      "iteration 2243, dc_loss: 0.027218565344810486, tv_loss: 0.03355851396918297\n",
      "iteration 2244, dc_loss: 0.02721378207206726, tv_loss: 0.033549144864082336\n",
      "iteration 2245, dc_loss: 0.02721773460507393, tv_loss: 0.03353704512119293\n",
      "iteration 2246, dc_loss: 0.027186406776309013, tv_loss: 0.03356725722551346\n",
      "iteration 2247, dc_loss: 0.027201594784855843, tv_loss: 0.033546071499586105\n",
      "iteration 2248, dc_loss: 0.027190441265702248, tv_loss: 0.0335594080388546\n",
      "iteration 2249, dc_loss: 0.027175024151802063, tv_loss: 0.03356745094060898\n",
      "iteration 2250, dc_loss: 0.027180293574929237, tv_loss: 0.03355369344353676\n",
      "iteration 2251, dc_loss: 0.027177082374691963, tv_loss: 0.033547293394804\n",
      "iteration 2252, dc_loss: 0.027163658291101456, tv_loss: 0.0335543192923069\n",
      "iteration 2253, dc_loss: 0.027154620736837387, tv_loss: 0.03355058655142784\n",
      "iteration 2254, dc_loss: 0.027174148708581924, tv_loss: 0.033532362431287766\n",
      "iteration 2255, dc_loss: 0.027126185595989227, tv_loss: 0.03357428312301636\n",
      "iteration 2256, dc_loss: 0.02716219425201416, tv_loss: 0.03354451060295105\n",
      "iteration 2257, dc_loss: 0.027122553437948227, tv_loss: 0.033594369888305664\n",
      "iteration 2258, dc_loss: 0.027157241478562355, tv_loss: 0.0335371159017086\n",
      "iteration 2259, dc_loss: 0.02711908146739006, tv_loss: 0.03357892856001854\n",
      "iteration 2260, dc_loss: 0.027171041816473007, tv_loss: 0.03352496400475502\n",
      "iteration 2261, dc_loss: 0.027131764218211174, tv_loss: 0.03358086571097374\n",
      "iteration 2262, dc_loss: 0.02719303034245968, tv_loss: 0.0335368737578392\n",
      "iteration 2263, dc_loss: 0.02715182490646839, tv_loss: 0.03358076140284538\n",
      "iteration 2264, dc_loss: 0.02721693366765976, tv_loss: 0.03351851925253868\n",
      "iteration 2265, dc_loss: 0.027108022943139076, tv_loss: 0.03359273821115494\n",
      "iteration 2266, dc_loss: 0.027154065668582916, tv_loss: 0.03351244702935219\n",
      "iteration 2267, dc_loss: 0.027063854038715363, tv_loss: 0.03359110653400421\n",
      "iteration 2268, dc_loss: 0.02713109366595745, tv_loss: 0.033521831035614014\n",
      "iteration 2269, dc_loss: 0.027086671441793442, tv_loss: 0.03356315940618515\n",
      "iteration 2270, dc_loss: 0.027089735493063927, tv_loss: 0.03353464975953102\n",
      "iteration 2271, dc_loss: 0.027067583054304123, tv_loss: 0.03354484587907791\n",
      "iteration 2272, dc_loss: 0.02705475315451622, tv_loss: 0.033554017543792725\n",
      "iteration 2273, dc_loss: 0.0270626749843359, tv_loss: 0.03354572877287865\n",
      "iteration 2274, dc_loss: 0.027018336579203606, tv_loss: 0.03359308838844299\n",
      "iteration 2275, dc_loss: 0.02704511396586895, tv_loss: 0.033566251397132874\n",
      "iteration 2276, dc_loss: 0.0270218662917614, tv_loss: 0.03356754034757614\n",
      "iteration 2277, dc_loss: 0.027016203850507736, tv_loss: 0.033550821244716644\n",
      "iteration 2278, dc_loss: 0.027018699795007706, tv_loss: 0.033563919365406036\n",
      "iteration 2279, dc_loss: 0.027020394802093506, tv_loss: 0.03356077894568443\n",
      "iteration 2280, dc_loss: 0.02699338085949421, tv_loss: 0.03357839584350586\n",
      "iteration 2281, dc_loss: 0.026979660615324974, tv_loss: 0.03356862813234329\n",
      "iteration 2282, dc_loss: 0.026989825069904327, tv_loss: 0.03356894105672836\n",
      "iteration 2283, dc_loss: 0.02698909491300583, tv_loss: 0.0335676409304142\n",
      "iteration 2284, dc_loss: 0.026960719376802444, tv_loss: 0.03358248248696327\n",
      "iteration 2285, dc_loss: 0.026964476332068443, tv_loss: 0.03356232866644859\n",
      "iteration 2286, dc_loss: 0.02696220763027668, tv_loss: 0.033560629934072495\n",
      "iteration 2287, dc_loss: 0.026962779462337494, tv_loss: 0.03356296196579933\n",
      "iteration 2288, dc_loss: 0.026947643607854843, tv_loss: 0.033565256744623184\n",
      "iteration 2289, dc_loss: 0.026943610981106758, tv_loss: 0.03355848789215088\n",
      "iteration 2290, dc_loss: 0.02694598026573658, tv_loss: 0.03354739770293236\n",
      "iteration 2291, dc_loss: 0.0269102081656456, tv_loss: 0.03357089310884476\n",
      "iteration 2292, dc_loss: 0.026943648234009743, tv_loss: 0.03352933004498482\n",
      "iteration 2293, dc_loss: 0.026901423931121826, tv_loss: 0.03356832265853882\n",
      "iteration 2294, dc_loss: 0.02691604010760784, tv_loss: 0.03354077413678169\n",
      "iteration 2295, dc_loss: 0.026906166225671768, tv_loss: 0.03354625403881073\n",
      "iteration 2296, dc_loss: 0.026904158294200897, tv_loss: 0.03354842960834503\n",
      "iteration 2297, dc_loss: 0.02689269743859768, tv_loss: 0.03356309235095978\n",
      "iteration 2298, dc_loss: 0.026883786544203758, tv_loss: 0.03357689455151558\n",
      "iteration 2299, dc_loss: 0.026883039623498917, tv_loss: 0.03355860337615013\n",
      "iteration 2300, dc_loss: 0.026868917047977448, tv_loss: 0.03355453535914421\n",
      "iteration 2301, dc_loss: 0.026876993477344513, tv_loss: 0.033550482243299484\n",
      "iteration 2302, dc_loss: 0.02687816135585308, tv_loss: 0.03355400636792183\n",
      "iteration 2303, dc_loss: 0.026846997439861298, tv_loss: 0.03357699140906334\n",
      "iteration 2304, dc_loss: 0.02684292010962963, tv_loss: 0.03356851264834404\n",
      "iteration 2305, dc_loss: 0.026842569932341576, tv_loss: 0.033554647117853165\n",
      "iteration 2306, dc_loss: 0.02684759348630905, tv_loss: 0.033543720841407776\n",
      "iteration 2307, dc_loss: 0.02682357095181942, tv_loss: 0.03356846794486046\n",
      "iteration 2308, dc_loss: 0.02684127911925316, tv_loss: 0.03354900702834129\n",
      "iteration 2309, dc_loss: 0.026826536282896996, tv_loss: 0.03355881944298744\n",
      "iteration 2310, dc_loss: 0.02681891992688179, tv_loss: 0.03355627879500389\n",
      "iteration 2311, dc_loss: 0.026805438101291656, tv_loss: 0.03355765342712402\n",
      "iteration 2312, dc_loss: 0.026808667927980423, tv_loss: 0.03355175256729126\n",
      "iteration 2313, dc_loss: 0.0267917700111866, tv_loss: 0.03356752544641495\n",
      "iteration 2314, dc_loss: 0.026796821504831314, tv_loss: 0.03355495631694794\n",
      "iteration 2315, dc_loss: 0.026789244264364243, tv_loss: 0.033559370785951614\n",
      "iteration 2316, dc_loss: 0.02678551897406578, tv_loss: 0.033561162650585175\n",
      "iteration 2317, dc_loss: 0.026776278391480446, tv_loss: 0.033562835305929184\n",
      "iteration 2318, dc_loss: 0.02678353153169155, tv_loss: 0.033550918102264404\n",
      "iteration 2319, dc_loss: 0.0267709381878376, tv_loss: 0.03355656936764717\n",
      "iteration 2320, dc_loss: 0.026770323514938354, tv_loss: 0.03355264663696289\n",
      "iteration 2321, dc_loss: 0.026764750480651855, tv_loss: 0.033550627529621124\n",
      "iteration 2322, dc_loss: 0.026787448674440384, tv_loss: 0.0335344523191452\n",
      "iteration 2323, dc_loss: 0.026743164286017418, tv_loss: 0.03357565402984619\n",
      "iteration 2324, dc_loss: 0.026812661439180374, tv_loss: 0.03353044390678406\n",
      "iteration 2325, dc_loss: 0.026751650497317314, tv_loss: 0.033610448241233826\n",
      "iteration 2326, dc_loss: 0.026830369606614113, tv_loss: 0.03353257477283478\n",
      "iteration 2327, dc_loss: 0.02676503360271454, tv_loss: 0.033616140484809875\n",
      "iteration 2328, dc_loss: 0.026915375143289566, tv_loss: 0.0334932878613472\n",
      "iteration 2329, dc_loss: 0.026788154616951942, tv_loss: 0.03365008533000946\n",
      "iteration 2330, dc_loss: 0.026951968669891357, tv_loss: 0.03346666321158409\n",
      "iteration 2331, dc_loss: 0.02677331119775772, tv_loss: 0.03364650532603264\n",
      "iteration 2332, dc_loss: 0.026898005977272987, tv_loss: 0.03347517549991608\n",
      "iteration 2333, dc_loss: 0.026694804430007935, tv_loss: 0.03364010155200958\n",
      "iteration 2334, dc_loss: 0.026730911806225777, tv_loss: 0.033533673733472824\n",
      "iteration 2335, dc_loss: 0.026670053601264954, tv_loss: 0.03356904909014702\n",
      "iteration 2336, dc_loss: 0.02665592171251774, tv_loss: 0.033576302230358124\n",
      "iteration 2337, dc_loss: 0.02675398625433445, tv_loss: 0.03350657597184181\n",
      "iteration 2338, dc_loss: 0.02666468173265457, tv_loss: 0.03360222652554512\n",
      "iteration 2339, dc_loss: 0.026751438155770302, tv_loss: 0.03350720927119255\n",
      "iteration 2340, dc_loss: 0.026638206094503403, tv_loss: 0.03360220789909363\n",
      "iteration 2341, dc_loss: 0.026705723255872726, tv_loss: 0.03350333496928215\n",
      "iteration 2342, dc_loss: 0.02662797085940838, tv_loss: 0.03356809914112091\n",
      "iteration 2343, dc_loss: 0.02662576362490654, tv_loss: 0.033558692783117294\n",
      "iteration 2344, dc_loss: 0.026662196964025497, tv_loss: 0.03353346884250641\n",
      "iteration 2345, dc_loss: 0.026597879827022552, tv_loss: 0.03359242528676987\n",
      "iteration 2346, dc_loss: 0.026656340807676315, tv_loss: 0.03353406488895416\n",
      "iteration 2347, dc_loss: 0.02661045640707016, tv_loss: 0.03356579318642616\n",
      "iteration 2348, dc_loss: 0.02663405053317547, tv_loss: 0.033532269299030304\n",
      "iteration 2349, dc_loss: 0.026605000719428062, tv_loss: 0.033548496663570404\n",
      "iteration 2350, dc_loss: 0.026585659012198448, tv_loss: 0.033556018024683\n",
      "iteration 2351, dc_loss: 0.026597466319799423, tv_loss: 0.03355201706290245\n",
      "iteration 2352, dc_loss: 0.026564152911305428, tv_loss: 0.03360305353999138\n",
      "iteration 2353, dc_loss: 0.026611048728227615, tv_loss: 0.033538222312927246\n",
      "iteration 2354, dc_loss: 0.026548022404313087, tv_loss: 0.033581651747226715\n",
      "iteration 2355, dc_loss: 0.026582082733511925, tv_loss: 0.03353676199913025\n",
      "iteration 2356, dc_loss: 0.026557156816124916, tv_loss: 0.0335640124976635\n",
      "iteration 2357, dc_loss: 0.026562636718153954, tv_loss: 0.03355855494737625\n",
      "iteration 2358, dc_loss: 0.02654479444026947, tv_loss: 0.033564090728759766\n",
      "iteration 2359, dc_loss: 0.02652701921761036, tv_loss: 0.03356822580099106\n",
      "iteration 2360, dc_loss: 0.026549795642495155, tv_loss: 0.03353048115968704\n",
      "iteration 2361, dc_loss: 0.026521459221839905, tv_loss: 0.03355816751718521\n",
      "iteration 2362, dc_loss: 0.026544969528913498, tv_loss: 0.03353527560830116\n",
      "iteration 2363, dc_loss: 0.026500215753912926, tv_loss: 0.033577729016542435\n",
      "iteration 2364, dc_loss: 0.026538526639342308, tv_loss: 0.03354375809431076\n",
      "iteration 2365, dc_loss: 0.02649451047182083, tv_loss: 0.0335836224257946\n",
      "iteration 2366, dc_loss: 0.026524199172854424, tv_loss: 0.033544838428497314\n",
      "iteration 2367, dc_loss: 0.026466762647032738, tv_loss: 0.03357860445976257\n",
      "iteration 2368, dc_loss: 0.026499826461076736, tv_loss: 0.03353739529848099\n",
      "iteration 2369, dc_loss: 0.026485580950975418, tv_loss: 0.03354474529623985\n",
      "iteration 2370, dc_loss: 0.02646910399198532, tv_loss: 0.03355514630675316\n",
      "iteration 2371, dc_loss: 0.026493147015571594, tv_loss: 0.03352196887135506\n",
      "iteration 2372, dc_loss: 0.026447506621479988, tv_loss: 0.03357076272368431\n",
      "iteration 2373, dc_loss: 0.02648022584617138, tv_loss: 0.033550843596458435\n",
      "iteration 2374, dc_loss: 0.02643568068742752, tv_loss: 0.03359782695770264\n",
      "iteration 2375, dc_loss: 0.02647951804101467, tv_loss: 0.03353556618094444\n",
      "iteration 2376, dc_loss: 0.026425037533044815, tv_loss: 0.033571161329746246\n",
      "iteration 2377, dc_loss: 0.02645958960056305, tv_loss: 0.03353137895464897\n",
      "iteration 2378, dc_loss: 0.02642187662422657, tv_loss: 0.033556342124938965\n",
      "iteration 2379, dc_loss: 0.026435354724526405, tv_loss: 0.033537548035383224\n",
      "iteration 2380, dc_loss: 0.026423489674925804, tv_loss: 0.03353709727525711\n",
      "iteration 2381, dc_loss: 0.026414213702082634, tv_loss: 0.03355250507593155\n",
      "iteration 2382, dc_loss: 0.026408562436699867, tv_loss: 0.03356926143169403\n",
      "iteration 2383, dc_loss: 0.0263962484896183, tv_loss: 0.03358330950140953\n",
      "iteration 2384, dc_loss: 0.026417210698127747, tv_loss: 0.033547960221767426\n",
      "iteration 2385, dc_loss: 0.026380419731140137, tv_loss: 0.03356770798563957\n",
      "iteration 2386, dc_loss: 0.026402950286865234, tv_loss: 0.033537667244672775\n",
      "iteration 2387, dc_loss: 0.026379289105534554, tv_loss: 0.03355893865227699\n",
      "iteration 2388, dc_loss: 0.026376893743872643, tv_loss: 0.03356650844216347\n",
      "iteration 2389, dc_loss: 0.02636924758553505, tv_loss: 0.03356868773698807\n",
      "iteration 2390, dc_loss: 0.02636273019015789, tv_loss: 0.03356414660811424\n",
      "iteration 2391, dc_loss: 0.026371896266937256, tv_loss: 0.03354267776012421\n",
      "iteration 2392, dc_loss: 0.02634674310684204, tv_loss: 0.033563707023859024\n",
      "iteration 2393, dc_loss: 0.026362024247646332, tv_loss: 0.03353724256157875\n",
      "iteration 2394, dc_loss: 0.026341894641518593, tv_loss: 0.0335543230175972\n",
      "iteration 2395, dc_loss: 0.026344509795308113, tv_loss: 0.033549170941114426\n",
      "iteration 2396, dc_loss: 0.026335295289754868, tv_loss: 0.03355289623141289\n",
      "iteration 2397, dc_loss: 0.02633696049451828, tv_loss: 0.03355349600315094\n",
      "iteration 2398, dc_loss: 0.026306863874197006, tv_loss: 0.033584654331207275\n",
      "iteration 2399, dc_loss: 0.02633942849934101, tv_loss: 0.033539362251758575\n",
      "iteration 2400, dc_loss: 0.026316111907362938, tv_loss: 0.033553920686244965\n",
      "iteration 2401, dc_loss: 0.026325548067688942, tv_loss: 0.03353872522711754\n",
      "iteration 2402, dc_loss: 0.026292309165000916, tv_loss: 0.03356120362877846\n",
      "iteration 2403, dc_loss: 0.02629193477332592, tv_loss: 0.03355366736650467\n",
      "iteration 2404, dc_loss: 0.026312272995710373, tv_loss: 0.033528659492731094\n",
      "iteration 2405, dc_loss: 0.02628338150680065, tv_loss: 0.03354332223534584\n",
      "iteration 2406, dc_loss: 0.026276659220457077, tv_loss: 0.03354234620928764\n",
      "iteration 2407, dc_loss: 0.026294592767953873, tv_loss: 0.03352069482207298\n",
      "iteration 2408, dc_loss: 0.026278384029865265, tv_loss: 0.03353484719991684\n",
      "iteration 2409, dc_loss: 0.026269875466823578, tv_loss: 0.033531248569488525\n",
      "iteration 2410, dc_loss: 0.02626989223062992, tv_loss: 0.03353374823927879\n",
      "iteration 2411, dc_loss: 0.026257917284965515, tv_loss: 0.033548083156347275\n",
      "iteration 2412, dc_loss: 0.02626483328640461, tv_loss: 0.03355021774768829\n",
      "iteration 2413, dc_loss: 0.026256315410137177, tv_loss: 0.03355855122208595\n",
      "iteration 2414, dc_loss: 0.026248760521411896, tv_loss: 0.03354766592383385\n",
      "iteration 2415, dc_loss: 0.02624414674937725, tv_loss: 0.03353719413280487\n",
      "iteration 2416, dc_loss: 0.02623438648879528, tv_loss: 0.03354603797197342\n",
      "iteration 2417, dc_loss: 0.02623807080090046, tv_loss: 0.033548757433891296\n",
      "iteration 2418, dc_loss: 0.026239216327667236, tv_loss: 0.033544812351465225\n",
      "iteration 2419, dc_loss: 0.0262349434196949, tv_loss: 0.03354291245341301\n",
      "iteration 2420, dc_loss: 0.026222992688417435, tv_loss: 0.033543724566698074\n",
      "iteration 2421, dc_loss: 0.026220479980111122, tv_loss: 0.03353726118803024\n",
      "iteration 2422, dc_loss: 0.026214372366666794, tv_loss: 0.03354174643754959\n",
      "iteration 2423, dc_loss: 0.026208920404314995, tv_loss: 0.03354405239224434\n",
      "iteration 2424, dc_loss: 0.026208698749542236, tv_loss: 0.03353366255760193\n",
      "iteration 2425, dc_loss: 0.026204532012343407, tv_loss: 0.033541131764650345\n",
      "iteration 2426, dc_loss: 0.026194903999567032, tv_loss: 0.03354164958000183\n",
      "iteration 2427, dc_loss: 0.02619781345129013, tv_loss: 0.033533044159412384\n",
      "iteration 2428, dc_loss: 0.026192767545580864, tv_loss: 0.033537667244672775\n",
      "iteration 2429, dc_loss: 0.026187101379036903, tv_loss: 0.03353624418377876\n",
      "iteration 2430, dc_loss: 0.02618248388171196, tv_loss: 0.0335451140999794\n",
      "iteration 2431, dc_loss: 0.026178037747740746, tv_loss: 0.03355489671230316\n",
      "iteration 2432, dc_loss: 0.026168320327997208, tv_loss: 0.03355442360043526\n",
      "iteration 2433, dc_loss: 0.026170426979660988, tv_loss: 0.03354260325431824\n",
      "iteration 2434, dc_loss: 0.026169970631599426, tv_loss: 0.03353654593229294\n",
      "iteration 2435, dc_loss: 0.026156265288591385, tv_loss: 0.03354572504758835\n",
      "iteration 2436, dc_loss: 0.026156457141041756, tv_loss: 0.03355081379413605\n",
      "iteration 2437, dc_loss: 0.026155980303883553, tv_loss: 0.03354498744010925\n",
      "iteration 2438, dc_loss: 0.026149949058890343, tv_loss: 0.03354200720787048\n",
      "iteration 2439, dc_loss: 0.026142384856939316, tv_loss: 0.03354593738913536\n",
      "iteration 2440, dc_loss: 0.026146981865167618, tv_loss: 0.03352908045053482\n",
      "iteration 2441, dc_loss: 0.02613896131515503, tv_loss: 0.03353259339928627\n",
      "iteration 2442, dc_loss: 0.02612459287047386, tv_loss: 0.03354959189891815\n",
      "iteration 2443, dc_loss: 0.026130754500627518, tv_loss: 0.03354361280798912\n",
      "iteration 2444, dc_loss: 0.02612093649804592, tv_loss: 0.03354443609714508\n",
      "iteration 2445, dc_loss: 0.026118367910385132, tv_loss: 0.03355870023369789\n",
      "iteration 2446, dc_loss: 0.026113471016287804, tv_loss: 0.03355475515127182\n",
      "iteration 2447, dc_loss: 0.026117641478776932, tv_loss: 0.03353803977370262\n",
      "iteration 2448, dc_loss: 0.02610631473362446, tv_loss: 0.03354043886065483\n",
      "iteration 2449, dc_loss: 0.026104610413312912, tv_loss: 0.03353424370288849\n",
      "iteration 2450, dc_loss: 0.02609926648437977, tv_loss: 0.03353805094957352\n",
      "iteration 2451, dc_loss: 0.02608462981879711, tv_loss: 0.03355003148317337\n",
      "iteration 2452, dc_loss: 0.026100071147084236, tv_loss: 0.03354213014245033\n",
      "iteration 2453, dc_loss: 0.026089049875736237, tv_loss: 0.033561330288648605\n",
      "iteration 2454, dc_loss: 0.026080738753080368, tv_loss: 0.03355078771710396\n",
      "iteration 2455, dc_loss: 0.026076504960656166, tv_loss: 0.03353644162416458\n",
      "iteration 2456, dc_loss: 0.026073182001709938, tv_loss: 0.033537235110998154\n",
      "iteration 2457, dc_loss: 0.0260754507035017, tv_loss: 0.033527765423059464\n",
      "iteration 2458, dc_loss: 0.02606327086687088, tv_loss: 0.0335419587790966\n",
      "iteration 2459, dc_loss: 0.0260605551302433, tv_loss: 0.03355111926794052\n",
      "iteration 2460, dc_loss: 0.026060014963150024, tv_loss: 0.033556848764419556\n",
      "iteration 2461, dc_loss: 0.02605808712542057, tv_loss: 0.03354814276099205\n",
      "iteration 2462, dc_loss: 0.026044076308608055, tv_loss: 0.03354543074965477\n",
      "iteration 2463, dc_loss: 0.026042941957712173, tv_loss: 0.03353665769100189\n",
      "iteration 2464, dc_loss: 0.026043077930808067, tv_loss: 0.03353406861424446\n",
      "iteration 2465, dc_loss: 0.026038547977805138, tv_loss: 0.03353952616453171\n",
      "iteration 2466, dc_loss: 0.026033267378807068, tv_loss: 0.03355185315012932\n",
      "iteration 2467, dc_loss: 0.026029638946056366, tv_loss: 0.033549606800079346\n",
      "iteration 2468, dc_loss: 0.02603224851191044, tv_loss: 0.03353864699602127\n",
      "iteration 2469, dc_loss: 0.026015836745500565, tv_loss: 0.03354450687766075\n",
      "iteration 2470, dc_loss: 0.026011543348431587, tv_loss: 0.03354039415717125\n",
      "iteration 2471, dc_loss: 0.026016587391495705, tv_loss: 0.033533792942762375\n",
      "iteration 2472, dc_loss: 0.026005469262599945, tv_loss: 0.03353799879550934\n",
      "iteration 2473, dc_loss: 0.026008788496255875, tv_loss: 0.03354154899716377\n",
      "iteration 2474, dc_loss: 0.025998767465353012, tv_loss: 0.033558424562215805\n",
      "iteration 2475, dc_loss: 0.025999674573540688, tv_loss: 0.03355091065168381\n",
      "iteration 2476, dc_loss: 0.025992058217525482, tv_loss: 0.03354445472359657\n",
      "iteration 2477, dc_loss: 0.025992069393396378, tv_loss: 0.0335388258099556\n",
      "iteration 2478, dc_loss: 0.02598576433956623, tv_loss: 0.03354085981845856\n",
      "iteration 2479, dc_loss: 0.025971921160817146, tv_loss: 0.033546701073646545\n",
      "iteration 2480, dc_loss: 0.0259797852486372, tv_loss: 0.03353998437523842\n",
      "iteration 2481, dc_loss: 0.025973642244935036, tv_loss: 0.033538781106472015\n",
      "iteration 2482, dc_loss: 0.025969073176383972, tv_loss: 0.03353753313422203\n",
      "iteration 2483, dc_loss: 0.0259640384465456, tv_loss: 0.03354048728942871\n",
      "iteration 2484, dc_loss: 0.025955654680728912, tv_loss: 0.033546268939971924\n",
      "iteration 2485, dc_loss: 0.025958670303225517, tv_loss: 0.03354373201727867\n",
      "iteration 2486, dc_loss: 0.02596021257340908, tv_loss: 0.03354104235768318\n",
      "iteration 2487, dc_loss: 0.025941144675016403, tv_loss: 0.033554721623659134\n",
      "iteration 2488, dc_loss: 0.0259446669369936, tv_loss: 0.03353911265730858\n",
      "iteration 2489, dc_loss: 0.025944802910089493, tv_loss: 0.03354262188076973\n",
      "iteration 2490, dc_loss: 0.025930130854249, tv_loss: 0.03354617953300476\n",
      "iteration 2491, dc_loss: 0.02593277581036091, tv_loss: 0.03353315219283104\n",
      "iteration 2492, dc_loss: 0.025928374379873276, tv_loss: 0.03353903815150261\n",
      "iteration 2493, dc_loss: 0.02592139132320881, tv_loss: 0.03353684023022652\n",
      "iteration 2494, dc_loss: 0.025927355512976646, tv_loss: 0.03352818638086319\n",
      "iteration 2495, dc_loss: 0.025916405022144318, tv_loss: 0.03353670984506607\n",
      "iteration 2496, dc_loss: 0.025914466008543968, tv_loss: 0.03353821113705635\n",
      "iteration 2497, dc_loss: 0.025911642238497734, tv_loss: 0.03354470431804657\n",
      "iteration 2498, dc_loss: 0.025902509689331055, tv_loss: 0.03356682136654854\n",
      "iteration 2499, dc_loss: 0.025891518220305443, tv_loss: 0.03356422111392021\n",
      "iteration 2500, dc_loss: 0.02589842677116394, tv_loss: 0.03354267030954361\n",
      "iteration 2501, dc_loss: 0.025890182703733444, tv_loss: 0.03354034945368767\n",
      "iteration 2502, dc_loss: 0.0258872639387846, tv_loss: 0.03354885056614876\n",
      "iteration 2503, dc_loss: 0.02589421719312668, tv_loss: 0.03354746103286743\n",
      "iteration 2504, dc_loss: 0.025874806568026543, tv_loss: 0.033550944179296494\n",
      "iteration 2505, dc_loss: 0.02587125077843666, tv_loss: 0.03354915231466293\n",
      "iteration 2506, dc_loss: 0.0258789099752903, tv_loss: 0.033534109592437744\n",
      "iteration 2507, dc_loss: 0.025863518938422203, tv_loss: 0.033546268939971924\n",
      "iteration 2508, dc_loss: 0.025868363678455353, tv_loss: 0.0335400365293026\n",
      "iteration 2509, dc_loss: 0.02586648054420948, tv_loss: 0.033536724746227264\n",
      "iteration 2510, dc_loss: 0.02585393749177456, tv_loss: 0.033543672412633896\n",
      "iteration 2511, dc_loss: 0.025849733501672745, tv_loss: 0.03354620561003685\n",
      "iteration 2512, dc_loss: 0.025851290673017502, tv_loss: 0.03353794664144516\n",
      "iteration 2513, dc_loss: 0.025841709226369858, tv_loss: 0.033546581864356995\n",
      "iteration 2514, dc_loss: 0.025841282680630684, tv_loss: 0.033538494259119034\n",
      "iteration 2515, dc_loss: 0.02584250457584858, tv_loss: 0.03353191167116165\n",
      "iteration 2516, dc_loss: 0.02583083137869835, tv_loss: 0.03353811800479889\n",
      "iteration 2517, dc_loss: 0.02582363784313202, tv_loss: 0.03354622423648834\n",
      "iteration 2518, dc_loss: 0.0258327703922987, tv_loss: 0.03353643789887428\n",
      "iteration 2519, dc_loss: 0.025816652923822403, tv_loss: 0.03355291858315468\n",
      "iteration 2520, dc_loss: 0.02581964060664177, tv_loss: 0.03355271369218826\n",
      "iteration 2521, dc_loss: 0.025813955813646317, tv_loss: 0.033548157662153244\n",
      "iteration 2522, dc_loss: 0.025814687833189964, tv_loss: 0.03353901579976082\n",
      "iteration 2523, dc_loss: 0.025800473988056183, tv_loss: 0.033545102924108505\n",
      "iteration 2524, dc_loss: 0.02580251917243004, tv_loss: 0.033534176647663116\n",
      "iteration 2525, dc_loss: 0.025804946199059486, tv_loss: 0.0335334911942482\n",
      "iteration 2526, dc_loss: 0.025786958634853363, tv_loss: 0.03355316445231438\n",
      "iteration 2527, dc_loss: 0.02579508349299431, tv_loss: 0.0335434228181839\n",
      "iteration 2528, dc_loss: 0.02578211948275566, tv_loss: 0.03355156630277634\n",
      "iteration 2529, dc_loss: 0.02578170970082283, tv_loss: 0.033544089645147324\n",
      "iteration 2530, dc_loss: 0.025778846815228462, tv_loss: 0.03354137763381004\n",
      "iteration 2531, dc_loss: 0.02577519789338112, tv_loss: 0.033541712909936905\n",
      "iteration 2532, dc_loss: 0.02577584609389305, tv_loss: 0.033536966890096664\n",
      "iteration 2533, dc_loss: 0.025763677433133125, tv_loss: 0.033543650060892105\n",
      "iteration 2534, dc_loss: 0.02575944922864437, tv_loss: 0.03354198485612869\n",
      "iteration 2535, dc_loss: 0.025762889534235, tv_loss: 0.033534616231918335\n",
      "iteration 2536, dc_loss: 0.025759005919098854, tv_loss: 0.03353163227438927\n",
      "iteration 2537, dc_loss: 0.025751346722245216, tv_loss: 0.033532608300447464\n",
      "iteration 2538, dc_loss: 0.025741657242178917, tv_loss: 0.03354224935173988\n",
      "iteration 2539, dc_loss: 0.025740280747413635, tv_loss: 0.03353595361113548\n",
      "iteration 2540, dc_loss: 0.02574996091425419, tv_loss: 0.03354049101471901\n",
      "iteration 2541, dc_loss: 0.02573052980005741, tv_loss: 0.03356495872139931\n",
      "iteration 2542, dc_loss: 0.025733059272170067, tv_loss: 0.033553317189216614\n",
      "iteration 2543, dc_loss: 0.025724725797772408, tv_loss: 0.0335470549762249\n",
      "iteration 2544, dc_loss: 0.025725189596414566, tv_loss: 0.03353295475244522\n",
      "iteration 2545, dc_loss: 0.025725850835442543, tv_loss: 0.03352950140833855\n",
      "iteration 2546, dc_loss: 0.025712184607982635, tv_loss: 0.03353600203990936\n",
      "iteration 2547, dc_loss: 0.025712711736559868, tv_loss: 0.033535394817590714\n",
      "iteration 2548, dc_loss: 0.025707120075821877, tv_loss: 0.03353741392493248\n",
      "iteration 2549, dc_loss: 0.025701621547341347, tv_loss: 0.033562108874320984\n",
      "iteration 2550, dc_loss: 0.025708751752972603, tv_loss: 0.03356214612722397\n",
      "iteration 2551, dc_loss: 0.025693627074360847, tv_loss: 0.03354697301983833\n",
      "iteration 2552, dc_loss: 0.025691673159599304, tv_loss: 0.033549580723047256\n",
      "iteration 2553, dc_loss: 0.025699619203805923, tv_loss: 0.03355421870946884\n",
      "iteration 2554, dc_loss: 0.025677721947431564, tv_loss: 0.03355797380208969\n",
      "iteration 2555, dc_loss: 0.025675876066088676, tv_loss: 0.03354687616229057\n",
      "iteration 2556, dc_loss: 0.025683041661977768, tv_loss: 0.03354896232485771\n",
      "iteration 2557, dc_loss: 0.025670664384961128, tv_loss: 0.03357327729463577\n",
      "iteration 2558, dc_loss: 0.025673888623714447, tv_loss: 0.03355112299323082\n",
      "iteration 2559, dc_loss: 0.025665052235126495, tv_loss: 0.03354065492749214\n",
      "iteration 2560, dc_loss: 0.02567066065967083, tv_loss: 0.03354845568537712\n",
      "iteration 2561, dc_loss: 0.02565951459109783, tv_loss: 0.0335550382733345\n",
      "iteration 2562, dc_loss: 0.025644419714808464, tv_loss: 0.03355320543050766\n",
      "iteration 2563, dc_loss: 0.025663049891591072, tv_loss: 0.03353175148367882\n",
      "iteration 2564, dc_loss: 0.025645839050412178, tv_loss: 0.033541489392519\n",
      "iteration 2565, dc_loss: 0.025639917701482773, tv_loss: 0.033543799072504044\n",
      "iteration 2566, dc_loss: 0.025645896792411804, tv_loss: 0.03353298455476761\n",
      "iteration 2567, dc_loss: 0.02563490904867649, tv_loss: 0.03354819118976593\n",
      "iteration 2568, dc_loss: 0.025625592097640038, tv_loss: 0.033561933785676956\n",
      "iteration 2569, dc_loss: 0.025633318349719048, tv_loss: 0.03353997319936752\n",
      "iteration 2570, dc_loss: 0.025626327842473984, tv_loss: 0.0335441455245018\n",
      "iteration 2571, dc_loss: 0.025622284039855003, tv_loss: 0.03354327008128166\n",
      "iteration 2572, dc_loss: 0.025623667985200882, tv_loss: 0.033534541726112366\n",
      "iteration 2573, dc_loss: 0.025611964985728264, tv_loss: 0.033536188304424286\n",
      "iteration 2574, dc_loss: 0.02560245990753174, tv_loss: 0.03354731574654579\n",
      "iteration 2575, dc_loss: 0.025607772171497345, tv_loss: 0.03353918716311455\n",
      "iteration 2576, dc_loss: 0.025603724643588066, tv_loss: 0.03354490175843239\n",
      "iteration 2577, dc_loss: 0.02559811621904373, tv_loss: 0.03355110436677933\n",
      "iteration 2578, dc_loss: 0.02559511363506317, tv_loss: 0.033550556749105453\n",
      "iteration 2579, dc_loss: 0.02559744007885456, tv_loss: 0.03353935480117798\n",
      "iteration 2580, dc_loss: 0.025578975677490234, tv_loss: 0.033549316227436066\n",
      "iteration 2581, dc_loss: 0.025582697242498398, tv_loss: 0.03353586047887802\n",
      "iteration 2582, dc_loss: 0.025581836700439453, tv_loss: 0.0335393100976944\n",
      "iteration 2583, dc_loss: 0.025569403544068336, tv_loss: 0.03354335576295853\n",
      "iteration 2584, dc_loss: 0.025577975437045097, tv_loss: 0.033526793122291565\n",
      "iteration 2585, dc_loss: 0.02557295560836792, tv_loss: 0.03353065997362137\n",
      "iteration 2586, dc_loss: 0.025563344359397888, tv_loss: 0.03353838250041008\n",
      "iteration 2587, dc_loss: 0.025561757385730743, tv_loss: 0.03353375196456909\n",
      "iteration 2588, dc_loss: 0.025554073974490166, tv_loss: 0.03353654220700264\n",
      "iteration 2589, dc_loss: 0.025555776432156563, tv_loss: 0.033534906804561615\n",
      "iteration 2590, dc_loss: 0.025545654818415642, tv_loss: 0.03356888145208359\n",
      "iteration 2591, dc_loss: 0.02554982900619507, tv_loss: 0.03355839475989342\n",
      "iteration 2592, dc_loss: 0.025536412373185158, tv_loss: 0.033543799072504044\n",
      "iteration 2593, dc_loss: 0.02554292418062687, tv_loss: 0.03354097157716751\n",
      "iteration 2594, dc_loss: 0.025535086169838905, tv_loss: 0.03356997296214104\n",
      "iteration 2595, dc_loss: 0.025523649528622627, tv_loss: 0.03356751427054405\n",
      "iteration 2596, dc_loss: 0.02553441934287548, tv_loss: 0.033529527485370636\n",
      "iteration 2597, dc_loss: 0.025525201112031937, tv_loss: 0.033544834703207016\n",
      "iteration 2598, dc_loss: 0.025516586378216743, tv_loss: 0.0335703119635582\n",
      "iteration 2599, dc_loss: 0.025524763390421867, tv_loss: 0.03355355188250542\n",
      "iteration 2600, dc_loss: 0.02550557255744934, tv_loss: 0.03355163335800171\n",
      "iteration 2601, dc_loss: 0.025507302954792976, tv_loss: 0.03354532644152641\n",
      "iteration 2602, dc_loss: 0.02550044283270836, tv_loss: 0.0335635282099247\n",
      "iteration 2603, dc_loss: 0.025501884520053864, tv_loss: 0.03355540335178375\n",
      "iteration 2604, dc_loss: 0.025513725355267525, tv_loss: 0.03353763371706009\n",
      "iteration 2605, dc_loss: 0.02547893300652504, tv_loss: 0.0335676334798336\n",
      "iteration 2606, dc_loss: 0.025490213185548782, tv_loss: 0.033569153398275375\n",
      "iteration 2607, dc_loss: 0.02550644427537918, tv_loss: 0.03353668004274368\n",
      "iteration 2608, dc_loss: 0.025469059124588966, tv_loss: 0.03356368467211723\n",
      "iteration 2609, dc_loss: 0.025469625368714333, tv_loss: 0.033568594604730606\n",
      "iteration 2610, dc_loss: 0.025487996637821198, tv_loss: 0.03354413062334061\n",
      "iteration 2611, dc_loss: 0.025471149012446404, tv_loss: 0.03355258330702782\n",
      "iteration 2612, dc_loss: 0.025468403473496437, tv_loss: 0.033544886857271194\n",
      "iteration 2613, dc_loss: 0.02546967752277851, tv_loss: 0.03354005888104439\n",
      "iteration 2614, dc_loss: 0.025464003905653954, tv_loss: 0.03355695307254791\n",
      "iteration 2615, dc_loss: 0.025454595685005188, tv_loss: 0.03355836123228073\n",
      "iteration 2616, dc_loss: 0.025450075045228004, tv_loss: 0.03354748338460922\n",
      "iteration 2617, dc_loss: 0.025457536801695824, tv_loss: 0.03354083374142647\n",
      "iteration 2618, dc_loss: 0.025438543409109116, tv_loss: 0.03355865925550461\n",
      "iteration 2619, dc_loss: 0.02544192038476467, tv_loss: 0.033554285764694214\n",
      "iteration 2620, dc_loss: 0.025453150272369385, tv_loss: 0.03353891149163246\n",
      "iteration 2621, dc_loss: 0.025428244844079018, tv_loss: 0.033549223095178604\n",
      "iteration 2622, dc_loss: 0.0254297386854887, tv_loss: 0.033547159284353256\n",
      "iteration 2623, dc_loss: 0.025437628850340843, tv_loss: 0.033536262810230255\n",
      "iteration 2624, dc_loss: 0.02542000263929367, tv_loss: 0.033552076667547226\n",
      "iteration 2625, dc_loss: 0.025416400283575058, tv_loss: 0.03355105221271515\n",
      "iteration 2626, dc_loss: 0.02541813813149929, tv_loss: 0.03354395553469658\n",
      "iteration 2627, dc_loss: 0.025426601991057396, tv_loss: 0.03353239595890045\n",
      "iteration 2628, dc_loss: 0.02540031634271145, tv_loss: 0.03355348855257034\n",
      "iteration 2629, dc_loss: 0.025410853326320648, tv_loss: 0.03353460133075714\n",
      "iteration 2630, dc_loss: 0.025401022285223007, tv_loss: 0.03353686258196831\n",
      "iteration 2631, dc_loss: 0.02540036104619503, tv_loss: 0.03353442996740341\n",
      "iteration 2632, dc_loss: 0.025391362607479095, tv_loss: 0.033536646515131\n",
      "iteration 2633, dc_loss: 0.025394940748810768, tv_loss: 0.03353460878133774\n",
      "iteration 2634, dc_loss: 0.02539198100566864, tv_loss: 0.0335482619702816\n",
      "iteration 2635, dc_loss: 0.025392286479473114, tv_loss: 0.033565275371074677\n",
      "iteration 2636, dc_loss: 0.025368565693497658, tv_loss: 0.0335749089717865\n",
      "iteration 2637, dc_loss: 0.02539770118892193, tv_loss: 0.03352167829871178\n",
      "iteration 2638, dc_loss: 0.025376660749316216, tv_loss: 0.033553093671798706\n",
      "iteration 2639, dc_loss: 0.025379788130521774, tv_loss: 0.03355734050273895\n",
      "iteration 2640, dc_loss: 0.025376001372933388, tv_loss: 0.033568594604730606\n",
      "iteration 2641, dc_loss: 0.025396492332220078, tv_loss: 0.03353360295295715\n",
      "iteration 2642, dc_loss: 0.025383803993463516, tv_loss: 0.03355663642287254\n",
      "iteration 2643, dc_loss: 0.025408253073692322, tv_loss: 0.0335330031812191\n",
      "iteration 2644, dc_loss: 0.025388101115822792, tv_loss: 0.03357475623488426\n",
      "iteration 2645, dc_loss: 0.025430195033550262, tv_loss: 0.03351946547627449\n",
      "iteration 2646, dc_loss: 0.025366416200995445, tv_loss: 0.03357579559087753\n",
      "iteration 2647, dc_loss: 0.02538973279297352, tv_loss: 0.03352484479546547\n",
      "iteration 2648, dc_loss: 0.02534474991261959, tv_loss: 0.03354828804731369\n",
      "iteration 2649, dc_loss: 0.025352533906698227, tv_loss: 0.03352534770965576\n",
      "iteration 2650, dc_loss: 0.025317782536149025, tv_loss: 0.03354683518409729\n",
      "iteration 2651, dc_loss: 0.025335099548101425, tv_loss: 0.03353557735681534\n",
      "iteration 2652, dc_loss: 0.025354696437716484, tv_loss: 0.033525899052619934\n",
      "iteration 2653, dc_loss: 0.02531779371201992, tv_loss: 0.03357987478375435\n",
      "iteration 2654, dc_loss: 0.025343354791402817, tv_loss: 0.0335431769490242\n",
      "iteration 2655, dc_loss: 0.025308454409241676, tv_loss: 0.03354915603995323\n",
      "iteration 2656, dc_loss: 0.02531713806092739, tv_loss: 0.03353313356637955\n",
      "iteration 2657, dc_loss: 0.025299271568655968, tv_loss: 0.03354702144861221\n",
      "iteration 2658, dc_loss: 0.025306036695837975, tv_loss: 0.033556655049324036\n",
      "iteration 2659, dc_loss: 0.025314785540103912, tv_loss: 0.03353970870375633\n",
      "iteration 2660, dc_loss: 0.025291871279478073, tv_loss: 0.03355037048459053\n",
      "iteration 2661, dc_loss: 0.025298766791820526, tv_loss: 0.033532362431287766\n",
      "iteration 2662, dc_loss: 0.025290733203291893, tv_loss: 0.03354644775390625\n",
      "iteration 2663, dc_loss: 0.025293180719017982, tv_loss: 0.033544015139341354\n",
      "iteration 2664, dc_loss: 0.025265634059906006, tv_loss: 0.033574458211660385\n",
      "iteration 2665, dc_loss: 0.025280961766839027, tv_loss: 0.03354880213737488\n",
      "iteration 2666, dc_loss: 0.025286545976996422, tv_loss: 0.033536359667778015\n",
      "iteration 2667, dc_loss: 0.025269338861107826, tv_loss: 0.03354727476835251\n",
      "iteration 2668, dc_loss: 0.02527577616274357, tv_loss: 0.03354162350296974\n",
      "iteration 2669, dc_loss: 0.0252615325152874, tv_loss: 0.033550772815942764\n",
      "iteration 2670, dc_loss: 0.02527027390897274, tv_loss: 0.03354117274284363\n",
      "iteration 2671, dc_loss: 0.02524625137448311, tv_loss: 0.03355320543050766\n",
      "iteration 2672, dc_loss: 0.025253454223275185, tv_loss: 0.03354334458708763\n",
      "iteration 2673, dc_loss: 0.025259481742978096, tv_loss: 0.03353210166096687\n",
      "iteration 2674, dc_loss: 0.025244731456041336, tv_loss: 0.03354869782924652\n",
      "iteration 2675, dc_loss: 0.02524239383637905, tv_loss: 0.03353925049304962\n",
      "iteration 2676, dc_loss: 0.025246581062674522, tv_loss: 0.03353384882211685\n",
      "iteration 2677, dc_loss: 0.02523181214928627, tv_loss: 0.03354600444436073\n",
      "iteration 2678, dc_loss: 0.02523544616997242, tv_loss: 0.03354440629482269\n",
      "iteration 2679, dc_loss: 0.025221100077033043, tv_loss: 0.033558353781700134\n",
      "iteration 2680, dc_loss: 0.025228355079889297, tv_loss: 0.03354203701019287\n",
      "iteration 2681, dc_loss: 0.02522161230444908, tv_loss: 0.033542852848768234\n",
      "iteration 2682, dc_loss: 0.02522435411810875, tv_loss: 0.03353677690029144\n",
      "iteration 2683, dc_loss: 0.025210769847035408, tv_loss: 0.03354522958397865\n",
      "iteration 2684, dc_loss: 0.02520926482975483, tv_loss: 0.03354831412434578\n",
      "iteration 2685, dc_loss: 0.025207320228219032, tv_loss: 0.033542390912771225\n",
      "iteration 2686, dc_loss: 0.025209784507751465, tv_loss: 0.033539436757564545\n",
      "iteration 2687, dc_loss: 0.025192884728312492, tv_loss: 0.033546727150678635\n",
      "iteration 2688, dc_loss: 0.02520137093961239, tv_loss: 0.03353746980428696\n",
      "iteration 2689, dc_loss: 0.025192677974700928, tv_loss: 0.03353990614414215\n",
      "iteration 2690, dc_loss: 0.025187203660607338, tv_loss: 0.03354939445853233\n",
      "iteration 2691, dc_loss: 0.02519466169178486, tv_loss: 0.033536989241838455\n",
      "iteration 2692, dc_loss: 0.02518061362206936, tv_loss: 0.033547576516866684\n",
      "iteration 2693, dc_loss: 0.025175221264362335, tv_loss: 0.03355292230844498\n",
      "iteration 2694, dc_loss: 0.02518206089735031, tv_loss: 0.03353853151202202\n",
      "iteration 2695, dc_loss: 0.02517770417034626, tv_loss: 0.03354225307703018\n",
      "iteration 2696, dc_loss: 0.025167003273963928, tv_loss: 0.03354869410395622\n",
      "iteration 2697, dc_loss: 0.025164926424622536, tv_loss: 0.033549342304468155\n",
      "iteration 2698, dc_loss: 0.025165895000100136, tv_loss: 0.033546384423971176\n",
      "iteration 2699, dc_loss: 0.02515680342912674, tv_loss: 0.03354470431804657\n",
      "iteration 2700, dc_loss: 0.02516128681600094, tv_loss: 0.033533673733472824\n",
      "iteration 2701, dc_loss: 0.025152334943413734, tv_loss: 0.03354299068450928\n",
      "iteration 2702, dc_loss: 0.02514326199889183, tv_loss: 0.03354663401842117\n",
      "iteration 2703, dc_loss: 0.025156117975711823, tv_loss: 0.03353283554315567\n",
      "iteration 2704, dc_loss: 0.025146935135126114, tv_loss: 0.0335305780172348\n",
      "iteration 2705, dc_loss: 0.02512413077056408, tv_loss: 0.03355656936764717\n",
      "iteration 2706, dc_loss: 0.025153206661343575, tv_loss: 0.033523984253406525\n",
      "iteration 2707, dc_loss: 0.02513018436729908, tv_loss: 0.033546239137649536\n",
      "iteration 2708, dc_loss: 0.02515343390405178, tv_loss: 0.03351498395204544\n",
      "iteration 2709, dc_loss: 0.02512749284505844, tv_loss: 0.03354368358850479\n",
      "iteration 2710, dc_loss: 0.025142095983028412, tv_loss: 0.03353526443243027\n",
      "iteration 2711, dc_loss: 0.025128424167633057, tv_loss: 0.03357230871915817\n",
      "iteration 2712, dc_loss: 0.02516830712556839, tv_loss: 0.03355599194765091\n",
      "iteration 2713, dc_loss: 0.025118308141827583, tv_loss: 0.033569760620594025\n",
      "iteration 2714, dc_loss: 0.025171278044581413, tv_loss: 0.03350987285375595\n",
      "iteration 2715, dc_loss: 0.025118667632341385, tv_loss: 0.033595919609069824\n",
      "iteration 2716, dc_loss: 0.02513880282640457, tv_loss: 0.03355341777205467\n",
      "iteration 2717, dc_loss: 0.02510581910610199, tv_loss: 0.03356511518359184\n",
      "iteration 2718, dc_loss: 0.025133537128567696, tv_loss: 0.03357211872935295\n",
      "iteration 2719, dc_loss: 0.025109009817242622, tv_loss: 0.033576905727386475\n",
      "iteration 2720, dc_loss: 0.025121347978711128, tv_loss: 0.03353669121861458\n",
      "iteration 2721, dc_loss: 0.025107627734541893, tv_loss: 0.03357210382819176\n",
      "iteration 2722, dc_loss: 0.0250824186950922, tv_loss: 0.033575721085071564\n",
      "iteration 2723, dc_loss: 0.025078075006604195, tv_loss: 0.0335574634373188\n",
      "iteration 2724, dc_loss: 0.025078296661376953, tv_loss: 0.03356602042913437\n",
      "iteration 2725, dc_loss: 0.025070950388908386, tv_loss: 0.03356114402413368\n",
      "iteration 2726, dc_loss: 0.0250734630972147, tv_loss: 0.03355427086353302\n",
      "iteration 2727, dc_loss: 0.02507079765200615, tv_loss: 0.03355531021952629\n",
      "iteration 2728, dc_loss: 0.025066548958420753, tv_loss: 0.03356337174773216\n",
      "iteration 2729, dc_loss: 0.02506418339908123, tv_loss: 0.03354916349053383\n",
      "iteration 2730, dc_loss: 0.025051478296518326, tv_loss: 0.033547114580869675\n",
      "iteration 2731, dc_loss: 0.025063462555408478, tv_loss: 0.03353939577937126\n",
      "iteration 2732, dc_loss: 0.02503308095037937, tv_loss: 0.03356693685054779\n",
      "iteration 2733, dc_loss: 0.02505459263920784, tv_loss: 0.03353997692465782\n",
      "iteration 2734, dc_loss: 0.025050215423107147, tv_loss: 0.03353991359472275\n",
      "iteration 2735, dc_loss: 0.025027593597769737, tv_loss: 0.033553313463926315\n",
      "iteration 2736, dc_loss: 0.02503482811152935, tv_loss: 0.03353692591190338\n",
      "iteration 2737, dc_loss: 0.025034872815012932, tv_loss: 0.033536821603775024\n",
      "iteration 2738, dc_loss: 0.025028588250279427, tv_loss: 0.03353483974933624\n",
      "iteration 2739, dc_loss: 0.025017311796545982, tv_loss: 0.03354450687766075\n",
      "iteration 2740, dc_loss: 0.025036901235580444, tv_loss: 0.03351709991693497\n",
      "iteration 2741, dc_loss: 0.025011712685227394, tv_loss: 0.03353993222117424\n",
      "iteration 2742, dc_loss: 0.02501589059829712, tv_loss: 0.03353957459330559\n",
      "iteration 2743, dc_loss: 0.025007350370287895, tv_loss: 0.03355851024389267\n",
      "iteration 2744, dc_loss: 0.024998147040605545, tv_loss: 0.03356758877635002\n",
      "iteration 2745, dc_loss: 0.02501041628420353, tv_loss: 0.03353683277964592\n",
      "iteration 2746, dc_loss: 0.024997450411319733, tv_loss: 0.033541448414325714\n",
      "iteration 2747, dc_loss: 0.025008514523506165, tv_loss: 0.03353158012032509\n",
      "iteration 2748, dc_loss: 0.024987323209643364, tv_loss: 0.033542342483997345\n",
      "iteration 2749, dc_loss: 0.024991123005747795, tv_loss: 0.03353426977992058\n",
      "iteration 2750, dc_loss: 0.02498217672109604, tv_loss: 0.03353698179125786\n",
      "iteration 2751, dc_loss: 0.02498854324221611, tv_loss: 0.033526454120874405\n",
      "iteration 2752, dc_loss: 0.024976084008812904, tv_loss: 0.03354036062955856\n",
      "iteration 2753, dc_loss: 0.024979839101433754, tv_loss: 0.03353216499090195\n",
      "iteration 2754, dc_loss: 0.024971099570393562, tv_loss: 0.033546630293130875\n",
      "iteration 2755, dc_loss: 0.024975040927529335, tv_loss: 0.03355433791875839\n",
      "iteration 2756, dc_loss: 0.024962082505226135, tv_loss: 0.03356577828526497\n",
      "iteration 2757, dc_loss: 0.024968884885311127, tv_loss: 0.03353719785809517\n",
      "iteration 2758, dc_loss: 0.02495688572525978, tv_loss: 0.03355059027671814\n",
      "iteration 2759, dc_loss: 0.024968795478343964, tv_loss: 0.03352678194642067\n",
      "iteration 2760, dc_loss: 0.024955151602625847, tv_loss: 0.03354300931096077\n",
      "iteration 2761, dc_loss: 0.02496037445962429, tv_loss: 0.033532943576574326\n",
      "iteration 2762, dc_loss: 0.024947689846158028, tv_loss: 0.033542875200510025\n",
      "iteration 2763, dc_loss: 0.02496117353439331, tv_loss: 0.033526461571455\n",
      "iteration 2764, dc_loss: 0.024936307221651077, tv_loss: 0.0335579477250576\n",
      "iteration 2765, dc_loss: 0.024971771985292435, tv_loss: 0.033519402146339417\n",
      "iteration 2766, dc_loss: 0.024938397109508514, tv_loss: 0.03356151655316353\n",
      "iteration 2767, dc_loss: 0.02497265301644802, tv_loss: 0.03353393077850342\n",
      "iteration 2768, dc_loss: 0.024943824857473373, tv_loss: 0.033568836748600006\n",
      "iteration 2769, dc_loss: 0.02496807463467121, tv_loss: 0.033532045781612396\n",
      "iteration 2770, dc_loss: 0.02493198588490486, tv_loss: 0.0335669070482254\n",
      "iteration 2771, dc_loss: 0.024973241612315178, tv_loss: 0.03351327404379845\n",
      "iteration 2772, dc_loss: 0.024914689362049103, tv_loss: 0.033566661179065704\n",
      "iteration 2773, dc_loss: 0.02495354600250721, tv_loss: 0.033521078526973724\n",
      "iteration 2774, dc_loss: 0.024899885058403015, tv_loss: 0.03356550261378288\n",
      "iteration 2775, dc_loss: 0.024935346096754074, tv_loss: 0.033524394035339355\n",
      "iteration 2776, dc_loss: 0.024885376915335655, tv_loss: 0.03356429561972618\n",
      "iteration 2777, dc_loss: 0.024916257709264755, tv_loss: 0.03353409469127655\n",
      "iteration 2778, dc_loss: 0.024904249235987663, tv_loss: 0.03353644162416458\n",
      "iteration 2779, dc_loss: 0.02488991618156433, tv_loss: 0.03354034945368767\n",
      "iteration 2780, dc_loss: 0.024896837770938873, tv_loss: 0.033529993146657944\n",
      "iteration 2781, dc_loss: 0.024883519858121872, tv_loss: 0.03354470059275627\n",
      "iteration 2782, dc_loss: 0.02488935925066471, tv_loss: 0.03354373201727867\n",
      "iteration 2783, dc_loss: 0.02488010935485363, tv_loss: 0.033563800156116486\n",
      "iteration 2784, dc_loss: 0.024881748482584953, tv_loss: 0.03355251997709274\n",
      "iteration 2785, dc_loss: 0.024865420535206795, tv_loss: 0.03355368226766586\n",
      "iteration 2786, dc_loss: 0.024872373789548874, tv_loss: 0.03354579955339432\n",
      "iteration 2787, dc_loss: 0.024862593039870262, tv_loss: 0.033559802919626236\n",
      "iteration 2788, dc_loss: 0.024875063449144363, tv_loss: 0.03354194387793541\n",
      "iteration 2789, dc_loss: 0.024852417409420013, tv_loss: 0.0335586853325367\n",
      "iteration 2790, dc_loss: 0.024866372346878052, tv_loss: 0.03353433683514595\n",
      "iteration 2791, dc_loss: 0.024850869551301003, tv_loss: 0.03354354202747345\n",
      "iteration 2792, dc_loss: 0.024846354499459267, tv_loss: 0.03354484215378761\n",
      "iteration 2793, dc_loss: 0.024851711466908455, tv_loss: 0.03353043273091316\n",
      "iteration 2794, dc_loss: 0.024837225675582886, tv_loss: 0.033541131764650345\n",
      "iteration 2795, dc_loss: 0.024846184998750687, tv_loss: 0.03353176638484001\n",
      "iteration 2796, dc_loss: 0.024835366755723953, tv_loss: 0.03354025259613991\n",
      "iteration 2797, dc_loss: 0.024837905541062355, tv_loss: 0.0335511788725853\n",
      "iteration 2798, dc_loss: 0.024830663576722145, tv_loss: 0.03356045484542847\n",
      "iteration 2799, dc_loss: 0.024823619052767754, tv_loss: 0.03355928510427475\n",
      "iteration 2800, dc_loss: 0.024823466315865517, tv_loss: 0.033538151532411575\n",
      "iteration 2801, dc_loss: 0.024825667962431908, tv_loss: 0.03353995084762573\n",
      "iteration 2802, dc_loss: 0.02481626160442829, tv_loss: 0.033540546894073486\n",
      "iteration 2803, dc_loss: 0.024818871170282364, tv_loss: 0.03353036195039749\n",
      "iteration 2804, dc_loss: 0.024810828268527985, tv_loss: 0.03353413566946983\n",
      "iteration 2805, dc_loss: 0.024801116436719894, tv_loss: 0.033542610704898834\n",
      "iteration 2806, dc_loss: 0.024813709780573845, tv_loss: 0.0335254929959774\n",
      "iteration 2807, dc_loss: 0.0248066745698452, tv_loss: 0.033535297960042953\n",
      "iteration 2808, dc_loss: 0.024790050461888313, tv_loss: 0.03354129567742348\n",
      "iteration 2809, dc_loss: 0.024807052686810493, tv_loss: 0.03352292627096176\n",
      "iteration 2810, dc_loss: 0.024809157475829124, tv_loss: 0.03352123498916626\n",
      "iteration 2811, dc_loss: 0.024787483736872673, tv_loss: 0.03354240208864212\n",
      "iteration 2812, dc_loss: 0.02478909119963646, tv_loss: 0.03354041278362274\n",
      "iteration 2813, dc_loss: 0.02479381486773491, tv_loss: 0.03352483734488487\n",
      "iteration 2814, dc_loss: 0.02478010021150112, tv_loss: 0.03353237733244896\n",
      "iteration 2815, dc_loss: 0.024781381711363792, tv_loss: 0.033533692359924316\n",
      "iteration 2816, dc_loss: 0.024786800146102905, tv_loss: 0.03353465348482132\n",
      "iteration 2817, dc_loss: 0.024777773767709732, tv_loss: 0.03354327753186226\n",
      "iteration 2818, dc_loss: 0.024775618687272072, tv_loss: 0.03353515639901161\n",
      "iteration 2819, dc_loss: 0.02477259933948517, tv_loss: 0.03353080153465271\n",
      "iteration 2820, dc_loss: 0.024770183488726616, tv_loss: 0.03352821618318558\n",
      "iteration 2821, dc_loss: 0.024767450988292694, tv_loss: 0.03353343531489372\n",
      "iteration 2822, dc_loss: 0.024764394387602806, tv_loss: 0.03353890776634216\n",
      "iteration 2823, dc_loss: 0.02476799488067627, tv_loss: 0.0335308276116848\n",
      "iteration 2824, dc_loss: 0.02476116642355919, tv_loss: 0.033531010150909424\n",
      "iteration 2825, dc_loss: 0.024755509570240974, tv_loss: 0.033527810126543045\n",
      "iteration 2826, dc_loss: 0.024749087169766426, tv_loss: 0.03353209048509598\n",
      "iteration 2827, dc_loss: 0.024754781275987625, tv_loss: 0.03352472186088562\n",
      "iteration 2828, dc_loss: 0.024757923558354378, tv_loss: 0.03352532163262367\n",
      "iteration 2829, dc_loss: 0.024745378643274307, tv_loss: 0.033549025654792786\n",
      "iteration 2830, dc_loss: 0.024743743240833282, tv_loss: 0.03354152664542198\n",
      "iteration 2831, dc_loss: 0.024741314351558685, tv_loss: 0.03352950140833855\n",
      "iteration 2832, dc_loss: 0.024739759042859077, tv_loss: 0.03352850303053856\n",
      "iteration 2833, dc_loss: 0.024740180000662804, tv_loss: 0.03353731334209442\n",
      "iteration 2834, dc_loss: 0.02473258785903454, tv_loss: 0.03355211764574051\n",
      "iteration 2835, dc_loss: 0.02473517134785652, tv_loss: 0.03353065997362137\n",
      "iteration 2836, dc_loss: 0.02472832426428795, tv_loss: 0.03352965787053108\n",
      "iteration 2837, dc_loss: 0.024727897718548775, tv_loss: 0.03352721780538559\n",
      "iteration 2838, dc_loss: 0.024723559617996216, tv_loss: 0.03354177251458168\n",
      "iteration 2839, dc_loss: 0.02471904642879963, tv_loss: 0.03354378789663315\n",
      "iteration 2840, dc_loss: 0.024725303053855896, tv_loss: 0.03352523595094681\n",
      "iteration 2841, dc_loss: 0.02471441589295864, tv_loss: 0.0335291288793087\n",
      "iteration 2842, dc_loss: 0.024713382124900818, tv_loss: 0.03352630138397217\n",
      "iteration 2843, dc_loss: 0.02471579797565937, tv_loss: 0.03352293744683266\n",
      "iteration 2844, dc_loss: 0.024706002324819565, tv_loss: 0.03353465721011162\n",
      "iteration 2845, dc_loss: 0.02470744214951992, tv_loss: 0.033534806221723557\n",
      "iteration 2846, dc_loss: 0.02470630407333374, tv_loss: 0.03353581950068474\n",
      "iteration 2847, dc_loss: 0.02470715343952179, tv_loss: 0.03353327512741089\n",
      "iteration 2848, dc_loss: 0.02469714917242527, tv_loss: 0.03354065492749214\n",
      "iteration 2849, dc_loss: 0.02469496987760067, tv_loss: 0.033533696085214615\n",
      "iteration 2850, dc_loss: 0.024694137275218964, tv_loss: 0.03353036195039749\n",
      "iteration 2851, dc_loss: 0.024692310020327568, tv_loss: 0.03352881222963333\n",
      "iteration 2852, dc_loss: 0.024695396423339844, tv_loss: 0.03352268785238266\n",
      "iteration 2853, dc_loss: 0.02468135952949524, tv_loss: 0.03353705257177353\n",
      "iteration 2854, dc_loss: 0.024679530411958694, tv_loss: 0.03353980556130409\n",
      "iteration 2855, dc_loss: 0.024687839671969414, tv_loss: 0.033531926572322845\n",
      "iteration 2856, dc_loss: 0.024684345349669456, tv_loss: 0.03353225067257881\n",
      "iteration 2857, dc_loss: 0.024671204388141632, tv_loss: 0.03353689983487129\n",
      "iteration 2858, dc_loss: 0.02467193454504013, tv_loss: 0.03352751582860947\n",
      "iteration 2859, dc_loss: 0.024675264954566956, tv_loss: 0.03352734446525574\n",
      "iteration 2860, dc_loss: 0.02466774918138981, tv_loss: 0.03352737054228783\n",
      "iteration 2861, dc_loss: 0.024668822064995766, tv_loss: 0.03352561220526695\n",
      "iteration 2862, dc_loss: 0.02466725930571556, tv_loss: 0.03352164849638939\n",
      "iteration 2863, dc_loss: 0.02466592937707901, tv_loss: 0.03351685032248497\n",
      "iteration 2864, dc_loss: 0.02465798147022724, tv_loss: 0.03352373465895653\n",
      "iteration 2865, dc_loss: 0.02465326525270939, tv_loss: 0.03352981433272362\n",
      "iteration 2866, dc_loss: 0.02465542033314705, tv_loss: 0.03352215513586998\n",
      "iteration 2867, dc_loss: 0.024651361629366875, tv_loss: 0.03352269157767296\n",
      "iteration 2868, dc_loss: 0.024650555104017258, tv_loss: 0.03352228179574013\n",
      "iteration 2869, dc_loss: 0.02464793622493744, tv_loss: 0.033525701612234116\n",
      "iteration 2870, dc_loss: 0.02464212104678154, tv_loss: 0.03353925794363022\n",
      "iteration 2871, dc_loss: 0.024644406512379646, tv_loss: 0.033540092408657074\n",
      "iteration 2872, dc_loss: 0.024639122188091278, tv_loss: 0.033539462834596634\n",
      "iteration 2873, dc_loss: 0.02463695779442787, tv_loss: 0.033526476472616196\n",
      "iteration 2874, dc_loss: 0.024638714268803596, tv_loss: 0.03352485969662666\n",
      "iteration 2875, dc_loss: 0.024632278829813004, tv_loss: 0.03353391960263252\n",
      "iteration 2876, dc_loss: 0.0246241707354784, tv_loss: 0.033544693142175674\n",
      "iteration 2877, dc_loss: 0.024623336270451546, tv_loss: 0.03354063630104065\n",
      "iteration 2878, dc_loss: 0.024633368477225304, tv_loss: 0.03351948782801628\n",
      "iteration 2879, dc_loss: 0.02462727390229702, tv_loss: 0.03352503851056099\n",
      "iteration 2880, dc_loss: 0.02461080066859722, tv_loss: 0.03353599086403847\n",
      "iteration 2881, dc_loss: 0.024619081988930702, tv_loss: 0.03352765366435051\n",
      "iteration 2882, dc_loss: 0.024616343900561333, tv_loss: 0.033533282577991486\n",
      "iteration 2883, dc_loss: 0.02460995502769947, tv_loss: 0.03354093059897423\n",
      "iteration 2884, dc_loss: 0.02461085468530655, tv_loss: 0.033533498644828796\n",
      "iteration 2885, dc_loss: 0.024618228897452354, tv_loss: 0.033518433570861816\n",
      "iteration 2886, dc_loss: 0.024601612240076065, tv_loss: 0.033533692359924316\n",
      "iteration 2887, dc_loss: 0.024595534428954124, tv_loss: 0.033532071858644485\n",
      "iteration 2888, dc_loss: 0.024603070691227913, tv_loss: 0.03352132812142372\n",
      "iteration 2889, dc_loss: 0.02459908276796341, tv_loss: 0.03352706506848335\n",
      "iteration 2890, dc_loss: 0.024597251787781715, tv_loss: 0.03352862969040871\n",
      "iteration 2891, dc_loss: 0.024596985429525375, tv_loss: 0.03353648632764816\n",
      "iteration 2892, dc_loss: 0.024593347683548927, tv_loss: 0.033537279814481735\n",
      "iteration 2893, dc_loss: 0.024581488221883774, tv_loss: 0.033543575555086136\n",
      "iteration 2894, dc_loss: 0.024582678452134132, tv_loss: 0.03353244438767433\n",
      "iteration 2895, dc_loss: 0.02458730898797512, tv_loss: 0.03352458029985428\n",
      "iteration 2896, dc_loss: 0.024585450068116188, tv_loss: 0.03352274000644684\n",
      "iteration 2897, dc_loss: 0.024574104696512222, tv_loss: 0.033532995730638504\n",
      "iteration 2898, dc_loss: 0.02457783930003643, tv_loss: 0.03353071212768555\n",
      "iteration 2899, dc_loss: 0.024578969925642014, tv_loss: 0.03354017063975334\n",
      "iteration 2900, dc_loss: 0.024569453671574593, tv_loss: 0.03354094550013542\n",
      "iteration 2901, dc_loss: 0.024559732526540756, tv_loss: 0.033539045602083206\n",
      "iteration 2902, dc_loss: 0.024575654417276382, tv_loss: 0.03352015092968941\n",
      "iteration 2903, dc_loss: 0.0245716143399477, tv_loss: 0.03352878615260124\n",
      "iteration 2904, dc_loss: 0.024549642577767372, tv_loss: 0.03355371952056885\n",
      "iteration 2905, dc_loss: 0.024556534364819527, tv_loss: 0.03353570029139519\n",
      "iteration 2906, dc_loss: 0.024564053863286972, tv_loss: 0.03352617099881172\n",
      "iteration 2907, dc_loss: 0.024555224925279617, tv_loss: 0.033530343323946\n",
      "iteration 2908, dc_loss: 0.02455243654549122, tv_loss: 0.03353855758905411\n",
      "iteration 2909, dc_loss: 0.024545861408114433, tv_loss: 0.03354253992438316\n",
      "iteration 2910, dc_loss: 0.02454676665365696, tv_loss: 0.033534176647663116\n",
      "iteration 2911, dc_loss: 0.024544591084122658, tv_loss: 0.03352825343608856\n",
      "iteration 2912, dc_loss: 0.024537742137908936, tv_loss: 0.03354257717728615\n",
      "iteration 2913, dc_loss: 0.02454257197678089, tv_loss: 0.0335405133664608\n",
      "iteration 2914, dc_loss: 0.024544158950448036, tv_loss: 0.03352748975157738\n",
      "iteration 2915, dc_loss: 0.02454478293657303, tv_loss: 0.03352287784218788\n",
      "iteration 2916, dc_loss: 0.0245206318795681, tv_loss: 0.0335402749478817\n",
      "iteration 2917, dc_loss: 0.024519864469766617, tv_loss: 0.03354232758283615\n",
      "iteration 2918, dc_loss: 0.02454458549618721, tv_loss: 0.03351888060569763\n",
      "iteration 2919, dc_loss: 0.024521516636013985, tv_loss: 0.03353823721408844\n",
      "iteration 2920, dc_loss: 0.024514751508831978, tv_loss: 0.03354283422231674\n",
      "iteration 2921, dc_loss: 0.024526486173272133, tv_loss: 0.03352253511548042\n",
      "iteration 2922, dc_loss: 0.024526631459593773, tv_loss: 0.033521201461553574\n",
      "iteration 2923, dc_loss: 0.024513132870197296, tv_loss: 0.03352564200758934\n",
      "iteration 2924, dc_loss: 0.02451058104634285, tv_loss: 0.03353112190961838\n",
      "iteration 2925, dc_loss: 0.024510033428668976, tv_loss: 0.033530913293361664\n",
      "iteration 2926, dc_loss: 0.024507444351911545, tv_loss: 0.03353913873434067\n",
      "iteration 2927, dc_loss: 0.024503527209162712, tv_loss: 0.033545371145009995\n",
      "iteration 2928, dc_loss: 0.02450789138674736, tv_loss: 0.03352886065840721\n",
      "iteration 2929, dc_loss: 0.02449995093047619, tv_loss: 0.03353402763605118\n",
      "iteration 2930, dc_loss: 0.024502146989107132, tv_loss: 0.03352304548025131\n",
      "iteration 2931, dc_loss: 0.024492062628269196, tv_loss: 0.033532340079545975\n",
      "iteration 2932, dc_loss: 0.02448919788002968, tv_loss: 0.033537089824676514\n",
      "iteration 2933, dc_loss: 0.024501046165823936, tv_loss: 0.03352094069123268\n",
      "iteration 2934, dc_loss: 0.02449011243879795, tv_loss: 0.0335417166352272\n",
      "iteration 2935, dc_loss: 0.024485578760504723, tv_loss: 0.03353653848171234\n",
      "iteration 2936, dc_loss: 0.024488799273967743, tv_loss: 0.03352910280227661\n",
      "iteration 2937, dc_loss: 0.024481700733304024, tv_loss: 0.033527638763189316\n",
      "iteration 2938, dc_loss: 0.024473505094647408, tv_loss: 0.033546701073646545\n",
      "iteration 2939, dc_loss: 0.024474019184708595, tv_loss: 0.033546026796102524\n",
      "iteration 2940, dc_loss: 0.02448311448097229, tv_loss: 0.03352835401892662\n",
      "iteration 2941, dc_loss: 0.024471819400787354, tv_loss: 0.033531565219163895\n",
      "iteration 2942, dc_loss: 0.024468878284096718, tv_loss: 0.03352953493595123\n",
      "iteration 2943, dc_loss: 0.024469979107379913, tv_loss: 0.03353719785809517\n",
      "iteration 2944, dc_loss: 0.024463357403874397, tv_loss: 0.0335402712225914\n",
      "iteration 2945, dc_loss: 0.02446298487484455, tv_loss: 0.03353404998779297\n",
      "iteration 2946, dc_loss: 0.024457361549139023, tv_loss: 0.03353092446923256\n",
      "iteration 2947, dc_loss: 0.02446010150015354, tv_loss: 0.03352672606706619\n",
      "iteration 2948, dc_loss: 0.024460159242153168, tv_loss: 0.03352085500955582\n",
      "iteration 2949, dc_loss: 0.024453677237033844, tv_loss: 0.033522479236125946\n",
      "iteration 2950, dc_loss: 0.02444811724126339, tv_loss: 0.033524755388498306\n",
      "iteration 2951, dc_loss: 0.024449089542031288, tv_loss: 0.03352634981274605\n",
      "iteration 2952, dc_loss: 0.02445019967854023, tv_loss: 0.03352325037121773\n",
      "iteration 2953, dc_loss: 0.024439344182610512, tv_loss: 0.03354748338460922\n",
      "iteration 2954, dc_loss: 0.024440739303827286, tv_loss: 0.03354349359869957\n",
      "iteration 2955, dc_loss: 0.024442831054329872, tv_loss: 0.033529266715049744\n",
      "iteration 2956, dc_loss: 0.024436982348561287, tv_loss: 0.03352617099881172\n",
      "iteration 2957, dc_loss: 0.024433886632323265, tv_loss: 0.03353121504187584\n",
      "iteration 2958, dc_loss: 0.0244317464530468, tv_loss: 0.033548176288604736\n",
      "iteration 2959, dc_loss: 0.024434372782707214, tv_loss: 0.03354218229651451\n",
      "iteration 2960, dc_loss: 0.02442321740090847, tv_loss: 0.03353424742817879\n",
      "iteration 2961, dc_loss: 0.02441972680389881, tv_loss: 0.03353071212768555\n",
      "iteration 2962, dc_loss: 0.024426210671663284, tv_loss: 0.03353060781955719\n",
      "iteration 2963, dc_loss: 0.024421248584985733, tv_loss: 0.033544667065143585\n",
      "iteration 2964, dc_loss: 0.024415193125605583, tv_loss: 0.03354281559586525\n",
      "iteration 2965, dc_loss: 0.024413639679551125, tv_loss: 0.033535003662109375\n",
      "iteration 2966, dc_loss: 0.02442193776369095, tv_loss: 0.03352425619959831\n",
      "iteration 2967, dc_loss: 0.02441377006471157, tv_loss: 0.03353840485215187\n",
      "iteration 2968, dc_loss: 0.024394892156124115, tv_loss: 0.03355506807565689\n",
      "iteration 2969, dc_loss: 0.024407222867012024, tv_loss: 0.03353142738342285\n",
      "iteration 2970, dc_loss: 0.024408426135778427, tv_loss: 0.03352675214409828\n",
      "iteration 2971, dc_loss: 0.024398453533649445, tv_loss: 0.033538818359375\n",
      "iteration 2972, dc_loss: 0.02440701425075531, tv_loss: 0.03352594003081322\n",
      "iteration 2973, dc_loss: 0.024397842586040497, tv_loss: 0.033532146364450455\n",
      "iteration 2974, dc_loss: 0.02439018338918686, tv_loss: 0.03353225812315941\n",
      "iteration 2975, dc_loss: 0.024388255551457405, tv_loss: 0.03352746739983559\n",
      "iteration 2976, dc_loss: 0.024389322847127914, tv_loss: 0.03353123739361763\n",
      "iteration 2977, dc_loss: 0.024391593411564827, tv_loss: 0.033531296998262405\n",
      "iteration 2978, dc_loss: 0.024386441335082054, tv_loss: 0.03353599086403847\n",
      "iteration 2979, dc_loss: 0.0243915356695652, tv_loss: 0.03352459892630577\n",
      "iteration 2980, dc_loss: 0.024377066642045975, tv_loss: 0.033532802015542984\n",
      "iteration 2981, dc_loss: 0.024373339489102364, tv_loss: 0.033535201102495193\n",
      "iteration 2982, dc_loss: 0.024377377703785896, tv_loss: 0.033527299761772156\n",
      "iteration 2983, dc_loss: 0.024367809295654297, tv_loss: 0.033530786633491516\n",
      "iteration 2984, dc_loss: 0.024371588602662086, tv_loss: 0.033525001257658005\n",
      "iteration 2985, dc_loss: 0.024378444999456406, tv_loss: 0.03351269289851189\n",
      "iteration 2986, dc_loss: 0.024369576945900917, tv_loss: 0.03353133425116539\n",
      "iteration 2987, dc_loss: 0.024358827620744705, tv_loss: 0.03354620933532715\n",
      "iteration 2988, dc_loss: 0.024359706789255142, tv_loss: 0.03353923186659813\n",
      "iteration 2989, dc_loss: 0.024357862770557404, tv_loss: 0.03353440389037132\n",
      "iteration 2990, dc_loss: 0.02436029724776745, tv_loss: 0.03352152183651924\n",
      "iteration 2991, dc_loss: 0.024359792470932007, tv_loss: 0.03352576494216919\n",
      "iteration 2992, dc_loss: 0.024350149556994438, tv_loss: 0.03353347256779671\n",
      "iteration 2993, dc_loss: 0.024347709491848946, tv_loss: 0.0335361622273922\n",
      "iteration 2994, dc_loss: 0.024349868297576904, tv_loss: 0.03353380784392357\n",
      "iteration 2995, dc_loss: 0.024341048672795296, tv_loss: 0.03353549912571907\n",
      "iteration 2996, dc_loss: 0.024348793551325798, tv_loss: 0.03352035582065582\n",
      "iteration 2997, dc_loss: 0.024344662204384804, tv_loss: 0.03352493792772293\n",
      "iteration 2998, dc_loss: 0.024329792708158493, tv_loss: 0.03353935480117798\n",
      "iteration 2999, dc_loss: 0.024340126663446426, tv_loss: 0.033529072999954224\n",
      "iteration 3000, dc_loss: 0.024342482909560204, tv_loss: 0.033527810126543045\n",
      "iteration 3001, dc_loss: 0.02432708628475666, tv_loss: 0.033547256141901016\n",
      "iteration 3002, dc_loss: 0.024317415431141853, tv_loss: 0.03354630991816521\n",
      "iteration 3003, dc_loss: 0.024335471913218498, tv_loss: 0.03352005034685135\n",
      "iteration 3004, dc_loss: 0.024326469749212265, tv_loss: 0.033527445048093796\n",
      "iteration 3005, dc_loss: 0.0243221502751112, tv_loss: 0.03352782502770424\n",
      "iteration 3006, dc_loss: 0.02432551234960556, tv_loss: 0.033537011593580246\n",
      "iteration 3007, dc_loss: 0.02431659772992134, tv_loss: 0.03354398533701897\n",
      "iteration 3008, dc_loss: 0.024313362315297127, tv_loss: 0.033534761518239975\n",
      "iteration 3009, dc_loss: 0.02431437000632286, tv_loss: 0.0335269458591938\n",
      "iteration 3010, dc_loss: 0.024308912456035614, tv_loss: 0.03352655470371246\n",
      "iteration 3011, dc_loss: 0.024309249594807625, tv_loss: 0.03352334350347519\n",
      "iteration 3012, dc_loss: 0.02430785447359085, tv_loss: 0.03352726250886917\n",
      "iteration 3013, dc_loss: 0.024304861202836037, tv_loss: 0.03353326395153999\n",
      "iteration 3014, dc_loss: 0.024302583187818527, tv_loss: 0.03353974595665932\n",
      "iteration 3015, dc_loss: 0.02430451661348343, tv_loss: 0.033525947481393814\n",
      "iteration 3016, dc_loss: 0.024288151413202286, tv_loss: 0.03353983536362648\n",
      "iteration 3017, dc_loss: 0.024292588233947754, tv_loss: 0.03353463113307953\n",
      "iteration 3018, dc_loss: 0.024297267198562622, tv_loss: 0.03352325037121773\n",
      "iteration 3019, dc_loss: 0.024295825511217117, tv_loss: 0.0335373692214489\n",
      "iteration 3020, dc_loss: 0.02429274283349514, tv_loss: 0.03353022411465645\n",
      "iteration 3021, dc_loss: 0.024283649399876595, tv_loss: 0.033533237874507904\n",
      "iteration 3022, dc_loss: 0.02428312599658966, tv_loss: 0.03352789208292961\n",
      "iteration 3023, dc_loss: 0.02428310364484787, tv_loss: 0.03352732211351395\n",
      "iteration 3024, dc_loss: 0.02427082695066929, tv_loss: 0.0335402712225914\n",
      "iteration 3025, dc_loss: 0.024282658472657204, tv_loss: 0.03352558612823486\n",
      "iteration 3026, dc_loss: 0.0242849662899971, tv_loss: 0.03352741524577141\n",
      "iteration 3027, dc_loss: 0.024274900555610657, tv_loss: 0.03352952003479004\n",
      "iteration 3028, dc_loss: 0.024264715611934662, tv_loss: 0.03354313597083092\n",
      "iteration 3029, dc_loss: 0.024282336235046387, tv_loss: 0.03352026268839836\n",
      "iteration 3030, dc_loss: 0.02426377311348915, tv_loss: 0.033530548214912415\n",
      "iteration 3031, dc_loss: 0.024267148226499557, tv_loss: 0.03353029489517212\n",
      "iteration 3032, dc_loss: 0.024271514266729355, tv_loss: 0.033534616231918335\n",
      "iteration 3033, dc_loss: 0.02426636591553688, tv_loss: 0.033537011593580246\n",
      "iteration 3034, dc_loss: 0.024259934201836586, tv_loss: 0.03354330360889435\n",
      "iteration 3035, dc_loss: 0.0242624469101429, tv_loss: 0.033527571707963943\n",
      "iteration 3036, dc_loss: 0.024253083392977715, tv_loss: 0.033533595502376556\n",
      "iteration 3037, dc_loss: 0.0242511797696352, tv_loss: 0.03353426605463028\n",
      "iteration 3038, dc_loss: 0.024252505972981453, tv_loss: 0.033526282757520676\n",
      "iteration 3039, dc_loss: 0.024245111271739006, tv_loss: 0.03352785483002663\n",
      "iteration 3040, dc_loss: 0.024235086515545845, tv_loss: 0.03353403881192207\n",
      "iteration 3041, dc_loss: 0.024247806519269943, tv_loss: 0.0335184745490551\n",
      "iteration 3042, dc_loss: 0.024239853024482727, tv_loss: 0.033521924167871475\n",
      "iteration 3043, dc_loss: 0.024228522554039955, tv_loss: 0.03353407606482506\n",
      "iteration 3044, dc_loss: 0.024240510538220406, tv_loss: 0.033534642308950424\n",
      "iteration 3045, dc_loss: 0.024240953847765923, tv_loss: 0.033539336174726486\n",
      "iteration 3046, dc_loss: 0.02422039769589901, tv_loss: 0.03354834020137787\n",
      "iteration 3047, dc_loss: 0.024231769144535065, tv_loss: 0.03352395072579384\n",
      "iteration 3048, dc_loss: 0.02423039637506008, tv_loss: 0.03352528437972069\n",
      "iteration 3049, dc_loss: 0.02421545423567295, tv_loss: 0.033549170941114426\n",
      "iteration 3050, dc_loss: 0.024220841005444527, tv_loss: 0.033539339900016785\n",
      "iteration 3051, dc_loss: 0.02421353943645954, tv_loss: 0.03353486582636833\n",
      "iteration 3052, dc_loss: 0.02421349100768566, tv_loss: 0.03353257104754448\n",
      "iteration 3053, dc_loss: 0.024218160659074783, tv_loss: 0.03352400287985802\n",
      "iteration 3054, dc_loss: 0.02421039342880249, tv_loss: 0.033531103283166885\n",
      "iteration 3055, dc_loss: 0.0242097619920969, tv_loss: 0.03353489935398102\n",
      "iteration 3056, dc_loss: 0.024205001071095467, tv_loss: 0.03353885933756828\n",
      "iteration 3057, dc_loss: 0.024205900728702545, tv_loss: 0.03353209048509598\n",
      "iteration 3058, dc_loss: 0.024199681356549263, tv_loss: 0.03352728113532066\n",
      "iteration 3059, dc_loss: 0.024202857166528702, tv_loss: 0.03352344036102295\n",
      "iteration 3060, dc_loss: 0.02419470250606537, tv_loss: 0.033532921224832535\n",
      "iteration 3061, dc_loss: 0.0241925660520792, tv_loss: 0.033537235110998154\n",
      "iteration 3062, dc_loss: 0.024197056889533997, tv_loss: 0.033541321754455566\n",
      "iteration 3063, dc_loss: 0.02419089525938034, tv_loss: 0.03353668004274368\n",
      "iteration 3064, dc_loss: 0.024187356233596802, tv_loss: 0.033529117703437805\n",
      "iteration 3065, dc_loss: 0.024187130853533745, tv_loss: 0.03352876752614975\n",
      "iteration 3066, dc_loss: 0.024181002750992775, tv_loss: 0.03353894501924515\n",
      "iteration 3067, dc_loss: 0.02418125607073307, tv_loss: 0.033539559692144394\n",
      "iteration 3068, dc_loss: 0.024178745225071907, tv_loss: 0.03353181108832359\n",
      "iteration 3069, dc_loss: 0.024181243032217026, tv_loss: 0.033525146543979645\n",
      "iteration 3070, dc_loss: 0.024176646023988724, tv_loss: 0.033523257821798325\n",
      "iteration 3071, dc_loss: 0.024176355451345444, tv_loss: 0.033521462231874466\n",
      "iteration 3072, dc_loss: 0.02416977658867836, tv_loss: 0.03353770077228546\n",
      "iteration 3073, dc_loss: 0.024160360917448997, tv_loss: 0.03355349600315094\n",
      "iteration 3074, dc_loss: 0.024176638573408127, tv_loss: 0.03352553769946098\n",
      "iteration 3075, dc_loss: 0.02416599728167057, tv_loss: 0.0335262231528759\n",
      "iteration 3076, dc_loss: 0.024161234498023987, tv_loss: 0.03353198990225792\n",
      "iteration 3077, dc_loss: 0.024160781875252724, tv_loss: 0.03353419527411461\n",
      "iteration 3078, dc_loss: 0.024158990010619164, tv_loss: 0.033535514026880264\n",
      "iteration 3079, dc_loss: 0.024162251502275467, tv_loss: 0.03353337198495865\n",
      "iteration 3080, dc_loss: 0.024162476882338524, tv_loss: 0.033529169857501984\n",
      "iteration 3081, dc_loss: 0.024155093356966972, tv_loss: 0.033533137291669846\n",
      "iteration 3082, dc_loss: 0.02415984496474266, tv_loss: 0.0335286483168602\n",
      "iteration 3083, dc_loss: 0.02415887638926506, tv_loss: 0.03352931886911392\n",
      "iteration 3084, dc_loss: 0.02416849695146084, tv_loss: 0.03351776674389839\n",
      "iteration 3085, dc_loss: 0.024151677265763283, tv_loss: 0.033534660935401917\n",
      "iteration 3086, dc_loss: 0.024164263159036636, tv_loss: 0.03352228179574013\n",
      "iteration 3087, dc_loss: 0.024135321378707886, tv_loss: 0.03354917839169502\n",
      "iteration 3088, dc_loss: 0.024145178496837616, tv_loss: 0.0335320308804512\n",
      "iteration 3089, dc_loss: 0.024137718603014946, tv_loss: 0.03352666646242142\n",
      "iteration 3090, dc_loss: 0.024130256846547127, tv_loss: 0.033528681844472885\n",
      "iteration 3091, dc_loss: 0.024125130847096443, tv_loss: 0.03353165462613106\n",
      "iteration 3092, dc_loss: 0.02413380891084671, tv_loss: 0.03352522477507591\n",
      "iteration 3093, dc_loss: 0.024136824533343315, tv_loss: 0.03352367877960205\n",
      "iteration 3094, dc_loss: 0.024113668128848076, tv_loss: 0.03354312479496002\n",
      "iteration 3095, dc_loss: 0.02412794530391693, tv_loss: 0.03353011980652809\n",
      "iteration 3096, dc_loss: 0.02411738783121109, tv_loss: 0.03353438153862953\n",
      "iteration 3097, dc_loss: 0.024121444672346115, tv_loss: 0.033528562635183334\n",
      "iteration 3098, dc_loss: 0.024112362414598465, tv_loss: 0.03353235498070717\n",
      "iteration 3099, dc_loss: 0.02411505952477455, tv_loss: 0.03352690115571022\n",
      "iteration 3100, dc_loss: 0.0241114292293787, tv_loss: 0.033526819199323654\n",
      "iteration 3101, dc_loss: 0.02410372905433178, tv_loss: 0.03353274613618851\n",
      "iteration 3102, dc_loss: 0.024109244346618652, tv_loss: 0.03352893888950348\n",
      "iteration 3103, dc_loss: 0.024099700152873993, tv_loss: 0.03354451432824135\n",
      "iteration 3104, dc_loss: 0.02410716377198696, tv_loss: 0.03353071212768555\n",
      "iteration 3105, dc_loss: 0.024092573672533035, tv_loss: 0.03353769704699516\n",
      "iteration 3106, dc_loss: 0.024095816537737846, tv_loss: 0.0335269495844841\n",
      "iteration 3107, dc_loss: 0.024098997935652733, tv_loss: 0.03352078050374985\n",
      "iteration 3108, dc_loss: 0.024092091247439384, tv_loss: 0.033527158200740814\n",
      "iteration 3109, dc_loss: 0.024090910330414772, tv_loss: 0.03353143483400345\n",
      "iteration 3110, dc_loss: 0.02408159151673317, tv_loss: 0.03354180231690407\n",
      "iteration 3111, dc_loss: 0.024087395519018173, tv_loss: 0.03353521600365639\n",
      "iteration 3112, dc_loss: 0.024082079529762268, tv_loss: 0.03353281319141388\n",
      "iteration 3113, dc_loss: 0.02408011071383953, tv_loss: 0.03352561220526695\n",
      "iteration 3114, dc_loss: 0.02407710812985897, tv_loss: 0.03352827951312065\n",
      "iteration 3115, dc_loss: 0.024082260206341743, tv_loss: 0.03351936116814613\n",
      "iteration 3116, dc_loss: 0.02407403104007244, tv_loss: 0.03353269025683403\n",
      "iteration 3117, dc_loss: 0.024066396057605743, tv_loss: 0.033540572971105576\n",
      "iteration 3118, dc_loss: 0.024070316925644875, tv_loss: 0.03353589400649071\n",
      "iteration 3119, dc_loss: 0.024064749479293823, tv_loss: 0.03353193774819374\n",
      "iteration 3120, dc_loss: 0.024072999134659767, tv_loss: 0.03352188691496849\n",
      "iteration 3121, dc_loss: 0.024057231843471527, tv_loss: 0.033532433211803436\n",
      "iteration 3122, dc_loss: 0.024070249870419502, tv_loss: 0.03352026268839836\n",
      "iteration 3123, dc_loss: 0.02405550703406334, tv_loss: 0.033537592738866806\n",
      "iteration 3124, dc_loss: 0.024055160582065582, tv_loss: 0.03353981301188469\n",
      "iteration 3125, dc_loss: 0.024050232023000717, tv_loss: 0.03353724256157875\n",
      "iteration 3126, dc_loss: 0.024062572047114372, tv_loss: 0.0335213765501976\n",
      "iteration 3127, dc_loss: 0.024041548371315002, tv_loss: 0.03354246914386749\n",
      "iteration 3128, dc_loss: 0.024054504930973053, tv_loss: 0.03351873159408569\n",
      "iteration 3129, dc_loss: 0.024049172177910805, tv_loss: 0.03352916240692139\n",
      "iteration 3130, dc_loss: 0.024048589169979095, tv_loss: 0.03352758288383484\n",
      "iteration 3131, dc_loss: 0.02404073439538479, tv_loss: 0.033529132604599\n",
      "iteration 3132, dc_loss: 0.024041369557380676, tv_loss: 0.033523108810186386\n",
      "iteration 3133, dc_loss: 0.024031223729252815, tv_loss: 0.03353094682097435\n",
      "iteration 3134, dc_loss: 0.024049248546361923, tv_loss: 0.03351091220974922\n",
      "iteration 3135, dc_loss: 0.02403271198272705, tv_loss: 0.033520329743623734\n",
      "iteration 3136, dc_loss: 0.02403753623366356, tv_loss: 0.033516813069581985\n",
      "iteration 3137, dc_loss: 0.02402748353779316, tv_loss: 0.033522605895996094\n",
      "iteration 3138, dc_loss: 0.024035530164837837, tv_loss: 0.03351261466741562\n",
      "iteration 3139, dc_loss: 0.02401990257203579, tv_loss: 0.033537983894348145\n",
      "iteration 3140, dc_loss: 0.024035213515162468, tv_loss: 0.03354077786207199\n",
      "iteration 3141, dc_loss: 0.024012157693505287, tv_loss: 0.03355839475989342\n",
      "iteration 3142, dc_loss: 0.02401926927268505, tv_loss: 0.033524274826049805\n",
      "iteration 3143, dc_loss: 0.02400873973965645, tv_loss: 0.03353067860007286\n",
      "iteration 3144, dc_loss: 0.02401779219508171, tv_loss: 0.03354131057858467\n",
      "iteration 3145, dc_loss: 0.024012606590986252, tv_loss: 0.03354905545711517\n",
      "iteration 3146, dc_loss: 0.024008341133594513, tv_loss: 0.033530913293361664\n",
      "iteration 3147, dc_loss: 0.024009807035326958, tv_loss: 0.03353162109851837\n",
      "iteration 3148, dc_loss: 0.024001456797122955, tv_loss: 0.033555082976818085\n",
      "iteration 3149, dc_loss: 0.023999743163585663, tv_loss: 0.033548466861248016\n",
      "iteration 3150, dc_loss: 0.024012796580791473, tv_loss: 0.033520448952913284\n",
      "iteration 3151, dc_loss: 0.0240015871822834, tv_loss: 0.033533964306116104\n",
      "iteration 3152, dc_loss: 0.02399599179625511, tv_loss: 0.033558689057826996\n",
      "iteration 3153, dc_loss: 0.024009553715586662, tv_loss: 0.033536944538354874\n",
      "iteration 3154, dc_loss: 0.024009039625525475, tv_loss: 0.033528443425893784\n",
      "iteration 3155, dc_loss: 0.02399907074868679, tv_loss: 0.03355322778224945\n",
      "iteration 3156, dc_loss: 0.02403264492750168, tv_loss: 0.033536966890096664\n",
      "iteration 3157, dc_loss: 0.024012157693505287, tv_loss: 0.033547308295965195\n",
      "iteration 3158, dc_loss: 0.024025406688451767, tv_loss: 0.0335271991789341\n",
      "iteration 3159, dc_loss: 0.02400815859436989, tv_loss: 0.033547256141901016\n",
      "iteration 3160, dc_loss: 0.02401365339756012, tv_loss: 0.03352821245789528\n",
      "iteration 3161, dc_loss: 0.02397322468459606, tv_loss: 0.033549755811691284\n",
      "iteration 3162, dc_loss: 0.023989349603652954, tv_loss: 0.03351961821317673\n",
      "iteration 3163, dc_loss: 0.02397102117538452, tv_loss: 0.0335322767496109\n",
      "iteration 3164, dc_loss: 0.02397862821817398, tv_loss: 0.033531710505485535\n",
      "iteration 3165, dc_loss: 0.023982038721442223, tv_loss: 0.03352788835763931\n",
      "iteration 3166, dc_loss: 0.02397461235523224, tv_loss: 0.03353239595890045\n",
      "iteration 3167, dc_loss: 0.023988384753465652, tv_loss: 0.03351166471838951\n",
      "iteration 3168, dc_loss: 0.023953909054398537, tv_loss: 0.033537764102220535\n",
      "iteration 3169, dc_loss: 0.023965606465935707, tv_loss: 0.033518098294734955\n",
      "iteration 3170, dc_loss: 0.023959774523973465, tv_loss: 0.03352077677845955\n",
      "iteration 3171, dc_loss: 0.023956527933478355, tv_loss: 0.03352898359298706\n",
      "iteration 3172, dc_loss: 0.02396395616233349, tv_loss: 0.033537693321704865\n",
      "iteration 3173, dc_loss: 0.023940613493323326, tv_loss: 0.03355536237359047\n",
      "iteration 3174, dc_loss: 0.023960920050740242, tv_loss: 0.03352439031004906\n",
      "iteration 3175, dc_loss: 0.02395327389240265, tv_loss: 0.033522095531225204\n",
      "iteration 3176, dc_loss: 0.023944934830069542, tv_loss: 0.03352817893028259\n",
      "iteration 3177, dc_loss: 0.023944107815623283, tv_loss: 0.03352484479546547\n",
      "iteration 3178, dc_loss: 0.023945240303874016, tv_loss: 0.03352859988808632\n",
      "iteration 3179, dc_loss: 0.023946713656187057, tv_loss: 0.033529169857501984\n",
      "iteration 3180, dc_loss: 0.02392110601067543, tv_loss: 0.03354489430785179\n",
      "iteration 3181, dc_loss: 0.023941755294799805, tv_loss: 0.0335254967212677\n",
      "iteration 3182, dc_loss: 0.023937324061989784, tv_loss: 0.033523816615343094\n",
      "iteration 3183, dc_loss: 0.02392786182463169, tv_loss: 0.033532992005348206\n",
      "iteration 3184, dc_loss: 0.023932715877890587, tv_loss: 0.033528830856084824\n",
      "iteration 3185, dc_loss: 0.02393031306564808, tv_loss: 0.03353573754429817\n",
      "iteration 3186, dc_loss: 0.023923393338918686, tv_loss: 0.033539291471242905\n",
      "iteration 3187, dc_loss: 0.02391832508146763, tv_loss: 0.03353152796626091\n",
      "iteration 3188, dc_loss: 0.02393641695380211, tv_loss: 0.03351200371980667\n",
      "iteration 3189, dc_loss: 0.023902179673314095, tv_loss: 0.033546674996614456\n",
      "iteration 3190, dc_loss: 0.023922165855765343, tv_loss: 0.03351704776287079\n",
      "iteration 3191, dc_loss: 0.023917969316244125, tv_loss: 0.03352685272693634\n",
      "iteration 3192, dc_loss: 0.02391021139919758, tv_loss: 0.03353836014866829\n",
      "iteration 3193, dc_loss: 0.02390342950820923, tv_loss: 0.033541373908519745\n",
      "iteration 3194, dc_loss: 0.02391628921031952, tv_loss: 0.03352414816617966\n",
      "iteration 3195, dc_loss: 0.02390669286251068, tv_loss: 0.033528801053762436\n",
      "iteration 3196, dc_loss: 0.023905465379357338, tv_loss: 0.03352213650941849\n",
      "iteration 3197, dc_loss: 0.023906780406832695, tv_loss: 0.03352423384785652\n",
      "iteration 3198, dc_loss: 0.023893844336271286, tv_loss: 0.03352813050150871\n",
      "iteration 3199, dc_loss: 0.023895058780908585, tv_loss: 0.03353250026702881\n",
      "iteration 3200, dc_loss: 0.0239058006554842, tv_loss: 0.03351860120892525\n",
      "iteration 3201, dc_loss: 0.023882608860731125, tv_loss: 0.033542387187480927\n",
      "iteration 3202, dc_loss: 0.02389375865459442, tv_loss: 0.033520158380270004\n",
      "iteration 3203, dc_loss: 0.023899327963590622, tv_loss: 0.03350820392370224\n",
      "iteration 3204, dc_loss: 0.023886242881417274, tv_loss: 0.033520475029945374\n",
      "iteration 3205, dc_loss: 0.023879481479525566, tv_loss: 0.03352833911776543\n",
      "iteration 3206, dc_loss: 0.023881657049059868, tv_loss: 0.03351728990674019\n",
      "iteration 3207, dc_loss: 0.023884935304522514, tv_loss: 0.033515650779008865\n",
      "iteration 3208, dc_loss: 0.02388393133878708, tv_loss: 0.033512189984321594\n",
      "iteration 3209, dc_loss: 0.023874936625361443, tv_loss: 0.03352177143096924\n",
      "iteration 3210, dc_loss: 0.023877888917922974, tv_loss: 0.033515799790620804\n",
      "iteration 3211, dc_loss: 0.023878997191786766, tv_loss: 0.03351646661758423\n",
      "iteration 3212, dc_loss: 0.0238688625395298, tv_loss: 0.03352661058306694\n",
      "iteration 3213, dc_loss: 0.023870479315519333, tv_loss: 0.033526599407196045\n",
      "iteration 3214, dc_loss: 0.023878533393144608, tv_loss: 0.03351707011461258\n",
      "iteration 3215, dc_loss: 0.023869113996624947, tv_loss: 0.033518992364406586\n",
      "iteration 3216, dc_loss: 0.02386346086859703, tv_loss: 0.03352172672748566\n",
      "iteration 3217, dc_loss: 0.023860344663262367, tv_loss: 0.03352260962128639\n",
      "iteration 3218, dc_loss: 0.023865023627877235, tv_loss: 0.033517587929964066\n",
      "iteration 3219, dc_loss: 0.023866649717092514, tv_loss: 0.03351818397641182\n",
      "iteration 3220, dc_loss: 0.023857062682509422, tv_loss: 0.03352685272693634\n",
      "iteration 3221, dc_loss: 0.023856081068515778, tv_loss: 0.03352705016732216\n",
      "iteration 3222, dc_loss: 0.023857783526182175, tv_loss: 0.03351599723100662\n",
      "iteration 3223, dc_loss: 0.02385466918349266, tv_loss: 0.03351585566997528\n",
      "iteration 3224, dc_loss: 0.023854004219174385, tv_loss: 0.033514320850372314\n",
      "iteration 3225, dc_loss: 0.023853229358792305, tv_loss: 0.03351159021258354\n",
      "iteration 3226, dc_loss: 0.023850228637456894, tv_loss: 0.03351490572094917\n",
      "iteration 3227, dc_loss: 0.0238481592386961, tv_loss: 0.033526066690683365\n",
      "iteration 3228, dc_loss: 0.023845475167036057, tv_loss: 0.033531807363033295\n",
      "iteration 3229, dc_loss: 0.02384110540151596, tv_loss: 0.033528175204992294\n",
      "iteration 3230, dc_loss: 0.023848840966820717, tv_loss: 0.03350911661982536\n",
      "iteration 3231, dc_loss: 0.023842688649892807, tv_loss: 0.03351305052638054\n",
      "iteration 3232, dc_loss: 0.023835569620132446, tv_loss: 0.03352130949497223\n",
      "iteration 3233, dc_loss: 0.023837221786379814, tv_loss: 0.03352073207497597\n",
      "iteration 3234, dc_loss: 0.023839371278882027, tv_loss: 0.033522892743349075\n",
      "iteration 3235, dc_loss: 0.023838765919208527, tv_loss: 0.03352677449584007\n",
      "iteration 3236, dc_loss: 0.023827549070119858, tv_loss: 0.033528588712215424\n",
      "iteration 3237, dc_loss: 0.02383195050060749, tv_loss: 0.03351621702313423\n",
      "iteration 3238, dc_loss: 0.02383829839527607, tv_loss: 0.03350953757762909\n",
      "iteration 3239, dc_loss: 0.023821713402867317, tv_loss: 0.033525947481393814\n",
      "iteration 3240, dc_loss: 0.023823207244277, tv_loss: 0.0335187204182148\n",
      "iteration 3241, dc_loss: 0.02382936142385006, tv_loss: 0.033510543406009674\n",
      "iteration 3242, dc_loss: 0.023824533447623253, tv_loss: 0.033511918038129807\n",
      "iteration 3243, dc_loss: 0.023817012086510658, tv_loss: 0.033524323254823685\n",
      "iteration 3244, dc_loss: 0.023817921057343483, tv_loss: 0.033529132604599\n",
      "iteration 3245, dc_loss: 0.02382510155439377, tv_loss: 0.0335247740149498\n",
      "iteration 3246, dc_loss: 0.023819968104362488, tv_loss: 0.033515896648168564\n",
      "iteration 3247, dc_loss: 0.023812759667634964, tv_loss: 0.033519845455884933\n",
      "iteration 3248, dc_loss: 0.023809541016817093, tv_loss: 0.03352893888950348\n",
      "iteration 3249, dc_loss: 0.02380860224366188, tv_loss: 0.03352685272693634\n",
      "iteration 3250, dc_loss: 0.023816555738449097, tv_loss: 0.03351645544171333\n",
      "iteration 3251, dc_loss: 0.02380559779703617, tv_loss: 0.03352384641766548\n",
      "iteration 3252, dc_loss: 0.02380126342177391, tv_loss: 0.03354005143046379\n",
      "iteration 3253, dc_loss: 0.023811502382159233, tv_loss: 0.03352099657058716\n",
      "iteration 3254, dc_loss: 0.023803355172276497, tv_loss: 0.033517707139253616\n",
      "iteration 3255, dc_loss: 0.02380061335861683, tv_loss: 0.03352900221943855\n",
      "iteration 3256, dc_loss: 0.023802949115633965, tv_loss: 0.03353079408407211\n",
      "iteration 3257, dc_loss: 0.023793445900082588, tv_loss: 0.033527810126543045\n",
      "iteration 3258, dc_loss: 0.02379857562482357, tv_loss: 0.033516623079776764\n",
      "iteration 3259, dc_loss: 0.023802723735570908, tv_loss: 0.033524662256240845\n",
      "iteration 3260, dc_loss: 0.023786721751093864, tv_loss: 0.033536672592163086\n",
      "iteration 3261, dc_loss: 0.023789387196302414, tv_loss: 0.03352811560034752\n",
      "iteration 3262, dc_loss: 0.023797601461410522, tv_loss: 0.03351516276597977\n",
      "iteration 3263, dc_loss: 0.023783626034855843, tv_loss: 0.03353642672300339\n",
      "iteration 3264, dc_loss: 0.023782921954989433, tv_loss: 0.03353404626250267\n",
      "iteration 3265, dc_loss: 0.023790795356035233, tv_loss: 0.033513061702251434\n",
      "iteration 3266, dc_loss: 0.023789217695593834, tv_loss: 0.03351428732275963\n",
      "iteration 3267, dc_loss: 0.023780900985002518, tv_loss: 0.03352924436330795\n",
      "iteration 3268, dc_loss: 0.023773837834596634, tv_loss: 0.03353000432252884\n",
      "iteration 3269, dc_loss: 0.023785589262843132, tv_loss: 0.03351626917719841\n",
      "iteration 3270, dc_loss: 0.023778894916176796, tv_loss: 0.033518023788928986\n",
      "iteration 3271, dc_loss: 0.023771772161126137, tv_loss: 0.03352431207895279\n",
      "iteration 3272, dc_loss: 0.023774191737174988, tv_loss: 0.03352753445506096\n",
      "iteration 3273, dc_loss: 0.023773150518536568, tv_loss: 0.033520203083753586\n",
      "iteration 3274, dc_loss: 0.023775704205036163, tv_loss: 0.03351204842329025\n",
      "iteration 3275, dc_loss: 0.023770367726683617, tv_loss: 0.03351660817861557\n",
      "iteration 3276, dc_loss: 0.02375948801636696, tv_loss: 0.03352564573287964\n",
      "iteration 3277, dc_loss: 0.02377087064087391, tv_loss: 0.03351425752043724\n",
      "iteration 3278, dc_loss: 0.023768559098243713, tv_loss: 0.03351587802171707\n",
      "iteration 3279, dc_loss: 0.023755550384521484, tv_loss: 0.03352896124124527\n",
      "iteration 3280, dc_loss: 0.023759419098496437, tv_loss: 0.03352274000644684\n",
      "iteration 3281, dc_loss: 0.02376309223473072, tv_loss: 0.033515360206365585\n",
      "iteration 3282, dc_loss: 0.023758379742503166, tv_loss: 0.0335153266787529\n",
      "iteration 3283, dc_loss: 0.023756759241223335, tv_loss: 0.03351561352610588\n",
      "iteration 3284, dc_loss: 0.023756511509418488, tv_loss: 0.03351422771811485\n",
      "iteration 3285, dc_loss: 0.023752344772219658, tv_loss: 0.03352334350347519\n",
      "iteration 3286, dc_loss: 0.023751281201839447, tv_loss: 0.03352691978216171\n",
      "iteration 3287, dc_loss: 0.02374902181327343, tv_loss: 0.03352392837405205\n",
      "iteration 3288, dc_loss: 0.02375037409365177, tv_loss: 0.033515334129333496\n",
      "iteration 3289, dc_loss: 0.023742208257317543, tv_loss: 0.033516574651002884\n",
      "iteration 3290, dc_loss: 0.0237417109310627, tv_loss: 0.033525191247463226\n",
      "iteration 3291, dc_loss: 0.023749208077788353, tv_loss: 0.03352276235818863\n",
      "iteration 3292, dc_loss: 0.02373850904405117, tv_loss: 0.033536527305841446\n",
      "iteration 3293, dc_loss: 0.023739362135529518, tv_loss: 0.03351963311433792\n",
      "iteration 3294, dc_loss: 0.023742370307445526, tv_loss: 0.033513396978378296\n",
      "iteration 3295, dc_loss: 0.023737194016575813, tv_loss: 0.03352593258023262\n",
      "iteration 3296, dc_loss: 0.023732395842671394, tv_loss: 0.03353078290820122\n",
      "iteration 3297, dc_loss: 0.023730242624878883, tv_loss: 0.03352884203195572\n",
      "iteration 3298, dc_loss: 0.023736394941806793, tv_loss: 0.03351440280675888\n",
      "iteration 3299, dc_loss: 0.02372940629720688, tv_loss: 0.03352750465273857\n",
      "iteration 3300, dc_loss: 0.023721851408481598, tv_loss: 0.03354682773351669\n",
      "iteration 3301, dc_loss: 0.02373659797012806, tv_loss: 0.03351510316133499\n",
      "iteration 3302, dc_loss: 0.02373180165886879, tv_loss: 0.033515043556690216\n",
      "iteration 3303, dc_loss: 0.023713266476988792, tv_loss: 0.03354229778051376\n",
      "iteration 3304, dc_loss: 0.02371985651552677, tv_loss: 0.033538758754730225\n",
      "iteration 3305, dc_loss: 0.023721998557448387, tv_loss: 0.033519990742206573\n",
      "iteration 3306, dc_loss: 0.023718731477856636, tv_loss: 0.033523865044116974\n",
      "iteration 3307, dc_loss: 0.02371925115585327, tv_loss: 0.033522024750709534\n",
      "iteration 3308, dc_loss: 0.02371319569647312, tv_loss: 0.03352684900164604\n",
      "iteration 3309, dc_loss: 0.02371555007994175, tv_loss: 0.033520691096782684\n",
      "iteration 3310, dc_loss: 0.0237171221524477, tv_loss: 0.033517178148031235\n",
      "iteration 3311, dc_loss: 0.02370942384004593, tv_loss: 0.033519092947244644\n",
      "iteration 3312, dc_loss: 0.023706920444965363, tv_loss: 0.03352853283286095\n",
      "iteration 3313, dc_loss: 0.0237099751830101, tv_loss: 0.03351892530918121\n",
      "iteration 3314, dc_loss: 0.023706478998064995, tv_loss: 0.03352092579007149\n",
      "iteration 3315, dc_loss: 0.023699548095464706, tv_loss: 0.03352471441030502\n",
      "iteration 3316, dc_loss: 0.023706402629613876, tv_loss: 0.03351128473877907\n",
      "iteration 3317, dc_loss: 0.023702900856733322, tv_loss: 0.033515624701976776\n",
      "iteration 3318, dc_loss: 0.023696457967162132, tv_loss: 0.03352580592036247\n",
      "iteration 3319, dc_loss: 0.023700356483459473, tv_loss: 0.033528052270412445\n",
      "iteration 3320, dc_loss: 0.023695165291428566, tv_loss: 0.03352309763431549\n",
      "iteration 3321, dc_loss: 0.023695001378655434, tv_loss: 0.03351762145757675\n",
      "iteration 3322, dc_loss: 0.02369818091392517, tv_loss: 0.03351501747965813\n",
      "iteration 3323, dc_loss: 0.023687124252319336, tv_loss: 0.03352432698011398\n",
      "iteration 3324, dc_loss: 0.023686667904257774, tv_loss: 0.03352169319987297\n",
      "iteration 3325, dc_loss: 0.0236912053078413, tv_loss: 0.03351849317550659\n",
      "iteration 3326, dc_loss: 0.0236882995814085, tv_loss: 0.033516280353069305\n",
      "iteration 3327, dc_loss: 0.023681454360485077, tv_loss: 0.033524490892887115\n",
      "iteration 3328, dc_loss: 0.023685792461037636, tv_loss: 0.033516474068164825\n",
      "iteration 3329, dc_loss: 0.023687148466706276, tv_loss: 0.03351246938109398\n",
      "iteration 3330, dc_loss: 0.023678362369537354, tv_loss: 0.03351780027151108\n",
      "iteration 3331, dc_loss: 0.02367318794131279, tv_loss: 0.033521514385938644\n",
      "iteration 3332, dc_loss: 0.02367752604186535, tv_loss: 0.03351633623242378\n",
      "iteration 3333, dc_loss: 0.02367635816335678, tv_loss: 0.033514708280563354\n",
      "iteration 3334, dc_loss: 0.023673422634601593, tv_loss: 0.03351990506052971\n",
      "iteration 3335, dc_loss: 0.023676613345742226, tv_loss: 0.03351671248674393\n",
      "iteration 3336, dc_loss: 0.023674044758081436, tv_loss: 0.03352276608347893\n",
      "iteration 3337, dc_loss: 0.023663757368922234, tv_loss: 0.03352995961904526\n",
      "iteration 3338, dc_loss: 0.02366444282233715, tv_loss: 0.03352418541908264\n",
      "iteration 3339, dc_loss: 0.023667672649025917, tv_loss: 0.03351620212197304\n",
      "iteration 3340, dc_loss: 0.023667311295866966, tv_loss: 0.033514004200696945\n",
      "iteration 3341, dc_loss: 0.023663276806473732, tv_loss: 0.03351481631398201\n",
      "iteration 3342, dc_loss: 0.023657897487282753, tv_loss: 0.03351786360144615\n",
      "iteration 3343, dc_loss: 0.023659326136112213, tv_loss: 0.03351648896932602\n",
      "iteration 3344, dc_loss: 0.0236569382250309, tv_loss: 0.033515918999910355\n",
      "iteration 3345, dc_loss: 0.023656761273741722, tv_loss: 0.033525481820106506\n",
      "iteration 3346, dc_loss: 0.023656656965613365, tv_loss: 0.033522870391607285\n",
      "iteration 3347, dc_loss: 0.023657822981476784, tv_loss: 0.03351597115397453\n",
      "iteration 3348, dc_loss: 0.023651931434869766, tv_loss: 0.03351452574133873\n",
      "iteration 3349, dc_loss: 0.023646419867873192, tv_loss: 0.0335213877260685\n",
      "iteration 3350, dc_loss: 0.023648932576179504, tv_loss: 0.03351646289229393\n",
      "iteration 3351, dc_loss: 0.02364496700465679, tv_loss: 0.03351961076259613\n",
      "iteration 3352, dc_loss: 0.0236480962485075, tv_loss: 0.03351573646068573\n",
      "iteration 3353, dc_loss: 0.02364201284945011, tv_loss: 0.03351717069745064\n",
      "iteration 3354, dc_loss: 0.023641157895326614, tv_loss: 0.033517271280288696\n",
      "iteration 3355, dc_loss: 0.023642271757125854, tv_loss: 0.033512718975543976\n",
      "iteration 3356, dc_loss: 0.023638198152184486, tv_loss: 0.03351658582687378\n",
      "iteration 3357, dc_loss: 0.023637909442186356, tv_loss: 0.03351966664195061\n",
      "iteration 3358, dc_loss: 0.023635905236005783, tv_loss: 0.03351815044879913\n",
      "iteration 3359, dc_loss: 0.023632396012544632, tv_loss: 0.03352102264761925\n",
      "iteration 3360, dc_loss: 0.02363094501197338, tv_loss: 0.033523328602313995\n",
      "iteration 3361, dc_loss: 0.023630205541849136, tv_loss: 0.03351845592260361\n",
      "iteration 3362, dc_loss: 0.023632418364286423, tv_loss: 0.0335148349404335\n",
      "iteration 3363, dc_loss: 0.023631621152162552, tv_loss: 0.033511143177747726\n",
      "iteration 3364, dc_loss: 0.023624546825885773, tv_loss: 0.033517025411129\n",
      "iteration 3365, dc_loss: 0.023622484877705574, tv_loss: 0.03352184593677521\n",
      "iteration 3366, dc_loss: 0.023621825501322746, tv_loss: 0.033518362790346146\n",
      "iteration 3367, dc_loss: 0.02362043410539627, tv_loss: 0.03351900354027748\n",
      "iteration 3368, dc_loss: 0.023623637855052948, tv_loss: 0.033511970192193985\n",
      "iteration 3369, dc_loss: 0.023620445281267166, tv_loss: 0.03351332247257233\n",
      "iteration 3370, dc_loss: 0.02361450158059597, tv_loss: 0.033518388867378235\n",
      "iteration 3371, dc_loss: 0.023612041026353836, tv_loss: 0.03351925313472748\n",
      "iteration 3372, dc_loss: 0.023613503202795982, tv_loss: 0.033519014716148376\n",
      "iteration 3373, dc_loss: 0.023613229393959045, tv_loss: 0.033517416566610336\n",
      "iteration 3374, dc_loss: 0.02361205965280533, tv_loss: 0.03351718932390213\n",
      "iteration 3375, dc_loss: 0.0236066747456789, tv_loss: 0.033519893884658813\n",
      "iteration 3376, dc_loss: 0.023605067282915115, tv_loss: 0.03351891040802002\n",
      "iteration 3377, dc_loss: 0.02360425889492035, tv_loss: 0.033519621938467026\n",
      "iteration 3378, dc_loss: 0.023606358096003532, tv_loss: 0.03351353481411934\n",
      "iteration 3379, dc_loss: 0.023603837937116623, tv_loss: 0.03351817652583122\n",
      "iteration 3380, dc_loss: 0.023600192740559578, tv_loss: 0.033517152070999146\n",
      "iteration 3381, dc_loss: 0.023600919172167778, tv_loss: 0.03351246938109398\n",
      "iteration 3382, dc_loss: 0.023598410189151764, tv_loss: 0.033519197255373\n",
      "iteration 3383, dc_loss: 0.023591943085193634, tv_loss: 0.03352702036499977\n",
      "iteration 3384, dc_loss: 0.023595210164785385, tv_loss: 0.033523157238960266\n",
      "iteration 3385, dc_loss: 0.023592988029122353, tv_loss: 0.03352339193224907\n",
      "iteration 3386, dc_loss: 0.023591775447130203, tv_loss: 0.033512987196445465\n",
      "iteration 3387, dc_loss: 0.023593636229634285, tv_loss: 0.03351306542754173\n",
      "iteration 3388, dc_loss: 0.02358824759721756, tv_loss: 0.03351771458983421\n",
      "iteration 3389, dc_loss: 0.023581519722938538, tv_loss: 0.03351898491382599\n",
      "iteration 3390, dc_loss: 0.023590130731463432, tv_loss: 0.0335095152258873\n",
      "iteration 3391, dc_loss: 0.023583559319376945, tv_loss: 0.033518221229314804\n",
      "iteration 3392, dc_loss: 0.023579804226756096, tv_loss: 0.03352503851056099\n",
      "iteration 3393, dc_loss: 0.023580171167850494, tv_loss: 0.03353077173233032\n",
      "iteration 3394, dc_loss: 0.023582665249705315, tv_loss: 0.03351708501577377\n",
      "iteration 3395, dc_loss: 0.02357112616300583, tv_loss: 0.033523187041282654\n",
      "iteration 3396, dc_loss: 0.023575598374009132, tv_loss: 0.03352129086852074\n",
      "iteration 3397, dc_loss: 0.02358218841254711, tv_loss: 0.033519960939884186\n",
      "iteration 3398, dc_loss: 0.023570546880364418, tv_loss: 0.03352334350347519\n",
      "iteration 3399, dc_loss: 0.02357041835784912, tv_loss: 0.033523350954055786\n",
      "iteration 3400, dc_loss: 0.02357202023267746, tv_loss: 0.03351835161447525\n",
      "iteration 3401, dc_loss: 0.023564085364341736, tv_loss: 0.03352535516023636\n",
      "iteration 3402, dc_loss: 0.0235699824988842, tv_loss: 0.03352195397019386\n",
      "iteration 3403, dc_loss: 0.023570120334625244, tv_loss: 0.03351828455924988\n",
      "iteration 3404, dc_loss: 0.023557079955935478, tv_loss: 0.03352518379688263\n",
      "iteration 3405, dc_loss: 0.023558422923088074, tv_loss: 0.03351902961730957\n",
      "iteration 3406, dc_loss: 0.023570546880364418, tv_loss: 0.03351335600018501\n",
      "iteration 3407, dc_loss: 0.02355116605758667, tv_loss: 0.0335237942636013\n",
      "iteration 3408, dc_loss: 0.023554831743240356, tv_loss: 0.03351851925253868\n",
      "iteration 3409, dc_loss: 0.023570319637656212, tv_loss: 0.03350625932216644\n",
      "iteration 3410, dc_loss: 0.023548120632767677, tv_loss: 0.033522412180900574\n",
      "iteration 3411, dc_loss: 0.02354445680975914, tv_loss: 0.03352586552500725\n",
      "iteration 3412, dc_loss: 0.023556793108582497, tv_loss: 0.03350941091775894\n",
      "iteration 3413, dc_loss: 0.023550162091851234, tv_loss: 0.03351668640971184\n",
      "iteration 3414, dc_loss: 0.023544959723949432, tv_loss: 0.03351685032248497\n",
      "iteration 3415, dc_loss: 0.023548582568764687, tv_loss: 0.033512748777866364\n",
      "iteration 3416, dc_loss: 0.023548057302832603, tv_loss: 0.033510711044073105\n",
      "iteration 3417, dc_loss: 0.023538703098893166, tv_loss: 0.033514898270368576\n",
      "iteration 3418, dc_loss: 0.023540882393717766, tv_loss: 0.03351884335279465\n",
      "iteration 3419, dc_loss: 0.02355010434985161, tv_loss: 0.033513836562633514\n",
      "iteration 3420, dc_loss: 0.02353321760892868, tv_loss: 0.033533867448568344\n",
      "iteration 3421, dc_loss: 0.023530621081590652, tv_loss: 0.03353520482778549\n",
      "iteration 3422, dc_loss: 0.023539165034890175, tv_loss: 0.03351469710469246\n",
      "iteration 3423, dc_loss: 0.023540275171399117, tv_loss: 0.03351077064871788\n",
      "iteration 3424, dc_loss: 0.02352546527981758, tv_loss: 0.03352579474449158\n",
      "iteration 3425, dc_loss: 0.023534713312983513, tv_loss: 0.033511683344841\n",
      "iteration 3426, dc_loss: 0.02353077381849289, tv_loss: 0.03351670131087303\n",
      "iteration 3427, dc_loss: 0.023521509021520615, tv_loss: 0.03353483974933624\n",
      "iteration 3428, dc_loss: 0.02353445626795292, tv_loss: 0.03351091966032982\n",
      "iteration 3429, dc_loss: 0.023522501811385155, tv_loss: 0.03351539000868797\n",
      "iteration 3430, dc_loss: 0.02351418510079384, tv_loss: 0.03352600336074829\n",
      "iteration 3431, dc_loss: 0.02352886088192463, tv_loss: 0.03351055458188057\n",
      "iteration 3432, dc_loss: 0.023518366739153862, tv_loss: 0.03351537138223648\n",
      "iteration 3433, dc_loss: 0.023516066372394562, tv_loss: 0.033522170037031174\n",
      "iteration 3434, dc_loss: 0.023526588454842567, tv_loss: 0.03350715711712837\n",
      "iteration 3435, dc_loss: 0.023513920605182648, tv_loss: 0.03351757302880287\n",
      "iteration 3436, dc_loss: 0.023505518212914467, tv_loss: 0.033522382378578186\n",
      "iteration 3437, dc_loss: 0.023515816777944565, tv_loss: 0.03351743519306183\n",
      "iteration 3438, dc_loss: 0.023508843034505844, tv_loss: 0.03351651504635811\n",
      "iteration 3439, dc_loss: 0.02351096272468567, tv_loss: 0.033509671688079834\n",
      "iteration 3440, dc_loss: 0.023509755730628967, tv_loss: 0.03351408988237381\n",
      "iteration 3441, dc_loss: 0.023501785472035408, tv_loss: 0.03352124243974686\n",
      "iteration 3442, dc_loss: 0.023510197177529335, tv_loss: 0.03350832313299179\n",
      "iteration 3443, dc_loss: 0.023500945419073105, tv_loss: 0.03351389989256859\n",
      "iteration 3444, dc_loss: 0.023494025692343712, tv_loss: 0.03352578729391098\n",
      "iteration 3445, dc_loss: 0.02350524067878723, tv_loss: 0.03351735323667526\n",
      "iteration 3446, dc_loss: 0.0235044714063406, tv_loss: 0.033520545810461044\n",
      "iteration 3447, dc_loss: 0.02349018305540085, tv_loss: 0.033531785011291504\n",
      "iteration 3448, dc_loss: 0.02349613420665264, tv_loss: 0.03351472690701485\n",
      "iteration 3449, dc_loss: 0.023500319570302963, tv_loss: 0.033510446548461914\n",
      "iteration 3450, dc_loss: 0.02348499745130539, tv_loss: 0.033527374267578125\n",
      "iteration 3451, dc_loss: 0.023491952568292618, tv_loss: 0.03351796045899391\n",
      "iteration 3452, dc_loss: 0.023488853126764297, tv_loss: 0.03351866081357002\n",
      "iteration 3453, dc_loss: 0.023483898490667343, tv_loss: 0.03352721408009529\n",
      "iteration 3454, dc_loss: 0.023491017520427704, tv_loss: 0.03351098671555519\n",
      "iteration 3455, dc_loss: 0.023487895727157593, tv_loss: 0.033512674272060394\n",
      "iteration 3456, dc_loss: 0.02348192222416401, tv_loss: 0.033525872975587845\n",
      "iteration 3457, dc_loss: 0.023479897528886795, tv_loss: 0.03353191912174225\n",
      "iteration 3458, dc_loss: 0.02348172292113304, tv_loss: 0.03352515399456024\n",
      "iteration 3459, dc_loss: 0.023471496999263763, tv_loss: 0.03352836146950722\n",
      "iteration 3460, dc_loss: 0.023481810465455055, tv_loss: 0.03351485729217529\n",
      "iteration 3461, dc_loss: 0.023480143398046494, tv_loss: 0.03352268412709236\n",
      "iteration 3462, dc_loss: 0.023465923964977264, tv_loss: 0.033536117523908615\n",
      "iteration 3463, dc_loss: 0.02347516641020775, tv_loss: 0.033516161143779755\n",
      "iteration 3464, dc_loss: 0.023478006944060326, tv_loss: 0.03350713104009628\n",
      "iteration 3465, dc_loss: 0.02346261590719223, tv_loss: 0.0335264727473259\n",
      "iteration 3466, dc_loss: 0.023469077423214912, tv_loss: 0.03352871909737587\n",
      "iteration 3467, dc_loss: 0.023465504869818687, tv_loss: 0.03353101760149002\n",
      "iteration 3468, dc_loss: 0.023462293669581413, tv_loss: 0.03352594003081322\n",
      "iteration 3469, dc_loss: 0.023465711623430252, tv_loss: 0.03351489081978798\n",
      "iteration 3470, dc_loss: 0.023463359102606773, tv_loss: 0.03352582827210426\n",
      "iteration 3471, dc_loss: 0.023454340174794197, tv_loss: 0.03353336080908775\n",
      "iteration 3472, dc_loss: 0.023457741364836693, tv_loss: 0.03351958841085434\n",
      "iteration 3473, dc_loss: 0.023465633392333984, tv_loss: 0.03351396322250366\n",
      "iteration 3474, dc_loss: 0.023454204201698303, tv_loss: 0.03351863473653793\n",
      "iteration 3475, dc_loss: 0.023452872410416603, tv_loss: 0.03351948782801628\n",
      "iteration 3476, dc_loss: 0.02345730923116207, tv_loss: 0.033514320850372314\n",
      "iteration 3477, dc_loss: 0.023442458361387253, tv_loss: 0.03352515771985054\n",
      "iteration 3478, dc_loss: 0.023447241634130478, tv_loss: 0.03351634368300438\n",
      "iteration 3479, dc_loss: 0.023460375145077705, tv_loss: 0.033501025289297104\n",
      "iteration 3480, dc_loss: 0.023440295830368996, tv_loss: 0.03351825103163719\n",
      "iteration 3481, dc_loss: 0.023442594334483147, tv_loss: 0.0335165299475193\n",
      "iteration 3482, dc_loss: 0.023454517126083374, tv_loss: 0.03350010886788368\n",
      "iteration 3483, dc_loss: 0.02344083972275257, tv_loss: 0.033510494977235794\n",
      "iteration 3484, dc_loss: 0.023428944870829582, tv_loss: 0.03352511301636696\n",
      "iteration 3485, dc_loss: 0.02344616688787937, tv_loss: 0.03351189196109772\n",
      "iteration 3486, dc_loss: 0.023439785465598106, tv_loss: 0.03353884071111679\n",
      "iteration 3487, dc_loss: 0.023434698581695557, tv_loss: 0.033533159643411636\n",
      "iteration 3488, dc_loss: 0.023437034338712692, tv_loss: 0.03351374343037605\n",
      "iteration 3489, dc_loss: 0.023438038304448128, tv_loss: 0.03352993354201317\n",
      "iteration 3490, dc_loss: 0.023426424711942673, tv_loss: 0.03354775905609131\n",
      "iteration 3491, dc_loss: 0.023422803729772568, tv_loss: 0.033529866486787796\n",
      "iteration 3492, dc_loss: 0.023431895300745964, tv_loss: 0.033538877964019775\n",
      "iteration 3493, dc_loss: 0.023433169350028038, tv_loss: 0.0335342176258564\n",
      "iteration 3494, dc_loss: 0.023421257734298706, tv_loss: 0.03352833539247513\n",
      "iteration 3495, dc_loss: 0.023420829325914383, tv_loss: 0.033539604395627975\n",
      "iteration 3496, dc_loss: 0.02342488057911396, tv_loss: 0.03353635594248772\n",
      "iteration 3497, dc_loss: 0.02341521717607975, tv_loss: 0.03353021666407585\n",
      "iteration 3498, dc_loss: 0.023424837738275528, tv_loss: 0.03352884575724602\n",
      "iteration 3499, dc_loss: 0.02342495135962963, tv_loss: 0.03353190794587135\n",
      "iteration 3500, dc_loss: 0.02340969629585743, tv_loss: 0.03353089839220047\n",
      "iteration 3501, dc_loss: 0.02341691218316555, tv_loss: 0.03352731093764305\n",
      "iteration 3502, dc_loss: 0.023425152525305748, tv_loss: 0.033520057797431946\n",
      "iteration 3503, dc_loss: 0.023401940241456032, tv_loss: 0.03353477269411087\n",
      "iteration 3504, dc_loss: 0.023405779153108597, tv_loss: 0.03352304548025131\n",
      "iteration 3505, dc_loss: 0.023417208343744278, tv_loss: 0.033516775816679\n",
      "iteration 3506, dc_loss: 0.023404225707054138, tv_loss: 0.033530332148075104\n",
      "iteration 3507, dc_loss: 0.023401662707328796, tv_loss: 0.03352699428796768\n",
      "iteration 3508, dc_loss: 0.023416154086589813, tv_loss: 0.033504363149404526\n",
      "iteration 3509, dc_loss: 0.023403897881507874, tv_loss: 0.0335170179605484\n",
      "iteration 3510, dc_loss: 0.02339913696050644, tv_loss: 0.03352140262722969\n",
      "iteration 3511, dc_loss: 0.023406755179166794, tv_loss: 0.03351486474275589\n",
      "iteration 3512, dc_loss: 0.023400789126753807, tv_loss: 0.0335245281457901\n",
      "iteration 3513, dc_loss: 0.02338382601737976, tv_loss: 0.033531785011291504\n",
      "iteration 3514, dc_loss: 0.02340497262775898, tv_loss: 0.033508773893117905\n",
      "iteration 3515, dc_loss: 0.02339821495115757, tv_loss: 0.03351069986820221\n",
      "iteration 3516, dc_loss: 0.023386286571621895, tv_loss: 0.033529069274663925\n",
      "iteration 3517, dc_loss: 0.023393496870994568, tv_loss: 0.03351924940943718\n",
      "iteration 3518, dc_loss: 0.023396799340844154, tv_loss: 0.03351668640971184\n",
      "iteration 3519, dc_loss: 0.02338765375316143, tv_loss: 0.033522721379995346\n",
      "iteration 3520, dc_loss: 0.02338036522269249, tv_loss: 0.033523593097925186\n",
      "iteration 3521, dc_loss: 0.023387692868709564, tv_loss: 0.03351227566599846\n",
      "iteration 3522, dc_loss: 0.02338448166847229, tv_loss: 0.033517394214868546\n",
      "iteration 3523, dc_loss: 0.02338416315615177, tv_loss: 0.03351350873708725\n",
      "iteration 3524, dc_loss: 0.023381691426038742, tv_loss: 0.03351801633834839\n",
      "iteration 3525, dc_loss: 0.02337121032178402, tv_loss: 0.033526334911584854\n",
      "iteration 3526, dc_loss: 0.023385848850011826, tv_loss: 0.03350921347737312\n",
      "iteration 3527, dc_loss: 0.023374876007437706, tv_loss: 0.03351837024092674\n",
      "iteration 3528, dc_loss: 0.02337525226175785, tv_loss: 0.03351082280278206\n",
      "iteration 3529, dc_loss: 0.023378824815154076, tv_loss: 0.03351160138845444\n",
      "iteration 3530, dc_loss: 0.023366494104266167, tv_loss: 0.03351818397641182\n",
      "iteration 3531, dc_loss: 0.02337116375565529, tv_loss: 0.03350848704576492\n",
      "iteration 3532, dc_loss: 0.02337578684091568, tv_loss: 0.03350485861301422\n",
      "iteration 3533, dc_loss: 0.02335890755057335, tv_loss: 0.03352167084813118\n",
      "iteration 3534, dc_loss: 0.023368969559669495, tv_loss: 0.033512264490127563\n",
      "iteration 3535, dc_loss: 0.023363333195447922, tv_loss: 0.03352141007781029\n",
      "iteration 3536, dc_loss: 0.023367593064904213, tv_loss: 0.033525414764881134\n",
      "iteration 3537, dc_loss: 0.023355994373559952, tv_loss: 0.03352758660912514\n",
      "iteration 3538, dc_loss: 0.0233658105134964, tv_loss: 0.033518679440021515\n",
      "iteration 3539, dc_loss: 0.02336161583662033, tv_loss: 0.03351574018597603\n",
      "iteration 3540, dc_loss: 0.02336062863469124, tv_loss: 0.03352905809879303\n",
      "iteration 3541, dc_loss: 0.023351235315203667, tv_loss: 0.03353307023644447\n",
      "iteration 3542, dc_loss: 0.023354291915893555, tv_loss: 0.033524058759212494\n",
      "iteration 3543, dc_loss: 0.023362543433904648, tv_loss: 0.03352278098464012\n",
      "iteration 3544, dc_loss: 0.023364214226603508, tv_loss: 0.03352687880396843\n",
      "iteration 3545, dc_loss: 0.023339150473475456, tv_loss: 0.033537913113832474\n",
      "iteration 3546, dc_loss: 0.023356182500720024, tv_loss: 0.03352271765470505\n",
      "iteration 3547, dc_loss: 0.023350253701210022, tv_loss: 0.03353375941514969\n",
      "iteration 3548, dc_loss: 0.023351842537522316, tv_loss: 0.03352303430438042\n",
      "iteration 3549, dc_loss: 0.023346539586782455, tv_loss: 0.033525943756103516\n",
      "iteration 3550, dc_loss: 0.02335156686604023, tv_loss: 0.033518508076667786\n",
      "iteration 3551, dc_loss: 0.023337554186582565, tv_loss: 0.0335330106317997\n",
      "iteration 3552, dc_loss: 0.023341001942753792, tv_loss: 0.0335177443921566\n",
      "iteration 3553, dc_loss: 0.023341167718172073, tv_loss: 0.03352135047316551\n",
      "iteration 3554, dc_loss: 0.023330776020884514, tv_loss: 0.03353118523955345\n",
      "iteration 3555, dc_loss: 0.023333987221121788, tv_loss: 0.0335322767496109\n",
      "iteration 3556, dc_loss: 0.023340795189142227, tv_loss: 0.03351040929555893\n",
      "iteration 3557, dc_loss: 0.02333141304552555, tv_loss: 0.033522892743349075\n",
      "iteration 3558, dc_loss: 0.023329993709921837, tv_loss: 0.0335257463157177\n",
      "iteration 3559, dc_loss: 0.0233295951038599, tv_loss: 0.03351878747344017\n",
      "iteration 3560, dc_loss: 0.023325949907302856, tv_loss: 0.03352665901184082\n",
      "iteration 3561, dc_loss: 0.023329976946115494, tv_loss: 0.03351251780986786\n",
      "iteration 3562, dc_loss: 0.02332364208996296, tv_loss: 0.0335211381316185\n",
      "iteration 3563, dc_loss: 0.0233265683054924, tv_loss: 0.03351554647088051\n",
      "iteration 3564, dc_loss: 0.0233134962618351, tv_loss: 0.033521659672260284\n",
      "iteration 3565, dc_loss: 0.023325985297560692, tv_loss: 0.0335136316716671\n",
      "iteration 3566, dc_loss: 0.023325176909565926, tv_loss: 0.03350640833377838\n",
      "iteration 3567, dc_loss: 0.023308245465159416, tv_loss: 0.03352327644824982\n",
      "iteration 3568, dc_loss: 0.02331714890897274, tv_loss: 0.03351911902427673\n",
      "iteration 3569, dc_loss: 0.02332390658557415, tv_loss: 0.033520448952913284\n",
      "iteration 3570, dc_loss: 0.023304004222154617, tv_loss: 0.03353329747915268\n",
      "iteration 3571, dc_loss: 0.02330826409161091, tv_loss: 0.03351890668272972\n",
      "iteration 3572, dc_loss: 0.023315316066145897, tv_loss: 0.03351190686225891\n",
      "iteration 3573, dc_loss: 0.023308757692575455, tv_loss: 0.03352416306734085\n",
      "iteration 3574, dc_loss: 0.02330748923122883, tv_loss: 0.03352481871843338\n",
      "iteration 3575, dc_loss: 0.0233078021556139, tv_loss: 0.03351801261305809\n",
      "iteration 3576, dc_loss: 0.02330085076391697, tv_loss: 0.03351769596338272\n",
      "iteration 3577, dc_loss: 0.02329578809440136, tv_loss: 0.03352459520101547\n",
      "iteration 3578, dc_loss: 0.023307165130972862, tv_loss: 0.03350747376680374\n",
      "iteration 3579, dc_loss: 0.023301392793655396, tv_loss: 0.03352358192205429\n",
      "iteration 3580, dc_loss: 0.023293396458029747, tv_loss: 0.033525656908750534\n",
      "iteration 3581, dc_loss: 0.023299530148506165, tv_loss: 0.03351457417011261\n",
      "iteration 3582, dc_loss: 0.02329450473189354, tv_loss: 0.033520184457302094\n",
      "iteration 3583, dc_loss: 0.023293526843190193, tv_loss: 0.03352559357881546\n",
      "iteration 3584, dc_loss: 0.02329462394118309, tv_loss: 0.033526461571455\n",
      "iteration 3585, dc_loss: 0.02328934520483017, tv_loss: 0.033519282937049866\n",
      "iteration 3586, dc_loss: 0.02328077331185341, tv_loss: 0.03352726995944977\n",
      "iteration 3587, dc_loss: 0.023294953629374504, tv_loss: 0.03351680561900139\n",
      "iteration 3588, dc_loss: 0.02329232543706894, tv_loss: 0.03352617099881172\n",
      "iteration 3589, dc_loss: 0.023281477391719818, tv_loss: 0.03352653980255127\n",
      "iteration 3590, dc_loss: 0.02327982895076275, tv_loss: 0.033523786813020706\n",
      "iteration 3591, dc_loss: 0.023287389427423477, tv_loss: 0.033522382378578186\n",
      "iteration 3592, dc_loss: 0.023281637579202652, tv_loss: 0.03352368623018265\n",
      "iteration 3593, dc_loss: 0.023273272439837456, tv_loss: 0.033527176827192307\n",
      "iteration 3594, dc_loss: 0.02328270487487316, tv_loss: 0.03351493924856186\n",
      "iteration 3595, dc_loss: 0.023287458345294, tv_loss: 0.03350919485092163\n",
      "iteration 3596, dc_loss: 0.023266244679689407, tv_loss: 0.033534783869981766\n",
      "iteration 3597, dc_loss: 0.023282775655388832, tv_loss: 0.03351667895913124\n",
      "iteration 3598, dc_loss: 0.023268451914191246, tv_loss: 0.03352300450205803\n",
      "iteration 3599, dc_loss: 0.023272009566426277, tv_loss: 0.03351781144738197\n",
      "iteration 3600, dc_loss: 0.023278269916772842, tv_loss: 0.03351397067308426\n",
      "iteration 3601, dc_loss: 0.0232823695987463, tv_loss: 0.03351566568017006\n",
      "iteration 3602, dc_loss: 0.02326417714357376, tv_loss: 0.03352212533354759\n",
      "iteration 3603, dc_loss: 0.0232655368745327, tv_loss: 0.03350977227091789\n",
      "iteration 3604, dc_loss: 0.023276343941688538, tv_loss: 0.03350460156798363\n",
      "iteration 3605, dc_loss: 0.023264292627573013, tv_loss: 0.033521611243486404\n",
      "iteration 3606, dc_loss: 0.02326129376888275, tv_loss: 0.03351534157991409\n",
      "iteration 3607, dc_loss: 0.02326652966439724, tv_loss: 0.03350566327571869\n",
      "iteration 3608, dc_loss: 0.023260125890374184, tv_loss: 0.033515240997076035\n",
      "iteration 3609, dc_loss: 0.023259397596120834, tv_loss: 0.033515069633722305\n",
      "iteration 3610, dc_loss: 0.023259304463863373, tv_loss: 0.033509623259305954\n",
      "iteration 3611, dc_loss: 0.02325759083032608, tv_loss: 0.03351166471838951\n",
      "iteration 3612, dc_loss: 0.023258617147803307, tv_loss: 0.03350576013326645\n",
      "iteration 3613, dc_loss: 0.023249972611665726, tv_loss: 0.03350920230150223\n",
      "iteration 3614, dc_loss: 0.023250754922628403, tv_loss: 0.03350940719246864\n",
      "iteration 3615, dc_loss: 0.023256167769432068, tv_loss: 0.03350207582116127\n",
      "iteration 3616, dc_loss: 0.02324659377336502, tv_loss: 0.03351176530122757\n",
      "iteration 3617, dc_loss: 0.02325361594557762, tv_loss: 0.033505260944366455\n",
      "iteration 3618, dc_loss: 0.02325556054711342, tv_loss: 0.03350552171468735\n",
      "iteration 3619, dc_loss: 0.023242540657520294, tv_loss: 0.03351385146379471\n",
      "iteration 3620, dc_loss: 0.023240378126502037, tv_loss: 0.03351454809308052\n",
      "iteration 3621, dc_loss: 0.023250309750437737, tv_loss: 0.033502381294965744\n",
      "iteration 3622, dc_loss: 0.023242345079779625, tv_loss: 0.033504974097013474\n",
      "iteration 3623, dc_loss: 0.02324017882347107, tv_loss: 0.033507321029901505\n",
      "iteration 3624, dc_loss: 0.02324255369603634, tv_loss: 0.033503029495477676\n",
      "iteration 3625, dc_loss: 0.023241575807332993, tv_loss: 0.03350536897778511\n",
      "iteration 3626, dc_loss: 0.023242535069584846, tv_loss: 0.03350793942809105\n",
      "iteration 3627, dc_loss: 0.02323046140372753, tv_loss: 0.03351950645446777\n",
      "iteration 3628, dc_loss: 0.023233462125062943, tv_loss: 0.03351924195885658\n",
      "iteration 3629, dc_loss: 0.023240668699145317, tv_loss: 0.033502452075481415\n",
      "iteration 3630, dc_loss: 0.023232093080878258, tv_loss: 0.033511534333229065\n",
      "iteration 3631, dc_loss: 0.023225432261824608, tv_loss: 0.033521514385938644\n",
      "iteration 3632, dc_loss: 0.023236965760588646, tv_loss: 0.03350793197751045\n",
      "iteration 3633, dc_loss: 0.0232393741607666, tv_loss: 0.03349975124001503\n",
      "iteration 3634, dc_loss: 0.023223882541060448, tv_loss: 0.03351184353232384\n",
      "iteration 3635, dc_loss: 0.023221470415592194, tv_loss: 0.0335194356739521\n",
      "iteration 3636, dc_loss: 0.02323250100016594, tv_loss: 0.03351324796676636\n",
      "iteration 3637, dc_loss: 0.02322738617658615, tv_loss: 0.03350900113582611\n",
      "iteration 3638, dc_loss: 0.023219633847475052, tv_loss: 0.03351534157991409\n",
      "iteration 3639, dc_loss: 0.023220820352435112, tv_loss: 0.03351571783423424\n",
      "iteration 3640, dc_loss: 0.023223279044032097, tv_loss: 0.03351708501577377\n",
      "iteration 3641, dc_loss: 0.023223236203193665, tv_loss: 0.03351336717605591\n",
      "iteration 3642, dc_loss: 0.02322029322385788, tv_loss: 0.03350537642836571\n",
      "iteration 3643, dc_loss: 0.023219017311930656, tv_loss: 0.03351378068327904\n",
      "iteration 3644, dc_loss: 0.023218076676130295, tv_loss: 0.033519625663757324\n",
      "iteration 3645, dc_loss: 0.02321256697177887, tv_loss: 0.03351544216275215\n",
      "iteration 3646, dc_loss: 0.023218873888254166, tv_loss: 0.03350887820124626\n",
      "iteration 3647, dc_loss: 0.02321840077638626, tv_loss: 0.03351757302880287\n",
      "iteration 3648, dc_loss: 0.02320852503180504, tv_loss: 0.03351867198944092\n",
      "iteration 3649, dc_loss: 0.023211592808365822, tv_loss: 0.033507611602544785\n",
      "iteration 3650, dc_loss: 0.023212911561131477, tv_loss: 0.033506840467453\n",
      "iteration 3651, dc_loss: 0.0232080165296793, tv_loss: 0.03351350128650665\n",
      "iteration 3652, dc_loss: 0.023206086829304695, tv_loss: 0.033517930656671524\n",
      "iteration 3653, dc_loss: 0.023204883560538292, tv_loss: 0.033512674272060394\n",
      "iteration 3654, dc_loss: 0.02320975624024868, tv_loss: 0.033501580357551575\n",
      "iteration 3655, dc_loss: 0.023208435624837875, tv_loss: 0.033505234867334366\n",
      "iteration 3656, dc_loss: 0.023200925439596176, tv_loss: 0.03351403400301933\n",
      "iteration 3657, dc_loss: 0.023203041404485703, tv_loss: 0.03351340815424919\n",
      "iteration 3658, dc_loss: 0.023201990872621536, tv_loss: 0.033509548753499985\n",
      "iteration 3659, dc_loss: 0.02319924905896187, tv_loss: 0.03350905701518059\n",
      "iteration 3660, dc_loss: 0.02319849096238613, tv_loss: 0.03350633755326271\n",
      "iteration 3661, dc_loss: 0.02320234477519989, tv_loss: 0.0335041880607605\n",
      "iteration 3662, dc_loss: 0.023196246474981308, tv_loss: 0.033513352274894714\n",
      "iteration 3663, dc_loss: 0.02318963035941124, tv_loss: 0.0335155688226223\n",
      "iteration 3664, dc_loss: 0.023196637630462646, tv_loss: 0.03350713104009628\n",
      "iteration 3665, dc_loss: 0.023197568953037262, tv_loss: 0.03350413218140602\n",
      "iteration 3666, dc_loss: 0.023191744461655617, tv_loss: 0.03350444510579109\n",
      "iteration 3667, dc_loss: 0.023187432438135147, tv_loss: 0.03351077437400818\n",
      "iteration 3668, dc_loss: 0.023189391940832138, tv_loss: 0.03350850194692612\n",
      "iteration 3669, dc_loss: 0.02319309301674366, tv_loss: 0.033499933779239655\n",
      "iteration 3670, dc_loss: 0.02318873628973961, tv_loss: 0.03350752592086792\n",
      "iteration 3671, dc_loss: 0.02318463660776615, tv_loss: 0.03350730985403061\n",
      "iteration 3672, dc_loss: 0.02318514697253704, tv_loss: 0.03350559249520302\n",
      "iteration 3673, dc_loss: 0.02318566106259823, tv_loss: 0.033503416925668716\n",
      "iteration 3674, dc_loss: 0.023182988166809082, tv_loss: 0.03350500762462616\n",
      "iteration 3675, dc_loss: 0.02317875251173973, tv_loss: 0.03350790962576866\n",
      "iteration 3676, dc_loss: 0.023183055222034454, tv_loss: 0.03350374102592468\n",
      "iteration 3677, dc_loss: 0.023184720426797867, tv_loss: 0.03350649029016495\n",
      "iteration 3678, dc_loss: 0.023173123598098755, tv_loss: 0.03351582959294319\n",
      "iteration 3679, dc_loss: 0.02317258156836033, tv_loss: 0.03351669758558273\n",
      "iteration 3680, dc_loss: 0.023182157427072525, tv_loss: 0.03350134938955307\n",
      "iteration 3681, dc_loss: 0.0231741052120924, tv_loss: 0.03350405395030975\n",
      "iteration 3682, dc_loss: 0.023169072344899178, tv_loss: 0.03351110219955444\n",
      "iteration 3683, dc_loss: 0.023177290335297585, tv_loss: 0.033503081649541855\n",
      "iteration 3684, dc_loss: 0.023175641894340515, tv_loss: 0.033508095890283585\n",
      "iteration 3685, dc_loss: 0.02317107655107975, tv_loss: 0.03351600095629692\n",
      "iteration 3686, dc_loss: 0.023162784054875374, tv_loss: 0.03352191299200058\n",
      "iteration 3687, dc_loss: 0.023169441148638725, tv_loss: 0.03350941464304924\n",
      "iteration 3688, dc_loss: 0.023171257227659225, tv_loss: 0.03350457176566124\n",
      "iteration 3689, dc_loss: 0.023166704922914505, tv_loss: 0.033511850982904434\n",
      "iteration 3690, dc_loss: 0.02316526509821415, tv_loss: 0.03351816162467003\n",
      "iteration 3691, dc_loss: 0.02316109836101532, tv_loss: 0.03350944072008133\n",
      "iteration 3692, dc_loss: 0.023162800818681717, tv_loss: 0.03351180627942085\n",
      "iteration 3693, dc_loss: 0.02316322736442089, tv_loss: 0.033505573868751526\n",
      "iteration 3694, dc_loss: 0.023155923932790756, tv_loss: 0.03351674601435661\n",
      "iteration 3695, dc_loss: 0.023159505799412727, tv_loss: 0.03350456431508064\n",
      "iteration 3696, dc_loss: 0.023163754492998123, tv_loss: 0.03350572660565376\n",
      "iteration 3697, dc_loss: 0.023156045004725456, tv_loss: 0.03350992128252983\n",
      "iteration 3698, dc_loss: 0.023154618218541145, tv_loss: 0.033509671688079834\n",
      "iteration 3699, dc_loss: 0.02315642684698105, tv_loss: 0.033512625843286514\n",
      "iteration 3700, dc_loss: 0.0231504924595356, tv_loss: 0.033509477972984314\n",
      "iteration 3701, dc_loss: 0.023154588416218758, tv_loss: 0.033505629748106\n",
      "iteration 3702, dc_loss: 0.02315533347427845, tv_loss: 0.03350963816046715\n",
      "iteration 3703, dc_loss: 0.023146698251366615, tv_loss: 0.033515267074108124\n",
      "iteration 3704, dc_loss: 0.02314644493162632, tv_loss: 0.0335099883377552\n",
      "iteration 3705, dc_loss: 0.02315480075776577, tv_loss: 0.03350329399108887\n",
      "iteration 3706, dc_loss: 0.023143790662288666, tv_loss: 0.03350701555609703\n",
      "iteration 3707, dc_loss: 0.023142486810684204, tv_loss: 0.03350816294550896\n",
      "iteration 3708, dc_loss: 0.023151837289333344, tv_loss: 0.03350216895341873\n",
      "iteration 3709, dc_loss: 0.023141810670495033, tv_loss: 0.03350909799337387\n",
      "iteration 3710, dc_loss: 0.02314087748527527, tv_loss: 0.03350960835814476\n",
      "iteration 3711, dc_loss: 0.02314150705933571, tv_loss: 0.03351103886961937\n",
      "iteration 3712, dc_loss: 0.023140788078308105, tv_loss: 0.03350895270705223\n",
      "iteration 3713, dc_loss: 0.023137887939810753, tv_loss: 0.0335056446492672\n",
      "iteration 3714, dc_loss: 0.02313845604658127, tv_loss: 0.03350742161273956\n",
      "iteration 3715, dc_loss: 0.023136399686336517, tv_loss: 0.033507008105516434\n",
      "iteration 3716, dc_loss: 0.023139819502830505, tv_loss: 0.0335003137588501\n",
      "iteration 3717, dc_loss: 0.023134510964155197, tv_loss: 0.03350387513637543\n",
      "iteration 3718, dc_loss: 0.023129157721996307, tv_loss: 0.033507779240608215\n",
      "iteration 3719, dc_loss: 0.023134896531701088, tv_loss: 0.03350280970335007\n",
      "iteration 3720, dc_loss: 0.02312943898141384, tv_loss: 0.03350397199392319\n",
      "iteration 3721, dc_loss: 0.023130757734179497, tv_loss: 0.03350263833999634\n",
      "iteration 3722, dc_loss: 0.023131195455789566, tv_loss: 0.03351105749607086\n",
      "iteration 3723, dc_loss: 0.023129373788833618, tv_loss: 0.03351551294326782\n",
      "iteration 3724, dc_loss: 0.02312324568629265, tv_loss: 0.03351639211177826\n",
      "iteration 3725, dc_loss: 0.02312655560672283, tv_loss: 0.0335044264793396\n",
      "iteration 3726, dc_loss: 0.023126259446144104, tv_loss: 0.03350302577018738\n",
      "iteration 3727, dc_loss: 0.023122195154428482, tv_loss: 0.03350850194692612\n",
      "iteration 3728, dc_loss: 0.023121673613786697, tv_loss: 0.03351057320833206\n",
      "iteration 3729, dc_loss: 0.023119540885090828, tv_loss: 0.033514413982629776\n",
      "iteration 3730, dc_loss: 0.023123662918806076, tv_loss: 0.03350451961159706\n",
      "iteration 3731, dc_loss: 0.02311825007200241, tv_loss: 0.03350580856204033\n",
      "iteration 3732, dc_loss: 0.023112406954169273, tv_loss: 0.033508967608213425\n",
      "iteration 3733, dc_loss: 0.02311931550502777, tv_loss: 0.03350120782852173\n",
      "iteration 3734, dc_loss: 0.023120049387216568, tv_loss: 0.033501334488391876\n",
      "iteration 3735, dc_loss: 0.023111704736948013, tv_loss: 0.033509403467178345\n",
      "iteration 3736, dc_loss: 0.023111935704946518, tv_loss: 0.03351464122533798\n",
      "iteration 3737, dc_loss: 0.02311551384627819, tv_loss: 0.03350787237286568\n",
      "iteration 3738, dc_loss: 0.02310958318412304, tv_loss: 0.03351243585348129\n",
      "iteration 3739, dc_loss: 0.023105375468730927, tv_loss: 0.03350932151079178\n",
      "iteration 3740, dc_loss: 0.023114318028092384, tv_loss: 0.033500995486974716\n",
      "iteration 3741, dc_loss: 0.023108873516321182, tv_loss: 0.033516619354486465\n",
      "iteration 3742, dc_loss: 0.02309923619031906, tv_loss: 0.03351825848221779\n",
      "iteration 3743, dc_loss: 0.023106534034013748, tv_loss: 0.033505771309137344\n",
      "iteration 3744, dc_loss: 0.02311578579246998, tv_loss: 0.0334973968565464\n",
      "iteration 3745, dc_loss: 0.023098068311810493, tv_loss: 0.03351516276597977\n",
      "iteration 3746, dc_loss: 0.02309403009712696, tv_loss: 0.033522527664899826\n",
      "iteration 3747, dc_loss: 0.023110877722501755, tv_loss: 0.03350178152322769\n",
      "iteration 3748, dc_loss: 0.02310119941830635, tv_loss: 0.03350706398487091\n",
      "iteration 3749, dc_loss: 0.023090912029147148, tv_loss: 0.03351671248674393\n",
      "iteration 3750, dc_loss: 0.023101598024368286, tv_loss: 0.03351699188351631\n",
      "iteration 3751, dc_loss: 0.023096684366464615, tv_loss: 0.033510223031044006\n",
      "iteration 3752, dc_loss: 0.023088810965418816, tv_loss: 0.03351917490363121\n",
      "iteration 3753, dc_loss: 0.023094812408089638, tv_loss: 0.03350809961557388\n",
      "iteration 3754, dc_loss: 0.02309730276465416, tv_loss: 0.03351050242781639\n",
      "iteration 3755, dc_loss: 0.02309376932680607, tv_loss: 0.03350541740655899\n",
      "iteration 3756, dc_loss: 0.023090921342372894, tv_loss: 0.03351093828678131\n",
      "iteration 3757, dc_loss: 0.02308754250407219, tv_loss: 0.03351354971528053\n",
      "iteration 3758, dc_loss: 0.023088719695806503, tv_loss: 0.03350897133350372\n",
      "iteration 3759, dc_loss: 0.023086892440915108, tv_loss: 0.03351130336523056\n",
      "iteration 3760, dc_loss: 0.023085476830601692, tv_loss: 0.03350670263171196\n",
      "iteration 3761, dc_loss: 0.02308613993227482, tv_loss: 0.03350457921624184\n",
      "iteration 3762, dc_loss: 0.023082369938492775, tv_loss: 0.0335065983235836\n",
      "iteration 3763, dc_loss: 0.023082289844751358, tv_loss: 0.03351221978664398\n",
      "iteration 3764, dc_loss: 0.023088183254003525, tv_loss: 0.03349970281124115\n",
      "iteration 3765, dc_loss: 0.023079322651028633, tv_loss: 0.03350750729441643\n",
      "iteration 3766, dc_loss: 0.023074576631188393, tv_loss: 0.033517707139253616\n",
      "iteration 3767, dc_loss: 0.023086659610271454, tv_loss: 0.03350154682993889\n",
      "iteration 3768, dc_loss: 0.023081110790371895, tv_loss: 0.033506911247968674\n",
      "iteration 3769, dc_loss: 0.023064401000738144, tv_loss: 0.03352026641368866\n",
      "iteration 3770, dc_loss: 0.02307814732193947, tv_loss: 0.033505164086818695\n",
      "iteration 3771, dc_loss: 0.02308589220046997, tv_loss: 0.033499013632535934\n",
      "iteration 3772, dc_loss: 0.02306368574500084, tv_loss: 0.0335262306034565\n",
      "iteration 3773, dc_loss: 0.023068411275744438, tv_loss: 0.033509183675050735\n",
      "iteration 3774, dc_loss: 0.02307741716504097, tv_loss: 0.033503033220767975\n",
      "iteration 3775, dc_loss: 0.023071197792887688, tv_loss: 0.03351068124175072\n",
      "iteration 3776, dc_loss: 0.023068109527230263, tv_loss: 0.03351498395204544\n",
      "iteration 3777, dc_loss: 0.02306814305484295, tv_loss: 0.0335138663649559\n",
      "iteration 3778, dc_loss: 0.023068435490131378, tv_loss: 0.03350329026579857\n",
      "iteration 3779, dc_loss: 0.02306106872856617, tv_loss: 0.03351035341620445\n",
      "iteration 3780, dc_loss: 0.02306288480758667, tv_loss: 0.03351453319191933\n",
      "iteration 3781, dc_loss: 0.023067409172654152, tv_loss: 0.03351042419672012\n",
      "iteration 3782, dc_loss: 0.02306215465068817, tv_loss: 0.03351395204663277\n",
      "iteration 3783, dc_loss: 0.023061860352754593, tv_loss: 0.03350777179002762\n",
      "iteration 3784, dc_loss: 0.023058760911226273, tv_loss: 0.033507220447063446\n",
      "iteration 3785, dc_loss: 0.02305760607123375, tv_loss: 0.03351670131087303\n",
      "iteration 3786, dc_loss: 0.023060377687215805, tv_loss: 0.03351622447371483\n",
      "iteration 3787, dc_loss: 0.023053985089063644, tv_loss: 0.033513229340314865\n",
      "iteration 3788, dc_loss: 0.02305128052830696, tv_loss: 0.03351227194070816\n",
      "iteration 3789, dc_loss: 0.023060491308569908, tv_loss: 0.03350617736577988\n",
      "iteration 3790, dc_loss: 0.023051314055919647, tv_loss: 0.03351529687643051\n",
      "iteration 3791, dc_loss: 0.023047933354973793, tv_loss: 0.03351340815424919\n",
      "iteration 3792, dc_loss: 0.023059386759996414, tv_loss: 0.03349771350622177\n",
      "iteration 3793, dc_loss: 0.023052353411912918, tv_loss: 0.033502720296382904\n",
      "iteration 3794, dc_loss: 0.023042064160108566, tv_loss: 0.033516790717840195\n",
      "iteration 3795, dc_loss: 0.023051371797919273, tv_loss: 0.03350519761443138\n",
      "iteration 3796, dc_loss: 0.02305205911397934, tv_loss: 0.033503998070955276\n",
      "iteration 3797, dc_loss: 0.02303951419889927, tv_loss: 0.0335153229534626\n",
      "iteration 3798, dc_loss: 0.023043567314743996, tv_loss: 0.033507056534290314\n",
      "iteration 3799, dc_loss: 0.023044629022479057, tv_loss: 0.03350277990102768\n",
      "iteration 3800, dc_loss: 0.02304147556424141, tv_loss: 0.03350653126835823\n",
      "iteration 3801, dc_loss: 0.023041490465402603, tv_loss: 0.03350559249520302\n",
      "iteration 3802, dc_loss: 0.023037055507302284, tv_loss: 0.03350567817687988\n",
      "iteration 3803, dc_loss: 0.02304476872086525, tv_loss: 0.03350182995200157\n",
      "iteration 3804, dc_loss: 0.023039713501930237, tv_loss: 0.033506862819194794\n",
      "iteration 3805, dc_loss: 0.023033607751131058, tv_loss: 0.03350928798317909\n",
      "iteration 3806, dc_loss: 0.023033857345581055, tv_loss: 0.033508241176605225\n",
      "iteration 3807, dc_loss: 0.02303725853562355, tv_loss: 0.03350052982568741\n",
      "iteration 3808, dc_loss: 0.023032817989587784, tv_loss: 0.03350546211004257\n",
      "iteration 3809, dc_loss: 0.023028071969747543, tv_loss: 0.033518243581056595\n",
      "iteration 3810, dc_loss: 0.023036042228341103, tv_loss: 0.03350745141506195\n",
      "iteration 3811, dc_loss: 0.023034481331706047, tv_loss: 0.03350527584552765\n",
      "iteration 3812, dc_loss: 0.023023223504424095, tv_loss: 0.03351151570677757\n",
      "iteration 3813, dc_loss: 0.023025821894407272, tv_loss: 0.03350409120321274\n",
      "iteration 3814, dc_loss: 0.023032132536172867, tv_loss: 0.03350049629807472\n",
      "iteration 3815, dc_loss: 0.023026125505566597, tv_loss: 0.033510562032461166\n",
      "iteration 3816, dc_loss: 0.023020615801215172, tv_loss: 0.03351416811347008\n",
      "iteration 3817, dc_loss: 0.02302713319659233, tv_loss: 0.03350507840514183\n",
      "iteration 3818, dc_loss: 0.023020992055535316, tv_loss: 0.033510033041238785\n",
      "iteration 3819, dc_loss: 0.023016909137368202, tv_loss: 0.033506520092487335\n",
      "iteration 3820, dc_loss: 0.02302107959985733, tv_loss: 0.03350420668721199\n",
      "iteration 3821, dc_loss: 0.023021262139081955, tv_loss: 0.03350570425391197\n",
      "iteration 3822, dc_loss: 0.023022742941975594, tv_loss: 0.03350423648953438\n",
      "iteration 3823, dc_loss: 0.023015286773443222, tv_loss: 0.03351297229528427\n",
      "iteration 3824, dc_loss: 0.02301664650440216, tv_loss: 0.03350730240345001\n",
      "iteration 3825, dc_loss: 0.023013653233647346, tv_loss: 0.03350815922021866\n",
      "iteration 3826, dc_loss: 0.02300785854458809, tv_loss: 0.03351205214858055\n",
      "iteration 3827, dc_loss: 0.023019522428512573, tv_loss: 0.03350267931818962\n",
      "iteration 3828, dc_loss: 0.023013291880488396, tv_loss: 0.03350844979286194\n",
      "iteration 3829, dc_loss: 0.023004110902547836, tv_loss: 0.033514440059661865\n",
      "iteration 3830, dc_loss: 0.023006679490208626, tv_loss: 0.033510759472846985\n",
      "iteration 3831, dc_loss: 0.023012841120362282, tv_loss: 0.03349805623292923\n",
      "iteration 3832, dc_loss: 0.02300916239619255, tv_loss: 0.03350489214062691\n",
      "iteration 3833, dc_loss: 0.023002395406365395, tv_loss: 0.033513184636831284\n",
      "iteration 3834, dc_loss: 0.023006649687886238, tv_loss: 0.03350908309221268\n",
      "iteration 3835, dc_loss: 0.023006321862339973, tv_loss: 0.03350456804037094\n",
      "iteration 3836, dc_loss: 0.023002266883850098, tv_loss: 0.03350423648953438\n",
      "iteration 3837, dc_loss: 0.02300184592604637, tv_loss: 0.03350422531366348\n",
      "iteration 3838, dc_loss: 0.022998306900262833, tv_loss: 0.03350377455353737\n",
      "iteration 3839, dc_loss: 0.02300027199089527, tv_loss: 0.03350113332271576\n",
      "iteration 3840, dc_loss: 0.023000990971922874, tv_loss: 0.03349870443344116\n",
      "iteration 3841, dc_loss: 0.022994276136159897, tv_loss: 0.03350558504462242\n",
      "iteration 3842, dc_loss: 0.022995900362730026, tv_loss: 0.03350195288658142\n",
      "iteration 3843, dc_loss: 0.02299531176686287, tv_loss: 0.03350474685430527\n",
      "iteration 3844, dc_loss: 0.02299482561647892, tv_loss: 0.033502012491226196\n",
      "iteration 3845, dc_loss: 0.02299283817410469, tv_loss: 0.033506590873003006\n",
      "iteration 3846, dc_loss: 0.022990547120571136, tv_loss: 0.03351140022277832\n",
      "iteration 3847, dc_loss: 0.022991513833403587, tv_loss: 0.03350505605340004\n",
      "iteration 3848, dc_loss: 0.022992772981524467, tv_loss: 0.03350811451673508\n",
      "iteration 3849, dc_loss: 0.022984579205513, tv_loss: 0.03351017087697983\n",
      "iteration 3850, dc_loss: 0.022985301911830902, tv_loss: 0.03351007029414177\n",
      "iteration 3851, dc_loss: 0.022989753633737564, tv_loss: 0.03350009769201279\n",
      "iteration 3852, dc_loss: 0.022986559197306633, tv_loss: 0.03350592777132988\n",
      "iteration 3853, dc_loss: 0.022985335439443588, tv_loss: 0.033501867204904556\n",
      "iteration 3854, dc_loss: 0.02298288606107235, tv_loss: 0.03349986672401428\n",
      "iteration 3855, dc_loss: 0.02298242412507534, tv_loss: 0.033503446727991104\n",
      "iteration 3856, dc_loss: 0.022981274873018265, tv_loss: 0.033503130078315735\n",
      "iteration 3857, dc_loss: 0.022979598492383957, tv_loss: 0.03350336477160454\n",
      "iteration 3858, dc_loss: 0.02297952212393284, tv_loss: 0.033509183675050735\n",
      "iteration 3859, dc_loss: 0.022974062711000443, tv_loss: 0.03351333364844322\n",
      "iteration 3860, dc_loss: 0.022975724190473557, tv_loss: 0.033510010689496994\n",
      "iteration 3861, dc_loss: 0.02297959104180336, tv_loss: 0.033501386642456055\n",
      "iteration 3862, dc_loss: 0.022981863468885422, tv_loss: 0.03349912911653519\n",
      "iteration 3863, dc_loss: 0.022964777424931526, tv_loss: 0.03351554647088051\n",
      "iteration 3864, dc_loss: 0.022969407960772514, tv_loss: 0.033506184816360474\n",
      "iteration 3865, dc_loss: 0.022977329790592194, tv_loss: 0.03349914774298668\n",
      "iteration 3866, dc_loss: 0.02296513505280018, tv_loss: 0.03351180627942085\n",
      "iteration 3867, dc_loss: 0.02296925149857998, tv_loss: 0.03350439295172691\n",
      "iteration 3868, dc_loss: 0.022973114624619484, tv_loss: 0.03350256010890007\n",
      "iteration 3869, dc_loss: 0.02296280488371849, tv_loss: 0.03351328894495964\n",
      "iteration 3870, dc_loss: 0.02296352945268154, tv_loss: 0.03350740671157837\n",
      "iteration 3871, dc_loss: 0.02297430858016014, tv_loss: 0.0334964394569397\n",
      "iteration 3872, dc_loss: 0.0229618139564991, tv_loss: 0.03350895643234253\n",
      "iteration 3873, dc_loss: 0.022958479821681976, tv_loss: 0.03350802883505821\n",
      "iteration 3874, dc_loss: 0.022963857278227806, tv_loss: 0.03350021317601204\n",
      "iteration 3875, dc_loss: 0.02296087145805359, tv_loss: 0.03350800648331642\n",
      "iteration 3876, dc_loss: 0.022958677262067795, tv_loss: 0.03351159766316414\n",
      "iteration 3877, dc_loss: 0.022956108674407005, tv_loss: 0.0335078239440918\n",
      "iteration 3878, dc_loss: 0.022957971319556236, tv_loss: 0.03351151943206787\n",
      "iteration 3879, dc_loss: 0.022954870015382767, tv_loss: 0.03350619599223137\n",
      "iteration 3880, dc_loss: 0.022957706823945045, tv_loss: 0.033503204584121704\n",
      "iteration 3881, dc_loss: 0.022956607863307, tv_loss: 0.033503711223602295\n",
      "iteration 3882, dc_loss: 0.02294834889471531, tv_loss: 0.03350837528705597\n",
      "iteration 3883, dc_loss: 0.022950680926442146, tv_loss: 0.03350835293531418\n",
      "iteration 3884, dc_loss: 0.022959575057029724, tv_loss: 0.033491816371679306\n",
      "iteration 3885, dc_loss: 0.022952532395720482, tv_loss: 0.03350111097097397\n",
      "iteration 3886, dc_loss: 0.022938480600714684, tv_loss: 0.03351334482431412\n",
      "iteration 3887, dc_loss: 0.022950921207666397, tv_loss: 0.03349658474326134\n",
      "iteration 3888, dc_loss: 0.022951524704694748, tv_loss: 0.033499009907245636\n",
      "iteration 3889, dc_loss: 0.022937925532460213, tv_loss: 0.03351282328367233\n",
      "iteration 3890, dc_loss: 0.02294626086950302, tv_loss: 0.033507753163576126\n",
      "iteration 3891, dc_loss: 0.02294670045375824, tv_loss: 0.03351392224431038\n",
      "iteration 3892, dc_loss: 0.022938642650842667, tv_loss: 0.0335138700902462\n",
      "iteration 3893, dc_loss: 0.022937770932912827, tv_loss: 0.033509183675050735\n",
      "iteration 3894, dc_loss: 0.02295261062681675, tv_loss: 0.03349439799785614\n",
      "iteration 3895, dc_loss: 0.022937249392271042, tv_loss: 0.03350917994976044\n",
      "iteration 3896, dc_loss: 0.022930385544896126, tv_loss: 0.03351648151874542\n",
      "iteration 3897, dc_loss: 0.022944994270801544, tv_loss: 0.0335034504532814\n",
      "iteration 3898, dc_loss: 0.022933410480618477, tv_loss: 0.033509511500597\n",
      "iteration 3899, dc_loss: 0.02292686328291893, tv_loss: 0.03351600840687752\n",
      "iteration 3900, dc_loss: 0.022938169538974762, tv_loss: 0.03350371867418289\n",
      "iteration 3901, dc_loss: 0.022938240319490433, tv_loss: 0.033495109528303146\n",
      "iteration 3902, dc_loss: 0.022927913814783096, tv_loss: 0.03351213410496712\n",
      "iteration 3903, dc_loss: 0.02292863465845585, tv_loss: 0.03351235017180443\n",
      "iteration 3904, dc_loss: 0.022932231426239014, tv_loss: 0.03350730985403061\n",
      "iteration 3905, dc_loss: 0.022924985736608505, tv_loss: 0.033510129898786545\n",
      "iteration 3906, dc_loss: 0.02293151058256626, tv_loss: 0.033500559628009796\n",
      "iteration 3907, dc_loss: 0.022926073521375656, tv_loss: 0.03350810334086418\n",
      "iteration 3908, dc_loss: 0.022920871153473854, tv_loss: 0.03351416811347008\n",
      "iteration 3909, dc_loss: 0.022928200662136078, tv_loss: 0.03350389748811722\n",
      "iteration 3910, dc_loss: 0.022925419732928276, tv_loss: 0.03349876403808594\n",
      "iteration 3911, dc_loss: 0.022918494418263435, tv_loss: 0.03350917249917984\n",
      "iteration 3912, dc_loss: 0.02292242832481861, tv_loss: 0.033504415303468704\n",
      "iteration 3913, dc_loss: 0.022918235510587692, tv_loss: 0.03350236639380455\n",
      "iteration 3914, dc_loss: 0.022917192429304123, tv_loss: 0.033505771309137344\n",
      "iteration 3915, dc_loss: 0.022920941933989525, tv_loss: 0.03350132331252098\n",
      "iteration 3916, dc_loss: 0.022914759814739227, tv_loss: 0.03350493684411049\n",
      "iteration 3917, dc_loss: 0.0229156706482172, tv_loss: 0.03350076451897621\n",
      "iteration 3918, dc_loss: 0.02291501872241497, tv_loss: 0.0335015207529068\n",
      "iteration 3919, dc_loss: 0.022908620536327362, tv_loss: 0.03350696340203285\n",
      "iteration 3920, dc_loss: 0.022915391251444817, tv_loss: 0.03349858894944191\n",
      "iteration 3921, dc_loss: 0.02291497029364109, tv_loss: 0.03350045531988144\n",
      "iteration 3922, dc_loss: 0.02290879748761654, tv_loss: 0.03351568803191185\n",
      "iteration 3923, dc_loss: 0.022904591634869576, tv_loss: 0.033515967428684235\n",
      "iteration 3924, dc_loss: 0.022914785891771317, tv_loss: 0.03350488469004631\n",
      "iteration 3925, dc_loss: 0.02290606126189232, tv_loss: 0.033508963882923126\n",
      "iteration 3926, dc_loss: 0.0229047704488039, tv_loss: 0.033512361347675323\n",
      "iteration 3927, dc_loss: 0.02290692739188671, tv_loss: 0.03351645916700363\n",
      "iteration 3928, dc_loss: 0.02289704605937004, tv_loss: 0.03351892903447151\n",
      "iteration 3929, dc_loss: 0.022901708260178566, tv_loss: 0.033509548753499985\n",
      "iteration 3930, dc_loss: 0.02290823496878147, tv_loss: 0.033505771309137344\n",
      "iteration 3931, dc_loss: 0.02289753593504429, tv_loss: 0.033516403287649155\n",
      "iteration 3932, dc_loss: 0.02289731428027153, tv_loss: 0.03351503610610962\n",
      "iteration 3933, dc_loss: 0.02290402539074421, tv_loss: 0.033501241356134415\n",
      "iteration 3934, dc_loss: 0.022898707538843155, tv_loss: 0.033509209752082825\n",
      "iteration 3935, dc_loss: 0.02289232425391674, tv_loss: 0.033514853566884995\n",
      "iteration 3936, dc_loss: 0.02289634756743908, tv_loss: 0.03350616618990898\n",
      "iteration 3937, dc_loss: 0.02289699763059616, tv_loss: 0.03350122645497322\n",
      "iteration 3938, dc_loss: 0.022890478372573853, tv_loss: 0.03350900113582611\n",
      "iteration 3939, dc_loss: 0.022892354056239128, tv_loss: 0.033506330102682114\n",
      "iteration 3940, dc_loss: 0.02289395034313202, tv_loss: 0.033502474427223206\n",
      "iteration 3941, dc_loss: 0.022889019921422005, tv_loss: 0.0335065983235836\n",
      "iteration 3942, dc_loss: 0.022890282794833183, tv_loss: 0.033501796424388885\n",
      "iteration 3943, dc_loss: 0.02288767881691456, tv_loss: 0.03350182995200157\n",
      "iteration 3944, dc_loss: 0.0228850357234478, tv_loss: 0.03350221365690231\n",
      "iteration 3945, dc_loss: 0.022888002917170525, tv_loss: 0.03349928930401802\n",
      "iteration 3946, dc_loss: 0.022887878119945526, tv_loss: 0.03349859267473221\n",
      "iteration 3947, dc_loss: 0.022880371659994125, tv_loss: 0.033505525439977646\n",
      "iteration 3948, dc_loss: 0.022886278107762337, tv_loss: 0.03349647670984268\n",
      "iteration 3949, dc_loss: 0.022880299016833305, tv_loss: 0.03350130468606949\n",
      "iteration 3950, dc_loss: 0.02287684753537178, tv_loss: 0.033510833978652954\n",
      "iteration 3951, dc_loss: 0.02288118377327919, tv_loss: 0.03350724279880524\n",
      "iteration 3952, dc_loss: 0.022884374484419823, tv_loss: 0.03350576385855675\n",
      "iteration 3953, dc_loss: 0.0228738971054554, tv_loss: 0.03350840136408806\n",
      "iteration 3954, dc_loss: 0.022876670584082603, tv_loss: 0.033501990139484406\n",
      "iteration 3955, dc_loss: 0.02287277765572071, tv_loss: 0.03350798785686493\n",
      "iteration 3956, dc_loss: 0.022872287780046463, tv_loss: 0.03350351005792618\n",
      "iteration 3957, dc_loss: 0.022879380732774734, tv_loss: 0.03349698707461357\n",
      "iteration 3958, dc_loss: 0.02286936342716217, tv_loss: 0.033504046499729156\n",
      "iteration 3959, dc_loss: 0.02286743000149727, tv_loss: 0.03351195901632309\n",
      "iteration 3960, dc_loss: 0.02287573739886284, tv_loss: 0.03349924832582474\n",
      "iteration 3961, dc_loss: 0.02287311851978302, tv_loss: 0.03350353240966797\n",
      "iteration 3962, dc_loss: 0.022860337048768997, tv_loss: 0.033513110131025314\n",
      "iteration 3963, dc_loss: 0.022870179265737534, tv_loss: 0.0335024893283844\n",
      "iteration 3964, dc_loss: 0.022869376465678215, tv_loss: 0.03350082039833069\n",
      "iteration 3965, dc_loss: 0.02286052145063877, tv_loss: 0.03350617736577988\n",
      "iteration 3966, dc_loss: 0.022863101214170456, tv_loss: 0.033502690494060516\n",
      "iteration 3967, dc_loss: 0.022867988795042038, tv_loss: 0.03349745273590088\n",
      "iteration 3968, dc_loss: 0.02285890281200409, tv_loss: 0.03350343927741051\n",
      "iteration 3969, dc_loss: 0.022862384095788002, tv_loss: 0.033499784767627716\n",
      "iteration 3970, dc_loss: 0.02285531349480152, tv_loss: 0.03350421413779259\n",
      "iteration 3971, dc_loss: 0.02285972237586975, tv_loss: 0.03349907696247101\n",
      "iteration 3972, dc_loss: 0.02286648005247116, tv_loss: 0.033495839685201645\n",
      "iteration 3973, dc_loss: 0.022852016612887383, tv_loss: 0.033503394573926926\n",
      "iteration 3974, dc_loss: 0.02285357005894184, tv_loss: 0.03350269794464111\n",
      "iteration 3975, dc_loss: 0.02285945415496826, tv_loss: 0.03349977359175682\n",
      "iteration 3976, dc_loss: 0.022851036861538887, tv_loss: 0.03351158648729324\n",
      "iteration 3977, dc_loss: 0.022848960012197495, tv_loss: 0.03351328894495964\n",
      "iteration 3978, dc_loss: 0.02285933308303356, tv_loss: 0.0335029736161232\n",
      "iteration 3979, dc_loss: 0.022846855223178864, tv_loss: 0.03351123631000519\n",
      "iteration 3980, dc_loss: 0.02284553274512291, tv_loss: 0.0335116982460022\n",
      "iteration 3981, dc_loss: 0.022854551672935486, tv_loss: 0.03349822387099266\n",
      "iteration 3982, dc_loss: 0.022844621911644936, tv_loss: 0.033507898449897766\n",
      "iteration 3983, dc_loss: 0.022843170911073685, tv_loss: 0.03350916504859924\n",
      "iteration 3984, dc_loss: 0.022849105298519135, tv_loss: 0.03350936248898506\n",
      "iteration 3985, dc_loss: 0.02284267172217369, tv_loss: 0.0335111990571022\n",
      "iteration 3986, dc_loss: 0.02283814735710621, tv_loss: 0.0335114561021328\n",
      "iteration 3987, dc_loss: 0.022846298292279243, tv_loss: 0.0335017628967762\n",
      "iteration 3988, dc_loss: 0.022845061495900154, tv_loss: 0.03350379690527916\n",
      "iteration 3989, dc_loss: 0.02283359505236149, tv_loss: 0.033517010509967804\n",
      "iteration 3990, dc_loss: 0.022844867780804634, tv_loss: 0.033502865582704544\n",
      "iteration 3991, dc_loss: 0.02284104749560356, tv_loss: 0.03350324183702469\n",
      "iteration 3992, dc_loss: 0.022831998765468597, tv_loss: 0.033511094748973846\n",
      "iteration 3993, dc_loss: 0.022839142009615898, tv_loss: 0.03350229188799858\n",
      "iteration 3994, dc_loss: 0.02283349446952343, tv_loss: 0.033508505672216415\n",
      "iteration 3995, dc_loss: 0.02282790280878544, tv_loss: 0.03351297602057457\n",
      "iteration 3996, dc_loss: 0.0228362325578928, tv_loss: 0.033499788492918015\n",
      "iteration 3997, dc_loss: 0.022838616743683815, tv_loss: 0.03349504992365837\n",
      "iteration 3998, dc_loss: 0.02282591536641121, tv_loss: 0.03350866585969925\n",
      "iteration 3999, dc_loss: 0.022827569395303726, tv_loss: 0.03350362181663513\n",
      "iteration 4000, dc_loss: 0.022835399955511093, tv_loss: 0.03349271044135094\n",
      "PSNR Value mt1: 35.77876077524474\n",
      "SSIM Value mt1: 0.7715722252164936\n"
     ]
    }
   ],
   "source": [
    "for dataset, ar in zip(datasets_list, [1, 2, 4, 8]):\n",
    "    device = torch.device(\"cuda\")\n",
    "    size = (640, 368)\n",
    "    from ese5934_project.models.SIREN import Siren, get_coordinates\n",
    "\n",
    "    coords = get_coordinates(size)\n",
    "    kspace, (mean, std), masked_kspace, mask, csm = dataset[15]\n",
    "    field = Siren(\n",
    "        size,\n",
    "        mean.to(device),\n",
    "        std.to(device),\n",
    "        in_features=2,\n",
    "        out_features=2,\n",
    "        hidden_features=256,\n",
    "        hidden_layers=4,\n",
    "        outermost_linear=True,\n",
    "        first_omega_0=25,\n",
    "        hidden_omega_0=25,\n",
    "    )\n",
    "    lr_scheduler = lambda t: 0.8 ** (t // 400) * 1e-4\n",
    "    optimizer = torchopt.adamw(lr=lr_scheduler)\n",
    "    # 1e-4 1.092077389\n",
    "    # 1e-3 0.08540542\n",
    "    params, image_list_SIREN = reconstruct(\n",
    "        field,\n",
    "        coords,\n",
    "        masked_kspace,\n",
    "        csm,\n",
    "        mask,\n",
    "        alpha=0.01,\n",
    "        optimizer=optimizer,\n",
    "        iterations=4000,\n",
    "        device=device,\n",
    "    )\n",
    "    psnr, ssim = Evaluate_MT1(image_gt, image_list_ADAM[-1])\n",
    "    SIREN_list.append((psnr, ssim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| self.basis_dims: array([32, 32, 32, 16, 16, 16])\n",
      "ic| self.basis_reso: array([16, 26, 35, 45, 54, 64])\n",
      "ic| self.freq_bands: tensor([40.0000, 24.6154, 18.2857, 14.2222, 11.8519, 10.0000], device='cuda:1')\n",
      "ic| self.bbox: tensor([[  0.,   0.],\n",
      "                       [640., 368.]], device='cuda:1')\n",
      "ic| self.coeff_reso: [20, 12]\n",
      "ic| coeffs.shape: torch.Size([1, 144, 20, 12])\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32, 16, 16]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 32, 26, 26]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 32, 35, 35]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 16, 45, 45]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 16, 54, 54]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 16, 64, 64]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====> total parameters:  257584\n",
      "tensor([[[6.0409e-04, 7.4484e-05]]]) tensor([[[1.2870, 1.2171]]])\n",
      "torch.Size([1, 1, 2]) torch.Size([1, 1, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| params.keys(): dict_keys(['coeffs', 'basises.0', 'basises.1', 'basises.2', 'basises.3', 'basises.4', 'basises.5', 'linear_mat.backbone.0.weight', 'linear_mat.backbone.0.bias', 'linear_mat.backbone.1.weight'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| self.basis_dims: array([32, 32, 32, 16, 16, 16])\n",
      "ic| self.basis_reso: array([16, 26, 35, 45, 54, 64])\n",
      "ic| self.freq_bands: tensor([40.0000, 24.6154, 18.2857, 14.2222, 11.8519, 10.0000], device='cuda:1')\n",
      "ic| self.bbox: tensor([[  0.,   0.],\n",
      "                       [640., 368.]], device='cuda:1')\n",
      "ic| self.coeff_reso: [20, 12]\n",
      "ic| coeffs.shape: torch.Size([1, 144, 20, 12])\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 32, 16, 16]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 32, 26, 26]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 32, 35, 35]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 16, 45, 45]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 16, 54, 54]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 16, 64, 64]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR Value mt1: 35.77876077524474\n",
      "SSIM Value mt1: 0.7715722252164936\n",
      "=====> total parameters:  257584\n",
      "tensor([[[6.0409e-04, 7.4484e-05]]]) tensor([[[1.2870, 1.2171]]])\n",
      "torch.Size([1, 1, 2]) torch.Size([1, 1, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| params.keys(): dict_keys(['coeffs', 'basises.0', 'basises.1', 'basises.2', 'basises.3', 'basises.4', 'basises.5', 'linear_mat.backbone.0.weight', 'linear_mat.backbone.0.bias', 'linear_mat.backbone.1.weight'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| self.basis_dims: array([32, 32, 32, 16, 16, 16])\n",
      "ic| self.basis_reso: array([16, 26, 35, 45, 54, 64])\n",
      "ic| self.freq_bands: tensor([40.0000, 24.6154, 18.2857, 14.2222, 11.8519, 10.0000], device='cuda:1')\n",
      "ic| self.bbox: tensor([[  0.,   0.],\n",
      "                       [640., 368.]], device='cuda:1')\n",
      "ic| self.coeff_reso: [20, 12]\n",
      "ic| coeffs.shape: torch.Size([1, 144, 20, 12])\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 32, 16, 16]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 32, 26, 26]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 32, 35, 35]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 16, 45, 45]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 16, 54, 54]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 16, 64, 64]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR Value mt1: 35.77876077524474\n",
      "SSIM Value mt1: 0.7715722252164936\n",
      "=====> total parameters:  257584\n",
      "tensor([[[6.0409e-04, 7.4484e-05]]]) tensor([[[1.2870, 1.2171]]])\n",
      "torch.Size([1, 1, 2]) torch.Size([1, 1, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| params.keys(): dict_keys(['coeffs', 'basises.0', 'basises.1', 'basises.2', 'basises.3', 'basises.4', 'basises.5', 'linear_mat.backbone.0.weight', 'linear_mat.backbone.0.bias', 'linear_mat.backbone.1.weight'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| self.basis_dims: array([32, 32, 32, 16, 16, 16])\n",
      "ic| self.basis_reso: array([16, 26, 35, 45, 54, 64])\n",
      "ic| self.freq_bands: tensor([40.0000, 24.6154, 18.2857, 14.2222, 11.8519, 10.0000], device='cuda:1')\n",
      "ic| self.bbox: tensor([[  0.,   0.],\n",
      "                       [640., 368.]], device='cuda:1')\n",
      "ic| self.coeff_reso: [20, 12]\n",
      "ic| coeffs.shape: torch.Size([1, 144, 20, 12])\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 32, 16, 16]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 32, 26, 26]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 32, 35, 35]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 16, 45, 45]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 16, 54, 54]\n",
      "ic| [1, basis_dim] + [reso] * self.in_dim: [1, 16, 64, 64]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR Value mt1: 35.77876077524474\n",
      "SSIM Value mt1: 0.7715722252164936\n",
      "=====> total parameters:  257584\n",
      "tensor([[[6.0409e-04, 7.4484e-05]]]) tensor([[[1.2870, 1.2171]]])\n",
      "torch.Size([1, 1, 2]) torch.Size([1, 1, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| params.keys(): dict_keys(['coeffs', 'basises.0', 'basises.1', 'basises.2', 'basises.3', 'basises.4', 'basises.5', 'linear_mat.backbone.0.weight', 'linear_mat.backbone.0.bias', 'linear_mat.backbone.1.weight'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "PSNR Value mt1: 35.77876077524474\n",
      "SSIM Value mt1: 0.7715722252164936\n"
     ]
    }
   ],
   "source": [
    "from ese5934_project.models.FactorFields import DictField, get_coordinates\n",
    "\n",
    "base_conf = OmegaConf.load(\"/bmrc-an-data/Chunxu/ese5934_project/configs/defaults.yaml\")\n",
    "second_conf = OmegaConf.load(\"/bmrc-an-data/Chunxu/ese5934_project/configs/image.yaml\")\n",
    "cfg = OmegaConf.merge(\n",
    "    base_conf,\n",
    "    second_conf,\n",
    ")\n",
    "for dataset, ar in zip(datasets_list, [1, 2, 4, 8]):\n",
    "    device = torch.device(\"cuda:1\")\n",
    "    size = (640, 368)\n",
    "    model = DictField(cfg, size, device)\n",
    "\n",
    "    coords = get_coordinates(size)\n",
    "    kspace, (mean, std), masked_kspace, mask, csm = dataset[15]\n",
    "    scheduler = lambda t: 0.8 ** (t // 400) * 5e-3\n",
    "    optimizer = torchopt.adam(lr=scheduler)\n",
    "\n",
    "    params, image_list = reconstruct(\n",
    "        model,\n",
    "        coords,\n",
    "        masked_kspace,\n",
    "        csm,\n",
    "        mask,\n",
    "        alpha=0.005,\n",
    "        optimizer=optimizer,\n",
    "        iterations=4000,\n",
    "        device=device,\n",
    "    )\n",
    "    psnr, ssim = Evaluate_MT1(image_gt, image_list_ADAM[-1])\n",
    "    DF_list.append((psnr, ssim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    {\"ADAM\": ADAM_list, \"SIREN\": SIREN_list, \"DF\": DF_list}, \"results_AR_1_2_4_8.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(35.239742876024216, 0.7594775679989919), (35.239742876024216, 0.7594775679989919), (35.239742876024216, 0.7594775679989919), (35.239742876024216, 0.7594775679989919), (35.77876077524474, 0.7715722252164936), (35.77876077524474, 0.7715722252164936), (35.77876077524474, 0.7715722252164936), (35.77876077524474, 0.7715722252164936)]\n"
     ]
    }
   ],
   "source": [
    "print(DF_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8db1953410>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([d[0] for d in ADAM_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openpyxl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexperiments/AR_results.xlsx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/bmrc-homes/nmrgrp/nmr201/micromamba/envs/python311/lib/python3.11/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/bmrc-homes/nmrgrp/nmr201/micromamba/envs/python311/lib/python3.11/site-packages/pandas/core/generic.py:2414\u001b[0m, in \u001b[0;36mNDFrame.to_excel\u001b[0;34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, inf_rep, freeze_panes, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   2401\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformats\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexcel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExcelFormatter\n\u001b[1;32m   2403\u001b[0m formatter \u001b[38;5;241m=\u001b[39m ExcelFormatter(\n\u001b[1;32m   2404\u001b[0m     df,\n\u001b[1;32m   2405\u001b[0m     na_rep\u001b[38;5;241m=\u001b[39mna_rep,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2412\u001b[0m     inf_rep\u001b[38;5;241m=\u001b[39minf_rep,\n\u001b[1;32m   2413\u001b[0m )\n\u001b[0;32m-> 2414\u001b[0m \u001b[43mformatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2415\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexcel_writer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2416\u001b[0m \u001b[43m    \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2417\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstartrow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstartrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2418\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstartcol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstartcol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2419\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfreeze_panes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreeze_panes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2420\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2422\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2423\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/bmrc-homes/nmrgrp/nmr201/micromamba/envs/python311/lib/python3.11/site-packages/pandas/io/formats/excel.py:943\u001b[0m, in \u001b[0;36mExcelFormatter.write\u001b[0;34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m    941\u001b[0m     need_save \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 943\u001b[0m     writer \u001b[38;5;241m=\u001b[39m \u001b[43mExcelWriter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    949\u001b[0m     need_save \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/bmrc-homes/nmrgrp/nmr201/micromamba/envs/python311/lib/python3.11/site-packages/pandas/io/excel/_openpyxl.py:57\u001b[0m, in \u001b[0;36mOpenpyxlWriter.__init__\u001b[0;34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     46\u001b[0m     path: FilePath \u001b[38;5;241m|\u001b[39m WriteExcelBuffer \u001b[38;5;241m|\u001b[39m ExcelWriter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Use the openpyxl module as the Excel writer.\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenpyxl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mworkbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Workbook\n\u001b[1;32m     59\u001b[0m     engine_kwargs \u001b[38;5;241m=\u001b[39m combine_kwargs(engine_kwargs, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     62\u001b[0m         path,\n\u001b[1;32m     63\u001b[0m         mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m         engine_kwargs\u001b[38;5;241m=\u001b[39mengine_kwargs,\n\u001b[1;32m     67\u001b[0m     )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'openpyxl'"
     ]
    }
   ],
   "source": [
    "results.to_excel(\"experiments/AR_results.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADAM\n",
      "PSNR Value mt1: 35.51589318336404\n",
      "SSIM Value mt1: 0.7751314202416238\n",
      "SIREN\n",
      "PSNR Value mt1: 28.169658116720203\n",
      "SSIM Value mt1: 0.8022582995267628\n",
      "DF\n",
      "PSNR Value mt1: 38.49677393524084\n",
      "SSIM Value mt1: 0.9359180332422214\n",
      "gt\n",
      "PSNR Value mt1: inf\n",
      "SSIM Value mt1: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bmrc-homes/nmrgrp/nmr201/micromamba/envs/python311/lib/python3.11/site-packages/skimage/metrics/simple_metrics.py:163: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return 10 * np.log10((data_range ** 2) / err)\n"
     ]
    }
   ],
   "source": [
    "for k, v in results.items():\n",
    "    print(k)\n",
    "    Evaluate_MT1(image_gt, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR Value mt2: 37.331352482059785\n",
      "SSIM Value mt2: 0.9279717622379655\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(37.331352482059785, 0.9279717622379655)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Evaluate_MT2(image_gt, image_list[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
